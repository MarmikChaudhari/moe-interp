{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "# from tqdm.auto import tqdm\n",
    "import datetime\n",
    "import gc\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"Get the optimal available device\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        # Enable TF32 for better performance on Ampere GPUs (A100, A6000, etc)\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        # Set memory allocation settings\n",
    "        torch.cuda.empty_cache()\n",
    "        # Enable CUDNN benchmarking for better performance\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed5a7558fa284d4c9d81a8a82147432f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DeepseekForCausalLM(\n",
       "  (model): DeepseekModel(\n",
       "    (embed_tokens): Embedding(102400, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0): DeepseekDecoderLayer(\n",
       "        (self_attn): DeepseekSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): DeepseekRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): DeepseekMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=10944, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=10944, bias=False)\n",
       "          (down_proj): Linear(in_features=10944, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): DeepseekRMSNorm()\n",
       "        (post_attention_layernorm): DeepseekRMSNorm()\n",
       "      )\n",
       "      (1-27): 27 x DeepseekDecoderLayer(\n",
       "        (self_attn): DeepseekSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): DeepseekRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): DeepseekMoE(\n",
       "          (experts): ModuleList(\n",
       "            (0-63): 64 x DeepseekMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
       "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (gate): MoEGate()\n",
       "          (shared_experts): DeepseekMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=2816, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=2816, bias=False)\n",
       "            (down_proj): Linear(in_features=2816, out_features=2048, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): DeepseekRMSNorm()\n",
       "        (post_attention_layernorm): DeepseekRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): DeepseekRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=102400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('deepseek-ai/deepseek-moe-16b-base',\n",
    "                                            trust_remote_code=True,\n",
    "                                            torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained('deepseek-ai/deepseek-moe-16b-base',\n",
    "                                            trust_remote_code=True)\n",
    "\n",
    "device = get_device()\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_chunks(file_path: str, chunk_size: int = 2048, tokenizer = None) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split input text file into chunks of specified token length with memory efficiency\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            if tokenizer:\n",
    "                # Process file in chunks to avoid loading entire file\n",
    "                chunk_text = \"\"\n",
    "                buffer = []\n",
    "                \n",
    "                for line in f:\n",
    "                    buffer.append(line)\n",
    "                    # Join buffer and check token length\n",
    "                    test_text = \"\".join(buffer)\n",
    "                    tokens = tokenizer.encode(test_text)\n",
    "                    \n",
    "                    # If we exceed max length, process the buffer\n",
    "                    if len(tokens) >= chunk_size:\n",
    "                        # Take only chunk_size tokens\n",
    "                        chunk_tokens = tokens[:chunk_size]\n",
    "                        chunks.append(tokenizer.decode(chunk_tokens))\n",
    "                        \n",
    "                        # Keep remainder for next chunk\n",
    "                        remainder_tokens = tokens[chunk_size:]\n",
    "                        buffer = [tokenizer.decode(remainder_tokens)]\n",
    "                \n",
    "                # Add remaining text if any\n",
    "                if buffer:\n",
    "                    final_text = \"\".join(buffer)\n",
    "                    final_tokens = tokenizer.encode(final_text)\n",
    "                    if len(final_tokens) > chunk_size:\n",
    "                        # Split remaining text into chunks of chunk_size\n",
    "                        for i in range(0, len(final_tokens), chunk_size):\n",
    "                            chunk_tokens = final_tokens[i:i + chunk_size]\n",
    "                            chunks.append(tokenizer.decode(chunk_tokens))\n",
    "                    else:\n",
    "                        chunks.append(final_text)\n",
    "            else:\n",
    "                words = []\n",
    "                for line in f:\n",
    "                    words.extend(line.split())\n",
    "                    while len(words) >= chunk_size:\n",
    "                        chunk = ' '.join(words[:chunk_size])\n",
    "                        chunks.append(chunk)\n",
    "                        words = words[chunk_size:]\n",
    "                if words:\n",
    "                    chunks.append(' '.join(words))\n",
    "                    \n",
    "        print(f\"Created {len(chunks)} chunks from the input file\")\n",
    "        return chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_expert_distribution(model, input_text: str, tokenizer, device: torch.device, \n",
    "                          num_shared_experts: int = 0, num_topk: int = 8) -> Dict[int, List[Tuple[str, int, float]]]:\n",
    "    \"\"\"\n",
    "    Get expert routing probabilities with proper handling of shared experts and top-k tracking\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = model.to(device)\n",
    "        \n",
    "        inputs = tokenizer(input_text, \n",
    "                         return_tensors=\"pt\",\n",
    "                         padding=True,\n",
    "                         truncation=True,\n",
    "                         max_length=2048)\n",
    "        \n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()} \n",
    "        \n",
    "        outputs = model(**inputs, \n",
    "                       output_attentions=False, \n",
    "                       output_hidden_states=True, \n",
    "                       return_dict=True)\n",
    "        \n",
    "        hidden_states = outputs.hidden_states\n",
    "        layer_distributions = {}\n",
    "\n",
    "        # Process each MoE layer\n",
    "        for layer_idx, layer in enumerate(model.model.layers[1:], 1):\n",
    "            moe_layer = layer.mlp\n",
    "            layer_hidden = hidden_states[layer_idx]\n",
    "            expert_distributions = []\n",
    "            \n",
    "            tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "            \n",
    "            # Get router logits for dynamic experts\n",
    "            router_logits = torch.matmul(\n",
    "                layer_hidden.float(),\n",
    "                moe_layer.gate.weight.t().float()\n",
    "            )\n",
    "            \n",
    "            # Get top-k routed experts (only from routed experts, not shared)\n",
    "            router_probs = torch.softmax(router_logits, dim=-1)\n",
    "            top_probs, top_indices = torch.topk(router_probs, k=min(num_topk, router_probs.size(-1)), dim=-1)\n",
    "\n",
    "            # Process results for each token\n",
    "            for i in range(len(tokens)):\n",
    "                if tokens[i] == tokenizer.pad_token:\n",
    "                    continue\n",
    "                    \n",
    "                if tokens[i].startswith('<') and tokens[i].endswith('>'):\n",
    "                    continue\n",
    "                    \n",
    "                clean_token = tokens[i].replace('Ġ', '')\n",
    "                \n",
    "                # Add shared experts first (0 to num_shared_experts-1)\n",
    "                for shared_idx in range(num_shared_experts):\n",
    "                    expert_distributions.append(\n",
    "                        (clean_token, f\"shared_{shared_idx}\", 1.0)  # Mark as shared expert\n",
    "                    )\n",
    "                \n",
    "                # Add routed experts separately (starting from 0)\n",
    "                for j in range(top_indices.size(-1)):\n",
    "                    expert_distributions.append(\n",
    "                        (clean_token, \n",
    "                         f\"routed_{int(top_indices[0,i,j].item())}\", # Mark as routed expert\n",
    "                         float(top_probs[0,i,j].item()))\n",
    "                    )\n",
    "            \n",
    "            layer_distributions[layer_idx] = expert_distributions\n",
    "\n",
    "        return layer_distributions\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_expert_distribution: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_expert_stats(distributions: Dict[int, List[Tuple[str, int, float]]], \n",
    "                     domain: str,\n",
    "                     output_dir: str = 'expert_stats'):\n",
    "    \"\"\"Save expert routing statistics with error handling\"\"\"\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_path = os.path.join(output_dir, f'{domain}_expert_stats.json')\n",
    "        \n",
    "        stats = {}\n",
    "        for layer_idx, layer_data in distributions.items():\n",
    "            expert_counts = defaultdict(int)\n",
    "            total = len(layer_data)\n",
    "            \n",
    "            if total == 0:\n",
    "                continue\n",
    "                \n",
    "            for _, expert_idx, _ in layer_data:\n",
    "                expert_counts[expert_idx] += 1\n",
    "                \n",
    "            expert_probs = {\n",
    "                str(expert): count/total \n",
    "                for expert, count in expert_counts.items()\n",
    "            }\n",
    "            stats[str(layer_idx)] = expert_probs\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(stats, f, indent=2)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving expert stats: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_token_distributions(expert_distributions):\n",
    "    \"\"\"\n",
    "    Convert expert distributions to token counts per expert per layer, separating shared and routed experts\n",
    "    \"\"\"\n",
    "    token_counts = {}\n",
    "    \n",
    "    # Process each layer\n",
    "    for layer_id, distributions in expert_distributions.items():\n",
    "        shared_counts = defaultdict(int)\n",
    "        routed_counts = defaultdict(int)\n",
    "        \n",
    "        # Count tokens per expert in this layer\n",
    "        for token, expert_id, prob in distributions:\n",
    "            if expert_id.startswith('shared_'):\n",
    "                expert_num = int(expert_id.split('_')[1])\n",
    "                shared_counts[expert_num] += 1\n",
    "            else:  # routed expert\n",
    "                expert_num = int(expert_id.split('_')[1])\n",
    "                routed_counts[expert_num] += 1\n",
    "            \n",
    "        token_counts[layer_id] = {\n",
    "            'shared': dict(shared_counts),\n",
    "            'routed': dict(routed_counts)\n",
    "        }\n",
    "        \n",
    "    return token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_layer_distribution(token_counts, layer_id: int, domain: str, dark_mode: bool = True, tokenizer=tokenizer):\n",
    "    \"\"\"\n",
    "    Create a bar graph showing token distribution across experts for a specific layer\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get expert counts for specified layer\n",
    "        layer_counts = token_counts.get(layer_id, {})\n",
    "        if not layer_counts:\n",
    "            raise ValueError(f\"No data found for layer {layer_id}\")\n",
    "            \n",
    "        # Create lists for x and y values\n",
    "        expert_ids = sorted(layer_counts.keys())\n",
    "        token_counts_list = [layer_counts[expert_id] for expert_id in expert_ids]\n",
    "        \n",
    "        # Create labels for experts\n",
    "        expert_labels = [f\"{expert_id}\" for expert_id in expert_ids]\n",
    "        \n",
    "        # Calculate totals\n",
    "        total_routing_decisions = sum(token_counts_list)\n",
    "        \n",
    "        file_path = f\"dataset/{domain}.txt\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        tokens = tokenizer(text)\n",
    "        actual_tokens = len(tokens['input_ids'])\n",
    "\n",
    "        # Create hover text\n",
    "        hover_text = [f\"Expert {expert_id}<br>Tokens: {count}<br>Percentage: {(count/total_routing_decisions*100):.1f}%\" \n",
    "                     for expert_id, count in zip(expert_ids, token_counts_list)]\n",
    "\n",
    "        # Create bar graph\n",
    "        fig = go.Figure(data=[\n",
    "            go.Bar(\n",
    "                x=expert_labels,\n",
    "                y=token_counts_list,\n",
    "                hovertext=hover_text,\n",
    "                hoverinfo='text',\n",
    "                marker_color='#636EFA'\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # Update layout\n",
    "        template = 'plotly_dark' if dark_mode else 'plotly_white'\n",
    "        fig.update_layout(\n",
    "            title=f'Token Distribution Across Experts in Layer {layer_id} ({domain})',\n",
    "            xaxis_title='Expert',\n",
    "            yaxis_title='Number of Routing Decisions',\n",
    "            width=1200,\n",
    "            height=600,\n",
    "            template=template,\n",
    "            showlegend=False,\n",
    "            xaxis={'tickangle': -45},\n",
    "            yaxis=dict(range=[0, actual_tokens])  # Set max on y-axis to actual tokens\n",
    "        )\n",
    "        \n",
    "        # Add annotations for both total routing decisions and actual tokens\n",
    "        fig.add_annotation(\n",
    "            text=f'Total Routing Decisions: {total_routing_decisions:,}<br>Actual Tokens: {actual_tokens:,}',\n",
    "            xref='paper', yref='paper',\n",
    "            x=1, y=1,\n",
    "            xanchor='right', yanchor='top',\n",
    "            showarrow=False,\n",
    "            font=dict(size=12)\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating bar plot: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_token_heatmap(token_counts, domain, n_experts=64, n_layers=27, tokenizer=None):\n",
    "    \"\"\"\n",
    "    Create heatmap of token percentages per expert per layer (routed experts only)\n",
    "    \"\"\"\n",
    "    # Initialize matrices for counts and percentages\n",
    "    count_matrix = np.zeros((n_layers, n_experts))\n",
    "    percentage_matrix = np.zeros((n_layers, n_experts))\n",
    "    \n",
    "    # Fill matrices with token counts and calculate percentages per layer\n",
    "    for layer_id, routed_counts in token_counts.items():\n",
    "        # Process routed experts (0-63)\n",
    "        for expert_id, count in routed_counts.items():\n",
    "            if expert_id < n_experts:\n",
    "                count_matrix[layer_id-1, expert_id] = count\n",
    "        \n",
    "        # Calculate percentages for this layer\n",
    "        layer_total = count_matrix[layer_id-1].sum()\n",
    "        if layer_total > 0:\n",
    "            percentage_matrix[layer_id-1] = (count_matrix[layer_id-1] / layer_total) * 100\n",
    "    \n",
    "    # Calculate total routing decisions\n",
    "    total_routing_decisions = int(count_matrix.sum())\n",
    "    \n",
    "    # Get actual tokens count\n",
    "    file_path = f\"dataset/{domain}.txt\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    tokens = tokenizer(text)\n",
    "    actual_tokens = len(tokens['input_ids'])\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=percentage_matrix,\n",
    "        x=[f'{i}' for i in range(n_experts)],  # 0-63 for all routed experts\n",
    "        y=[f'{i+1}' for i in range(n_layers)],\n",
    "        colorscale='Viridis',\n",
    "        hoverongaps=False,\n",
    "        hovertemplate='Layer %{y}<br>Expert %{x}<br>' +\n",
    "                      'Percentage: %{z:.1f}%<br>' +\n",
    "                      'Tokens: %{customdata:d}<extra></extra>',\n",
    "        customdata=count_matrix.astype(int)\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'Token Distribution Heatmap for {domain} (Percentages per Layer)',\n",
    "        xaxis_title='Expert Index (0-63)',\n",
    "        yaxis_title='Layer',\n",
    "        width=1200,\n",
    "        height=800,\n",
    "        template='plotly_dark',\n",
    "        margin=dict(t=100)\n",
    "    )\n",
    "    \n",
    "    fig.add_annotation(\n",
    "        text=f'Total Routing Decisions: {total_routing_decisions:,}<br>Actual Tokens: {actual_tokens:,}',\n",
    "        xref='paper', yref='paper',\n",
    "        x=1.0,\n",
    "        y=1.1,\n",
    "        xanchor='right',\n",
    "        yanchor='top',\n",
    "        showarrow=False,\n",
    "        font=dict(size=12)\n",
    "    )\n",
    "    \n",
    "    return fig, count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_expert_distribution(domain, device='cpu'):\n",
    "    \"\"\"    \n",
    "    args :\n",
    "        json_path: Path to the JSON file containing expert counts\n",
    "        device: Device to use for tensor operations ('cuda' or 'cpu')\n",
    "    output : heatmap showing distribution of tokens across experts and layers as percentages\n",
    "    \"\"\"\n",
    "    json_path = f'plots-deepseek/{domain}_count_matrix.npy'\n",
    "\n",
    "    # Load the matrix directly from npy file \n",
    "    count_matrix = np.load(json_path)\n",
    "    \n",
    "    # Convert raw counts to percentages per layer\n",
    "    percentage_matrix = np.zeros_like(count_matrix, dtype=float)\n",
    "    for layer in range(count_matrix.shape[0]):\n",
    "        layer_total = count_matrix[layer].sum()\n",
    "        if layer_total > 0:  # Avoid division by zero\n",
    "            percentage_matrix[layer] = (count_matrix[layer] / layer_total) * 100\n",
    "    \n",
    "    # Create heatmap with adjusted expert indices\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=percentage_matrix,\n",
    "        x=[f'{i+1}' for i in range(percentage_matrix.shape[1])],\n",
    "        y=[f'{i+1}' for i in range(percentage_matrix.shape[0])],\n",
    "        colorscale='Viridis',\n",
    "        hoverongaps=False,\n",
    "        hovertemplate='Layer %{y}<br>Expert %{x}<br>Percentage: %{z:.1f}%<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'Percentage Distribution of Tokens Across Experts and Layers ({domain})',\n",
    "        xaxis_title='Expert Index',\n",
    "        yaxis_title='Layer',\n",
    "        width=1200,\n",
    "        height=800,\n",
    "        template='plotly_dark'\n",
    "    )\n",
    "    \n",
    "    return fig, count_matrix\n",
    "def plot_layer_distribution(token_counts, layer_id: int, domain: str, dark_mode: bool = True, tokenizer=None):\n",
    "    \"\"\"Create a bar graph showing percentage distribution of tokens across experts for a specific layer\"\"\"\n",
    "    try:\n",
    "        # Get expert counts for specified layer\n",
    "        layer_counts = token_counts.get(layer_id, {})\n",
    "        if not layer_counts:\n",
    "            raise ValueError(f\"No data found for layer {layer_id}\")\n",
    "            \n",
    "        # Create lists for x and y values\n",
    "        expert_ids = sorted(layer_counts.keys())\n",
    "        token_counts_list = [layer_counts[expert_id] for expert_id in expert_ids]\n",
    "        \n",
    "        # Calculate percentages\n",
    "        total_routing_decisions = sum(token_counts_list)\n",
    "        percentages = [(count/total_routing_decisions * 100) for count in token_counts_list]\n",
    "        \n",
    "        # Create labels for experts\n",
    "        expert_labels = [f\"{expert_id}\" for expert_id in expert_ids]\n",
    "\n",
    "        # Get actual tokens for reference\n",
    "        file_path = f\"dataset/{domain}.txt\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        tokens = tokenizer(text)\n",
    "        actual_tokens = len(tokens['input_ids'])\n",
    "\n",
    "        # Create hover text with both counts and percentages\n",
    "        hover_text = [f\"Expert {expert_id}<br>Tokens: {count}<br>Percentage: {percentage:.1f}%\" \n",
    "                     for expert_id, count, percentage in zip(expert_ids, token_counts_list, percentages)]\n",
    "\n",
    "        # Create bar graph\n",
    "        fig = go.Figure(data=[\n",
    "            go.Bar(\n",
    "                x=expert_labels,\n",
    "                y=percentages,  # Now plotting percentages instead of raw counts\n",
    "                hovertext=hover_text,\n",
    "                hoverinfo='text',\n",
    "                marker_color='#636EFA'\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # Update layout\n",
    "        template = 'plotly_dark' if dark_mode else 'plotly_white'\n",
    "        fig.update_layout(\n",
    "            title=f'Token Distribution Across Experts in Layer {layer_id} ({domain})',\n",
    "            xaxis_title='Expert',\n",
    "            yaxis_title='Percentage of Total Tokens (%)',\n",
    "            width=1200,\n",
    "            height=600,\n",
    "            template=template,\n",
    "            showlegend=False,\n",
    "            xaxis={'tickangle': -45},\n",
    "            yaxis=dict(range=[0, 100])  # Set y-axis range to 0-100%\n",
    "        )\n",
    "        \n",
    "        # Add annotations for totals\n",
    "        fig.add_annotation(\n",
    "            text=f'Total Routing Decisions: {total_routing_decisions:,}<br>Actual Tokens: {actual_tokens:,}',\n",
    "            xref='paper', yref='paper',\n",
    "            x=1, y=1,\n",
    "            xanchor='right', yanchor='top',\n",
    "            showarrow=False,\n",
    "            font=dict(size=12)\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating bar plot: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_layer_distribution(token_counts, layer_id: int, domain: str, dark_mode: bool = True, tokenizer=None):\n",
    "    \"\"\"Create a bar graph showing percentage distribution of tokens across routed experts (0-63) for a specific layer\"\"\"\n",
    "    try:\n",
    "        # Get expert counts for specified layer - token_counts is already the routed experts dict\n",
    "        if not token_counts:\n",
    "            raise ValueError(f\"No data found for layer {layer_id}\")\n",
    "            \n",
    "        # Create lists for x and y values - ensure we handle all experts 0-63\n",
    "        token_counts_list = [token_counts.get(i, 0) for i in range(64)]  # Initialize all experts 0-63\n",
    "        \n",
    "        # Calculate percentages\n",
    "        total_routing_decisions = sum(token_counts_list)\n",
    "        if total_routing_decisions == 0:\n",
    "            raise ValueError(f\"No routing decisions found for layer {layer_id}\")\n",
    "        percentages = [(count/total_routing_decisions * 100) for count in token_counts_list]\n",
    "        \n",
    "        # Create labels for experts\n",
    "        expert_labels = [f\"{i}\" for i in range(64)]  # Labels 0-63\n",
    "\n",
    "        # Get actual tokens for reference\n",
    "        file_path = f\"dataset/{domain}.txt\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        tokens = tokenizer(text)\n",
    "        actual_tokens = len(tokens['input_ids'])\n",
    "\n",
    "        # Create hover text with both counts and percentages\n",
    "        hover_text = [f\"Expert {expert_id}<br>Tokens: {count}<br>Percentage: {percentage:.1f}%\" \n",
    "                     for expert_id, count, percentage in zip(expert_labels, token_counts_list, percentages)]\n",
    "\n",
    "        # Create bar graph\n",
    "        fig = go.Figure(data=[\n",
    "            go.Bar(\n",
    "                x=expert_labels,\n",
    "                y=percentages,\n",
    "                hovertext=hover_text,\n",
    "                hoverinfo='text',\n",
    "                marker_color='#636EFA'\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # Update layout\n",
    "        template = 'plotly_dark' if dark_mode else 'plotly_white'\n",
    "        fig.update_layout(\n",
    "            title=f'Token Distribution Across Experts in Layer {layer_id} ({domain})',\n",
    "            xaxis_title='Expert Index (0-63)',\n",
    "            yaxis_title='Percentage of Total Tokens (%)',\n",
    "            width=1200,\n",
    "            height=600,\n",
    "            template=template,\n",
    "            showlegend=False,\n",
    "            xaxis={'tickangle': -45},\n",
    "            yaxis=dict(range=[0, 100])  # Set y-axis range to 0-100%\n",
    "        )\n",
    "        \n",
    "        # Add annotations for totals\n",
    "        fig.add_annotation(\n",
    "            text=f'Total Routing Decisions: {total_routing_decisions:,}<br>Actual Tokens: {actual_tokens:,}',\n",
    "            xref='paper', yref='paper',\n",
    "            x=1, y=1,\n",
    "            xanchor='right', yanchor='top',\n",
    "            showarrow=False,\n",
    "            font=dict(size=12)\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating bar plot: {e}\")\n",
    "        traceback.print_exc()  # Add this to get more detailed error information\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(\n",
    "    plot_type: str = 'heatmap',\n",
    "    file_path: str = None,\n",
    "    model = None,\n",
    "    tokenizer = None,\n",
    "    domain: str = 'test1',\n",
    "    chunk_size: int = 2048,\n",
    "    num_shared_experts: int = 2,  \n",
    "    num_topk: int = 6,\n",
    "    layer_id: int = 1,\n",
    "    device: torch.device = None,\n",
    "    force_recompute: bool = False\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Main analysis function with caching of computed results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create cache filename that includes configuration\n",
    "        cache_name = f'{domain}_s{num_shared_experts}_k{num_topk}'\n",
    "        npy_path = f'plots-deepseek/{cache_name}_count_matrix.npy'\n",
    "        \n",
    "        # Adjust matrix size to accommodate all experts (including index 63)\n",
    "        total_experts = 64  # Update this to match the model's total number of experts\n",
    "        \n",
    "        # Check for existing results with matching parameters\n",
    "        if os.path.exists(npy_path) and not force_recompute:\n",
    "            print(f\"Loading existing results for {domain} with {num_shared_experts} shared experts and top-k={num_topk}\")\n",
    "            data = np.load(npy_path, allow_pickle=True).item()\n",
    "            shared_matrix = data['shared']\n",
    "            routed_matrix = data['routed']\n",
    "            \n",
    "            # Reconstruct token_counts\n",
    "            token_counts = {}\n",
    "            for layer in range(shared_matrix.shape[0]):\n",
    "                token_counts[layer + 1] = {\n",
    "                    'shared': {i: int(shared_matrix[layer, i]) for i in range(shared_matrix.shape[1]) if shared_matrix[layer, i] > 0},\n",
    "                    'routed': {i: int(routed_matrix[layer, i]) for i in range(routed_matrix.shape[1]) if routed_matrix[layer, i] > 0}\n",
    "                }\n",
    "        else:\n",
    "            print(f\"Computing new results for {domain} with {num_shared_experts} shared experts and top-k={num_topk}\")\n",
    "            \n",
    "            if device is None:\n",
    "                device = get_device()\n",
    "            print(f\"Using device: {device}\")\n",
    "            \n",
    "            chunks = prepare_text_chunks(file_path, chunk_size, tokenizer)\n",
    "            if not chunks:\n",
    "                raise ValueError(\"No text chunks generated\")\n",
    "                \n",
    "            # Process chunks\n",
    "            all_distributions = defaultdict(list)\n",
    "            total_chunks = len(chunks)\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                torch.cuda.empty_cache()\n",
    "                print(f\"Processing chunk {i+1}/{total_chunks}\")\n",
    "                \n",
    "                chunk_dist = get_expert_distribution(\n",
    "                    model, \n",
    "                    chunk,\n",
    "                    tokenizer,\n",
    "                    device,\n",
    "                    num_shared_experts=num_shared_experts,\n",
    "                    num_topk=num_topk\n",
    "                )\n",
    "                \n",
    "                for layer, layer_data in chunk_dist.items():\n",
    "                    all_distributions[layer].extend(layer_data)\n",
    "                    \n",
    "            token_counts = analyze_token_distributions(all_distributions)\n",
    "            \n",
    "            # Create matrices for saving with correct dimensions\n",
    "            shared_matrix = np.zeros((27, num_shared_experts))\n",
    "            routed_matrix = np.zeros((27, total_experts - num_shared_experts))  # Adjusted for total experts\n",
    "            \n",
    "            for layer in range(27):\n",
    "                layer_counts = token_counts.get(layer + 1, {'shared': {}, 'routed': {}})\n",
    "                for i, count in layer_counts['shared'].items():\n",
    "                    if i < num_shared_experts:  # Bounds check\n",
    "                        shared_matrix[layer, i] = count\n",
    "                for i, count in layer_counts['routed'].items():\n",
    "                    if i < (total_experts - num_shared_experts):  # Bounds check\n",
    "                        routed_matrix[layer, i] = count\n",
    "            \n",
    "            # Save results\n",
    "            os.makedirs('plots-deepseek', exist_ok=True)\n",
    "            np.save(npy_path, {'shared': shared_matrix, 'routed': routed_matrix})\n",
    "            \n",
    "        # Create visualization\n",
    "        if plot_type == 'heatmap':\n",
    "            # Extract routed experts data from the nested structure\n",
    "            routed_counts = {}\n",
    "            for layer_id, layer_data in token_counts.items():\n",
    "                routed_counts[layer_id] = layer_data['routed']\n",
    "                \n",
    "            fig, matrix = plot_token_heatmap(  \n",
    "                token_counts=routed_counts,  # Pass extracted routed experts data\n",
    "                domain=domain,\n",
    "                n_experts=64,  # Fixed number of experts for DeepSeek\n",
    "                n_layers=27,   # Fixed number of layers\n",
    "                tokenizer=tokenizer\n",
    "            )\n",
    "        else:\n",
    "            fig = plot_layer_distribution(\n",
    "                token_counts=token_counts[layer_id]['routed'],  # Pass routed experts for specific layer\n",
    "                layer_id=layer_id,\n",
    "                domain=domain,\n",
    "                tokenizer=tokenizer\n",
    "            )\n",
    "            \n",
    "        # Save plots with configuration in filename\n",
    "        if plot_type == 'heatmap':\n",
    "            fig.write_html(f'plots-deepseek/{cache_name}_heatmap.html')\n",
    "            fig.write_image(f'plots-deepseek/{cache_name}_heatmap.png')\n",
    "        else:\n",
    "            # fig.write_html(f'plots-deepseek/{cache_name}_layer{layer_id}_bargraph.html')\n",
    "            # fig.write_image(f'plots-deepseek/{cache_name}_layer{layer_id}_bargraph.png')\n",
    "            pass\n",
    "            \n",
    "        return fig, {'shared': shared_matrix, 'routed': routed_matrix}, token_counts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in analyze_dataset: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze with proper bounds checking\n",
    "fig, count_matrix, token_counts = analyze_dataset(\n",
    "    plot_type='heatmap', # or 'bar'\n",
    "    file_path='dataset/humaneval.txt',\n",
    "    model= model,\n",
    "    tokenizer= tokenizer,\n",
    "    domain='humaneval',\n",
    "    chunk_size=1024,\n",
    "    num_shared_experts=1,\n",
    "    num_topk=7,\n",
    "    force_recompute=False\n",
    "    # layer_id=1 # only for bar plot\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, count_matrix, token_counts = analyze_dataset(\n",
    "    plot_type='heatmap', # or 'bar'\n",
    "    file_path='dataset/piqa.txt',\n",
    "    model= model,\n",
    "    tokenizer= tokenizer,\n",
    "    domain='piqa',\n",
    "    chunk_size=1024,\n",
    "    num_shared_experts=1, \n",
    "    num_topk=7,\n",
    "    force_recompute=False\n",
    "    # layer_id=1 # only for bar plot\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze with proper bounds checking\n",
    "fig, count_matrix, token_counts = analyze_dataset(\n",
    "    plot_type='heatmap', # or 'bar'\n",
    "    file_path='dataset/gsm8k.txt',\n",
    "    model= model,\n",
    "    tokenizer= tokenizer,\n",
    "    domain='gsm8k',\n",
    "    chunk_size=1024,\n",
    "    num_shared_experts=1, \n",
    "    num_topk=7,\n",
    "    force_recompute=False\n",
    "    # layer_id=1 # only for bar plot\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, count_matrix, token_counts = analyze_dataset(\n",
    "    plot_type='heatmap', # or 'bar'\n",
    "    file_path='dataset/arc_easy.txt',\n",
    "    model= model,\n",
    "    tokenizer= tokenizer,\n",
    "    domain='arc_easy',\n",
    "    chunk_size=1024,\n",
    "    num_shared_experts=1, \n",
    "    num_topk=7,\n",
    "    force_recompute=False\n",
    "    # layer_id=1 # only for bar plot\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, count_matrix, token_counts = analyze_dataset(\n",
    "    plot_type='bar',\n",
    "    file_path='dataset/arc_easy.txt',\n",
    "    model= model,\n",
    "    tokenizer= tokenizer,\n",
    "    domain='arc_easy',\n",
    "    chunk_size=1024,\n",
    "    num_shared_experts=1, \n",
    "    num_topk=7,\n",
    "    force_recompute=False,\n",
    "    layer_id=1 # only for bar plot\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
