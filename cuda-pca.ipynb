{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    \"\"\"get the device to use for model and tokenizer\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return device\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    # Set default device to CUDA if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load model with CUDA optimizations\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",  # Automatically handle model parallelism\n",
    "        use_flash_attention_2=True,\n",
    "    )\n",
    "    \n",
    "    # Move model to GPU\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Enable CUDA optimizations\n",
    "    if device.type == \"cuda\":\n",
    "        model = model.half()  # Convert to FP16 for GPU memory efficiency\n",
    "        torch.backends.cudnn.benchmark = True  # Enable CUDNN autotuner\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Initialize model on GPU\n",
    "model, tokenizer = load_model(\"deepseek-ai/deepseek-moe-16b-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, input_text, max_length=70):\n",
    "    # Encoding can happen on CPU since it's just string processing\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    # Then move to GPU after encoding\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    \n",
    "    # outputs = model(input_ids, output_router_logits=True) \n",
    "    outputs = model(input_ids,)\n",
    "    \n",
    "    # Generate on GPU\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        use_cache=True,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    print(\"generated text :\")\n",
    "    # Move to CPU for decoding since tokenizer expects CPU tensors\n",
    "    for token in output[0].cpu():\n",
    "        print(tokenizer.decode(token, skip_special_tokens=True), end='', flush=True)\n",
    "    print()\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_router_logits(model, input_ids):\n",
    "    \"\"\"get router logits from model forward pass\"\"\"\n",
    "    \n",
    "    # Get hidden states for each layer\n",
    "    outputs = model(input_ids, output_hidden_states=True)\n",
    "    hidden_states = outputs.hidden_states\n",
    "    \n",
    "    # Initialize tensor to store all router logits on GPU\n",
    "    num_layers = len(model.model.layers)\n",
    "    first_layer = next(layer for layer in model.model.layers if hasattr(layer.mlp, 'gate'))\n",
    "    num_experts = first_layer.mlp.gate.weight.shape[0]\n",
    "    bsz, seq_len, hidden_dim = hidden_states[0].shape\n",
    "    router_logits = torch.zeros((num_layers, bsz * seq_len, num_experts), device=model.device)\n",
    "    \n",
    "    # For each MoE layer, calculate router logits\n",
    "    layer_count = 0\n",
    "    for layer_idx, layer in enumerate(model.model.layers):\n",
    "        # Check if this layer uses MoE\n",
    "        if hasattr(layer.mlp, 'gate'):\n",
    "            # Get hidden states before MoE layer\n",
    "            layer_hidden = hidden_states[layer_idx]\n",
    "            \n",
    "            # Calculate router logits using layer's gate\n",
    "            flat_hidden = layer_hidden.view(-1, hidden_dim)\n",
    "            logits = torch.nn.functional.linear(flat_hidden, layer.mlp.gate.weight.to(model.device))\n",
    "            \n",
    "            # Store logits in the tensor\n",
    "            router_logits[layer_count] = logits\n",
    "            layer_count += 1\n",
    "                \n",
    "    router_logits = router_logits[:layer_count]  \n",
    "    print(f\"router logits shape : {router_logits.shape}\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return router_logits\n",
    "\n",
    "def get_last_token_router_probs(router_logits, layer_idx):\n",
    "    \"\"\"Get router probabilities for the last token in a specified layer\"\"\"\n",
    "    layer_logits = router_logits[layer_idx]  # Shape: [sequence_length, num_experts]\n",
    "    last_token_logits = layer_logits[-1]  # get last token logits\n",
    "    routing_probs = torch.nn.functional.softmax(last_token_logits, dim=-1)\n",
    "    \n",
    "    return routing_probs\n",
    "\n",
    "def topk(router_probs, k):\n",
    "    \"\"\"zero out all components except top k router probabilities\"\"\"\n",
    "    values, indices = torch.topk(router_probs, k)\n",
    "    zeroed_probs = torch.zeros_like(router_probs, device=router_probs.device)\n",
    "    zeroed_probs[indices] = values\n",
    "    \n",
    "    return zeroed_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_router_probs_matrix(model, prompts, k=8):\n",
    "    \"\"\"get router probs matrix for all tokens and last token for multiple inputs across all layers\n",
    "    shape of all_router_logits : [num_prompts, num_layers-1, max_seq_len, num_experts]\n",
    "    shape of last_token_prob_matrix : [num_layers-1, num_prompts, num_experts]\n",
    "    \"\"\"\n",
    "    num_prompts = len(prompts)\n",
    "    num_experts = 64\n",
    "    num_layers = 28\n",
    "    last_token_prob_matrix = torch.zeros((num_layers-1, num_prompts, num_experts))\n",
    "    \n",
    "    # Store router logits for all tokens\n",
    "    all_router_logits = []\n",
    "    max_seq_len = 0\n",
    "    \n",
    "    # First pass to get max sequence length\n",
    "    for prompt in prompts:\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        seq_len = input_ids.shape[1]\n",
    "        max_seq_len = max(max_seq_len, seq_len)\n",
    "    \n",
    "    # Get router probs for each prompt\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        router_logits = get_router_logits(model, input_ids)\n",
    "        \n",
    "        # Pad router logits to max sequence length\n",
    "        curr_seq_len = router_logits.shape[1]\n",
    "        if curr_seq_len < max_seq_len:\n",
    "            padding = torch.zeros((router_logits.shape[0], max_seq_len - curr_seq_len, num_experts))\n",
    "            router_logits = torch.cat([router_logits, padding], dim=1)\n",
    "            \n",
    "        all_router_logits.append(router_logits)\n",
    "        \n",
    "        # Get probs for each layer's last token\n",
    "        for layer_idx in range(1,num_layers):\n",
    "            probs = get_last_token_router_probs(router_logits, layer_idx-1)\n",
    "            top_probs = topk(probs, k=k)\n",
    "            last_token_prob_matrix[layer_idx-1, i] = top_probs\n",
    "            \n",
    "    # Stack all router logits into single tensor\n",
    "    all_router_logits = torch.stack(all_router_logits)\n",
    "\n",
    "    # Clear GPU memory after processing all prompts\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return last_token_prob_matrix, all_router_logits\n",
    "\n",
    "\n",
    "\n",
    "def get_last_token(prompt):\n",
    "    \"\"\"get the last token of a prompt using the tokenizer\"\"\"\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    last_token = tokenizer.decode([tokens[-1]])\n",
    "    return last_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_visualize(prompts):\n",
    "    \"\"\"perform PCA visualization on router probabilities for a list of prompts\"\"\"\n",
    "    \n",
    "    # Get router probability matrix using helper functions\n",
    "    last_token_prob_matrix, all_router_logits = get_router_probs_matrix(model, prompts, k=64)\n",
    "    router_prob_matrix_np = last_token_prob_matrix.detach().numpy()\n",
    "\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=3)\n",
    "    pca_result = pca.fit_transform(router_prob_matrix_np)\n",
    "\n",
    "    print(\"\\nPCA results:\")\n",
    "    print(f\"explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "    print(f\"cumulative explained variance: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "    print(\"\\nPCA transformed data shape:\", pca_result.shape)\n",
    "\n",
    "    # Get last token of each prompt\n",
    "    last_tokens = [get_last_token(prompt) for prompt in prompts]\n",
    "\n",
    "    # Create 3D scatter plot\n",
    "    fig = go.Figure(data=[go.Scatter3d(\n",
    "        x=pca_result[:, 0],\n",
    "        y=pca_result[:, 1],\n",
    "        z=pca_result[:, 2],\n",
    "        mode='markers+text',\n",
    "        text=last_tokens,\n",
    "        textposition=\"top center\",\n",
    "        marker=dict(\n",
    "            size=10,\n",
    "            opacity=0.8\n",
    "        )\n",
    "    )])\n",
    "\n",
    "    # Update layout for 3D\n",
    "    fig.update_layout(\n",
    "        title='3D PCA of Router Probabilities',\n",
    "        scene=dict(\n",
    "            xaxis_title='first principal component',\n",
    "            yaxis_title='second principal component', \n",
    "            zaxis_title='third principal component',\n",
    "            xaxis=dict(showgrid=True, gridwidth=1, gridcolor='LightGray'),\n",
    "            yaxis=dict(showgrid=True, gridwidth=1, gridcolor='LightGray'),\n",
    "            zaxis=dict(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
    "        ),\n",
    "        width=1000,\n",
    "        height=800,\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(router_prob_matrix, idx1, idx2):\n",
    "\n",
    "    # Get the probability vectors for the two tokens\n",
    "    vec1 = router_prob_matrix[idx1]\n",
    "    vec2 = router_prob_matrix[idx2]\n",
    "    \n",
    "    # Check if inputs are already torch tensors\n",
    "    if not isinstance(vec1, torch.Tensor):\n",
    "        vec1 = torch.from_numpy(vec1).float()\n",
    "    if not isinstance(vec2, torch.Tensor):\n",
    "        vec2 = torch.from_numpy(vec2).float()\n",
    "    \n",
    "    # Compute cosine similarity using torch.nn.functional\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(vec1.unsqueeze(0), vec2.unsqueeze(0))\n",
    "    \n",
    "    return cos_sim.item()\n",
    "\n",
    "# print(f\"Cosine similarity between tokens 2 and 3: {compute_cosine_similarity(router_prob_matrix, 2, 3):.4f}\")\n",
    "# print(f\"Cosine similarity between tokens 0 and 5: {compute_cosine_similarity(router_prob_matrix, 0, 5):.4f}\")\n",
    "# print(f\"Cosine similarity between tokens 10 and 15: {compute_cosine_similarity(router_prob_matrix, 10, 15):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompts_from_txt(txt_file_path,  domain = 'english', output_path=f'english.json'):\n",
    "    \"\"\" read prompts from a txt file and save them in json format. \"\"\"\n",
    "    \n",
    "    with open(txt_file_path, 'r', encoding='utf-8') as f:\n",
    "        prompts = [line.strip() for line in f.readlines() if line.strip()]\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump({f\"{domain}\": prompts}, f, indent=4)\n",
    "        \n",
    "    print(f\"{domain} prompts saved to {output_path}\")\n",
    "    return prompts\n",
    "\n",
    "def parse_code_blocks(txt_file_path, output_path='code.json', domain='code'):\n",
    "    \"\"\"parse code blocks between ``` markers from a text file and save them in json format.\"\"\"\n",
    "    code_blocks = []\n",
    "    current_block = []\n",
    "    in_block = False\n",
    "    \n",
    "    with open(txt_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip().startswith('```'):\n",
    "                if in_block:\n",
    "                    # Current block is complete, save it and start new block\n",
    "                    if current_block:\n",
    "                        code_blocks.append('\\n'.join(current_block))\n",
    "                    current_block = []\n",
    "                # Always start a new block since ``` only indicates start\n",
    "                in_block = True\n",
    "                current_block = []\n",
    "            elif in_block:\n",
    "                # Add line to current block\n",
    "                current_block.append(line.rstrip())\n",
    "    \n",
    "    # Save final block if exists\n",
    "    if current_block:\n",
    "        code_blocks.append('\\n'.join(current_block))\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump({domain: code_blocks}, f, indent=4)\n",
    "        \n",
    "    print(f\"code blocks saved to {output_path}\")\n",
    "    return code_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompts_from_json(json_file_path):\n",
    "    \"\"\"load prompts from a json file and return them as a list.\"\"\"\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    # get the first (and only) value from the dictionary\n",
    "    # since the json structure is {\"domain\": [prompts]}\n",
    "    prompts = list(data.values())[0]\n",
    "    return prompts\n",
    "\n",
    "def prepare_multi_domain_prompts(domain_files, output_path='all_domain_prompts.json'):\n",
    "    \"\"\"\n",
    "    prepare a json file containing prompts from multiple domains.\n",
    "    \n",
    "    args:\n",
    "        domain_files: Dict mapping domain names to lists of tuples (file_path, parser_func)\n",
    "            where parser_func is a function that takes a file path and returns a list of prompts\n",
    "            \n",
    "    example:\n",
    "        domain_files = {\n",
    "            'code': [('code.txt', parse_code_blocks)], \n",
    "            'english': [('english.txt', prepare_prompts_from_txt)]\n",
    "        }\n",
    "    \"\"\"\n",
    "    all_prompts = {}\n",
    "    \n",
    "    for domain, file_list in domain_files.items():\n",
    "        domain_prompts = []\n",
    "        for _, prompts in file_list:\n",
    "            # Use load_prompts_from_json if prompts is a dict\n",
    "            if not isinstance(prompts, list):\n",
    "                prompts = load_prompts_from_json(prompts)\n",
    "            domain_prompts.extend(prompts)\n",
    "                \n",
    "        all_prompts[domain] = domain_prompts\n",
    "        \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_prompts, f, indent=4)\n",
    "        \n",
    "    print(f\"all domain prompts saved to {output_path}\")\n",
    "    return all_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_all_to_list(all_prompts):\n",
    "    \"\"\"\n",
    "    combines prompts from all domains into a single list of domain-specific prompt lists.\n",
    "    returns a list in the format [[domain1_prompts], [domain2_prompts], ...].\n",
    "    \"\"\"\n",
    "    # Create list of domain-specific prompt lists\n",
    "    combined_prompts = [\n",
    "        prompts for prompts in all_prompts.values()\n",
    "    ]\n",
    "        \n",
    "    total_prompts = sum(len(prompts) for prompts in combined_prompts)\n",
    "    print(f\"total prompts: {total_prompts}\")\n",
    "    print(f\"prompts per domain:\")\n",
    "    for domain, prompts in zip(all_prompts.keys(), combined_prompts):\n",
    "        print(f\"  {domain}: {len(prompts)}\")\n",
    "        \n",
    "    return combined_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_visualize_all_domains(router_prob_matrix,combined_prompts, layer_id=None):\n",
    "    \"\"\"\n",
    "    perform PCA visualization on router probability matrix for all domains together.\n",
    "    colors points based on which domain list they came from.\n",
    "    \n",
    "    args:\n",
    "        combined_prompts: List of lists of prompts, where each inner list represents a domain\n",
    "                         [code_prompts, english_prompts, french_prompts, ...]\n",
    "        layer_idx: if None, visualizes all layers but has to be >0 ALWAYS if used\n",
    "    \"\"\"\n",
    "    # Flatten prompts list while tracking domain indices\n",
    "    all_prompts = []\n",
    "    domain_colors = []\n",
    "    \n",
    "    # Generate enough distinct colors for all domains\n",
    "    num_domains = len(combined_prompts)\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, num_domains))\n",
    "    \n",
    "    for domain_idx, domain_prompts in enumerate(combined_prompts):\n",
    "        all_prompts.extend(domain_prompts)\n",
    "        domain_colors.extend([colors[domain_idx]] * len(domain_prompts))\n",
    "    \n",
    "    \n",
    "    # If layer_idx is None, visualize all layers\n",
    "    if layer_id is None:\n",
    "        # num_layers = router_prob_matrix.shape[0]\n",
    "        # print(f\"num layers : {num_layers}\")\n",
    "        \n",
    "        for layer_idx in range(1,28):\n",
    "            print(f\"layer {layer_idx} router probs shape : {router_prob_matrix[layer_idx-1].shape}\")\n",
    "            # Perform PCA for current layer\n",
    "            pca = PCA(n_components=3)\n",
    "            layer_probs = router_prob_matrix[layer_idx-1].detach().numpy()\n",
    "            pca_result = pca.fit_transform(layer_probs)\n",
    "\n",
    "            print(f\"\\nPCA results for layer {layer_idx}:\")\n",
    "            print(f\"explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "            print(f\"cumulative explained variance: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "            print(\"\\nPCA transformed data shape:\", pca_result.shape)\n",
    "\n",
    "            # Get last tokens for each prompt\n",
    "            last_tokens = [get_last_token(prompt) for prompt in all_prompts]\n",
    "\n",
    "            # Create 3D scatter plot with domain-specific colors\n",
    "            fig = go.Figure(data=[go.Scatter3d(\n",
    "                x=pca_result[:, 0],\n",
    "                y=pca_result[:, 1], \n",
    "                z=pca_result[:, 2],\n",
    "                mode='markers+text',\n",
    "                text=last_tokens,\n",
    "                textposition=\"top center\",\n",
    "                marker=dict(\n",
    "                    size=10,\n",
    "                    opacity=0.8,\n",
    "                    color=[f'rgb({int(c[0]*255)},{int(c[1]*255)},{int(c[2]*255)})' for c in domain_colors]\n",
    "                )\n",
    "            )])\n",
    "\n",
    "            # Update layout for better 3D visualization\n",
    "            fig.update_layout(\n",
    "                title=f'PCA visualization of router probabilities across domains for layer {layer_idx}',\n",
    "                scene=dict(\n",
    "                    xaxis_title='PC1',\n",
    "                    yaxis_title='PC2', \n",
    "                    zaxis_title='PC3'\n",
    "                ),\n",
    "                width=1000,\n",
    "                height=800,\n",
    "                showlegend=False\n",
    "            )\n",
    "\n",
    "            # Save figure as HTML and PNG for each layer\n",
    "            os.makedirs('pca', exist_ok=True)\n",
    "            fig.write_html(f'pca/pca_visualization_layer_{layer_idx}.html')\n",
    "            fig.write_image(f'pca/pca_visualization_layer_{layer_idx}.png')\n",
    "\n",
    "            fig.show()\n",
    "    else:\n",
    "        # Original single layer visualization\n",
    "        layer_probs = router_prob_matrix[layer_id-1].detach().numpy()\n",
    "        pca = PCA(n_components=3)\n",
    "        pca_result = pca.fit_transform(layer_probs)\n",
    "\n",
    "        print(\"\\nPCA results:\")\n",
    "        print(f\"explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "        print(f\"cumulative explained variance: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "        print(\"\\nPCA transformed data shape:\", pca_result.shape)\n",
    "\n",
    "        # Get last tokens for each prompt\n",
    "        last_tokens = [get_last_token(prompt) for prompt in all_prompts]\n",
    "\n",
    "        # Create 3D scatter plot with domain-specific colors\n",
    "        fig = go.Figure(data=[go.Scatter3d(\n",
    "            x=pca_result[:, 0],\n",
    "            y=pca_result[:, 1], \n",
    "            z=pca_result[:, 2],\n",
    "            mode='markers+text',\n",
    "            text=last_tokens,\n",
    "            textposition=\"top center\",\n",
    "            marker=dict(\n",
    "                size=10,\n",
    "                opacity=0.8,\n",
    "                color=[f'rgb({int(c[0]*255)},{int(c[1]*255)},{int(c[2]*255)})' for c in domain_colors]\n",
    "            )\n",
    "        )])\n",
    "\n",
    "        # Update layout for better 3D visualization\n",
    "        fig.update_layout(\n",
    "            title=f'PCA visualization of router probabilities across domains for layer {layer_id}',\n",
    "            scene=dict(\n",
    "                xaxis_title='PC1',\n",
    "                yaxis_title='PC2', \n",
    "                zaxis_title='PC3'\n",
    "            ),\n",
    "            width=1000,\n",
    "            height=800,\n",
    "            showlegend=False\n",
    "        )\n",
    "\n",
    "        fig.show()\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english prompts saved to english.json\n",
      "french prompts saved to french.json\n",
      "code blocks saved to code.json\n",
      "total code prompts : 200\n",
      "total english prompts : 200\n",
      "code blocks saved to code.json\n",
      "english prompts saved to english.json\n",
      "french prompts saved to french.json\n",
      "all domain prompts saved to all_prompts.json\n",
      "total domains : 3\n",
      "total prompts: 600\n",
      "prompts per domain:\n",
      "  code: 200\n",
      "  english: 200\n",
      "  french: 200\n"
     ]
    }
   ],
   "source": [
    "prepare_prompts_from_txt('interp-data/engl-lit.txt', domain='english', output_path='english.json')\n",
    "prepare_prompts_from_txt('interp-data/french.txt', domain='french', output_path='french.json')\n",
    "parse_code_blocks('interp-data/code.txt', 'code.json', domain='code')\n",
    "\n",
    "code_prompts = load_prompts_from_json(json_file_path='code.json')\n",
    "print(f\"total code prompts : {len(code_prompts)}\")\n",
    "english_prompts = load_prompts_from_json(json_file_path='english.json')\n",
    "print(f'total english prompts : {len(english_prompts)}')\n",
    "\n",
    "\n",
    "domain_files = {\n",
    "    'code': [('interp-data/code.txt', parse_code_blocks(txt_file_path='interp-data/code.txt', output_path='code.json', domain='code'))],\n",
    "    'english': [('interp-data/engl-lit.txt', prepare_prompts_from_txt(txt_file_path='interp-data/engl-lit.txt', output_path='english.json', domain='english'))],\n",
    "    'french': [('interp-data/french.txt', prepare_prompts_from_txt(txt_file_path='interp-data/french.txt', output_path='french.json', domain='french'))]\n",
    "}\n",
    "all_prompts = prepare_multi_domain_prompts(domain_files, output_path='all_prompts.json')\n",
    "print(f\"total domains : {len(all_prompts)}\")\n",
    "\n",
    "# convert all prompts to a single list of domain-specific prompt lists\n",
    "combined_prompts = convert_all_to_list(all_prompts)\n",
    "\n",
    "# # Create test version with only first 5 prompts per domain\n",
    "# test_combined_prompts = [\n",
    "#     domain_prompts[:2] for domain_prompts in combined_prompts\n",
    "# ]\n",
    "\n",
    "# print(test_combined_prompts[0]) # Print first 5 prompts from first domain\n",
    "# print(f' total prompts in first domain : {len(test_combined_prompts[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in PCA visualizations layers are 0 indexed but in the code they are 1 indexed because of the router probs matrix !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "router logits shape : torch.Size([27, 26, 64])\n",
      "router logits shape : torch.Size([27, 19, 64])\n",
      "router logits shape : torch.Size([27, 28, 64])\n",
      "router logits shape : torch.Size([27, 25, 64])\n",
      "router logits shape : torch.Size([27, 34, 64])\n",
      "router logits shape : torch.Size([27, 27, 64])\n"
     ]
    }
   ],
   "source": [
    "k = 64\n",
    "prompts=[p for domain in combined_prompts for p in domain]\n",
    "print(f\"total prompts : {len(prompts)}\")\n",
    "prompts = prompts[:150]\n",
    "last_token_prob_matrix, all_router_logits = get_router_probs_matrix(model, prompts=prompts,k=k)\n",
    "# Save router probability matrix\n",
    "torch.save(last_token_prob_matrix, 'combined_last_token_prob_matrix-1.pt')\n",
    "# save all_router_logits\n",
    "torch.save(all_router_logits, 'combined_all_router_logits-1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts=[p for domain in combined_prompts for p in domain]\n",
    "print(f\"Processing {len(prompts)} prompts...\")\n",
    "\n",
    "# Initialize tensor to store all hidden states\n",
    "all_hidden_states = []\n",
    "\n",
    "# First pass to find max sequence length\n",
    "max_seq_len = 0\n",
    "for prompt in prompts:\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    max_seq_len = max(max_seq_len, input_ids.shape[1])\n",
    "\n",
    "print(f\"Max sequence length: {max_seq_len}\")\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    \n",
    "    # Get model outputs with hidden states\n",
    "    outputs = model(input_ids, output_hidden_states=True)\n",
    "    \n",
    "    # Get hidden states for all layers and move to CPU\n",
    "    hidden_states = [state.detach().cpu() for state in outputs.hidden_states]\n",
    "    \n",
    "    # Pad each hidden state to max_seq_len\n",
    "    padded_states = []\n",
    "    for state in hidden_states:\n",
    "        # Create padding\n",
    "        pad_size = max_seq_len - state.shape[1]  # Changed from state.shape[2]\n",
    "        if pad_size > 0:\n",
    "            padding = torch.zeros((state.shape[0], pad_size, state.shape[2]), dtype=state.dtype)  # Removed extra dimension\n",
    "            padded_state = torch.cat([state, padding], dim=1)  # Changed dim from 2 to 1\n",
    "            padded_states.append(padded_state)\n",
    "        else:\n",
    "            padded_states.append(state)\n",
    "    \n",
    "    # Stack all layers' hidden states for this prompt\n",
    "    prompt_hidden_states = torch.stack(padded_states)\n",
    "    \n",
    "    # Add to list\n",
    "    all_hidden_states.append(prompt_hidden_states)\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    del outputs\n",
    "    del hidden_states\n",
    "    del input_ids\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"Processed {i+1}/{len(prompts)} prompts\")\n",
    "\n",
    "# Stack all prompts' hidden states into one big tensor on CPU\n",
    "# Shape will be: [num_prompts, num_layers, sequence_length, hidden_size]\n",
    "all_hidden_states = torch.stack(all_hidden_states)\n",
    "\n",
    "print(f\"Final tensor shape: {all_hidden_states.shape}\")\n",
    "\n",
    "# Save the tensor\n",
    "torch.save(all_hidden_states, 'all_prompts_hidden_states.pt')\n",
    "print(\"Saved hidden states tensor to all_prompts_hidden_states.pt\")\n",
    "\n",
    "# Clear final tensor from memory\n",
    "del all_hidden_states\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all domains together\n",
    "# layer_id = None\n",
    "# fig = pca_visualize_all_domains(router_prob_matrix=last_token_prob_matrix,combined_prompts=combined_prompts, layer_id=layer_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_text = \"what is principal component analysis ?\"\n",
    "# outputs = generate_text(model, tokenizer, input_text)\n",
    "\n",
    "# test_prompts = [\n",
    "#     \"The quick brown fox\",\n",
    "#     \"1+1=\",\n",
    "#     \"the grey cat\",\n",
    "#     \"the grey elephant\",\n",
    "#     \"2*8\",\n",
    "#     \"def hello_world() : \\n    print('hello world')\",\n",
    "#     \"what is principal component analysis\",\n",
    "#     'what is capital of india',\n",
    "#     'sqrt 16',\n",
    "#     'void bubbleSort(int arr[], int n) {',\n",
    "#     'def is_prime(n):',\n",
    "#     'if n <= 1:',\n",
    "#     'return False',\n",
    "#     'for i in range(2, int(n**0.5) + 1):',\n",
    "#     'if n % i == 0:',\n",
    "#     'return False',\n",
    "#     'return True',\n",
    "#     \"china\",\n",
    "#     \"the united states of america\",\n",
    "#     \"london\",\n",
    "#     \"tokyo\",\n",
    "#     'paris'\n",
    "# ]\n",
    "\n",
    "# layer_idx = 0\n",
    "# k = 64\n",
    "# router_logits = get_router_logits(model, input_ids=tokenizer.encode(input_text, return_tensors=\"pt\"))\n",
    "# probs = get_last_token_router_probs(router_logits, layer_idx=layer_idx)\n",
    "# print(f\"router probs shape: {probs.shape}, sum: {probs.sum():.2f}\")\n",
    "# print(f'router probs : {probs}')\n",
    "# top_probs = topk(probs, k=k) \n",
    "# print(f\"top {k} probs : {top_probs}\")\n",
    "# print(f\"top {k} probs sum : {top_probs.sum():.2f}\")\n",
    "\n",
    "\n",
    "# router_prob_matrix = get_router_probs_matrix(model, prompts = test_prompts,k=k)\n",
    "# print(f\"router probability matrix shape : {router_prob_matrix.shape}\")\n",
    "# print(\"\\nrouter probs matrix :\")\n",
    "# print(router_prob_matrix[:])\n",
    "# print(f\"\\nverify each row sums to 1 : {router_prob_matrix.sum(dim=1)}\")\n",
    "\n",
    "\n",
    "# last_tokens = [get_last_token(prompt) for prompt in test_prompts]\n",
    "# print(last_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
