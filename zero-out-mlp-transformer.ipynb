{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OLMoForCausalLM(\n",
       "  (model): OLMo(\n",
       "    (transformer): ModuleDict(\n",
       "      (wte): Embedding(50304, 2048)\n",
       "      (emb_drop): Dropout(p=0.0, inplace=False)\n",
       "      (ln_f): LayerNorm()\n",
       "      (blocks): ModuleList(\n",
       "        (0-15): 16 x OLMoSequentialBlock(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (act): SwiGLU()\n",
       "          (attn_out): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (ff_out): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (att_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (ff_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (attn_norm): LayerNorm()\n",
       "          (ff_norm): LayerNorm()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"allenai/OLMo-1B\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B\", trust_remote_code=True)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mlp_weights(model, layer_idx):\n",
    "    \"\"\"\n",
    "    Print the weights of an MLP at a given layer.\n",
    "    \n",
    "    Args:\n",
    "        model: The OLMo model\n",
    "        layer_idx: Index of the layer containing the MLP\n",
    "    \"\"\"\n",
    "    ff_proj = f'model.transformer.blocks.{layer_idx}.ff_proj.weight'\n",
    "    ff_out = f'model.transformer.blocks.{layer_idx}.ff_out.weight'\n",
    "    \n",
    "    state_dict = model.state_dict()\n",
    "    \n",
    "    print(f\"\\nMLP weights for layer {layer_idx}:\")\n",
    "    print(\"\\nFF Projection:\")\n",
    "    print(state_dict[ff_proj])\n",
    "    print(state_dict[ff_proj].shape)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"\\nFF Output:\") \n",
    "    print(state_dict[ff_out])\n",
    "    print(state_dict[ff_out].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_mlp(model, layer_idx):\n",
    "    \"\"\"\n",
    "    Zero out MLP weights in a specific layer of the OLMo model.\n",
    "    \n",
    "    Args:\n",
    "        model: The OLMo model\n",
    "        layer_idx: Index of the layer to zero out\n",
    "    \"\"\"\n",
    "    # Access the transformer blocks\n",
    "    decoder_layers = model.model.transformer.blocks\n",
    "    print(f\"Zeroing out MLP in layer {layer_idx}\")\n",
    "    \n",
    "    # Verify index is valid\n",
    "    num_layers = len(decoder_layers)\n",
    "    if layer_idx >= num_layers:\n",
    "        raise ValueError(f\"Layer index out of range. Model has {num_layers} layers.\")\n",
    "    \n",
    "    # Get the layer\n",
    "    layer = decoder_layers[layer_idx]\n",
    "    \n",
    "    # Zero out ff projection weights\n",
    "    layer.ff_proj.weight.data.zero_()\n",
    "    \n",
    "    # Zero out ff output weights\n",
    "    layer.ff_out.weight.data.zero_()\n",
    "    \n",
    "    return {\n",
    "        'zeroed_mlp': {\n",
    "            'layer': layer_idx\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_multiple_mlps(model, layer_indices):\n",
    "    \"\"\"\n",
    "    Zero out MLPs in multiple layers of the OLMo model.\n",
    "    \n",
    "    Args:\n",
    "        model: The OLMo model\n",
    "        layer_indices: List of layer indices to zero out\n",
    "    \"\"\"\n",
    "    zeroed = []\n",
    "    for layer_idx in layer_indices:\n",
    "        zero_info = zero_mlp(model, layer_idx)\n",
    "        zeroed.append(zero_info)\n",
    "        print(f\"Zeroed MLP in layer {layer_idx}\")\n",
    "    return zeroed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_new_tokens=100):\n",
    "    \"\"\"Generate text from a prompt.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.6,\n",
    "        do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original generation:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Continue this text in a natural and coherent way:\n",
      "\n",
      "The quantum physicist stared at the readout in disbelief. The entanglement\n",
      "patterns showed something unprecedented - a stable quantum state that lasted\n",
      "for a long time.\n",
      "\n",
      "The physicist thought about the possibilities, but he was too tired to\n",
      "think about them further.\n",
      "\n",
      "He was a senior scientist at a university. He had been working on this\n",
      "project for several months. The first step was to create a quantum\n",
      "entanglement. The second step was to observe it. The third step was to\n",
      "measure the entanglement. The fourth step was to calculate the entanglement\n",
      "of the quantum state.\n",
      "\n",
      "The fifth step was\n"
     ]
    }
   ],
   "source": [
    "# Test generation before swapping\n",
    "prompt = \"\"\"\n",
    "Continue this text in a natural and coherent way:\n",
    "\n",
    "The quantum physicist stared at the readout in disbelief. The entanglement\n",
    "patterns showed something unprecedented - a stable quantum state that lasted\n",
    "\"\"\"\n",
    "\n",
    "print()\n",
    "print(\"Original generation:\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "original_output = generate_text(model, tokenizer, prompt)\n",
    "print(original_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MLP weights for layer 0:\n",
      "\n",
      "FF Projection:\n",
      "tensor([[-1.2955e-03, -2.2828e-03,  5.8700e-03,  ..., -4.1715e-03,\n",
      "         -2.9691e-03,  3.5815e-03],\n",
      "        [-7.4904e-03,  5.2179e-05, -3.0653e-03,  ..., -6.2542e-03,\n",
      "         -6.7004e-03,  3.1544e-03],\n",
      "        [-1.2344e-02, -9.9137e-03, -1.5885e-03,  ..., -5.1158e-03,\n",
      "         -5.1641e-03,  1.0260e-02],\n",
      "        ...,\n",
      "        [-1.8434e-03,  1.0538e-02,  1.5211e-02,  ..., -1.5467e-02,\n",
      "         -4.5518e-03, -1.6748e-03],\n",
      "        [ 9.3506e-04, -2.2135e-03, -8.7500e-03,  ..., -1.9706e-03,\n",
      "         -8.7817e-03, -4.2292e-03],\n",
      "        [ 1.3079e-02, -6.1314e-03, -1.2436e-02,  ...,  3.9528e-03,\n",
      "          7.4631e-03, -1.9468e-03]])\n",
      "torch.Size([16384, 2048])\n",
      "\n",
      "\n",
      "\n",
      "FF Output:\n",
      "tensor([[-5.0323e-04,  5.9398e-03, -2.1946e-03,  ...,  1.4586e-02,\n",
      "         -1.7366e-03,  4.7926e-03],\n",
      "        [-1.1657e-02, -3.1330e-03, -8.0331e-04,  ...,  7.7663e-04,\n",
      "          1.0677e-02, -4.8340e-03],\n",
      "        [ 1.5075e-03,  4.4474e-04, -1.4024e-03,  ..., -4.0307e-03,\n",
      "          7.4193e-04,  2.2806e-03],\n",
      "        ...,\n",
      "        [-1.4468e-02,  5.5693e-03, -6.3896e-03,  ..., -2.4977e-03,\n",
      "          1.1380e-03, -2.0009e-03],\n",
      "        [-1.4459e-02,  2.1440e-05, -1.7939e-03,  ..., -6.7479e-03,\n",
      "         -1.1632e-03, -7.1097e-03],\n",
      "        [ 2.1105e-03, -2.9090e-03,  5.2821e-03,  ..., -3.6961e-03,\n",
      "          2.8238e-03, -4.7801e-03]])\n",
      "torch.Size([2048, 8192])\n",
      "\n",
      "MLP weights for layer 1:\n",
      "\n",
      "FF Projection:\n",
      "tensor([[ 7.7826e-03,  5.9513e-04,  3.0032e-03,  ..., -2.7562e-05,\n",
      "         -3.3554e-04, -1.5863e-02],\n",
      "        [ 1.5511e-02,  1.6985e-02, -3.0576e-03,  ...,  9.6881e-03,\n",
      "          5.3660e-03,  9.6499e-04],\n",
      "        [-2.2951e-04,  4.6680e-03, -4.6625e-03,  ...,  2.2134e-03,\n",
      "          5.5331e-03, -8.3459e-03],\n",
      "        ...,\n",
      "        [ 7.8161e-03,  3.4275e-03, -4.8885e-03,  ...,  1.8150e-02,\n",
      "          5.7679e-03,  6.6568e-03],\n",
      "        [-7.7287e-03, -6.0238e-03, -9.1761e-03,  ...,  9.3783e-03,\n",
      "         -6.1900e-03,  2.4203e-03],\n",
      "        [-3.1727e-03, -1.1215e-02, -6.4773e-03,  ...,  1.8631e-03,\n",
      "          2.2056e-03,  9.3109e-04]])\n",
      "torch.Size([16384, 2048])\n",
      "\n",
      "\n",
      "\n",
      "FF Output:\n",
      "tensor([[ 0.0161,  0.0172,  0.0043,  ..., -0.0073,  0.0018,  0.0127],\n",
      "        [ 0.0082,  0.0121,  0.0087,  ..., -0.0068,  0.0024,  0.0079],\n",
      "        [ 0.0077,  0.0018, -0.0028,  ..., -0.0026,  0.0042,  0.0010],\n",
      "        ...,\n",
      "        [ 0.0015,  0.0053,  0.0025,  ...,  0.0018, -0.0112, -0.0067],\n",
      "        [ 0.0019, -0.0010,  0.0022,  ...,  0.0042, -0.0005, -0.0067],\n",
      "        [-0.0017, -0.0047, -0.0091,  ..., -0.0025,  0.0001,  0.0001]])\n",
      "torch.Size([2048, 8192])\n"
     ]
    }
   ],
   "source": [
    "# Print some weights before swapping (optional)\n",
    "print_mlp_weights(model, 0)\n",
    "print_mlp_weights(model, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeroing out MLP in layer 0\n",
      "Zeroed MLP in layer 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'zeroed_mlp': {'layer': 0}}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_indices = [0] \n",
    "zero_multiple_mlps(model, layer_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MLP weights for layer 0:\n",
      "\n",
      "FF Projection:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([16384, 2048])\n",
      "\n",
      "\n",
      "\n",
      "FF Output:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([2048, 8192])\n",
      "\n",
      "MLP weights for layer 1:\n",
      "\n",
      "FF Projection:\n",
      "tensor([[ 7.7826e-03,  5.9513e-04,  3.0032e-03,  ..., -2.7562e-05,\n",
      "         -3.3554e-04, -1.5863e-02],\n",
      "        [ 1.5511e-02,  1.6985e-02, -3.0576e-03,  ...,  9.6881e-03,\n",
      "          5.3660e-03,  9.6499e-04],\n",
      "        [-2.2951e-04,  4.6680e-03, -4.6625e-03,  ...,  2.2134e-03,\n",
      "          5.5331e-03, -8.3459e-03],\n",
      "        ...,\n",
      "        [ 7.8161e-03,  3.4275e-03, -4.8885e-03,  ...,  1.8150e-02,\n",
      "          5.7679e-03,  6.6568e-03],\n",
      "        [-7.7287e-03, -6.0238e-03, -9.1761e-03,  ...,  9.3783e-03,\n",
      "         -6.1900e-03,  2.4203e-03],\n",
      "        [-3.1727e-03, -1.1215e-02, -6.4773e-03,  ...,  1.8631e-03,\n",
      "          2.2056e-03,  9.3109e-04]])\n",
      "torch.Size([16384, 2048])\n",
      "\n",
      "\n",
      "\n",
      "FF Output:\n",
      "tensor([[ 0.0161,  0.0172,  0.0043,  ..., -0.0073,  0.0018,  0.0127],\n",
      "        [ 0.0082,  0.0121,  0.0087,  ..., -0.0068,  0.0024,  0.0079],\n",
      "        [ 0.0077,  0.0018, -0.0028,  ..., -0.0026,  0.0042,  0.0010],\n",
      "        ...,\n",
      "        [ 0.0015,  0.0053,  0.0025,  ...,  0.0018, -0.0112, -0.0067],\n",
      "        [ 0.0019, -0.0010,  0.0022,  ...,  0.0042, -0.0005, -0.0067],\n",
      "        [-0.0017, -0.0047, -0.0091,  ..., -0.0025,  0.0001,  0.0001]])\n",
      "torch.Size([2048, 8192])\n"
     ]
    }
   ],
   "source": [
    "# Print some weights after swapping (optional)\n",
    "print_mlp_weights(model, 0)\n",
    "print_mlp_weights(model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation after zeroing out MLPs in layer [0]:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Continue this text in a natural and coherent way:\n",
      "\n",
      "The quantum physicist stared at the readout in disbelief. The entanglement\n",
      "patterns showed something unprecedented - a stable quantum state that lasted\n",
      "weeks, and was not be be used with the same same results of the\n",
      "Quantum state that lasted for so many years.\n",
      "The entanglement was so stable and so that this was not.\n",
      "The entanglement was so stable and so that this was not.\n",
      "The entanglement was so stable.\n",
      "The entanglement was so stable that it did not be used with the\n",
      "The entanglement was so stable.\n",
      "The entanglement was so stable.\n",
      "The entanglement was so that it was so stable.\n",
      "The entanglement\n"
     ]
    }
   ],
   "source": [
    "# Test generation after swapping\n",
    "print(f\"\\nGeneration after zeroing out MLPs in layer {layer_indices}:\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "modified_output = generate_text(model, tokenizer, prompt)\n",
    "print(modified_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
