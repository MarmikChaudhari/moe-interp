{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OLMoForCausalLM(\n",
       "  (model): OLMo(\n",
       "    (transformer): ModuleDict(\n",
       "      (wte): Embedding(50304, 2048)\n",
       "      (emb_drop): Dropout(p=0.0, inplace=False)\n",
       "      (ln_f): LayerNorm()\n",
       "      (blocks): ModuleList(\n",
       "        (0-15): 16 x OLMoSequentialBlock(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (act): SwiGLU()\n",
       "          (attn_out): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (ff_out): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (att_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (ff_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (attn_norm): LayerNorm()\n",
       "          (ff_norm): LayerNorm()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"allenai/OLMo-1B\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B\", trust_remote_code=True)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mlp_weights(model, layer_idx):\n",
    "    \"\"\"\n",
    "    Print the weights of an MLP at a given layer.\n",
    "    \n",
    "    Args:\n",
    "        model: The OLMo model\n",
    "        layer_idx: Index of the layer containing the MLP\n",
    "    \"\"\"\n",
    "    ff_proj = f'model.transformer.blocks.{layer_idx}.ff_proj.weight'\n",
    "    ff_out = f'model.transformer.blocks.{layer_idx}.ff_out.weight'\n",
    "    \n",
    "    state_dict = model.state_dict()\n",
    "    \n",
    "    print(f\"\\nMLP weights for layer {layer_idx}:\")\n",
    "    print(\"\\nFF Projection:\")\n",
    "    print(state_dict[ff_proj])\n",
    "    print(state_dict[ff_proj].shape)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"\\nFF Output:\") \n",
    "    print(state_dict[ff_out])\n",
    "    print(state_dict[ff_out].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def swap_mlps(model, layer_idx_1, layer_idx_2):\n",
    "    \"\"\"\n",
    "    Swap MLPs between two layers in the OLMo model.\n",
    "    \n",
    "    Args:\n",
    "        model: The OLMo model\n",
    "        layer_idx_1: Index of the first layer\n",
    "        layer_idx_2: Index of the second layer\n",
    "    \"\"\"\n",
    "    # Access the transformer blocks\n",
    "    decoder_layers = model.model.transformer.blocks\n",
    "    print(f\"Swapping MLPs between layers {layer_idx_1} and {layer_idx_2}\")\n",
    "    \n",
    "    # Verify indices are valid\n",
    "    num_layers = len(decoder_layers)\n",
    "    if layer_idx_1 >= num_layers or layer_idx_2 >= num_layers:\n",
    "        raise ValueError(f\"Layer index out of range. Model has {num_layers} layers.\")\n",
    "    \n",
    "    # Get the MLP blocks from both layers\n",
    "    layer_1 = decoder_layers[layer_idx_1]\n",
    "    layer_2 = decoder_layers[layer_idx_2]\n",
    "    \n",
    "    # Swap ff projection weights\n",
    "    layer_1.ff_proj.weight.data, layer_2.ff_proj.weight.data = \\\n",
    "        layer_2.ff_proj.weight.data.clone(), layer_1.ff_proj.weight.data.clone()\n",
    "        \n",
    "    # Swap ff output weights\n",
    "    layer_1.ff_out.weight.data, layer_2.ff_out.weight.data = \\\n",
    "        layer_2.ff_out.weight.data.clone(), layer_1.ff_out.weight.data.clone()\n",
    "    \n",
    "    return {\n",
    "        'swapped_mlps': {\n",
    "            'layer_1': layer_idx_1,\n",
    "            'layer_2': layer_idx_2\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def swap_multiple_mlps(model, layer_pairs):\n",
    "    \"\"\"\n",
    "    Swap multiple pairs of MLPs in the OLMo model.\n",
    "    \n",
    "    Args:\n",
    "        model: The OLMo model\n",
    "        layer_pairs: List of tuples, each containing two layer indices to swap\n",
    "    \"\"\"\n",
    "    swaps = []\n",
    "    for layer_1, layer_2 in layer_pairs:\n",
    "        swap_info = swap_mlps(model, layer_1, layer_2)\n",
    "        swaps.append(swap_info)\n",
    "        print(f\"Swapped MLPs between layers {layer_1} and {layer_2}\")\n",
    "    return swaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_new_tokens=100):\n",
    "    \"\"\"Generate text from a prompt.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.6,\n",
    "        do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original generation:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Continue this text in a natural and coherent way:\n",
      "\n",
      "The quantum physicist stared at the readout in disbelief. The entanglement\n",
      "patterns showed something unprecedented - a stable quantum state that lasted\n",
      "for millions of years.\n",
      "\n",
      "(Source: https://www.wired.com/2016/11/quantum-entangled-molecules-time-travel/)\n",
      "\n",
      "I think that this is a good example of how a narrative or story can be\n",
      "used to expand the mind. I think that this is a good example of a way of\n",
      "explaining quantum mechanics in a way that is easily understood.\n",
      "\n",
      "I think that this is a good example of how a narrative or story\n"
     ]
    }
   ],
   "source": [
    "# Test generation before swapping\n",
    "prompt = \"\"\"\n",
    "Continue this text in a natural and coherent way:\n",
    "\n",
    "The quantum physicist stared at the readout in disbelief. The entanglement\n",
    "patterns showed something unprecedented - a stable quantum state that lasted\n",
    "\"\"\"\n",
    "\n",
    "print()\n",
    "print(\"Original generation:\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "original_output = generate_text(model, tokenizer, prompt)\n",
    "print(original_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MLP weights for layer 0:\n",
      "\n",
      "FF Projection:\n",
      "tensor([[-1.2955e-03, -2.2828e-03,  5.8700e-03,  ..., -4.1715e-03,\n",
      "         -2.9691e-03,  3.5815e-03],\n",
      "        [-7.4904e-03,  5.2179e-05, -3.0653e-03,  ..., -6.2542e-03,\n",
      "         -6.7004e-03,  3.1544e-03],\n",
      "        [-1.2344e-02, -9.9137e-03, -1.5885e-03,  ..., -5.1158e-03,\n",
      "         -5.1641e-03,  1.0260e-02],\n",
      "        ...,\n",
      "        [-1.8434e-03,  1.0538e-02,  1.5211e-02,  ..., -1.5467e-02,\n",
      "         -4.5518e-03, -1.6748e-03],\n",
      "        [ 9.3506e-04, -2.2135e-03, -8.7500e-03,  ..., -1.9706e-03,\n",
      "         -8.7817e-03, -4.2292e-03],\n",
      "        [ 1.3079e-02, -6.1314e-03, -1.2436e-02,  ...,  3.9528e-03,\n",
      "          7.4631e-03, -1.9468e-03]])\n",
      "torch.Size([16384, 2048])\n",
      "\n",
      "\n",
      "\n",
      "FF Output:\n",
      "tensor([[-5.0323e-04,  5.9398e-03, -2.1946e-03,  ...,  1.4586e-02,\n",
      "         -1.7366e-03,  4.7926e-03],\n",
      "        [-1.1657e-02, -3.1330e-03, -8.0331e-04,  ...,  7.7663e-04,\n",
      "          1.0677e-02, -4.8340e-03],\n",
      "        [ 1.5075e-03,  4.4474e-04, -1.4024e-03,  ..., -4.0307e-03,\n",
      "          7.4193e-04,  2.2806e-03],\n",
      "        ...,\n",
      "        [-1.4468e-02,  5.5693e-03, -6.3896e-03,  ..., -2.4977e-03,\n",
      "          1.1380e-03, -2.0009e-03],\n",
      "        [-1.4459e-02,  2.1440e-05, -1.7939e-03,  ..., -6.7479e-03,\n",
      "         -1.1632e-03, -7.1097e-03],\n",
      "        [ 2.1105e-03, -2.9090e-03,  5.2821e-03,  ..., -3.6961e-03,\n",
      "          2.8238e-03, -4.7801e-03]])\n",
      "torch.Size([2048, 8192])\n",
      "\n",
      "MLP weights for layer 1:\n",
      "\n",
      "FF Projection:\n",
      "tensor([[ 7.7826e-03,  5.9513e-04,  3.0032e-03,  ..., -2.7562e-05,\n",
      "         -3.3554e-04, -1.5863e-02],\n",
      "        [ 1.5511e-02,  1.6985e-02, -3.0576e-03,  ...,  9.6881e-03,\n",
      "          5.3660e-03,  9.6499e-04],\n",
      "        [-2.2951e-04,  4.6680e-03, -4.6625e-03,  ...,  2.2134e-03,\n",
      "          5.5331e-03, -8.3459e-03],\n",
      "        ...,\n",
      "        [ 7.8161e-03,  3.4275e-03, -4.8885e-03,  ...,  1.8150e-02,\n",
      "          5.7679e-03,  6.6568e-03],\n",
      "        [-7.7287e-03, -6.0238e-03, -9.1761e-03,  ...,  9.3783e-03,\n",
      "         -6.1900e-03,  2.4203e-03],\n",
      "        [-3.1727e-03, -1.1215e-02, -6.4773e-03,  ...,  1.8631e-03,\n",
      "          2.2056e-03,  9.3109e-04]])\n",
      "torch.Size([16384, 2048])\n",
      "\n",
      "\n",
      "\n",
      "FF Output:\n",
      "tensor([[ 0.0161,  0.0172,  0.0043,  ..., -0.0073,  0.0018,  0.0127],\n",
      "        [ 0.0082,  0.0121,  0.0087,  ..., -0.0068,  0.0024,  0.0079],\n",
      "        [ 0.0077,  0.0018, -0.0028,  ..., -0.0026,  0.0042,  0.0010],\n",
      "        ...,\n",
      "        [ 0.0015,  0.0053,  0.0025,  ...,  0.0018, -0.0112, -0.0067],\n",
      "        [ 0.0019, -0.0010,  0.0022,  ...,  0.0042, -0.0005, -0.0067],\n",
      "        [-0.0017, -0.0047, -0.0091,  ..., -0.0025,  0.0001,  0.0001]])\n",
      "torch.Size([2048, 8192])\n"
     ]
    }
   ],
   "source": [
    "# Print some weights before swapping (optional)\n",
    "print_mlp_weights(model, 0)\n",
    "print_mlp_weights(model, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swapping MLPs between layers 0 and 1\n",
      "Swapped MLPs between layers 0 and 1\n",
      "Swapping MLPs between layers 2 and 3\n",
      "Swapped MLPs between layers 2 and 3\n",
      "Swapping MLPs between layers 4 and 5\n",
      "Swapped MLPs between layers 4 and 5\n",
      "Swapping MLPs between layers 6 and 7\n",
      "Swapped MLPs between layers 6 and 7\n",
      "Swapping MLPs between layers 8 and 9\n",
      "Swapped MLPs between layers 8 and 9\n"
     ]
    }
   ],
   "source": [
    "# Perform MLP swaps\n",
    "layer_pairs = [(0, 1), (2, 3), (4, 5), (6, 7), (8, 9)]  # Example layer pairs\n",
    "swap_results = swap_multiple_mlps(model, layer_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MLP weights for layer 0:\n",
      "\n",
      "FF Projection:\n",
      "tensor([[-1.2955e-03, -2.2828e-03,  5.8700e-03,  ..., -4.1715e-03,\n",
      "         -2.9691e-03,  3.5815e-03],\n",
      "        [-7.4904e-03,  5.2179e-05, -3.0653e-03,  ..., -6.2542e-03,\n",
      "         -6.7004e-03,  3.1544e-03],\n",
      "        [-1.2344e-02, -9.9137e-03, -1.5885e-03,  ..., -5.1158e-03,\n",
      "         -5.1641e-03,  1.0260e-02],\n",
      "        ...,\n",
      "        [-1.8434e-03,  1.0538e-02,  1.5211e-02,  ..., -1.5467e-02,\n",
      "         -4.5518e-03, -1.6748e-03],\n",
      "        [ 9.3506e-04, -2.2135e-03, -8.7500e-03,  ..., -1.9706e-03,\n",
      "         -8.7817e-03, -4.2292e-03],\n",
      "        [ 1.3079e-02, -6.1314e-03, -1.2436e-02,  ...,  3.9528e-03,\n",
      "          7.4631e-03, -1.9468e-03]])\n",
      "torch.Size([16384, 2048])\n",
      "\n",
      "\n",
      "\n",
      "FF Output:\n",
      "tensor([[-5.0323e-04,  5.9398e-03, -2.1946e-03,  ...,  1.4586e-02,\n",
      "         -1.7366e-03,  4.7926e-03],\n",
      "        [-1.1657e-02, -3.1330e-03, -8.0331e-04,  ...,  7.7663e-04,\n",
      "          1.0677e-02, -4.8340e-03],\n",
      "        [ 1.5075e-03,  4.4474e-04, -1.4024e-03,  ..., -4.0307e-03,\n",
      "          7.4193e-04,  2.2806e-03],\n",
      "        ...,\n",
      "        [-1.4468e-02,  5.5693e-03, -6.3896e-03,  ..., -2.4977e-03,\n",
      "          1.1380e-03, -2.0009e-03],\n",
      "        [-1.4459e-02,  2.1440e-05, -1.7939e-03,  ..., -6.7479e-03,\n",
      "         -1.1632e-03, -7.1097e-03],\n",
      "        [ 2.1105e-03, -2.9090e-03,  5.2821e-03,  ..., -3.6961e-03,\n",
      "          2.8238e-03, -4.7801e-03]])\n",
      "torch.Size([2048, 8192])\n",
      "\n",
      "MLP weights for layer 1:\n",
      "\n",
      "FF Projection:\n",
      "tensor([[ 7.7826e-03,  5.9513e-04,  3.0032e-03,  ..., -2.7562e-05,\n",
      "         -3.3554e-04, -1.5863e-02],\n",
      "        [ 1.5511e-02,  1.6985e-02, -3.0576e-03,  ...,  9.6881e-03,\n",
      "          5.3660e-03,  9.6499e-04],\n",
      "        [-2.2951e-04,  4.6680e-03, -4.6625e-03,  ...,  2.2134e-03,\n",
      "          5.5331e-03, -8.3459e-03],\n",
      "        ...,\n",
      "        [ 7.8161e-03,  3.4275e-03, -4.8885e-03,  ...,  1.8150e-02,\n",
      "          5.7679e-03,  6.6568e-03],\n",
      "        [-7.7287e-03, -6.0238e-03, -9.1761e-03,  ...,  9.3783e-03,\n",
      "         -6.1900e-03,  2.4203e-03],\n",
      "        [-3.1727e-03, -1.1215e-02, -6.4773e-03,  ...,  1.8631e-03,\n",
      "          2.2056e-03,  9.3109e-04]])\n",
      "torch.Size([16384, 2048])\n",
      "\n",
      "\n",
      "\n",
      "FF Output:\n",
      "tensor([[ 0.0161,  0.0172,  0.0043,  ..., -0.0073,  0.0018,  0.0127],\n",
      "        [ 0.0082,  0.0121,  0.0087,  ..., -0.0068,  0.0024,  0.0079],\n",
      "        [ 0.0077,  0.0018, -0.0028,  ..., -0.0026,  0.0042,  0.0010],\n",
      "        ...,\n",
      "        [ 0.0015,  0.0053,  0.0025,  ...,  0.0018, -0.0112, -0.0067],\n",
      "        [ 0.0019, -0.0010,  0.0022,  ...,  0.0042, -0.0005, -0.0067],\n",
      "        [-0.0017, -0.0047, -0.0091,  ..., -0.0025,  0.0001,  0.0001]])\n",
      "torch.Size([2048, 8192])\n"
     ]
    }
   ],
   "source": [
    "# Print some weights after swapping (optional)\n",
    "print_mlp_weights(model, 0)\n",
    "print_mlp_weights(model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation after swapping MLPs:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Continue this text in a natural and coherent way:\n",
      "\n",
      "The quantum physicist stared at the readout in disbelief. The entanglement\n",
      "patterns showed something unprecedented - a stable quantum state that lasted\n",
      "ended).\n",
      "and continued.\n",
      "alalp).3 hours.8 hours.8.1.1.5.8).3 hours.8.5.8.8.4.8.8.8.8.8.8dand.T.im.T.T.T.T.T.T.T.T.T.T.T.T.T.T.T.T.T.T.m.T.T.\n"
     ]
    }
   ],
   "source": [
    "# Test generation after swapping\n",
    "print(\"\\nGeneration after swapping MLPs:\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "modified_output = generate_text(model, tokenizer, prompt)\n",
    "print(modified_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zero out weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_mlp(model, layer_idx):\n",
    "    \"\"\"\n",
    "    Zero out MLP weights in a specific layer of the OLMo model.\n",
    "    \n",
    "    Args:\n",
    "        model: The OLMo model\n",
    "        layer_idx: Index of the layer to zero out\n",
    "    \"\"\"\n",
    "    # Access the transformer blocks\n",
    "    decoder_layers = model.model.transformer.blocks\n",
    "    print(f\"Zeroing out MLP in layer {layer_idx}\")\n",
    "    \n",
    "    # Verify index is valid\n",
    "    num_layers = len(decoder_layers)\n",
    "    if layer_idx >= num_layers:\n",
    "        raise ValueError(f\"Layer index out of range. Model has {num_layers} layers.\")\n",
    "    \n",
    "    # Get the layer\n",
    "    layer = decoder_layers[layer_idx]\n",
    "    \n",
    "    # Zero out ff projection weights\n",
    "    layer.ff_proj.weight.data.zero_()\n",
    "    \n",
    "    # Zero out ff output weights\n",
    "    layer.ff_out.weight.data.zero_()\n",
    "    \n",
    "    return {\n",
    "        'zeroed_mlp': {\n",
    "            'layer': layer_idx\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_multiple_mlps(model, layer_indices):\n",
    "    \"\"\"\n",
    "    Zero out MLPs in multiple layers of the OLMo model.\n",
    "    \n",
    "    Args:\n",
    "        model: The OLMo model\n",
    "        layer_indices: List of layer indices to zero out\n",
    "    \"\"\"\n",
    "    zeroed = []\n",
    "    for layer_idx in layer_indices:\n",
    "        zero_info = zero_mlp(model, layer_idx)\n",
    "        zeroed.append(zero_info)\n",
    "        print(f\"Zeroed MLP in layer {layer_idx}\")\n",
    "    return zeroed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
