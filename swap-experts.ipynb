{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ[\"PYTORCH_TRANSFORMERS_SDP_BACKEND\"] = \"flash\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314102585b934d8991f7717e32f195b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_model(model_name=\"allenai/OLMoE-1B-7B-0924\"):\n",
    "    # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_expert_weights(model, layer_idx, expert_idx):\n",
    "    \"\"\"\n",
    "    Print the weights of a specific expert MLP at a given layer.\n",
    "    \n",
    "    Args:\n",
    "        model: The OLMoE model\n",
    "        layer_idx: Index of the layer containing the expert\n",
    "        expert_idx: Index of the expert within the layer\n",
    "    \"\"\"\n",
    "    gate_proj = f'model.layers.{layer_idx}.mlp.experts.{expert_idx}.gate_proj.weight'\n",
    "    up_proj = f'model.layers.{layer_idx}.mlp.experts.{expert_idx}.up_proj.weight'\n",
    "    down_proj = f'model.layers.{layer_idx}.mlp.experts.{expert_idx}.down_proj.weight'\n",
    "    \n",
    "    print(\"\\nGate Projection:\")\n",
    "    print(model.state_dict()[gate_proj])\n",
    "    print(\"\\nUp Projection:\") \n",
    "    print(model.state_dict()[up_proj])\n",
    "    print(\"\\nDown Projection:\")\n",
    "    print(model.state_dict()[down_proj])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gate Projection:\n",
      "tensor([[-0.0118,  0.0150,  0.0190,  ..., -0.0022, -0.0043,  0.0098],\n",
      "        [-0.0156,  0.0110,  0.0079,  ..., -0.0136, -0.0059,  0.0064],\n",
      "        [-0.0004, -0.0051, -0.0077,  ...,  0.0092,  0.0153, -0.0466],\n",
      "        ...,\n",
      "        [-0.0208, -0.0040,  0.0033,  ...,  0.0048, -0.0037,  0.0115],\n",
      "        [-0.0066,  0.0152,  0.0057,  ..., -0.0050, -0.0275, -0.0078],\n",
      "        [ 0.0004, -0.0062,  0.0060,  ...,  0.0011, -0.0302,  0.0057]])\n",
      "\n",
      "Up Projection:\n",
      "tensor([[ 0.0038, -0.0003,  0.0136,  ..., -0.0016,  0.0109, -0.0172],\n",
      "        [ 0.0078, -0.0044,  0.0027,  ...,  0.0194,  0.0127,  0.0271],\n",
      "        [ 0.0020,  0.0182,  0.0090,  ...,  0.0281, -0.0231, -0.0129],\n",
      "        ...,\n",
      "        [ 0.0015,  0.0172,  0.0036,  ...,  0.0123, -0.0040, -0.0101],\n",
      "        [-0.0208,  0.0119,  0.0157,  ..., -0.0388,  0.0206, -0.0027],\n",
      "        [-0.0113,  0.0016, -0.0238,  ..., -0.0243, -0.0132,  0.0069]])\n",
      "\n",
      "Down Projection:\n",
      "tensor([[-0.0053,  0.0125, -0.0048,  ...,  0.0077, -0.0144, -0.0030],\n",
      "        [-0.0013, -0.0056, -0.0166,  ...,  0.0171,  0.0208,  0.0033],\n",
      "        [-0.0078,  0.0154,  0.0001,  ...,  0.0117,  0.0144, -0.0110],\n",
      "        ...,\n",
      "        [ 0.0017,  0.0017,  0.0095,  ...,  0.0122, -0.0188, -0.0248],\n",
      "        [ 0.0071, -0.0049,  0.0067,  ...,  0.0068,  0.0078, -0.0057],\n",
      "        [-0.0159,  0.0187, -0.0058,  ..., -0.0193,  0.0003,  0.0077]])\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Gate Projection:\n",
      "tensor([[ 0.0047,  0.0144,  0.0098,  ..., -0.0136,  0.0004, -0.0096],\n",
      "        [ 0.0166,  0.0229, -0.0249,  ...,  0.0093,  0.0170, -0.0167],\n",
      "        [-0.0054, -0.0134,  0.0197,  ..., -0.0031,  0.0035,  0.0121],\n",
      "        ...,\n",
      "        [ 0.0183, -0.0057,  0.0187,  ..., -0.0131,  0.0139, -0.0266],\n",
      "        [-0.0187,  0.0104,  0.0003,  ...,  0.0239, -0.0026, -0.0034],\n",
      "        [ 0.0164,  0.0256, -0.0092,  ...,  0.0055, -0.0097,  0.0016]])\n",
      "\n",
      "Up Projection:\n",
      "tensor([[-0.0283, -0.0043,  0.0032,  ..., -0.0098,  0.0206,  0.0082],\n",
      "        [ 0.0025, -0.0062, -0.0189,  ...,  0.0159, -0.0078,  0.0192],\n",
      "        [ 0.0066, -0.0167, -0.0043,  ..., -0.0079, -0.0193, -0.0078],\n",
      "        ...,\n",
      "        [ 0.0074,  0.0029,  0.0161,  ..., -0.0123,  0.0339, -0.0276],\n",
      "        [-0.0248,  0.0226,  0.0152,  ...,  0.0004,  0.0189,  0.0175],\n",
      "        [ 0.0018,  0.0214, -0.0118,  ..., -0.0059, -0.0383,  0.0023]])\n",
      "\n",
      "Down Projection:\n",
      "tensor([[-0.0134,  0.0007,  0.0071,  ...,  0.0142, -0.0272, -0.0029],\n",
      "        [-0.0099, -0.0106, -0.0054,  ...,  0.0137,  0.0457, -0.0067],\n",
      "        [-0.0055, -0.0084, -0.0052,  ..., -0.0210,  0.0046, -0.0143],\n",
      "        ...,\n",
      "        [-0.0017,  0.0106,  0.0021,  ...,  0.0022,  0.0171,  0.0120],\n",
      "        [ 0.0118,  0.0115, -0.0010,  ...,  0.0076, -0.0024, -0.0280],\n",
      "        [ 0.0178,  0.0094, -0.0085,  ..., -0.0337, -0.0161,  0.0038]])\n"
     ]
    }
   ],
   "source": [
    "# Print weights for layer 0 expert 0\n",
    "print_expert_weights(model, layer_idx=0, expert_idx=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")  # Separator for readability\n",
    "\n",
    "# Print weights for layer 0 expert 1\n",
    "print_expert_weights(model, layer_idx=0, expert_idx=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the model with a prompt\n",
    "# prompt = (\"\"\" \n",
    "# Continue the poem naturally and coherently, maintaining consistency with the rhyme scheme, diction and imagery. Match the poem's tone and style precisely.\n",
    "\n",
    "# we measure rainfall in memories now\n",
    "# count droplets like endangered species\n",
    "# my grandmother's garden is underwater\n",
    "# but the roses still bloom, phosphorescent\n",
    "# in depths where submarines chart\n",
    "# the coordinates of lost cities, while above                  \n",
    "# \"\"\")\n",
    "\n",
    "# # Convert the prompt to inputs and run a forward pass\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "# # Generate output (since it's a causal LM, we need to generate text from input)\n",
    "# outputs = model.generate(\n",
    "#     inputs['input_ids'],  # Only provide input_ids to generate\n",
    "#     attention_mask=inputs['attention_mask'],  # Add attention mask to not attend to padding tokens\n",
    "#     max_new_tokens=156,    # Generate 1024 new tokens\n",
    "#     temperature=0.6,       # Control randomness\n",
    "#     # top_k=100,  # Use top-k sampling\n",
    "#     do_sample=True,        # Use sampling instead of greedy decoding\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "#     pad_token_id=tokenizer.eos_token_id  # Set padding token\n",
    "# )\n",
    "\n",
    "# # Decode the generated output\n",
    "# generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# # Print the original prompt and generated response\n",
    "# print(\"Prompt:\", prompt)\n",
    "# print(\"\\nGenerated response :\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_experts(model, expert_idx, target_layer_idx, source_layer_idx=0, source_expert_idx=0):\n",
    "    \"\"\"\n",
    "    Swap experts between two layers in the OLMoE model.\n",
    "    \n",
    "    Args:\n",
    "        model: The OLMoE model\n",
    "        expert_idx: Index of the expert in target layer to swap with\n",
    "        target_layer_idx: Index of the layer containing the expert to swap with\n",
    "        source_layer_idx: Index of the source layer (default 0)\n",
    "        source_expert_idx: Index of the source expert (default 0)\n",
    "\n",
    "    \"\"\"\n",
    "    # Access the decoder layers\n",
    "    decoder_layers = model.model.layers\n",
    "    print(decoder_layers[0].mlp.experts[0].gate_proj.weight.shape)\n",
    "    \n",
    "    # Verify indices are valid\n",
    "    num_layers = len(decoder_layers)\n",
    "    if target_layer_idx >= num_layers or source_layer_idx >= num_layers:\n",
    "        raise ValueError(f\"Layer index out of range. Model has {num_layers} layers.\")\n",
    "    \n",
    "    # Get the MoE blocks from both layers\n",
    "    source_moe = decoder_layers[source_layer_idx].mlp\n",
    "    target_moe = decoder_layers[target_layer_idx].mlp\n",
    "    \n",
    "    # Verify expert indices are valid\n",
    "    num_experts = len(source_moe.experts)\n",
    "    if expert_idx >= num_experts or source_expert_idx >= num_experts:\n",
    "        raise ValueError(f\"Expert index out of range. Each layer has {num_experts} experts.\")\n",
    "        \n",
    "    # Swap the expert weights\n",
    "    source_expert = source_moe.experts[source_expert_idx]\n",
    "    target_expert = target_moe.experts[expert_idx]\n",
    "    \n",
    "    # Swap gate projection weights\n",
    "    source_expert.gate_proj.weight, target_expert.gate_proj.weight = \\\n",
    "        target_expert.gate_proj.weight, source_expert.gate_proj.weight\n",
    "        \n",
    "    # Swap up projection weights\n",
    "    source_expert.up_proj.weight, target_expert.up_proj.weight = \\\n",
    "        target_expert.up_proj.weight, source_expert.up_proj.weight\n",
    "        \n",
    "    # Swap down projection weights  \n",
    "    source_expert.down_proj.weight, target_expert.down_proj.weight = \\\n",
    "        target_expert.down_proj.weight, source_expert.down_proj.weight\n",
    "    \n",
    "    return {\n",
    "        'swapped_experts': {\n",
    "            'source': {\n",
    "                'layer': source_layer_idx,\n",
    "                'expert': source_expert_idx\n",
    "            },\n",
    "            'target': {\n",
    "                'layer': target_layer_idx,\n",
    "                'expert': expert_idx\n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gemini logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def swap_experts(model, expert_idx, target_layer_idx, source_layer_idx=0, source_expert_idx=0):\n",
    "#     \"\"\"\n",
    "#     Swap experts between two layers in the OLMoE model.\n",
    "\n",
    "#     Args:\n",
    "#         model: The OLMoE model\n",
    "#         expert_idx: Index of the expert in target layer to swap with\n",
    "#         target_layer_idx: Index of the layer containing the expert to swap with\n",
    "#         source_layer_idx: Index of the source layer (default 0)\n",
    "#         source_expert_idx: Index of the source expert (default 0)\n",
    "#     \"\"\"\n",
    "#     # Access the decoder layers\n",
    "#     decoder_layers = model.model.layers\n",
    "#     print(decoder_layers[0].mlp.experts[0].gate_proj.weight.shape)\n",
    "\n",
    "#     # Verify indices are valid\n",
    "#     num_layers = len(decoder_layers)\n",
    "#     if target_layer_idx >= num_layers or source_layer_idx >= num_layers:\n",
    "#         raise ValueError(f\"Layer index out of range. Model has {num_layers} layers.\")\n",
    "\n",
    "#     # Get the MoE blocks from both layers\n",
    "#     source_moe = decoder_layers[source_layer_idx].mlp\n",
    "#     target_moe = decoder_layers[target_layer_idx].mlp\n",
    "\n",
    "#     # Verify expert indices are valid\n",
    "#     num_experts = len(source_moe.experts)\n",
    "#     if expert_idx >= num_experts or source_expert_idx >= num_experts:\n",
    "#         raise ValueError(f\"Expert index out of range. Each layer has {num_experts} experts.\")\n",
    "\n",
    "#     # Swap the expert weights\n",
    "#     source_expert = source_moe.experts[source_expert_idx]\n",
    "#     target_expert = target_moe.experts[expert_idx]\n",
    "\n",
    "#     # Swap gate projection weights\n",
    "#     temp_gate_proj_weight = source_expert.gate_proj.weight.data.clone()\n",
    "#     source_expert.gate_proj.weight.data = target_expert.gate_proj.weight.data.clone()\n",
    "#     target_expert.gate_proj.weight.data = temp_gate_proj_weight\n",
    "\n",
    "#     # Swap up projection weights\n",
    "#     temp_up_proj_weight = source_expert.up_proj.weight.data.clone()\n",
    "#     source_expert.up_proj.weight.data = target_expert.up_proj.weight.data.clone()\n",
    "#     target_expert.up_proj.weight.data = temp_up_proj_weight\n",
    "\n",
    "#     # Swap down projection weights\n",
    "#     temp_down_proj_weight = source_expert.down_proj.weight.data.clone()\n",
    "#     source_expert.down_proj.weight.data = target_expert.down_proj.weight.data.clone()\n",
    "#     target_expert.down_proj.weight.data = temp_down_proj_weight\n",
    "    \n",
    "#     return {\n",
    "#         'swapped_experts': {\n",
    "#             'source': {\n",
    "#                 'layer': source_layer_idx,\n",
    "#                 'expert': source_expert_idx\n",
    "#             },\n",
    "#             'target': {\n",
    "#                 'layer': target_layer_idx,\n",
    "#                 'expert': expert_idx\n",
    "#             }\n",
    "#         }\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 0, top expert 0 with bottom expert 1\n"
     ]
    }
   ],
   "source": [
    "# Create lists of experts to swap\n",
    "top_experts_list = [0] # layer 15\n",
    "bottom_experts_list = [1] # layer 15\n",
    "\n",
    "layer_idx = 0\n",
    "# Swap experts at each index\n",
    "for i in range(len(top_experts_list)):\n",
    "    swap_experts(model, expert_idx=top_experts_list[i], target_layer_idx=layer_idx, source_layer_idx=layer_idx, source_expert_idx=bottom_experts_list[i])\n",
    "    print(f\"Swapped experts at layer {layer_idx}, top expert {top_experts_list[i]} with bottom expert {bottom_experts_list[i]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gate Projection:\n",
      "tensor([[ 0.0047,  0.0144,  0.0098,  ..., -0.0136,  0.0004, -0.0096],\n",
      "        [ 0.0166,  0.0229, -0.0249,  ...,  0.0093,  0.0170, -0.0167],\n",
      "        [-0.0054, -0.0134,  0.0197,  ..., -0.0031,  0.0035,  0.0121],\n",
      "        ...,\n",
      "        [ 0.0183, -0.0057,  0.0187,  ..., -0.0131,  0.0139, -0.0266],\n",
      "        [-0.0187,  0.0104,  0.0003,  ...,  0.0239, -0.0026, -0.0034],\n",
      "        [ 0.0164,  0.0256, -0.0092,  ...,  0.0055, -0.0097,  0.0016]])\n",
      "\n",
      "Up Projection:\n",
      "tensor([[-0.0283, -0.0043,  0.0032,  ..., -0.0098,  0.0206,  0.0082],\n",
      "        [ 0.0025, -0.0062, -0.0189,  ...,  0.0159, -0.0078,  0.0192],\n",
      "        [ 0.0066, -0.0167, -0.0043,  ..., -0.0079, -0.0193, -0.0078],\n",
      "        ...,\n",
      "        [ 0.0074,  0.0029,  0.0161,  ..., -0.0123,  0.0339, -0.0276],\n",
      "        [-0.0248,  0.0226,  0.0152,  ...,  0.0004,  0.0189,  0.0175],\n",
      "        [ 0.0018,  0.0214, -0.0118,  ..., -0.0059, -0.0383,  0.0023]])\n",
      "\n",
      "Down Projection:\n",
      "tensor([[-0.0134,  0.0007,  0.0071,  ...,  0.0142, -0.0272, -0.0029],\n",
      "        [-0.0099, -0.0106, -0.0054,  ...,  0.0137,  0.0457, -0.0067],\n",
      "        [-0.0055, -0.0084, -0.0052,  ..., -0.0210,  0.0046, -0.0143],\n",
      "        ...,\n",
      "        [-0.0017,  0.0106,  0.0021,  ...,  0.0022,  0.0171,  0.0120],\n",
      "        [ 0.0118,  0.0115, -0.0010,  ...,  0.0076, -0.0024, -0.0280],\n",
      "        [ 0.0178,  0.0094, -0.0085,  ..., -0.0337, -0.0161,  0.0038]])\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Gate Projection:\n",
      "tensor([[-0.0118,  0.0150,  0.0190,  ..., -0.0022, -0.0043,  0.0098],\n",
      "        [-0.0156,  0.0110,  0.0079,  ..., -0.0136, -0.0059,  0.0064],\n",
      "        [-0.0004, -0.0051, -0.0077,  ...,  0.0092,  0.0153, -0.0466],\n",
      "        ...,\n",
      "        [-0.0208, -0.0040,  0.0033,  ...,  0.0048, -0.0037,  0.0115],\n",
      "        [-0.0066,  0.0152,  0.0057,  ..., -0.0050, -0.0275, -0.0078],\n",
      "        [ 0.0004, -0.0062,  0.0060,  ...,  0.0011, -0.0302,  0.0057]])\n",
      "\n",
      "Up Projection:\n",
      "tensor([[ 0.0038, -0.0003,  0.0136,  ..., -0.0016,  0.0109, -0.0172],\n",
      "        [ 0.0078, -0.0044,  0.0027,  ...,  0.0194,  0.0127,  0.0271],\n",
      "        [ 0.0020,  0.0182,  0.0090,  ...,  0.0281, -0.0231, -0.0129],\n",
      "        ...,\n",
      "        [ 0.0015,  0.0172,  0.0036,  ...,  0.0123, -0.0040, -0.0101],\n",
      "        [-0.0208,  0.0119,  0.0157,  ..., -0.0388,  0.0206, -0.0027],\n",
      "        [-0.0113,  0.0016, -0.0238,  ..., -0.0243, -0.0132,  0.0069]])\n",
      "\n",
      "Down Projection:\n",
      "tensor([[-0.0053,  0.0125, -0.0048,  ...,  0.0077, -0.0144, -0.0030],\n",
      "        [-0.0013, -0.0056, -0.0166,  ...,  0.0171,  0.0208,  0.0033],\n",
      "        [-0.0078,  0.0154,  0.0001,  ...,  0.0117,  0.0144, -0.0110],\n",
      "        ...,\n",
      "        [ 0.0017,  0.0017,  0.0095,  ...,  0.0122, -0.0188, -0.0248],\n",
      "        [ 0.0071, -0.0049,  0.0067,  ...,  0.0068,  0.0078, -0.0057],\n",
      "        [-0.0159,  0.0187, -0.0058,  ..., -0.0193,  0.0003,  0.0077]])\n"
     ]
    }
   ],
   "source": [
    "# Print weights for layer 0 expert 0\n",
    "print_expert_weights(model, layer_idx=0, expert_idx=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")  # Separator for readability\n",
    "\n",
    "# Print weights for layer 0 expert 1\n",
    "print_expert_weights(model, layer_idx=0, expert_idx=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:     \n",
      "Continue this text in a natural and coherent way, maintaining consistency with the style, \n",
      "terminology, and logical flow of the preceding text.\n",
      "          \n",
      "\\title{Quantum Error Mitigation in NISQ Devices}\n",
      "\\begin{abstract}\n",
      "We present a novel approach to error mitigation in noisy intermediate-scale quantum (NISQ) devices. \n",
      "Our method introduces a scaling framework for quantum channels that preserves gate fidelity while reducing environmental noise.\n",
      "\\end{abstract}\n",
      "\\section{Introduction}\n",
      "Recent advances in NISQ devices have demonstrated both promise and limitations in quantum computation. \n",
      "The primary challenge remains decoherence, which introduces errors in quantum operations. We propose a channel scaling approach \n",
      "$\\mathcal{N}(\\rho) = e^{-\\lambda t}\\rho$ \n",
      "that provides a systematic way to\n",
      "\n",
      "\n",
      "Generated response:     \n",
      "Continue this text in a natural and coherent way, maintaining consistency with the style, \n",
      "terminology, and logical flow of the preceding text.\n",
      "          \n",
      "\\title{Quantum Error Mitigation in NISQ Devices}\n",
      "\\begin{abstract}\n",
      "We present a novel approach to error mitigation in noisy intermediate-scale quantum (NISQ) devices. \n",
      "Our method introduces a scaling framework for quantum channels that preserves gate fidelity while reducing environmental noise.\n",
      "\\end{abstract}\n",
      "\\section{Introduction}\n",
      "Recent advances in NISQ devices have demonstrated both promise and limitations in quantum computation. \n",
      "The primary challenge remains decoherence, which introduces errors in quantum operations. We propose a channel scaling approach \n",
      "$\\mathcal{N}(\\rho) = e^{-\\lambda t}\\rho$ \n",
      "that provides a systematic way to\n",
      "mitigate noise and maintain fidelity.\n",
      "\n",
      "We first present a general framework for qubit error mitigation, followed by a discussion of the proposed scaling approach. \n",
      "We then describe a general quantum channel scaling method and its application to quantum state tomography.\n",
      "Finally, we apply the scaling method to the implementation of quantum error mitigation in a quantum device. \n",
      "The scaling method is presented in the Supplementary Material.\n",
      "\n",
      "\\section{Error Mitigation in NISQ Devices}\n",
      "\\label{sec:error_mitigation}\n",
      "\\subsection{General Framework}\n",
      "\\label{sec:general_framework}\n",
      "We consider a quantum channel $\\mathcal{N}(\\rho) = e^{-\\lambda t}\\rho$ with a time dependent parameter $\\lambda(t)$ that\n"
     ]
    }
   ],
   "source": [
    "# Test the model with a prompt\n",
    "prompt = (\"\"\"    \n",
    "Continue this text in a natural and coherent way, maintaining consistency with the style, \n",
    "terminology, and logical flow of the preceding text.\n",
    "          \n",
    "\\\\title{Quantum Error Mitigation in NISQ Devices}\n",
    "\\\\begin{abstract}\n",
    "We present a novel approach to error mitigation in noisy intermediate-scale quantum (NISQ) devices. \n",
    "Our method introduces a scaling framework for quantum channels that preserves gate fidelity while reducing environmental noise.\n",
    "\\end{abstract}\n",
    "\\section{Introduction}\n",
    "Recent advances in NISQ devices have demonstrated both promise and limitations in quantum computation. \n",
    "The primary challenge remains decoherence, which introduces errors in quantum operations. We propose a channel scaling approach \n",
    "$\\mathcal{N}(\\\\rho) = e^{-\\lambda t}\\\\rho$ \n",
    "that provides a systematic way to\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Convert the prompt to inputs and run a forward pass\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "# Generate output (since it's a causal LM, we need to generate text from input)\n",
    "outputs = model.generate(\n",
    "    inputs['input_ids'],  # Only provide input_ids to generate\n",
    "    attention_mask=inputs['attention_mask'],  # Add attention mask to not attend to padding tokens\n",
    "    max_new_tokens=156,    # Generate 100 new tokens\n",
    "    temperature=0.6,       # Control randomness\n",
    "    do_sample=True,        # Use sampling instead of greedy decoding\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id  # Set padding token\n",
    ")\n",
    "\n",
    "# Decode the generated output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the original prompt and generated response\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"\\nGenerated response:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in model.state_dict().keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sorting expert dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 8 keys: [17, 1, 34, 44, 50, 45, 30, 54]\n",
      "Last 8 keys: [18, 15, 37, 26, 7, 38, 21, 51]\n"
     ]
    }
   ],
   "source": [
    "# layer 14\n",
    "example_dict = {60: 56, 58: 290, 24: 111, 6: 286, 36: 34, 11: 80, 9: 474, 52: 84, 34: 32, 19: 61, 7: 66, 1: 18, 50: 37, 4: 132, 5: 20, 22: 17, 29: 4, 42: 62, 17: 76, 54: 53, 46: 33, 51: 52, 26: 3, 39: 25, 47: 20, 13: 3, 18: 25, 23: 14, 16: 44, 28: 13, 12: 10, 57: 36, 40: 27, 63: 35, 35: 42, 38: 55, 48: 63, 20: 32, 15: 44, 31: 7, 44: 5, 14: 5, 41: 4, 45: 12, 33: 41, 62: 16, 10: 5, 56: 24, 61: 6, 37: 26, 53: 25, 49: 31, 59: 51, 21: 23, 3: 4, 8: 16, 25: 1, 2: 8, 43: 2, 0: 7, 32: 13, 27: 1, 55: 1}\n",
    "\n",
    "# Sorting by values in decreasing order\n",
    "sorted_items = sorted(example_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# Extract keys from the sorted items\n",
    "sorted_keys = [item[0] for item in sorted_items]\n",
    "# Get the first 8 keys and the last 8 keys\n",
    "first_8_keys = sorted_keys[:8]\n",
    "last_8_keys = sorted_keys[-8:]\n",
    "\n",
    "# Output the two lists\n",
    "print(\"First 8 keys:\", first_8_keys)\n",
    "print(\"Last 8 keys:\", last_8_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
