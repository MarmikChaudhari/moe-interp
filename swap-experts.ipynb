{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ[\"PYTORCH_TRANSFORMERS_SDP_BACKEND\"] = \"flash\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748f9256c3c543a08759bbe204dfe710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_model(model_name=\"allenai/OLMoE-1B-7B-0924\"):\n",
    "    # Load the model with device mapping\n",
    "    # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"cpu\",  # Automatically use CUDA if available\n",
    "        # torch_dtype=torch.float16,  # Use half precision for better CUDA memory usage\n",
    "        offload_folder=\"./offload\",  # Temporary storage for offloaded layers\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with a prompt\n",
    "prompt = (\"\"\" \n",
    "Continue the poem naturally and coherently, maintaining consistency with the rhyme scheme, diction and imagery. Match the poem's tone and style precisely.\n",
    "\n",
    "we measure rainfall in memories now\n",
    "count droplets like endangered species\n",
    "my grandmother's garden is underwater\n",
    "but the roses still bloom, phosphorescent\n",
    "in depths where submarines chart\n",
    "the coordinates of lost cities, while above                  \n",
    "\"\"\")\n",
    "\n",
    "# Convert the prompt to inputs and run a forward pass\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "# Generate output (since it's a causal LM, we need to generate text from input)\n",
    "outputs = model.generate(\n",
    "    inputs['input_ids'],  # Only provide input_ids to generate\n",
    "    attention_mask=inputs['attention_mask'],  # Add attention mask to not attend to padding tokens\n",
    "    max_new_tokens=156,    # Generate 1024 new tokens\n",
    "    temperature=0.6,       # Control randomness\n",
    "    # top_k=100,  # Use top-k sampling\n",
    "    do_sample=True,        # Use sampling instead of greedy decoding\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id  # Set padding token\n",
    ")\n",
    "\n",
    "# Decode the generated output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the original prompt and generated response\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"\\nGenerated response :\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_experts(model, expert_idx, target_layer_idx, source_layer_idx=0, source_expert_idx=0):\n",
    "    \"\"\"\n",
    "    Swap experts between two layers in the OLMoE model.\n",
    "    \n",
    "    Args:\n",
    "        model: The OLMoE model\n",
    "        expert_idx: Index of the expert in target layer to swap with\n",
    "        target_layer_idx: Index of the layer containing the expert to swap with\n",
    "        source_layer_idx: Index of the source layer (default 0)\n",
    "        source_expert_idx: Index of the source expert (default 0)\n",
    "\n",
    "    \"\"\"\n",
    "    # Access the decoder layers\n",
    "    decoder_layers = model.model.layers\n",
    "    print(decoder_layers[0].mlp.experts[0].gate_proj.weight.shape)\n",
    "    \n",
    "    # Verify indices are valid\n",
    "    num_layers = len(decoder_layers)\n",
    "    if target_layer_idx >= num_layers or source_layer_idx >= num_layers:\n",
    "        raise ValueError(f\"Layer index out of range. Model has {num_layers} layers.\")\n",
    "    \n",
    "    # Get the MoE blocks from both layers\n",
    "    source_moe = decoder_layers[source_layer_idx].mlp\n",
    "    target_moe = decoder_layers[target_layer_idx].mlp\n",
    "    \n",
    "    # Verify expert indices are valid\n",
    "    num_experts = len(source_moe.experts)\n",
    "    if expert_idx >= num_experts or source_expert_idx >= num_experts:\n",
    "        raise ValueError(f\"Expert index out of range. Each layer has {num_experts} experts.\")\n",
    "        \n",
    "    # Swap the expert weights\n",
    "    source_expert = source_moe.experts[source_expert_idx]\n",
    "    target_expert = target_moe.experts[expert_idx]\n",
    "    \n",
    "    # Swap gate projection weights\n",
    "    source_expert.gate_proj.weight, target_expert.gate_proj.weight = \\\n",
    "        target_expert.gate_proj.weight, source_expert.gate_proj.weight\n",
    "        \n",
    "    # Swap up projection weights\n",
    "    source_expert.up_proj.weight, target_expert.up_proj.weight = \\\n",
    "        target_expert.up_proj.weight, source_expert.up_proj.weight\n",
    "        \n",
    "    # Swap down projection weights  \n",
    "    source_expert.down_proj.weight, target_expert.down_proj.weight = \\\n",
    "        target_expert.down_proj.weight, source_expert.down_proj.weight\n",
    "    \n",
    "    return {\n",
    "        'swapped_experts': {\n",
    "            'source': {\n",
    "                'layer': source_layer_idx,\n",
    "                'expert': source_expert_idx\n",
    "            },\n",
    "            'target': {\n",
    "                'layer': target_layer_idx,\n",
    "                'expert': expert_idx\n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swap_experts(model, expert_idx=34, target_layer_idx=0, source_layer_idx=0, source_expert_idx=49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with a prompt\n",
    "prompt = (\"\"\"\n",
    "\\title{Quantum Error Mitigation in NISQ Devices}\n",
    "\\begin{abstract}\n",
    "We present a novel approach to error mitigation in noisy intermediate-scale quantum (NISQ) devices. \n",
    "Our method introduces a scaling framework for quantum channels that preserves gate fidelity while reducing environmental noise.\n",
    "\\end{abstract}\n",
    "\\section{Introduction}\n",
    "Recent advances in NISQ devices have demonstrated both promise and limitations in quantum computation. \n",
    "The primary challenge remains decoherence, which introduces errors in quantum operations. We propose a channel scaling approach \n",
    "$\\mathcal{N}(\\rho) = e^{-\\lambda t}\\rho$ \n",
    "that provides a systematic way to ...\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Convert the prompt to inputs and run a forward pass\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "# Generate output (since it's a causal LM, we need to generate text from input)\n",
    "outputs = model.generate(\n",
    "    inputs['input_ids'],  # Only provide input_ids to generate\n",
    "    attention_mask=inputs['attention_mask'],  # Add attention mask to not attend to padding tokens\n",
    "    max_new_tokens=100,    # Generate 100 new tokens\n",
    "    temperature=0.7,       # Control randomness\n",
    "    do_sample=True,        # Use sampling instead of greedy decoding\n",
    "    pad_token_id=tokenizer.eos_token_id  # Set padding token\n",
    ")\n",
    "\n",
    "# Decode the generated output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the original prompt and generated response\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"\\nGenerated response:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in model.state_dict().keys():\n",
    "#     print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
