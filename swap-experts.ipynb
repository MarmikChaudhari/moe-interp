{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ[\"PYTORCH_TRANSFORMERS_SDP_BACKEND\"] = \"flash\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4914a8892844f05b749e3041b50e494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_model(model_name=\"allenai/OLMoE-1B-7B-0924\"):\n",
    "    # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_expert_weights(model, layer_idx, expert_idx):\n",
    "    \"\"\"\n",
    "    Print the weights of a specific expert MLP at a given layer.\n",
    "    \n",
    "    Args:\n",
    "        model: The OLMoE model\n",
    "        layer_idx: Index of the layer containing the expert\n",
    "        expert_idx: Index of the expert within the layer\n",
    "    \"\"\"\n",
    "    gate_proj = f'model.layers.{layer_idx}.mlp.experts.{expert_idx}.gate_proj.weight'\n",
    "    up_proj = f'model.layers.{layer_idx}.mlp.experts.{expert_idx}.up_proj.weight'\n",
    "    down_proj = f'model.layers.{layer_idx}.mlp.experts.{expert_idx}.down_proj.weight'\n",
    "    \n",
    "    print(\"\\nGate Projection:\")\n",
    "    print(model.state_dict()[gate_proj])\n",
    "    print(\"\\nUp Projection:\") \n",
    "    print(model.state_dict()[up_proj])\n",
    "    print(\"\\nDown Projection:\")\n",
    "    print(model.state_dict()[down_proj])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gate Projection:\n",
      "tensor([[-0.0118,  0.0150,  0.0190,  ..., -0.0022, -0.0043,  0.0098],\n",
      "        [-0.0156,  0.0110,  0.0079,  ..., -0.0136, -0.0059,  0.0064],\n",
      "        [-0.0004, -0.0051, -0.0077,  ...,  0.0092,  0.0153, -0.0466],\n",
      "        ...,\n",
      "        [-0.0208, -0.0040,  0.0033,  ...,  0.0048, -0.0037,  0.0115],\n",
      "        [-0.0066,  0.0152,  0.0057,  ..., -0.0050, -0.0275, -0.0078],\n",
      "        [ 0.0004, -0.0062,  0.0060,  ...,  0.0011, -0.0302,  0.0057]])\n",
      "\n",
      "Up Projection:\n",
      "tensor([[ 0.0038, -0.0003,  0.0136,  ..., -0.0016,  0.0109, -0.0172],\n",
      "        [ 0.0078, -0.0044,  0.0027,  ...,  0.0194,  0.0127,  0.0271],\n",
      "        [ 0.0020,  0.0182,  0.0090,  ...,  0.0281, -0.0231, -0.0129],\n",
      "        ...,\n",
      "        [ 0.0015,  0.0172,  0.0036,  ...,  0.0123, -0.0040, -0.0101],\n",
      "        [-0.0208,  0.0119,  0.0157,  ..., -0.0388,  0.0206, -0.0027],\n",
      "        [-0.0113,  0.0016, -0.0238,  ..., -0.0243, -0.0132,  0.0069]])\n",
      "\n",
      "Down Projection:\n",
      "tensor([[-0.0053,  0.0125, -0.0048,  ...,  0.0077, -0.0144, -0.0030],\n",
      "        [-0.0013, -0.0056, -0.0166,  ...,  0.0171,  0.0208,  0.0033],\n",
      "        [-0.0078,  0.0154,  0.0001,  ...,  0.0117,  0.0144, -0.0110],\n",
      "        ...,\n",
      "        [ 0.0017,  0.0017,  0.0095,  ...,  0.0122, -0.0188, -0.0248],\n",
      "        [ 0.0071, -0.0049,  0.0067,  ...,  0.0068,  0.0078, -0.0057],\n",
      "        [-0.0159,  0.0187, -0.0058,  ..., -0.0193,  0.0003,  0.0077]])\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Gate Projection:\n",
      "tensor([[ 0.0047,  0.0144,  0.0098,  ..., -0.0136,  0.0004, -0.0096],\n",
      "        [ 0.0166,  0.0229, -0.0249,  ...,  0.0093,  0.0170, -0.0167],\n",
      "        [-0.0054, -0.0134,  0.0197,  ..., -0.0031,  0.0035,  0.0121],\n",
      "        ...,\n",
      "        [ 0.0183, -0.0057,  0.0187,  ..., -0.0131,  0.0139, -0.0266],\n",
      "        [-0.0187,  0.0104,  0.0003,  ...,  0.0239, -0.0026, -0.0034],\n",
      "        [ 0.0164,  0.0256, -0.0092,  ...,  0.0055, -0.0097,  0.0016]])\n",
      "\n",
      "Up Projection:\n",
      "tensor([[-0.0283, -0.0043,  0.0032,  ..., -0.0098,  0.0206,  0.0082],\n",
      "        [ 0.0025, -0.0062, -0.0189,  ...,  0.0159, -0.0078,  0.0192],\n",
      "        [ 0.0066, -0.0167, -0.0043,  ..., -0.0079, -0.0193, -0.0078],\n",
      "        ...,\n",
      "        [ 0.0074,  0.0029,  0.0161,  ..., -0.0123,  0.0339, -0.0276],\n",
      "        [-0.0248,  0.0226,  0.0152,  ...,  0.0004,  0.0189,  0.0175],\n",
      "        [ 0.0018,  0.0214, -0.0118,  ..., -0.0059, -0.0383,  0.0023]])\n",
      "\n",
      "Down Projection:\n",
      "tensor([[-0.0134,  0.0007,  0.0071,  ...,  0.0142, -0.0272, -0.0029],\n",
      "        [-0.0099, -0.0106, -0.0054,  ...,  0.0137,  0.0457, -0.0067],\n",
      "        [-0.0055, -0.0084, -0.0052,  ..., -0.0210,  0.0046, -0.0143],\n",
      "        ...,\n",
      "        [-0.0017,  0.0106,  0.0021,  ...,  0.0022,  0.0171,  0.0120],\n",
      "        [ 0.0118,  0.0115, -0.0010,  ...,  0.0076, -0.0024, -0.0280],\n",
      "        [ 0.0178,  0.0094, -0.0085,  ..., -0.0337, -0.0161,  0.0038]])\n"
     ]
    }
   ],
   "source": [
    "# Print weights for layer 0 expert 0\n",
    "print_expert_weights(model, layer_idx=0, expert_idx=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")  # Separator for readability\n",
    "\n",
    "# Print weights for layer 0 expert 1\n",
    "print_expert_weights(model, layer_idx=0, expert_idx=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the model with a prompt on vanilla model\n",
    "# prompt = (\"\"\" \n",
    "# Continue the poem naturally and coherently, maintaining consistency with the rhyme scheme, diction and imagery. Match the poem's tone and style precisely.\n",
    "\n",
    "# we measure rainfall in memories now\n",
    "# count droplets like endangered species\n",
    "# my grandmother's garden is underwater\n",
    "# but the roses still bloom, phosphorescent\n",
    "# in depths where submarines chart\n",
    "# the coordinates of lost cities, while above                  \n",
    "# \"\"\")\n",
    "\n",
    "# # Convert the prompt to inputs and run a forward pass\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "# # Generate output (since it's a causal LM, we need to generate text from input)\n",
    "# outputs = model.generate(\n",
    "#     inputs['input_ids'],  # Only provide input_ids to generate\n",
    "#     attention_mask=inputs['attention_mask'],  # Add attention mask to not attend to padding tokens\n",
    "#     max_new_tokens=156,    # Generate 1024 new tokens\n",
    "#     temperature=0.6,       # Control randomness\n",
    "#     # top_k=100,  # Use top-k sampling\n",
    "#     do_sample=True,        # Use sampling instead of greedy decoding\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "#     pad_token_id=tokenizer.eos_token_id  # Set padding token\n",
    "# )\n",
    "\n",
    "# # Decode the generated output\n",
    "# generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# # Print the original prompt and generated response\n",
    "# print(\"Prompt:\", prompt)\n",
    "# print(\"\\nGenerated response :\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_experts(model, expert_idx, target_layer_idx, source_layer_idx=0, source_expert_idx=0):\n",
    "    \"\"\"\n",
    "    Swap experts between two layers in the OLMoE model.\n",
    "    \n",
    "    Args:\n",
    "        model: The OLMoE model\n",
    "        expert_idx: Index of the expert in target layer to swap with\n",
    "        target_layer_idx: Index of the layer containing the expert to swap with\n",
    "        source_layer_idx: Index of the source layer (default 0)\n",
    "        source_expert_idx: Index of the source expert (default 0)\n",
    "\n",
    "    \"\"\"\n",
    "    # Access the decoder layers\n",
    "    decoder_layers = model.model.layers\n",
    "    print(decoder_layers[0].mlp.experts[0].gate_proj.weight.shape)\n",
    "    \n",
    "    # Verify indices are valid\n",
    "    num_layers = len(decoder_layers)\n",
    "    if target_layer_idx >= num_layers or source_layer_idx >= num_layers:\n",
    "        raise ValueError(f\"Layer index out of range. Model has {num_layers} layers.\")\n",
    "    \n",
    "    # Get the MoE blocks from both layers\n",
    "    source_moe = decoder_layers[source_layer_idx].mlp\n",
    "    target_moe = decoder_layers[target_layer_idx].mlp\n",
    "    \n",
    "    # Verify expert indices are valid\n",
    "    num_experts = len(source_moe.experts)\n",
    "    if expert_idx >= num_experts or source_expert_idx >= num_experts:\n",
    "        raise ValueError(f\"Expert index out of range. Each layer has {num_experts} experts.\")\n",
    "        \n",
    "    # Swap the expert weights\n",
    "    source_expert = source_moe.experts[source_expert_idx]\n",
    "    target_expert = target_moe.experts[expert_idx]\n",
    "    \n",
    "    # Swap gate projection weights\n",
    "    source_expert.gate_proj.weight, target_expert.gate_proj.weight = \\\n",
    "        target_expert.gate_proj.weight, source_expert.gate_proj.weight\n",
    "        \n",
    "    # Swap up projection weights\n",
    "    source_expert.up_proj.weight, target_expert.up_proj.weight = \\\n",
    "        target_expert.up_proj.weight, source_expert.up_proj.weight\n",
    "        \n",
    "    # Swap down projection weights  \n",
    "    source_expert.down_proj.weight, target_expert.down_proj.weight = \\\n",
    "        target_expert.down_proj.weight, source_expert.down_proj.weight\n",
    "    \n",
    "    return {\n",
    "        'swapped_experts': {\n",
    "            'source': {\n",
    "                'layer': source_layer_idx,\n",
    "                'expert': source_expert_idx\n",
    "            },\n",
    "            'target': {\n",
    "                'layer': target_layer_idx,\n",
    "                'expert': expert_idx\n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gemini logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def swap_experts(model, expert_idx, target_layer_idx, source_layer_idx=0, source_expert_idx=0):\n",
    "#     \"\"\"\n",
    "#     Swap experts between two layers in the OLMoE model.\n",
    "\n",
    "#     Args:\n",
    "#         model: The OLMoE model\n",
    "#         expert_idx: Index of the expert in target layer to swap with\n",
    "#         target_layer_idx: Index of the layer containing the expert to swap with\n",
    "#         source_layer_idx: Index of the source layer (default 0)\n",
    "#         source_expert_idx: Index of the source expert (default 0)\n",
    "#     \"\"\"\n",
    "#     # Access the decoder layers\n",
    "#     decoder_layers = model.model.layers\n",
    "#     print(decoder_layers[0].mlp.experts[0].gate_proj.weight.shape)\n",
    "\n",
    "#     # Verify indices are valid\n",
    "#     num_layers = len(decoder_layers)\n",
    "#     if target_layer_idx >= num_layers or source_layer_idx >= num_layers:\n",
    "#         raise ValueError(f\"Layer index out of range. Model has {num_layers} layers.\")\n",
    "\n",
    "#     # Get the MoE blocks from both layers\n",
    "#     source_moe = decoder_layers[source_layer_idx].mlp\n",
    "#     target_moe = decoder_layers[target_layer_idx].mlp\n",
    "\n",
    "#     # Verify expert indices are valid\n",
    "#     num_experts = len(source_moe.experts)\n",
    "#     if expert_idx >= num_experts or source_expert_idx >= num_experts:\n",
    "#         raise ValueError(f\"Expert index out of range. Each layer has {num_experts} experts.\")\n",
    "\n",
    "#     # Swap the expert weights\n",
    "#     source_expert = source_moe.experts[source_expert_idx]\n",
    "#     target_expert = target_moe.experts[expert_idx]\n",
    "\n",
    "#     # Swap gate projection weights\n",
    "#     temp_gate_proj_weight = source_expert.gate_proj.weight.data.clone()\n",
    "#     source_expert.gate_proj.weight.data = target_expert.gate_proj.weight.data.clone()\n",
    "#     target_expert.gate_proj.weight.data = temp_gate_proj_weight\n",
    "\n",
    "#     # Swap up projection weights\n",
    "#     temp_up_proj_weight = source_expert.up_proj.weight.data.clone()\n",
    "#     source_expert.up_proj.weight.data = target_expert.up_proj.weight.data.clone()\n",
    "#     target_expert.up_proj.weight.data = temp_up_proj_weight\n",
    "\n",
    "#     # Swap down projection weights\n",
    "#     temp_down_proj_weight = source_expert.down_proj.weight.data.clone()\n",
    "#     source_expert.down_proj.weight.data = target_expert.down_proj.weight.data.clone()\n",
    "#     target_expert.down_proj.weight.data = temp_down_proj_weight\n",
    "    \n",
    "#     return {\n",
    "#         'swapped_experts': {\n",
    "#             'source': {\n",
    "#                 'layer': source_layer_idx,\n",
    "#                 'expert': source_expert_idx\n",
    "#             },\n",
    "#             'target': {\n",
    "#                 'layer': target_layer_idx,\n",
    "#                 'expert': expert_idx\n",
    "#             }\n",
    "#         }\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 0, top expert 0 with bottom expert 43 in layer 0\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 0, top expert 6 with bottom expert 20 in layer 0\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 0, top expert 36 with bottom expert 63 in layer 0\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 0, top expert 41 with bottom expert 7 in layer 0\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 0, top expert 52 with bottom expert 58 in layer 0\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 0, top expert 10 with bottom expert 34 in layer 0\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 0, top expert 49 with bottom expert 39 in layer 0\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 0, top expert 21 with bottom expert 9 in layer 0\n"
     ]
    }
   ],
   "source": [
    "# # Create lists of experts to swap\n",
    "# top_experts_list = [0] # layer 15\n",
    "# bottom_experts_list = [1] # layer 15\n",
    "\n",
    "# layer_idx = 0\n",
    "# # Swap experts at each index\n",
    "# for i in range(len(top_experts_list)):\n",
    "#     swap_experts(model, expert_idx=top_experts_list[i], target_layer_idx=layer_idx, source_layer_idx=layer_idx, source_expert_idx=bottom_experts_list[i])\n",
    "#     print(f\"Swapped experts at layer {layer_idx}, top expert {top_experts_list[i]} with bottom expert {bottom_experts_list[i]}\")\n",
    "\n",
    "\n",
    "# Create lists of experts to swap\n",
    "top_experts_list = [0, 6, 36, 41, 52, 10, 49, 21] # layer 0\n",
    "bottom_experts_list = [43, 20, 63, 7, 58, 34, 39, 9] # layer 0\n",
    "\n",
    "layer_idx_1 = 0\n",
    "layer_idx_2 = 0\n",
    "\n",
    "# Swap experts at each index\n",
    "for i in range(len(top_experts_list)):\n",
    "    swap_experts(model, expert_idx=top_experts_list[i], target_layer_idx=layer_idx_2, source_layer_idx=layer_idx_1, source_expert_idx=bottom_experts_list[i])\n",
    "    print(f\"Swapped experts at layer {layer_idx_1}, top expert {top_experts_list[i]} with bottom expert {bottom_experts_list[i]} in layer {layer_idx_2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 1, top expert 47 with bottom expert 3 in layer 1\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 1, top expert 18 with bottom expert 6 in layer 1\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 1, top expert 61 with bottom expert 33 in layer 1\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 1, top expert 25 with bottom expert 52 in layer 1\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 1, top expert 5 with bottom expert 56 in layer 1\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 1, top expert 16 with bottom expert 10 in layer 1\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 1, top expert 27 with bottom expert 43 in layer 1\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 1, top expert 7 with bottom expert 57 in layer 1\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 2, top expert 60 with bottom expert 53 in layer 2\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 2, top expert 10 with bottom expert 4 in layer 2\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 2, top expert 61 with bottom expert 55 in layer 2\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 2, top expert 45 with bottom expert 41 in layer 2\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 2, top expert 40 with bottom expert 2 in layer 2\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 2, top expert 30 with bottom expert 8 in layer 2\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 2, top expert 15 with bottom expert 51 in layer 2\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 2, top expert 26 with bottom expert 59 in layer 2\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 3, top expert 35 with bottom expert 1 in layer 3\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 3, top expert 9 with bottom expert 44 in layer 3\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 3, top expert 6 with bottom expert 24 in layer 3\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 3, top expert 62 with bottom expert 25 in layer 3\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 3, top expert 15 with bottom expert 60 in layer 3\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 3, top expert 51 with bottom expert 26 in layer 3\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 3, top expert 19 with bottom expert 16 in layer 3\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 3, top expert 43 with bottom expert 17 in layer 3\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 4, top expert 17 with bottom expert 36 in layer 4\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 4, top expert 21 with bottom expert 13 in layer 4\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 4, top expert 6 with bottom expert 54 in layer 4\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 4, top expert 27 with bottom expert 12 in layer 4\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 4, top expert 2 with bottom expert 57 in layer 4\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 4, top expert 14 with bottom expert 18 in layer 4\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 4, top expert 25 with bottom expert 15 in layer 4\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 4, top expert 55 with bottom expert 32 in layer 4\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 5, top expert 0 with bottom expert 32 in layer 5\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 5, top expert 60 with bottom expert 47 in layer 5\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 5, top expert 31 with bottom expert 51 in layer 5\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 5, top expert 57 with bottom expert 24 in layer 5\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 5, top expert 37 with bottom expert 49 in layer 5\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 5, top expert 17 with bottom expert 63 in layer 5\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 5, top expert 21 with bottom expert 15 in layer 5\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 5, top expert 2 with bottom expert 26 in layer 5\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 6, top expert 57 with bottom expert 25 in layer 6\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 6, top expert 62 with bottom expert 21 in layer 6\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 6, top expert 18 with bottom expert 45 in layer 6\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 6, top expert 36 with bottom expert 34 in layer 6\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 6, top expert 52 with bottom expert 14 in layer 6\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 6, top expert 40 with bottom expert 39 in layer 6\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 6, top expert 4 with bottom expert 11 in layer 6\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 6, top expert 31 with bottom expert 56 in layer 6\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 7, top expert 17 with bottom expert 43 in layer 7\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 7, top expert 58 with bottom expert 47 in layer 7\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 7, top expert 35 with bottom expert 41 in layer 7\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 7, top expert 2 with bottom expert 27 in layer 7\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 7, top expert 4 with bottom expert 53 in layer 7\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 7, top expert 59 with bottom expert 6 in layer 7\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 7, top expert 21 with bottom expert 51 in layer 7\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 7, top expert 61 with bottom expert 8 in layer 7\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 8, top expert 16 with bottom expert 9 in layer 8\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 8, top expert 54 with bottom expert 25 in layer 8\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 8, top expert 14 with bottom expert 33 in layer 8\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 8, top expert 18 with bottom expert 4 in layer 8\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 8, top expert 32 with bottom expert 47 in layer 8\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 8, top expert 3 with bottom expert 62 in layer 8\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 8, top expert 37 with bottom expert 19 in layer 8\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 8, top expert 6 with bottom expert 11 in layer 8\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 9, top expert 5 with bottom expert 61 in layer 9\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 9, top expert 8 with bottom expert 10 in layer 9\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 9, top expert 6 with bottom expert 0 in layer 9\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 9, top expert 4 with bottom expert 60 in layer 9\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 9, top expert 28 with bottom expert 35 in layer 9\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 9, top expert 14 with bottom expert 58 in layer 9\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 9, top expert 7 with bottom expert 56 in layer 9\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 9, top expert 20 with bottom expert 37 in layer 9\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 10, top expert 56 with bottom expert 53 in layer 10\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 10, top expert 43 with bottom expert 63 in layer 10\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 10, top expert 11 with bottom expert 57 in layer 10\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 10, top expert 59 with bottom expert 33 in layer 10\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 10, top expert 22 with bottom expert 1 in layer 10\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 10, top expert 60 with bottom expert 31 in layer 10\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 10, top expert 13 with bottom expert 10 in layer 10\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 10, top expert 28 with bottom expert 54 in layer 10\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 11, top expert 47 with bottom expert 55 in layer 11\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 11, top expert 23 with bottom expert 29 in layer 11\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 11, top expert 27 with bottom expert 49 in layer 11\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 11, top expert 51 with bottom expert 14 in layer 11\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 11, top expert 54 with bottom expert 53 in layer 11\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 11, top expert 33 with bottom expert 19 in layer 11\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 11, top expert 52 with bottom expert 17 in layer 11\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 11, top expert 25 with bottom expert 59 in layer 11\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 12, top expert 43 with bottom expert 18 in layer 12\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 12, top expert 55 with bottom expert 19 in layer 12\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 12, top expert 59 with bottom expert 27 in layer 12\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 12, top expert 38 with bottom expert 61 in layer 12\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 12, top expert 31 with bottom expert 45 in layer 12\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 12, top expert 58 with bottom expert 3 in layer 12\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 12, top expert 47 with bottom expert 37 in layer 12\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 12, top expert 44 with bottom expert 0 in layer 12\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 13, top expert 2 with bottom expert 56 in layer 13\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 13, top expert 32 with bottom expert 17 in layer 13\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 13, top expert 5 with bottom expert 40 in layer 13\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 13, top expert 20 with bottom expert 1 in layer 13\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 13, top expert 25 with bottom expert 48 in layer 13\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 13, top expert 22 with bottom expert 52 in layer 13\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 13, top expert 55 with bottom expert 21 in layer 13\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 13, top expert 61 with bottom expert 36 in layer 13\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 14, top expert 9 with bottom expert 41 in layer 14\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 14, top expert 58 with bottom expert 3 in layer 14\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 14, top expert 6 with bottom expert 26 in layer 14\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 14, top expert 4 with bottom expert 13 in layer 14\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 14, top expert 24 with bottom expert 43 in layer 14\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 14, top expert 52 with bottom expert 25 in layer 14\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 14, top expert 11 with bottom expert 27 in layer 14\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 14, top expert 17 with bottom expert 55 in layer 14\n"
     ]
    }
   ],
   "source": [
    "# Create lists of experts to swap\n",
    "top_experts_list = [47, 18, 61, 25, 5, 16, 27, 7]\n",
    "bottom_experts_list = [3, 6, 33, 52, 56, 10, 43, 57]\n",
    "\n",
    "layer_idx_1 = 1\n",
    "layer_idx_2 = 1\n",
    "\n",
    "# Swap experts at each index\n",
    "for i in range(len(top_experts_list)):\n",
    "    swap_experts(model, expert_idx=top_experts_list[i], target_layer_idx=layer_idx_2, source_layer_idx=layer_idx_1, source_expert_idx=bottom_experts_list[i])\n",
    "    print(f\"Swapped experts at layer {layer_idx_1}, top expert {top_experts_list[i]} with bottom expert {bottom_experts_list[i]} in layer {layer_idx_2}\")\n",
    "\n",
    "\n",
    "# Create lists of experts to swap\n",
    "top_experts_list = [60, 10, 61, 45, 40, 30, 15, 26]\n",
    "bottom_experts_list = [53, 4, 55, 41, 2, 8, 51, 59]\n",
    "\n",
    "layer_idx_1 = 2\n",
    "layer_idx_2 = 2\n",
    "\n",
    "# Swap experts at each index\n",
    "for i in range(len(top_experts_list)):\n",
    "    swap_experts(model, expert_idx=top_experts_list[i], target_layer_idx=layer_idx_2, source_layer_idx=layer_idx_1, source_expert_idx=bottom_experts_list[i])\n",
    "    print(f\"Swapped experts at layer {layer_idx_1}, top expert {top_experts_list[i]} with bottom expert {bottom_experts_list[i]} in layer {layer_idx_2}\")\n",
    "\n",
    "\n",
    "# Create lists of experts to swap\n",
    "top_experts_list = [35, 9, 6, 62, 15, 51, 19, 43]\n",
    "bottom_experts_list = [1, 44, 24, 25, 60, 26, 16, 17]\n",
    "\n",
    "layer_idx_1 = 3\n",
    "layer_idx_2 = 3\n",
    "\n",
    "# Swap experts at each index\n",
    "for i in range(len(top_experts_list)):\n",
    "    swap_experts(model, expert_idx=top_experts_list[i], target_layer_idx=layer_idx_2, source_layer_idx=layer_idx_1, source_expert_idx=bottom_experts_list[i])\n",
    "    print(f\"Swapped experts at layer {layer_idx_1}, top expert {top_experts_list[i]} with bottom expert {bottom_experts_list[i]} in layer {layer_idx_2}\")\n",
    "\n",
    "\n",
    "# Create lists of experts to swap\n",
    "top_experts_list = [17, 21, 6, 27, 2, 14, 25, 55]\n",
    "bottom_experts_list = [36, 13, 54, 12, 57, 18, 15, 32]\n",
    "\n",
    "layer_idx_1 = 4\n",
    "layer_idx_2 = 4\n",
    "\n",
    "# Swap experts at each index\n",
    "for i in range(len(top_experts_list)):\n",
    "    swap_experts(model, expert_idx=top_experts_list[i], target_layer_idx=layer_idx_2, source_layer_idx=layer_idx_1, source_expert_idx=bottom_experts_list[i])\n",
    "    print(f\"Swapped experts at layer {layer_idx_1}, top expert {top_experts_list[i]} with bottom expert {bottom_experts_list[i]} in layer {layer_idx_2}\")\n",
    "\n",
    "\n",
    "# Create lists of experts to swap\n",
    "top_experts_list = [0, 60, 31, 57, 37, 17, 21, 2]\n",
    "bottom_experts_list = [32, 47, 51, 24, 49, 63, 15, 26]\n",
    "\n",
    "layer_idx_1 = 5\n",
    "layer_idx_2 = 5\n",
    "\n",
    "# Swap experts at each index\n",
    "for i in range(len(top_experts_list)):\n",
    "    swap_experts(model, expert_idx=top_experts_list[i], target_layer_idx=layer_idx_2, source_layer_idx=layer_idx_1, source_expert_idx=bottom_experts_list[i])\n",
    "    print(f\"Swapped experts at layer {layer_idx_1}, top expert {top_experts_list[i]} with bottom expert {bottom_experts_list[i]} in layer {layer_idx_2}\")\n",
    "\n",
    "\n",
    "# Create lists of experts to swap\n",
    "top_experts_list = [57, 62, 18, 36, 52, 40, 4, 31]\n",
    "bottom_experts_list = [25, 21, 45, 34, 14, 39, 11, 56]\n",
    "\n",
    "layer_idx_1 = 6\n",
    "layer_idx_2 = 6\n",
    "\n",
    "# Swap experts at each index\n",
    "for i in range(len(top_experts_list)):\n",
    "    swap_experts(model, expert_idx=top_experts_list[i], target_layer_idx=layer_idx_2, source_layer_idx=layer_idx_1, source_expert_idx=bottom_experts_list[i])\n",
    "    print(f\"Swapped experts at layer {layer_idx_1}, top expert {top_experts_list[i]} with bottom expert {bottom_experts_list[i]} in layer {layer_idx_2}\")\n",
    "\n",
    "\n",
    "# Create lists of experts to swap\n",
    "top_experts_list = [17, 58, 35, 2, 4, 59, 21, 61]\n",
    "bottom_experts_list = [43, 47, 41, 27, 53, 6, 51, 8]\n",
    "\n",
    "layer_idx_1 = 7\n",
    "layer_idx_2 = 7\n",
    "\n",
    "# Swap experts at each index\n",
    "for i in range(len(top_experts_list)):\n",
    "    swap_experts(model, expert_idx=top_experts_list[i], target_layer_idx=layer_idx_2, source_layer_idx=layer_idx_1, source_expert_idx=bottom_experts_list[i])\n",
    "    print(f\"Swapped experts at layer {layer_idx_1}, top expert {top_experts_list[i]} with bottom expert {bottom_experts_list[i]} in layer {layer_idx_2}\")\n",
    "\n",
    "\n",
    "# Create lists of experts to swap\n",
    "top_experts_list = [16, 54, 14, 18, 32, 3, 37, 6]\n",
    "bottom_experts_list = [9, 25, 33, 4, 47, 62, 19, 11]\n",
    "\n",
    "layer_idx_1 = 8\n",
    "layer_idx_2 = 8\n",
    "\n",
    "# Swap experts at each index\n",
    "for i in range(len(top_experts_list)):\n",
    "    swap_experts(model, expert_idx=top_experts_list[i], target_layer_idx=layer_idx_2, source_layer_idx=layer_idx_1, source_expert_idx=bottom_experts_list[i])\n",
    "    print(f\"Swapped experts at layer {layer_idx_1}, top expert {top_experts_list[i]} with bottom expert {bottom_experts_list[i]} in layer {layer_idx_2}\")\n",
    "\n",
    "\n",
    "# Create lists of experts to swap\n",
    "top_experts_list = [5, 8, 6, 4, 28, 14, 7, 20]\n",
    "bottom_experts_list = [61, 10, 0, 60, 35, 58, 56, 37]\n",
    "\n",
    "layer_idx_1 = 9\n",
    "layer_idx_2 = 9\n",
    "\n",
    "# Swap experts at each index\n",
    "for i in range(len(top_experts_list)):\n",
    "    swap_experts(model, expert_idx=top_experts_list[i], target_layer_idx=layer_idx_2, source_layer_idx=layer_idx_1, source_expert_idx=bottom_experts_list[i])\n",
    "    print(f\"Swapped experts at layer {layer_idx_1}, top expert {top_experts_list[i]} with bottom expert {bottom_experts_list[i]} in layer {layer_idx_2}\")\n",
    "\n",
    "\n",
    "# Create lists of experts to swap\n",
    "top_experts_list = [56, 43, 11, 59, 22, 60, 13, 28]\n",
    "bottom_experts_list = [53, 63, 57, 33, 1, 31, 10, 54]\n",
    "\n",
    "layer_idx_1 = 10\n",
    "layer_idx_2 = 10\n",
    "\n",
    "# Swap experts at each index\n",
    "for i in range(len(top_experts_list)):\n",
    "    swap_experts(model, expert_idx=top_experts_list[i], target_layer_idx=layer_idx_2, source_layer_idx=layer_idx_1, source_expert_idx=bottom_experts_list[i])\n",
    "    print(f\"Swapped experts at layer {layer_idx_1}, top expert {top_experts_list[i]} with bottom expert {bottom_experts_list[i]} in layer {layer_idx_2}\")\n",
    "\n",
    "\n",
    "# Create lists of experts to swap\n",
    "top_experts_list = [47, 23, 27, 51, 54, 33, 52, 25]\n",
    "bottom_experts_list = [55, 29, 49, 14, 53, 19, 17, 59]\n",
    "\n",
    "layer_idx_1 = 11\n",
    "layer_idx_2 = 11\n",
    "\n",
    "# Swap experts at each index\n",
    "for i in range(len(top_experts_list)):\n",
    "    swap_experts(model, expert_idx=top_experts_list[i], target_layer_idx=layer_idx_2, source_layer_idx=layer_idx_1, source_expert_idx=bottom_experts_list[i])\n",
    "    print(f\"Swapped experts at layer {layer_idx_1}, top expert {top_experts_list[i]} with bottom expert {bottom_experts_list[i]} in layer {layer_idx_2}\")\n",
    "\n",
    "\n",
    "# Create lists of experts to swap\n",
    "top_experts_list = [43, 55, 59, 38, 31, 58, 47, 44]\n",
    "bottom_experts_list = [18, 19, 27, 61, 45, 3, 37, 0]\n",
    "\n",
    "layer_idx_1 = 12\n",
    "layer_idx_2 = 12\n",
    "\n",
    "# Swap experts at each index\n",
    "for i in range(len(top_experts_list)):\n",
    "    swap_experts(model, expert_idx=top_experts_list[i], target_layer_idx=layer_idx_2, source_layer_idx=layer_idx_1, source_expert_idx=bottom_experts_list[i])\n",
    "    print(f\"Swapped experts at layer {layer_idx_1}, top expert {top_experts_list[i]} with bottom expert {bottom_experts_list[i]} in layer {layer_idx_2}\")\n",
    "\n",
    "\n",
    "# Create lists of experts to swap\n",
    "top_experts_list = [2, 32, 5, 20, 25, 22, 55, 61]\n",
    "bottom_experts_list = [56, 17, 40, 1, 48, 52, 21, 36]\n",
    "\n",
    "layer_idx_1 = 13\n",
    "layer_idx_2 = 13\n",
    "\n",
    "# Swap experts at each index\n",
    "for i in range(len(top_experts_list)):\n",
    "    swap_experts(model, expert_idx=top_experts_list[i], target_layer_idx=layer_idx_2, source_layer_idx=layer_idx_1, source_expert_idx=bottom_experts_list[i])\n",
    "    print(f\"Swapped experts at layer {layer_idx_1}, top expert {top_experts_list[i]} with bottom expert {bottom_experts_list[i]} in layer {layer_idx_2}\")\n",
    "\n",
    "\n",
    "# Create lists of experts to swap\n",
    "top_experts_list = [9, 58, 6, 4, 24, 52, 11, 17]\n",
    "bottom_experts_list = [41, 3, 26, 13, 43, 25, 27, 55]\n",
    "\n",
    "layer_idx_1 = 14\n",
    "layer_idx_2 = 14\n",
    "\n",
    "# Swap experts at each index\n",
    "for i in range(len(top_experts_list)):\n",
    "    swap_experts(model, expert_idx=top_experts_list[i], target_layer_idx=layer_idx_2, source_layer_idx=layer_idx_1, source_expert_idx=bottom_experts_list[i])\n",
    "    print(f\"Swapped experts at layer {layer_idx_1}, top expert {top_experts_list[i]} with bottom expert {bottom_experts_list[i]} in layer {layer_idx_2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 15, top expert 17 with bottom expert 18 in layer 15\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 15, top expert 1 with bottom expert 15 in layer 15\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 15, top expert 34 with bottom expert 37 in layer 15\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 15, top expert 44 with bottom expert 26 in layer 15\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 15, top expert 50 with bottom expert 7 in layer 15\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 15, top expert 45 with bottom expert 38 in layer 15\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 15, top expert 30 with bottom expert 21 in layer 15\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 15, top expert 54 with bottom expert 51 in layer 15\n"
     ]
    }
   ],
   "source": [
    "# Create lists of experts to swap\n",
    "top_experts_list = [17, 1, 34, 44, 50, 45, 30, 54] # layer 15\n",
    "bottom_experts_list = [18, 15, 37, 26, 7, 38, 21, 51] # layer 15\n",
    "\n",
    "layer_idx_1 = 15\n",
    "layer_idx_2 = 15\n",
    "\n",
    "# Swap experts at each index\n",
    "for i in range(len(top_experts_list)):\n",
    "    swap_experts(model, expert_idx=top_experts_list[i], target_layer_idx=layer_idx_2, source_layer_idx=layer_idx_1, source_expert_idx=bottom_experts_list[i])\n",
    "    print(f\"Swapped experts at layer {layer_idx_1}, top expert {top_experts_list[i]} with bottom expert {bottom_experts_list[i]} in layer {layer_idx_2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print weights for layer 0 expert 0\n",
    "# print_expert_weights(model, layer_idx=0, expert_idx=0)\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80 + \"\\n\")  # Separator for readability\n",
    "\n",
    "# # Print weights for layer 0 expert 1\n",
    "# print_expert_weights(model, layer_idx=0, expert_idx=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:     \n",
      "Continue this text in a natural and coherent way, maintaining consistency with the style, \n",
      "terminology, and logical flow of the preceding text.\n",
      "          \n",
      "\\title{Quantum Error Mitigation in NISQ Devices}\n",
      "\\begin{abstract}\n",
      "We present a novel approach to error mitigation in noisy intermediate-scale quantum (NISQ) devices. \n",
      "Our method introduces a scaling framework for quantum channels that preserves gate fidelity while reducing environmental noise.\n",
      "\\end{abstract}\n",
      "\\section{Introduction}\n",
      "Recent advances in NISQ devices have demonstrated both promise and limitations in quantum computation. \n",
      "The primary challenge remains decoherence, which introduces errors in quantum operations. We propose a channel scaling approach \n",
      "$\\mathcal{N}(\\rho) = e^{-\\lambda t}\\rho$ \n",
      "that provides a systematic way to\n",
      "\n",
      "\n",
      "Generated response:     \n",
      "Continue this text in a natural and coherent way, maintaining consistency with the style, \n",
      "terminology, and logical flow of the preceding text.\n",
      "          \n",
      "\\title{Quantum Error Mitigation in NISQ Devices}\n",
      "\\begin{abstract}\n",
      "We present a novel approach to error mitigation in noisy intermediate-scale quantum (NISQ) devices. \n",
      "Our method introduces a scaling framework for quantum channels that preserves gate fidelity while reducing environmental noise.\n",
      "\\end{abstract}\n",
      "\\section{Introduction}\n",
      "Recent advances in NISQ devices have demonstrated both promise and limitations in quantum computation. \n",
      "The primary challenge remains decoherence, which introduces errors in quantum operations. We propose a channel scaling approach \n",
      "$\\mathcal{N}(\\rho) = e^{-\\lambda t}\\rho$ \n",
      "that provides a systematic way to\n",
      "\n",
      "reduce this.\n",
      "\n",
      "\n",
      ". We are proposing a scaling approach that introduces a new way to reduce environmental noise in quantum channels.\n",
      "a.\n",
      "\n",
      "’. ”.\n",
      "\n",
      ". ”.\n",
      ".\n",
      ".\n",
      "..\n",
      "\n",
      "..\n",
      "’.\n",
      "\n",
      ".\n",
      "\n",
      "..\n",
      "\n",
      "..\n",
      "\n",
      "..\n",
      "\n",
      "..\n",
      "\n",
      "..\n",
      "\n",
      "..\n",
      "\n",
      ". and their\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "...\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      ". \n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model with a prompt\n",
    "prompt = (\"\"\"    \n",
    "Continue this text in a natural and coherent way, maintaining consistency with the style, \n",
    "terminology, and logical flow of the preceding text.\n",
    "          \n",
    "\\\\title{Quantum Error Mitigation in NISQ Devices}\n",
    "\\\\begin{abstract}\n",
    "We present a novel approach to error mitigation in noisy intermediate-scale quantum (NISQ) devices. \n",
    "Our method introduces a scaling framework for quantum channels that preserves gate fidelity while reducing environmental noise.\n",
    "\\end{abstract}\n",
    "\\section{Introduction}\n",
    "Recent advances in NISQ devices have demonstrated both promise and limitations in quantum computation. \n",
    "The primary challenge remains decoherence, which introduces errors in quantum operations. We propose a channel scaling approach \n",
    "$\\mathcal{N}(\\\\rho) = e^{-\\lambda t}\\\\rho$ \n",
    "that provides a systematic way to\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Convert the prompt to inputs and run a forward pass\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "# Generate output (since it's a causal LM, we need to generate text from input)\n",
    "outputs = model.generate(\n",
    "    inputs['input_ids'],  # Only provide input_ids to generate\n",
    "    attention_mask=inputs['attention_mask'],  # Add attention mask to not attend to padding tokens\n",
    "    max_new_tokens=156,    # Generate 100 new tokens\n",
    "    temperature=0.6,       # Control randomness\n",
    "    do_sample=True,        # Use sampling instead of greedy decoding\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id  # Set padding token\n",
    ")\n",
    "\n",
    "# Decode the generated output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the original prompt and generated response\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"\\nGenerated response:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### top 8 and bottom 8 experts for each layer for `inputs.txt`\n",
    "Layer `0`\n",
    "First 8 keys: `[0, 6, 36, 41, 52, 10, 49, 21]`\n",
    "Last 8 keys: `[43, 20, 63, 7, 58, 34, 39, 9]`\n",
    "\n",
    "Layer `1`\n",
    "First 8 keys: `[47, 18, 61, 25, 5, 16, 27, 7]`\n",
    "Last 8 keys: `[3, 6, 33, 52, 56, 10, 43, 57]`\n",
    "\n",
    "Layer `2`\n",
    "First 8 keys: `[60, 10, 61, 45, 40, 30, 15, 26]`\n",
    "Last 8 keys: `[53, 4, 55, 41, 2, 8, 51, 59]`\n",
    "\n",
    "Layer `3`\n",
    "First 8 keys: `[35, 9, 6, 62, 15, 51, 19, 43]`\n",
    "Last 8 keys: `[1, 44, 24, 25, 60, 26, 16, 17]`\n",
    "\n",
    "Layer `4`\n",
    "First 8 keys: `[17, 21, 6, 27, 2, 14, 25, 55]`\n",
    "Last 8 keys: `[36, 13, 54, 12, 57, 18, 15, 32]`\n",
    "\n",
    "Layer `5`\n",
    "First 8 keys: `[0, 60, 31, 57, 37, 17, 21, 2]`\n",
    "Last 8 keys: `[32, 47, 51, 24, 49, 63, 15, 26]`\n",
    "\n",
    "Layer `6`\n",
    "First 8 keys: `[57, 62, 18, 36, 52, 40, 4, 31]`\n",
    "Last 8 keys: `[25, 21, 45, 34, 14, 39, 11, 56]`\n",
    "\n",
    "Layer `7`\n",
    "First 8 keys: `[17, 58, 35, 2, 4, 59, 21, 61]`\n",
    "Last 8 keys: `[43, 47, 41, 27, 53, 6, 51, 8]`\n",
    "\n",
    "Layer `8`\n",
    "First 8 keys: `[16, 54, 14, 18, 32, 3, 37, 6]`\n",
    "Last 8 keys: `[9, 25, 33, 4, 47, 62, 19, 11]`\n",
    "\n",
    "Layer `9`\n",
    "First 8 keys: `[5, 8, 6, 4, 28, 14, 7, 20]`\n",
    "Last 8 keys: `[61, 10, 0, 60, 35, 58, 56, 37]`\n",
    "\n",
    "Layer `10`\n",
    "First 8 keys: `[56, 43, 11, 59, 22, 60, 13, 28]`\n",
    "Last 8 keys: `[53, 63, 57, 33, 1, 31, 10, 54]`\n",
    "\n",
    "Layer `11`\n",
    "First 8 keys: `[47, 23, 27, 51, 54, 33, 52, 25]`\n",
    "Last 8 keys: `[55, 29, 49, 14, 53, 19, 17, 59]`\n",
    "\n",
    "Layer `12`\n",
    "First 8 keys: `[43, 55, 59, 38, 31, 58, 47, 44]`\n",
    "Last 8 keys: `[18, 19, 27, 61, 45, 3, 37, 0]`\n",
    "\n",
    "Layer `13`\n",
    "First 8 keys: `[2, 32, 5, 20, 25, 22, 55, 61]`\n",
    "Last 8 keys: `[56, 17, 40, 1, 48, 52, 21, 36]`\n",
    "\n",
    "Layer `14`\n",
    "First 8 keys: `[9, 58, 6, 4, 24, 52, 11, 17]`\n",
    "Last 8 keys: `[41, 3, 26, 13, 43, 25, 27, 55]`\n",
    "\n",
    "Layer `15`\n",
    "First 8 keys: `[17, 1, 34, 44, 50, 45, 30, 54]`\n",
    "Last 8 keys: `[18, 15, 37, 26, 7, 38, 21, 51]`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sorting expert dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 8 keys: [17, 21, 6, 27, 2, 14, 25, 55]\n",
      "Last 8 keys: [36, 13, 54, 12, 57, 18, 15, 32]\n"
     ]
    }
   ],
   "source": [
    "# layer 3\n",
    "example_dict =  {27: 151, 19: 19, 55: 86, 6: 162, 25: 105, 41: 50, 48: 26, 17: 753, 3: 44, 21: 275, 7: 63, 61: 23, 39: 31, 59: 16, 2: 141, 43: 14, 10: 11, 62: 14, 33: 32, 11: 16, 52: 34, 14: 106, 35: 35, 5: 44, 60: 27, 24: 23, 58: 18, 63: 9, 51: 38, 8: 23, 4: 10, 16: 11, 23: 12, 13: 6, 22: 14, 38: 21, 34: 26, 26: 28, 29: 9, 50: 20, 46: 16, 9: 21, 37: 22, 28: 9, 53: 48, 1: 30, 20: 21, 45: 36, 57: 3, 42: 45, 56: 12, 44: 13, 30: 24, 40: 12, 0: 21, 18: 3, 54: 6, 12: 5, 36: 7, 15: 2, 32: 1}\n",
    "\n",
    "\n",
    "# Sorting by values in decreasing order\n",
    "sorted_items = sorted(example_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# Extract keys from the sorted items\n",
    "sorted_keys = [item[0] for item in sorted_items]\n",
    "# Get the first 8 keys and the last 8 keys\n",
    "first_8_keys = sorted_keys[:8]\n",
    "last_8_keys = sorted_keys[-8:]\n",
    "\n",
    "# Output the two lists\n",
    "print(\"First 8 keys:\", first_8_keys)\n",
    "print(\"Last 8 keys:\", last_8_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
