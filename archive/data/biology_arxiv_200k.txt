Successful predictions are among the most compelling validations of any
model. Extracting falsifiable predictions from nonlinear multiparameter models
is complicated by the fact that such models are commonly sloppy, possessing
sensitivities to different parameter combinations that range over many decades.
Here we discuss how sloppiness affects the sorts of data that best constrain
model predictions, makes linear uncertainty approximations dangerous, and
introduces computational difficulties in Monte-Carlo uncertainty analysis. We
also present a useful test problem and suggest refinements to the standards by
which models are communicated.

In most vertebrate species, the body axis is generated by the formation of
repeated transient structures called somites. This spatial periodicity in
somitogenesis has been related to the temporally sustained oscillations in
certain mRNAs and their associated gene products in the cells forming the
presomatic mesoderm. The mechanism underlying these oscillations have been
identified as due to the delays involved in the synthesis of mRNA and
translation into protein molecules [J. Lewis, Current Biol. {\bf 13}, 1398
(2003)]. In addition, in the zebrafish embryo intercellular Notch signalling
couples these oscillators and a longitudinal positional information signal in
the form of an Fgf8 gradient exists that could be used to transform these
coupled temporal oscillations into the observed spatial periodicity of somites.
Here we consider a simple model based on this known biology and study its
consequences for somitogenesis. Comparison is made with the known properties of
somite formation in the zebrafish embryo . We also study the effects of
localized Fgf8 perturbations on somite patterning.

We have developed a linearization method to investigate the subthreshold
oscillatory behaviors in nonlinear autonomous systems. By considering firstly
the neuronal system as an example, we show that this theoretical approach can
predict quantitatively the subthreshold oscillatory activities, including the
damping coefficients and the oscillatory frequencies which are in good
agreement with those observed in experiments. Then we generalize the
linearization method to an arbitrary autonomous nonlinear system. The detailed
extension of this theoretical approach is also presented and further discussed.

The widespread use of genetic testing in high risk pregnancies has created
strong interest in rapid and accurate molecular diagnostics for common
chromosomal aneuploidies. We show here that digital polymerase chain reaction
(dPCR) can be used for accurate measurement of trisomy 21 (Down's Syndrome),
the most common human aneuploidy. dPCR is generally applicable to any
aneuploidy, does not depend on allelic distribution or gender, and is able to
detect signals in the presence of mosaics or contaminating maternal DNA.

Biologists are leading current research on genome characterization
(sequencing, alignment, transcription), providing a huge quantity of raw data
about many genome organisms. Extracting knowledge from this raw data is an
important process for biologists, using usually data mining approaches.
However, it is difficult to deals with these genomic information using actual
bioinformatics data mining tools, because data are heterogeneous, huge in
quantity and geographically distributed. In this paper, we present a new
approach between data mining and virtual reality visualization, called visual
data mining. Indeed Virtual Reality becomes ripe, with efficient display
devices and intuitive interaction in an immersive context. Moreover, biologists
use to work with 3D representation of their molecules, but in a desktop
context. We present a software solution, Genome3DExplorer, which addresses the
problem of genomic data visualization, of scene management and interaction.
This solution is based on a well-adapted graphical and interaction paradigm,
where local and global topological characteristics of data are easily visible,
on the contrary to traditional genomic database browsers, always focused on the
zoom and details level.

This paper presents a stability test for a class of interconnected nonlinear
systems motivated by biochemical reaction networks. One of the main results
determines global asymptotic stability of the network from the diagonal
stability of a "dissipativity matrix" which incorporates information about the
passivity properties of the subsystems, the interconnection structure of the
network, and the signs of the interconnection terms. This stability test
encompasses the "secant criterion" for cyclic networks presented in our
previous paper, and extends it to a general interconnection structure
represented by a graph. A second main result allows one to accommodate state
products. This extension makes the new stability criterion applicable to a
broader class of models, even in the case of cyclic systems. The new stability
test is illustrated on a mitogen activated protein kinase (MAPK) cascade model,
and on a branched interconnection structure motivated by metabolic networks.
Finally, another result addresses the robustness of stability in the presence
of diffusion terms in a compartmental system made out of identical systems.

Mechanistic home range models are important tools in modeling animal dynamics
in spatially-complex environments. We introduce a class of stochastic models
for animal movement in a habitat of varying preference. Such models interpolate
between spatially-implicit resource selection analysis (RSA) and
advection-diffusion models, possessing these two models as limiting cases. We
find a closed-form solution for the steady-state (equilibrium) probability
distribution u* using a factorization of the redistribution operator into
symmetric and diagonal parts. How space use is controlled by the preference
function w then depends on the characteristic width of the redistribution
kernel: when w changes rapidly compared to this width, u* ~ w, whereas on
global scales large compared to this width, u* ~ w^2. We analyse the behavior
at discontinuities in w which occur at habitat type boundaries. We simulate the
dynamics of space use given two-dimensional prey-availability data and explore
the effect of the redistribution kernel width. Our factorization allows such
numerical simulations to be done extremely fast; we expect this to aid the
computationally-intensive task of model parameter fitting and inverse modeling.

Transcription networks, and other directed networks can be characterized by
some topological observables such as for example subgraph occurrence (network
motifs). In order to perform such kind of analysis, it is necessary to be able
to generate suitable randomized network ensembles. Typically, one considers
null networks with the same degree sequences of the original ones. The commonly
used algorithms sometimes have long convergence times, and sampling problems.
We present here an alternative, based on a variant of the importance sampling
Montecarlo developed by Chen et al. [1].

It is basic question in biology and other fields to identify the char-
acteristic properties that on one hand are shared by structures from a
particular realm, like gene regulation, protein-protein interaction or neu- ral
networks or foodwebs, and that on the other hand distinguish them from other
structures. We introduce and apply a general method, based on the spectrum of
the normalized graph Laplacian, that yields repre- sentations, the spectral
plots, that allow us to find and visualize such properties systematically. We
present such visualizations for a wide range of biological networks and compare
them with those for networks derived from theoretical schemes. The differences
that we find are quite striking and suggest that the search for universal
properties of biological networks should be complemented by an understanding of
more specific features of biological organization principles at different
scales.

The classical attenuation regulation of gene expression in bacteria is
considered. We propose to represent the secondary RNA structure in the leader
region of a gene or an operon by a term, and we give a probabilistic term
rewriting system modeling the whole process of such a regulation.

Escherichia coli is a motile bacterium that moves up a chemoattractant
gradient by performing a biased random walk composed of alternating runs and
tumbles. Previous models of run and tumble chemotaxis neglect one or more
features of the motion, namely (i) a cell cannot directly detect a
chemoattractant gradient but rather makes temporal comparisons of
chemoattractant concentration, (ii) rather than being entirely random, tumbles
exhibit persistence of direction, meaning that the new direction after a tumble
is more likely to be in the forward hemisphere, and (iii) rotational Brownian
motion makes it impossible for an E. coli cell to swim in a straight line
during a run. This paper presents an analytic calculation of the chemotactic
drift velocity taking account of (i), (ii) and (iii), for weak chemotaxis. The
analytic results are verified by Monte Carlo simulation. The results reveal a
synergy between temporal comparisons and persistence that enhances the drift
velocity, while rotational Brownian motion reduces the drift velocity.

An eutactic star, in a n-dimensional space, is a set of N vectors which can
be viewed as the projection of N orthogonal vectors in a N-dimensional space.
By adequately associating a star of vectors to a particular sea urchin we
propose that a measure of the eutacticity of the star constitutes a measure of
the regularity of the sea urchin. Then we study changes of regularity
(eutacticity) in a macroevolutive and taxonomic level of sea urchins belonging
to the Echinoidea Class. An analysis considering changes through geological
time suggests a high degree of regularity in the shape of these organisms
through their evolution. Rare deviations from regularity measured in
Holasteroida order are discussed.

Statistical mechanics is one of the most powerful and elegant tools in the
quantitative sciences. One key virtue of statistical mechanics is that it is
designed to examine large systems with many interacting degrees of freedom,
providing a clue that it might have some bearing on the analysis of the
molecules of living matter. As a result of data on biological systems becoming
increasingly quantitative, there is a concomitant demand that the models set
forth to describe biological systems be themselves quantitative. We describe
how statistical mechanics is part of the quantitative toolkit that is needed to
respond to such data. The power of statistical mechanics is not limited to
traditional physical and chemical problems and there are a host of interesting
ways in which these ideas can be applied in biology. This article reports on
our efforts to teach statistical mechanics to life science students and
provides a framework for others interested in bringing these tools to a
nontraditional audience in the life sciences.

MOTIVATION: Microarray technology makes it possible to measure thousands of
variables and to compare their values under hundreds of conditions. Once
microarray data are quantified, normalized and classified, the analysis phase
is essentially a manual and subjective task based on visual inspection of
classes in the light of the vast amount of information available. Currently,
data interpretation clearly constitutes the bottleneck of such analyses and
there is an obvious need for tools able to fill the gap between data processed
with mathematical methods and existing biological knowledge. RESULTS: THEA
(Tools for High-throughput Experiments Analysis) is an integrated information
processing system allowing convenient handling of data. It allows to
automatically annotate data issued from classification systems with selected
biological information coming from a knowledge base and to either manually
search and browse through these annotations or automatically generate
meaningful generalizations according to statistical criteria (data mining).
AVAILABILITY: The software is available on the website http://thea.unice.fr/

In this paper we address a general parameter estimation methodology for an
extended biokinetic degradation model [1] for poorly degradable
micropollutants. In particular we concentrate on parameter estimation of the
micropollutant degradation sub-model by specialised microorganisms. In this
case we focus on the case when only substrate degradation data are available
and prove the structural identifiability of the model. Further we consider the
problem of practical identifiability and propose experimental and related
numerical methods for unambiguous parameter estimation based on multiple
substrate degradation curves with different initial concentrations. Finally by
means of simulated pseudo-experiments we have found convincing indications that
the proposed algorithm is stable and yields appropriate parameter estimates
even in unfavourable regimes.

Predicting interactions between small molecules and proteins is a crucial
ingredient of the drug discovery process. In particular, accurate predictive
models are increasingly used to preselect potential lead compounds from large
molecule databases, or to screen for side-effects. While classical in silico
approaches focus on predicting interactions with a given specific target, new
chemogenomics approaches adopt cross-target views. Building on recent
developments in the use of kernel methods in bio- and chemoinformatics, we
present a systematic framework to screen the chemical space of small molecules
for interaction with the biological space of proteins. We show that this
framework allows information sharing across the targets, resulting in a
dramatic improvement of ligand prediction accuracy for three important classes
of drug targets: enzymes, GPCR and ion channels.

BACKGROUND: One of the most evident achievements of bioinformatics is the
development of methods that transfer biological knowledge from characterised
proteins to uncharacterised sequences. This mode of protein function assignment
is mostly based on the detection of sequence similarity and the premise that
functional properties are conserved during evolution. Most automatic approaches
developed to date rely on the identification of clusters of homologous proteins
and the mapping of new proteins onto these clusters, which are expected to
share functional characteristics. RESULTS: Here, we inverse the logic of this
process, by considering the mapping of sequences directly to a functional
classification instead of mapping functions to a sequence clustering. In this
mode, the starting point is a database of labelled proteins according to a
functional classification scheme, and the subsequent use of sequence similarity
allows defining the membership of new proteins to these functional classes. In
this framework, we define the Correspondence Indicators as measures of
relationship between sequence and function and further formulate two Bayesian
approaches to estimate the probability for a sequence of unknown function to
belong to a functional class. This approach allows the parametrisation of
different sequence search strategies and provides a direct measure of
annotation error rates. We validate this approach with a database of enzymes
labelled by their corresponding four-digit EC numbers and analyse specific
cases. CONCLUSION: The performance of this method is significantly higher than
the simple strategy consisting in transferring the annotation from the highest
scoring BLAST match and is expected to find applications in automated
functional annotation pipelines.

In many biochemical processes, proteins bound to DNA at distant sites are
brought into close proximity by loops in the underlying DNA. For example, the
function of some gene-regulatory proteins depends on such DNA looping
interactions. We present a new technique for characterizing the kinetics of
loop formation in vitro, as observed using the tethered particle method, and
apply it to experimental data on looping induced by lambda repressor. Our
method uses a modified (diffusive) hidden Markov analysis that directly
incorporates the Brownian motion of the observed tethered bead. We compare
looping lifetimes found with our method (which we find are consistent over a
range of sampling frequencies) to those obtained via the traditional
threshold-crossing analysis (which can vary depending on how the raw data are
filtered in the time domain). Our method does not involve any time filtering
and can detect sudden changes in looping behavior. For example, we show how our
method can identify transitions between long-lived, kinetically distinct states
that would otherwise be difficult to discern.

We propose a general framework for converting global and local similarities
between biological sequences to quasi-metrics. In contrast to previous works,
our formulation allows asymmetric distances, originating from uneven weighting
of strings, that may induce non-trivial partial orders on sets of biosequences.
Furthermore, the $\ell^p$-type distances considered are more general than
traditional generalized string edit distances corresponding to the $\ell^1$
case, and enable conversion of sequence similarities to distances for a much
wider class of scoring schemes. Our constructions require much less restrictive
gap penalties than the ones regularly used. Numerous examples are provided to
illustrate the concepts introduced and their potential applications.

Environment specific substitution tables have been used effectively for
distinguishing structural and functional constraints on proteins and thereby
identify their active sites (Chelliah et al. (2004)). This work explores
whether a similar approach can be used to identify specificity determining
residues (SDRs) responsible for cofactor dependence, substrate specificity or
subtle catalytic variations. We combine structure-sequence information and
functional annotation from various data sources to create structural alignments
for homologous enzymes and functional partitions therein. We develop a scoring
procedure to predict SDRs and assess their accuracy using information from
bound specific ligands and published literature.

Transforming growth factor (TGF) $\beta$ is known to have properties of both
a tumor suppressor and a tumor promoter. While it inhibits cell proliferation,
it also increases cell motility and decreases cell--cell adhesion. Coupling
mathematical modeling and experiments, we investigate the growth and motility
of oncogene--expressing human mammary epithelial cells under exposure to
TGF--$\beta$. We use a version of the well--known Fisher--Kolmogorov equation,
and prescribe a procedure for its parametrization. We quantify the simultaneous
effects of TGF--$\beta$ to increase the tendency of individual cells and cell
clusters to move randomly and to decrease overall population growth. We
demonstrate that in experiments with TGF--$\beta$ treated cells \textit{in
vitro}, TGF--$\beta$ increases cell motility by a factor of 2 and decreases
cell proliferation by a factor of 1/2 in comparison with untreated cells.

Fast, efficient and reliable algorithms for pairwise alignment of protein
structures are in ever increasing demand for analyzing the rapidly growing data
of protein structures. CLePAPS is a tool developed for this purpose. It
distinguishes itself from other existing algorithms by the use of
conformational letters, which are discretized states of 3D segmental structural
states. A letter corresponds to a cluster of combinations of the three angles
formed by C_alpha pseudobonds of four contiguous residues. A substitution
matrix called CLESUM is available to measure similarity between any two such
letters. CLePAPS regards an aligned fragment pair (AFP) as an ungapped string
pair with a high sum of pairwise CLESUM scores. Using CLESUM scores as the
similarity measure, CLePAPS searches for AFPs by simple string comparison. The
transformation which best superimposes a highly similar AFP can be used to
superimpose the structure pairs under comparison. A highly scored AFP which is
consistent with several other AFPs determines an initial alignment. CLePAPS
then joins consistent AFPs guided by their similarity scores to extend the
alignment by several `zoom-in' iteration steps. A follow-up refinement produces
the final alignment. CLePAPS does not implement dynamic programming. The
utility of CLePAPS is tested on various protein structure pairs.

The key to understanding a protein's function often lies in its
conformational dynamics. We develop a coarse-grained variational model to
investigate the interplay between structural transitions, conformational
flexibility and function of N-terminal calmodulin (nCaM) domain. In this model,
two energy basins corresponding to the ``closed'' apo conformation and ``open''
holo conformation of nCaM domain are connected by a uniform interpolation
parameter. The resulting detailed transition route from our model is largely
consistent with the recently proposed EF$\beta$-scaffold mechanism in EF-hand
family proteins. We find that the N-terminal part in calcium binding loops I
and II shows higher flexibility than the C-terminal part which form this
EF$\beta$-scaffold structure. The structural transition of binding loops I and
II are compared in detail. Our model predicts that binding loop II, with higher
flexibility and early structural change than binding loop I, dominates the
conformational transition in nCaM domain.

Various physical properties such as dipole moment, heat of formation and
energy of the most stable formation of nucleotides and bases were calculated by
PM3 (modified neglect of diatomic overlap, parametric method number 3) and AM1
(Austin model 1) methods. As distinct from previous calculations, for
nucleotides the interaction with neighbours is taken into account up to
gradient of convergence equaling 1. The dependences of these variables from the
place in the codon and the determinative degree were obtained. The difference
of these variables for codons and anticodons is shown.

A multitude of measures have been proposed to quantify the similarity between
protein 3-D structure. Among these measures, contact map overlap (CMO)
maximization deserved sustained attention during past decade because it offers
a fine estimation of the natural homology relation between proteins. Despite
this large involvement of the bioinformatics and computer science community,
the performance of known algorithms remains modest. Due to the complexity of
the problem, they got stuck on relatively small instances and are not
applicable for large scale comparison. This paper offers a clear improvement
over past methods in this respect. We present a new integer programming model
for CMO and propose an exact B &B algorithm with bounds computed by solving
Lagrangian relaxation. The efficiency of the approach is demonstrated on a
popular small benchmark (Skolnick set, 40 domains). On this set our algorithm
significantly outperforms the best existing exact algorithms, and yet provides
lower and upper bounds of better quality. Some hard CMO instances have been
solved for the first time and within reasonable time limits. From the values of
the running time and the relative gap (relative difference between upper and
lower bounds), we obtained the right classification for this test. These
encouraging result led us to design a harder benchmark to better assess the
classification capability of our approach. We constructed a large scale set of
300 protein domains (a subset of ASTRAL database) that we have called Proteus
300. Using the relative gap of any of the 44850 couples as a similarity
measure, we obtained a classification in very good agreement with SCOP. Our
algorithm provides thus a powerful classification tool for large structure
databases.

This note discusses a theoretical issue regarding the application of the
"Modular Response Analysis" method to quasi-steady state (rather than
steady-state) data.

By convention, and even more often, as an unintentional consequence of
design, time distributions of latency and infectious durations in stochastic
epidemic simulations are often exponential. The skewed distribtion typically
leads to unrealistically short times. We examine the effects of altering the
distribution latency and infectious times by comparing the key results after
simulation with exponential and gamma distributions in a homogeneous mixing
model aswell as a model with regional divisions connected by a travel intensity
matrix. We show a delay in spread with more realistic latency times and offer
an explanation of the effect.

Sensitivity analysis is an effective tool for systematically identifying
specific perturbations in parameters that have significant effects on the
behavior of a given biosystem, at the scale investigated. In this work, using a
two-dimensional, multiscale non-small cell lung cancer (NSCLC) model, we
examine the effects of perturbations in system parameters which span both
molecular and cellular levels, i.e. across scales of interest. This is achieved
by first linking molecular and cellular activities and then assessing the
influence of parameters at the molecular level on the tumor's spatio-temporal
expansion rate, which serves as the output behavior at the cellular level.
Overall, the algorithm operated reliably over relatively large variations of
most parameters, hence confirming the robustness of the model. However, three
pathway components (proteins PKC, MEK, and ERK) and eleven reaction steps were
determined to be of critical importance by employing a sensitivity coefficient
as an evaluation index. Each of these sensitive parameters exhibited a similar
changing pattern in that a relatively larger increase or decrease in its value
resulted in a lesser influence on the system's cellular performance. This study
provides a novel cross-scaled approach to analyzing sensitivities of
computational model parameters and proposes its application to
interdisciplinary biomarker studies.

Heavy-tailed or power-law distributions are becoming increasingly common in
biological literature. A wide range of biological data has been fitted to
distributions with heavy tails. Many of these studies use simple fitting
methods to find the parameters in the distribution, which can give highly
misleading results. The potential pitfalls that can occur when using these
methods are pointed out, and a step-by-step guide to fitting power-law
distributions and assessing their goodness-of-fit is offered.

Despite recent molecular technique improvements, biological knowledge remains
incomplete. Reasoning on living systems hence implies to integrate
heterogeneous and partial informations. Although current investigations
successfully focus on qualitative behaviors of macromolecular networks, others
approaches show partial quantitative informations like protein concentration
variations over times. We consider that both informations, qualitative and
quantitative, have to be combined into a modeling method to provide a better
understanding of the biological system. We propose here such a method using a
probabilistic-like approach. After its exhaustive description, we illustrate
its advantages by modeling the carbon starvation response in Escherichia coli.
In this purpose, we build an original qualitative model based on available
observations. After the formal verification of its qualitative properties, the
probabilistic model shows quantitative results corresponding to biological
expectations which confirm the interest of our probabilistic approach.

Many complex biological, social, and economical networks show topologies
drastically differing from random graphs. But, what is a complex network, i.e.\
how can one quantify the complexity of a graph? Here the Offdiagonal Complexity
(OdC), a new, and computationally cheap, measure of complexity is defined,
based on the node-node link cross-distribution, whose nondiagonal elements
characterize the graph structure beyond link distribution, cluster coefficient
and average path length. The OdC apporach is applied to the {\sl Helicobacter
pylori} protein interaction network and randomly rewired surrogates thereof. In
addition, OdC is used to characterize the spatial complexity of cell
aggregates. We investigate the earliest embryo development states of
Caenorhabditis elegans. The development states of the premorphogenetic phase
are represented by symmetric binary-valued cell connection matrices with
dimension growing from 4 to 385. These matrices can be interpreted as adjacency
matrix of an undirected graph, or network. The OdC approach allows to describe
quantitatively the complexity of the cell aggregate geometry.

Two recent streams of work suggest that pairwise interactions may be
sufficient to capture the complexity of biological systems ranging from protein
structure to networks of neurons. In one approach, possible amino acid
sequences in a family of proteins are generated by Monte Carlo annealing of a
"Hamiltonian" that forces pairwise correlations among amino acid substitutions
to be close to the observed correlations. In the other approach, the observed
correlations among pairs of neurons are used to construct a maximum entropy
model for the states of the network as a whole. We show that, in certain
limits, these two approaches are mathematically equivalent, and we comment on
open problems suggested by this framework

Given a metabolic network in terms of its metabolites and reactions, our goal
is to efficiently compute the minimal knock out sets of reactions required to
block a given behaviour. We describe an algorithm which improves the
computation of these knock out sets when the elementary modes (minimal
functional subsystems) of the network are given. We also describe an algorithm
which computes both the knock out sets and the elementary modes containing the
blocked reactions directly from the description of the network and whose
worst-case computational complexity is better than the algorithms currently in
use for these problems. Computational results are included.

Over the last decade, a large variety of clustering algorithms have been
developed to detect coregulatory relationships among genes from microarray gene
expression data. Model based clustering approaches have emerged as
statistically well grounded methods, but the properties of these algorithms
when applied to large-scale data sets are not always well understood. An
in-depth analysis can reveal important insights about the performance of the
algorithm, the expected quality of the output clusters, and the possibilities
for extracting more relevant information out of a particular data set. We have
extended an existing algorithm for model based clustering of genes to
simultaneously cluster genes and conditions, and used three large compendia of
gene expression data for S. cerevisiae to analyze its properties. The algorithm
uses a Bayesian approach and a Gibbs sampling procedure to iteratively update
the cluster assignment of each gene and condition. For large-scale data sets,
the posterior distribution is strongly peaked on a limited number of
equiprobable clusterings. A GO annotation analysis shows that these local
maxima are all biologically equally significant, and that simultaneously
clustering genes and conditions performs better than only clustering genes and
assuming independent conditions. A collection of distinct equivalent
clusterings can be summarized as a weighted graph on the set of genes, from
which we extract fuzzy, overlapping clusters using a graph spectral method. The
cores of these fuzzy clusters contain tight sets of strongly coexpressed genes,
while the overlaps exhibit relations between genes showing only partial
coexpression.

The G-protein coupled receptor (GPCR) superfamily is currently the largest
class of therapeutic targets. \textit{In silico} prediction of interactions
between GPCRs and small molecules is therefore a crucial step in the drug
discovery process, which remains a daunting task due to the difficulty to
characterize the 3D structure of most GPCRs, and to the limited amount of known
ligands for some members of the superfamily. Chemogenomics, which attempts to
characterize interactions between all members of a target class and all small
molecules simultaneously, has recently been proposed as an interesting
alternative to traditional docking or ligand-based virtual screening
strategies. We propose new methods for in silico chemogenomics and validate
them on the virtual screening of GPCRs. The methods represent an extension of a
recently proposed machine learning strategy, based on support vector machines
(SVM), which provides a flexible framework to incorporate various information
sources on the biological space of targets and on the chemical space of small
molecules. We investigate the use of 2D and 3D descriptors for small molecules,
and test a variety of descriptors for GPCRs. We show fo instance that
incorporating information about the known hierarchical classification of the
target family and about key residues in their inferred binding pockets
significantly improves the prediction accuracy of our model. In particular we
are able to predict ligands of orphan GPCRs with an estimated accuracy of
78.1%.

A composite, exponential relaxation function, modulated by a periodic
component, was used to fit to an experimental time series of blood glucose
levels. The 11 parameters function that allows for the detection of a possible
rhythm transition was fitted to the experimental time series using a genetic
algorithm. It has been found that the relaxation from a hyperglycemic condition
following a change in the anti-diabetic treatment, can be characterized by a
change from an initial 12 hours ultradian rhythm to a near-24 hours circadian
rhythm.

In many experiments, the aim is to deduce an underlying multi-substate on-off
kinetic scheme (KS) from the statistical properties of a two-state trajectory.
However, the mapping of a KS into a two-state trajectory leads to the loss of
information about the KS, and so, in many cases, more than one KS can be
associated with the data. We recently showed that the optimal way to solve this
problem is to use canonical forms of reduced dimensions (RD). RD forms are
on-off networks with connections only between substates of different states,
where the connections can have non-exponential waiting time probability density
functions (WT-PDFs). In theory, only a single RD form can be associated with
the data. To utilize RD forms in the analysis of the data, a RD form should be
associated with the data. Here, we give a toolbox for building a RD form from a
finite two-state trajectory. The methods in the toolbox are based on known
statistical methods in data analysis, combined with statistical methods and
numerical algorithms designed specifically for the current problem. Our toolbox
is self-contained - it builds a mechanism based only on the information it
extracts from the data, and its implementation on the data is fast (analyzing a
10^6 cycle trajectory from a thirty-parameter mechanism takes a couple of hours
on a PC with a 2.66 GHz processor). The toolbox is automated and is freely
available for academic research upon electronic request.

MOTIVATION: The use of oligonucleotide microarray technology requires a very
detailed attention to the design of specific probes spotted on the solid phase.
These problems are far from being commonplace since they refer to complex
physicochemical constraints. Whereas there are more and more publicly available
programs for microarray oligonucleotide design, most of them use the same
algorithm or criteria to design oligos, with only little variation. RESULTS: We
show that classical approaches used in oligo design software may be inefficient
under certain experimental conditions, especially when dealing with complex
target mixtures. Indeed, our biological model is a human obligate parasite, the
microsporidia Encephalitozoon cuniculi. Targets that are extracted from
biological samples are composed of a mixture of pathogen transcripts and host
cell transcripts. We propose a new approach to design oligonucleotides which
combines good specificity with a potentially high sensitivity. This approach is
original in the biological point of view as well as in the algorithmic point of
view. We also present an experimental validation of this new strategy by
comparing results obtained with standard oligos and with our composite oligos.
A specific E.cuniculi microarray will overcome the difficulty to discriminate
the parasite mRNAs from the host cell mRNAs demonstrating the power of the
microarray approach to elucidate the lifestyle of an intracellular pathogen
using mix mRNAs.

Living cells are the product of gene expression programs that involve the
regulated transcription of thousands of genes. The elucidation of
transcriptional regulatory networks in thus needed to understand the cell's
working mechanism, and can for example be useful for the discovery of novel
therapeutic targets. Although several methods have been proposed to infer gene
regulatory networks from gene expression data, a recent comparison on a
large-scale benchmark experiment revealed that most current methods only
predict a limited number of known regulations at a reasonable precision level.
We propose SIRENE, a new method for the inference of gene regulatory networks
from a compendium of expression data. The method decomposes the problem of gene
regulatory network inference into a large number of local binary classification
problems, that focus on separating target genes from non-targets for each TF.
SIRENE is thus conceptually simple and computationally efficient. We test it on
a benchmark experiment aimed at predicting regulations in E. coli, and show
that it retrieves of the order of 6 times more known regulations than other
state-of-the-art inference methods.

We define the complexity of DNA sequences as the information content per
nucleotide, calculated by means of some Lempel-Ziv data compression algorithm.
It is possible to use the statistics of the complexity values of the functional
regions of different complete genomes to distinguish among genomes of different
domains of life (Archaea, Bacteria and Eukarya). We shall focus on the
distribution function of the complexity of noncoding regions. We show that the
three domains may be plotted in separate regions within the two-dimensional
space where the axes are the skewness coefficient and the curtosis coefficient
of the aforementioned distribution. Preliminary results on 15 genomes are
introduced.

Models of the dynamics of cellular interaction networks have become
increasingly larger in recent years. Formal verification based on model
checking provides a powerful technology to keep up with this increase in scale
and complexity. The application of model-checking approaches is hampered,
however, by the difficulty for non-expert users to formulate appropriate
questions in temporal logic. In order to deal with this problem, we propose the
use of patterns, that is, high-level query templates that capture recurring
biological questions and that can be automatically translated into temporal
logic. The applicability of the developed set of patterns has been investigated
by the analysis of an extended model of the network of global regulators
controlling the carbon starvation response in Escherichia coli.

Combination therapies are often needed for effective clinical outcomes in the
management of complex diseases, but presently they are generally based on
empirical clinical experience. Here we suggest a novel application of search
algorithms, originally developed for digital communication, modified to
optimize combinations of therapeutic interventions. In biological experiments
measuring the restoration of the decline with age in heart function and
exercise capacity in Drosophila melanogaster, we found that search algorithms
correctly identified optimal combinations of four drugs with only one third of
the tests performed in a fully factorial search. In experiments identifying
combinations of three doses of up to six drugs for selective killing of human
cancer cells, search algorithms resulted in a highly significant enrichment of
selective combinations compared with random searches. In simulations using a
network model of cell death, we found that the search algorithms identified the
optimal combinations of 6-9 interventions in 80-90% of tests, compared with
15-30% for an equivalent random search. These findings suggest that modified
search algorithms from information theory have the potential to enhance the
discovery of novel therapeutic drug combinations. This report also helps to
frame a biomedical problem that will benefit from an interdisciplinary effort
and suggests a general strategy for its solution.

Any cutting-edge scientific research project requires a myriad of
computational tools for data generation, management, analysis and
visualization. Python is a flexible and extensible scientific programming
platform that offered the perfect solution in our recent comparative genomics
investigation (J. B. Lucks, D. R. Nelson, G. Kudla, J. B. Plotkin. Genome
landscapes and bacteriophage codon usage, PLoS Computational Biology, 4,
1000001, 2008). In this paper, we discuss the challenges of this project, and
how the combined power of Biopython, Matplotlib and SWIG were utilized for the
required computational tasks. We finish by discussing how python goes beyond
being a convenient programming language, and promotes good scientific practice
by enabling clean code, integration with professional programming techniques
such as unit testing, and strong data provenance.

Summary: In anticipation of the individualized proteomics era and the need to
integrate knowledge from disease studies, we have augmented our peptide
identification software RAId DbS to take into account annotated single amino
acid polymorphisms, post-translational modifications, and their documented
disease associations while analyzing a tandem mass spectrum. To facilitate new
discoveries, RAId DbS allows users to conduct searches permitting novel
polymorphisms. Availability: The webserver link is http://www.ncbi.nlm.nih.gov/
/CBBResearch/qmbp/raid dbs/index.html. The relevant databases and binaries of
RAId DbS for Linux, Windows, and Mac OS X are available from the same web page.
Contact: yyu@ncbi.nlm.nih.gov

Models of reaction chemistry based on the stochastic simulation algorithm
(SSA) have become a crucial tool for simulating complicated biological reaction
networks due to their ability to handle extremely complicated reaction networks
and to represent noise in small-scale chemistry. These methods can, however,
become highly inefficient for stiff reaction systems, those in which different
reaction channels operate on widely varying time scales. In this paper, we
develop two methods for accelerating sampling in SSA models: an exact method
and a scheme allowing for sampling accuracy up to any arbitrary error bound.
Both methods depend on analysis of the eigenvalues of continuous time Markov
model graphs that define the behavior of the SSA. We demonstrate these methods
for the specific application of sampling breakage times for multiply-connected
bond networks, a class of stiff system important to models of self-assembly
processes. We show theoretically and empirically that our eigenvalue methods
provide substantially reduced sampling times for a wide range of network
breakage models. These techniques are also likely to have broad use in
accelerating SSA models so as to apply them to systems and parameter ranges
that are currently computationally intractable.

Statistical inference of genetic regulatory networks is essential for
understanding temporal interactions of regulatory elements inside the cells.
For inferences of large networks, identification of network structure is
typical achieved under the assumption of sparsity of the networks.
  When the number of time points in the expression experiment is not too small,
we propose to infer the parameters in the ordinary differential equations using
the techniques from functional data analysis (FDA) by regarding the observed
time course expression data as continuous-time curves. For networks with a
large number of genes, we take advantage of the sparsity of the networks by
penalizing the linear coefficients with a L_1 norm. The ability of the
algorithm to infer network structure is demonstrated using the cell-cycle time
course data for Saccharomyces cerevisiae.

In Proteomics, only the de novo peptide sequencing approach allows a partial
amino acid sequence of a peptide to be found from a MS/MS spectrum. In this
article a preliminary work is presented to discover a complete protein sequence
from spectral data (MS and MS/MS spectra). For the moment, our approach only
uses MS spectra. A Genetic Algorithm (GA) has been designed with a new
evaluation function which works directly with a complete MS spectrum as input
and not with a mass list like the other methods using this kind of data. Thus
the mono isotopic peak extraction step which needs a human intervention is
deleted. The goal of this approach is to discover the sequence of unknown
proteins and to allow a better understanding of the differences between
experimental proteins and proteins from databases.

Some problems with the mathematical analysis on which the UK Non-Native
Organism Risk Assessment Scheme is based are outlined.

This report presents the implementation of a protein sequence comparison
algorithm specifically designed for speeding up time consuming part on parallel
hardware such as SSE instructions, multicore architectures or graphic boards.
Three programs have been developed: PLAST-P, TPLAST-N and PLAST-X. They provide
equivalent results compared to the NCBI BLAST family programs (BLAST-P,
TBLAST-N and BLAST-X) with a speed-up factor ranging from 5 to 10.

Information theory is a branch of probability and statistics involving the
analysis of communications. Information theory enables us to analyze and
quantify the information content of predictions made in the context of plant
disease management and related disciplines. In this article, some applications
of information theory in plant disease management are outlined.

The functioning of many biochemical networks is often robust -- remarkably
stable under changes in external conditions and internal reaction parameters.
Much recent work on robustness and evolvability has focused on the structure of
neutral spaces, in which system behavior remains invariant to mutations.
Recently we have shown that the collective behavior of multiparameter models is
most often 'sloppy': insensitive to changes except along a few 'stiff'
combinations of parameters, with an enormous sloppy neutral subspace.
Robustness is often assumed to be an emergent evolved property, but the
sloppiness natural to biochemical networks offers an alternative non-adaptive
explanation. Conversely, ideas developed to study evolvability in robust
systems can be usefully extended to characterize sloppy systems.

We review a recent trend in computational systems biology which aims at using
pattern recognition algorithms to infer the structure of large-scale biological
networks from heterogeneous genomic data. We present several strategies that
have been proposed and that lead to different pattern recognition problems and
algorithms. The strenght of these approaches is illustrated on the
reconstruction of metabolic, protein-protein and regulatory networks of model
organisms. In all cases, state-of-the-art performance is reported.

In this short note, we analyze the assumptions made by McDougal et al (2006),
both explicit and implicit, in their estimation of the proportion of "true
recent infections" using the BED CEIA. This enables us to write down
expressions for the sensitivity, short term specificity and long term
specificity of a test for recent infection defined by a BED ODn below a
threshold. We then derive an identity which shows the relationship between
these parameters, allowing the elimination of sensitivity and short term
specificity from an expression relating the proportion of "true recent
infections" to the proportion of seropositive individuals testing below
threshold. This has two important consequences. Firstly, the simplified formula
is substantially more amenable to calibration. Secondly, naively treating the
parameters as independent would lead to an incorrect estimate of uncertainty
due to imperfect calibration.

We provide a complete thermodynamic solution of a 1D hopping model in the
presence of a random potential by obtaining the density of states. Since the
partition function is related to the density of states by a Laplace transform,
the density of states determines completely the thermodynamic behavior of the
system. We have also shown that the transfer matrix technique, or the so-called
dynamic programming, used to obtain the density of states in the 1D hopping
model may be generalized to tackle a long-standing problem in statistical
significance assessment for one of the most important proteomic tasks - peptide
sequencing using tandem mass spectrometry data.

Statistically meaningful comparison/combination of peptide identification
results from various search methods is impeded by the lack of a universal
statistical standard. Providing an E-value calibration protocol, we
demonstrated earlier the feasibility of translating either the score or
heuristic E-value reported by any method into the textbook-defined E-value,
which may serve as the universal statistical standard. This protocol, although
robust, may lose spectrum-specific statistics and might require a new
calibration when changes in experimental setup occur. To mitigate these issues,
we developed a new MS/MS search tool, RAId_aPS, that is able to provide
spectrum-specific E-values for additive scoring functions. Given a selection of
scoring functions out of RAId score, K-score, Hyperscore and XCorr, RAId_aPS
generates the corresponding score histograms of all possible peptides using
dynamic programming. Using these score histograms to assign E-values enables a
calibration-free protocol for accurate significance assignment for each scoring
function. RAId_aPS features four different modes: (i) compute the total number
of possible peptides for a given molecular mass range, (ii) generate the score
histogram given a MS/MS spectrum and a scoring function, (iii) reassign
E-values for a list of candidate peptides given a MS/MS spectrum and the
scoring functions chosen, and (iv) perform database searches using selected
scoring functions. In modes (iii) and (iv), RAId_aPS is also capable of
combining results from different scoring functions using spectrum-specific
statistics. The web link is
http://www.ncbi.nlm.nih.gov/CBBresearch/Yu/raid_aps/index.html. Relevant
binaries for Linux, Windows, and Mac OS X are available from the same page.

Networks of person-person contacts form the substrate along which infectious
diseases spread. Most network-based studies of the spread focus on the impact
of variations in degree (the number of contacts an individual has). However,
other effects such as clustering, variations in infectiousness or
susceptibility, or variations in closeness of contacts may play a significant
role. We develop analytic techniques to predict how these effects alter the
growth rate, probability, and size of epidemics and validate the predictions
with a realistic social network. We find that (for given degree distribution
and average transmissibility) clustering is the dominant factor controlling the
growth rate, heterogeneity in infectiousness is the dominant factor controlling
the probability of an epidemic, and heterogeneity in susceptibility is the
dominant factor controlling the size of an epidemic. Edge weights (measuring
closeness or duration of contacts) have impact only if correlations exist
between different edges. Combined, these effects can play a minor role in
reinforcing one another, with the impact of clustering largest when the
population is maximally heterogeneous or if the closer contacts are also
strongly clustered. Our most significant contribution is a systematic way to
address clustering in infectious disease models, and our results have a number
of implications for the design of interventions.

The Tribolium genome contains 21 nuclear receptors, representing all of the
six known subfamilies. When compared to other species, this first complete set
for a Coleoptera reveals a strong conservation of the number and identity of
nuclear receptors in holometabolous insects. Two novelties are observed: the
atypical NR0 gene knirps is present only in brachyceran flies, while the NR2E6
gene is found only in Tribolium and in Apis. Using a quantitative analysis of
the evolutionary rate, we discovered that nuclear receptors could be divided
into two groups. In one group of 13 proteins, the rates follow the trend of the
Mecopterida genome-wide acceleration. In a second group of five nuclear
receptors, all acting together at the top of the ecdysone cascade, we observed
an overacceleration of the evolutionary rate during the early divergence of
Mecopterida. We thus extended our analysis to the twelve classic ecdysone
transcriptional regulators and found that six of them (ECR, USP, HR3, E75, HR4
and Kr-h1) underwent an overacceleration at the base of the Mecopterida
lineage. By contrast, E74, E93, BR, HR39, FTZ-F1 and E78 do not show this
divergence. We suggest that coevolution occurred within a network of regulators
that control the ecdysone cascade. The advent of Tribolium as a powerful model
should allow a better understanding of this evolution.

In this report we review modern nonlinearity methods that can be used in the
preterm birth analysis. The nonlinear analysis of uterine contraction signals
can provide information regarding physiological changes during the menstrual
cycle and pregnancy. This information can be used both for the preterm birth
prediction and the preterm labor control.
  Keywords: preterm birth, complex data analysis, nonlinear methods

Clustering is a concept used in a huge variety of applications. We review a
conceptually very simple algorithm for hierarchical clustering called in the
following the {\it mutual information clustering} (MIC) algorithm. It uses
mutual information (MI) as a similarity measure and exploits its grouping
property: The MI between three objects X, Y, and Z is equal to the sum of the
MI between X and Y, plus the MI between Z and the combined object (XY). We use
MIC both in the Shannon (probabilistic) version of information theory, where
the "objects" are probability distributions represented by random samples, and
in the Kolmogorov (algorithmic) version, where the "objects" are symbol
sequences. We apply our method to the construction of phylogenetic trees from
mitochondrial DNA sequences and we reconstruct the fetal ECG from the output of
independent components analysis (ICA) applied to the ECG of a pregnant woman.

We discuss the property of a.e. and in mean convergence of the Kohonen
algorithm considered as a stochastic process. The various conditions ensuring
the a.e. convergence are described and the connection with the rate decay of
the learning parameter is analyzed. The rate of convergence is discussed for
different choices of learning parameters. We proof rigorously that the rate of
decay of the learning parameter which is most used in the applications is a
sufficient condition for a.e. convergence and we check it numerically. The aim
of the paper is also to clarify the state of the art on the convergence
property of the algorithm in view of the growing number of applications of the
Kohonen neural networks. We apply our theorem and considerations to the case of
genetic classification which is a rapidly developing field.

We propose that certain patterns (scars) -- theoretically and numerically
predicted to be formed by electrons arranged on a sphere to minimize the
repulsive Coulomb potential (the Thomson problem) and experimentally found in
spherical crystals formed by self-assembled polystyrene beads (an instance of
the {\it generalized} Thomson problem) -- could be relevant to extend the
classic Caspar and Klug construction for icosahedrally-shaped virus capsids.
The main idea is that scars could be produced on the capsid at an intermediate
stage of its evolution and the release of the bending energy present in scars
into stretching energy could allow for shape-changes. The conjecture can be
tested in experiments and/or in numerical simulations.

We apply Markov chain lumping techniques to aggregate codons from an
empirical substitution matrix. The standard genetic code as well as higher
order amino acid substitution groups are identified. Since the aggregates are
derived from first principles they do not rely on system dependent assumptions
made beforehand, e.g. regarding criteria on what should constitute an amino
acid group. We therefore argue that the acquired aggregations more accurately
capture the multi-level structure of the substitution dynamics than alternative
techniques.

We apply the concept of subset seeds proposed in [1] to similarity search in
protein sequences. The main question studied is the design of efficient seed
alphabets to construct seeds with optimal sensitivity/selectivity trade-offs.
We propose several different design methods and use them to construct several
alphabets.We then perform an analysis of seeds built over those alphabet and
compare them with the standard Blastp seeding method [2,3], as well as with the
family of vector seeds proposed in [4]. While the formalism of subset seed is
less expressive (but less costly to implement) than the accumulative principle
used in Blastp and vector seeds, our seeds show a similar or even better
performance than Blastp on Bernoulli models of proteins compatible with the
common BLOSUM62 matrix.

Molecular docking is an essential tool for drug design. It helps the
scientist to rapidly know if two molecules, respectively called ligand and
receptor, can be combined together to obtain a stable complex. We propose a new
multi-objective model combining an energy term and a surface term to gain such
complexes. The aim of our model is to provide complexes with a low energy and
low surface. This model has been validated with two multi-objective genetic
algorithms on instances from the literature dedicated to the docking
benchmarking.

Recent advances in experimental neuroscience allow, for the first time,
non-invasive studies of the white matter tracts in the human central nervous
system, thus making available cutting-edge brain anatomical data describing
these global connectivity patterns. This new, non-invasive, technique uses
magnetic resonance imaging to construct a snap-shot of the cortical network
within the living human brain. Here, we report on the initial success of a new
weighted network communicability measure in distinguishing local and global
differences between diseased patients and controls. This approach builds on
recent advances in network science, where an underlying connectivity structure
is used as a means to measure the ease with which information can flow between
nodes. One advantage of our method is that it deals directly with the
real-valued connectivity data, thereby avoiding the need to discretise the
corresponding adjacency matrix, that is, to round weights up to 1 or down to 0,
depending upon some threshold value. Experimental results indicate that the new
approach is able to highlight biologically relevant features that are not
immediately apparent from the raw connectivity data.

An approach for multiplex qualitative and quantitative microarray-based PCR
analysis has been proposed. The characteristics of PCR executed on a gel-based
oligonucleotide microarray with immobilized forward primers and a single common
reverse primer in solution were investigated for several DNA targets. One-stage
multiplex on-chip PCR was studied for simultaneous amplification of herpes
simplex viruses types 1 and 2, cytomegalovirus DNA, and bacteriophage lambda
DNA as an internal control. Additionally the joint analysis of increased number
of targets (with addition of Chlamydia trachomatis, Mycoplasma hominis, and
Ureaplasma urealyticum DNA) was done in two-stage version of assay: first stage
was in-tube PCR with target-specific primers, while the reverse ones contained
5'-adapter region; the second stage was on-chip amplification with immobilized
target-specific forward primers and adapter as common reverse primer in
solution. The possible application of one-stage reaction for human cDNA
analysis was additionally demonstrated with utilization of a common
poly-T-containing primer in solution. SYBR green I; and Cy-5 labeled dUTP were
used for real-time and end-point detection of specific PCR products. The
efficiencies of both one-stage and two-stage reactions was shown to be strongly
dependent on magnesium and primers concentrations. Quantitative PCR in the both
versions was studied with 10-fold serial dilutions of phage lambda DNA. The
method enabled detection of 6 DNA copies per reaction for both versions of
assay. The quantitative interval for one-stage reaction covered eight orders of
concentration. The revealed significant effect of gel pad size on microarray
PCR effectiveness has been discussed.

The Automated Protein Structure Analysis (APSA) method is used for the
classification of supersecondary structures. Basis for the classification is
the encoding of three-dimensional (3D) residue conformations into a 16-letter
code (3D-1D projection). It is shown that the letter code of the protein makes
it possible to reconstruct its overall shape without ambiguity (1D-3D
translation). Accordingly, the letter code is used for the development of
classification rules that distinguish supersecondary structures by the
properties of their turns and the orientation of the flanking helix or strand
structures. The orientations of turn and flanking structures are collected in
an octant system that helps to specify 196 supersecondary groups for
(alpha,alpha)-, (alpha,beta)-, (beta,alpha)-, (beta,beta)-class. 391 protein
chains leading to 2499 super secondary structures were analyzed. Frequently
occurring super secondary structures are identified with the help of the octant
classification system and explained on the basis of their letter and
classification codes.

A new method for the Automated Protein Structure Analysis (APSA) is derived,
which simplifies the protein backbone to a smooth curve in 3-dimensional space.
For the purpose of obtaining this smooth line each amino acid is represented by
its C$_{\alpha}$ atom, which serves as suitable anchor point for a cubic spline
fit. The backbone line is characterized by arc length $s$, curvature
$\kappa(s)$, and torsion $\tau(s)$. The $\kappa(s)$ and $\tau(s)$ diagrams of
the protein backbone suppress, because of the level of coarse graining applied,
details of the bond framework of the backbone, however reveal accurately all
secondary structure features of a protein. Advantages of APSA are its
quantitative representation and analysis of 3-dimensional structure in form of
2-dimensional curvature and torsion patterns, its easy visualization of
complicated conformational features, and its general applicability. Typical
differences between 3$_{10}$-,$\alpha$-, $\pi$-helices, and $\beta$-strands are
quantified with the help of the $\kappa(s)$ and $\tau(s)$ diagrams. For a test
set of 20 proteins, 63 % of all helical residues and 48.5 % of all extended
residues are identified to be in ideal conformational environments with the
help of APSA. APSA is compared with other methods for protein structure
analysis and its applicability to higher levels of protein structure is
discussed.

We have investigated the binding interaction between the bacteriophage lambda
repressor CI and its target DNA using total internal reflection fluorescence
microscopy. Large, step-wise changes in the intensity of the red fluorescent
protein fused to CI were observed as it associated and dissociated from
individually labeled single molecule DNA targets. The stochastic association
and dissociation were characterized by Poisson statistics. Dark and bright
intervals were measured for thousands of individual events. The exponential
distribution of the intervals allowed direct determination of the association
and dissociation rate constants, ka and kd respectively. We resolved in detail
how ka and kd varied as a function of 3 control parameters, the DNA length L,
the CI dimer concentration, and the binding affinity. Our results show that
although interaction with non-operator DNA sequences are observable, CI binding
to the operator site is not dependent on the length of flanking non-operator
DNA.

This note studies feedforward circuits as models for perfect adaptation to
step signals in biological systems. A global convergence theorem is proved in a
general framework, which includes examples from the literature as particular
cases. A notable aspect of these circuits is that they do not adapt to pulse
signals, because they display a memory phenomenon. Estimates are given of the
magnitude of this effect.

This article is addressing a recurrent problem in biology: mining newly built
large scale networks. Our approach consists in comparing these new networks to
well known ones. The visual backbone of this comparative analysis is provided
by a network classification hierarchy. This method makes sense when dealing
with metabolic networks since comparison could be done using pathways
(clusters). Moreover each network models an organism and it exists organism
classification such as taxonomies. Video demonstration:
http://www.labri.fr/perso/bourqui/video.wmv

Random Threshold Networks (RTNs) are an idealized model of diluted, non
symmetric spin glasses, neural networks or gene regulatory networks. RTNs also
serve as an interesting general example of any coordinated causal system. Here
we study the conditions for maximal information transfer and behavior diversity
in RTNs. These conditions are likely to play a major role in physical and
biological systems, perhaps serving as important selective traits in biological
systems. We show that the pairwise mutual information is maximized in
dynamically critical networks. Also, we show that the correlated behavior
diversity is maximized for slightly chaotic networks, close to the critical
region. Importantly, critical networks maximize coordinated, diverse dynamical
behavior across the network and across time: the information transmission
between source and receiver nodes and the diversity of dynamical behaviors,
when measured with a time delay between the source and receiver, are maximized
for critical networks.

We apply the concept of subset seeds proposed in [1] to similarity search in
protein sequences. The main question studied is the design of efficient seed
alphabets to construct seeds with optimal sensitivity/selectivity trade-offs.
We propose several different design methods and use them to construct several
alphabets. We then perform a comparative analysis of seeds built over those
alphabets and compare them with the standard BLASTP seeding method [2], [3], as
well as with the family of vector seeds proposed in [4]. While the formalism of
subset seeds is less expressive (but less costly to implement) than the
cumulative principle used in BLASTP and vector seeds, our seeds show a similar
or even better performance than BLASTP on Bernoulli models of proteins
compatible with the common BLOSUM62 matrix. Finally, we perform a large-scale
benchmarking of our seeds against several main databases of protein alignments.
Here again, the results show a comparable or better performance of our seeds
vs. BLASTP.

We study a method of seed-based lossless filtration for approximate string
matching and related bioinformatics applications. The method is based on a
simultaneous use of several spaced seeds rather than a single seed as studied
by Burkhardt and K\"arkk\"ainen [1]. We present algorithms to compute several
important parameters of seed families, study their combinatorial properties,
and describe several techniques to construct efficient families. We also report
a large-scale application of the proposed technique to the problem of
oligonucleotide selection for an EST sequence database.

A basic assumption of molecular biology is that proteins sharing close
three-dimensional (3D) structures are likely to share a common function and in
most cases derive from a same ancestor. Computing the similarity between two
protein structures is therefore a crucial task and has been extensively
investigated. Evaluating the similarity of two proteins can be done by finding
an optimal one-to-one matching between their components, which is equivalent to
identifying a maximum weighted clique in a specific "alignment graph". In this
paper we present a new integer programming formulation for solving such clique
problems. The model has been implemented using the ILOG CPLEX Callable Library.
In addition, we designed a dedicated branch and bound algorithm for solving the
maximum cardinality clique problem. Both approaches have been integrated in
VAST (Vector Alignment Search Tool) - a software for aligning protein 3D
structures largely used in NCBI (National Center for Biotechnology
Information). The original VAST clique solver uses the well known Bron and
Kerbosh algorithm (BK). Our computational results on real life protein
alignment instances show that our branch and bound algorithm is up to 116 times
faster than BK for the largest proteins.

CoPreTHi is a Java based web application, which combines the results of
methods that predict the location of transmembrane segments in protein
sequences into a joint prediction histogram. Clearly, the joint prediction
algorithm, produces superior quality results than individual prediction
schemes. The program is available at http://o2.db.uoa.gr/CoPreTHi

We present a novel method that predicts transmembrane domains in proteins
using solely information contained in the sequence itself. The PRED-TMR
algorithm described, refines a standard hydrophobicity analysis with a
detection of potential termini ('edges', starts and ends) of transmembrane
regions. This allows one both to discard highly hydrophobic regions not
delimited by clear start and end configurations and to confirm putative
transmembrane segments not distinguishable by their hydrophobic composition.
The accuracy obtained on a test set of 101 non-homologous transmembrane
proteins with reliable topologies compares well with that of other popular
existing methods. Only a slight decrease in prediction accuracy was observed
when the algorithm was applied to all transmembrane proteins of the SwissProt
database (release 35). A WWW server running the PRED-TMR algorithm is available
at http://o2.db.uoa. gr/PRED-TMR/

Summary : FT is a tool written in C++, which implements the Fourier analysis
method to locate periodicities in aminoacid or DNA sequences. It is provided
for free public use on a WWW server with a Java interface. Availability : The
server address is http://o2.db.uoa.gr/FT Contact : shamodr@atlas.uoa.gr

A cascading system of hierarchical, artificial neural networks (named
PRED-CLASS) is presented for the generalized classification of proteins into
four distinct classes-transmembrane, fibrous, globular, and mixed-from
information solely encoded in their amino acid sequences. The architecture of
the individual component networks is kept very simple, reducing the number of
free parameters (network synaptic weights) for faster training, improved
generalization, and the avoidance of data overfitting. Capturing information
from as few as 50 protein sequences spread among the four target classes (6
transmembrane, 10 fibrous, 13 globular, and 17 mixed), PRED-CLASS was able to
obtain 371 correct predictions out of a set of 387 proteins (success rate
approximately 96%) unambiguously assigned into one of the target classes. The
application of PRED-CLASS to several test sets and complete proteomes of
several organisms demonstrates that such a method could serve as a valuable
tool in the annotation of genomic open reading frames with no functional
assignment or as a preliminary step in fold recognition and ab initio structure
prediction methods. Detailed results obtained for various data sets and
completed genomes, along with a web sever running the PRED-CLASS algorithm, can
be accessed over the World Wide Web at http://o2.biol.uoa.gr/PRED-CLASS

Current research in biology heavily depends on the availability and efficient
use of information. In order to build new knowledge, various sources of
biological data must often be combined. Semantic Web technologies, which
provide a common framework allowing data to be shared and reused between
applications, can be applied to the management of disseminated biological data.
However, due to some specificities of biological data, the application of these
technologies to life science constitutes a real challenge. Through a use case
of biological data integration, we show in this paper that current Semantic Web
technologies start to become mature and can be applied for the development of
large applications. However, in order to get the best from these technologies,
improvements are needed both at the level of tool performance and knowledge
modeling.

This work presents a simple artificial neural network which classifies
proteins into two classes from their sequences alone: the membrane protein
class and the non-membrane protein class. This may be important in the
functional assignment and analysis of open reading frames (ORF's) identified in
complete genomes and, especially, those ORF's that correspond to proteins with
unknown function. The network described here has a simple hierarchical
feed-forward topology and a limited number of neurons which makes it very fast.
By using only information contained in 11 protein sequences, the method was
able to identify, with 100% accuracy, all membrane proteins with reliable
topologies collected from several papers in the literature. Applied to a test
set of 995 globular, water-soluble proteins, the neural network classified
falsely 23 of them in the membrane protein class (97.7% of correct assignment).
The method was also applied to the complete SWISS-PROT database with
considerable success and on ORF's of several complete genomes. The neural
network developed was associated with the PRED-TMR algorithm (Pasquier,C.,
Promponas,V.J., Palaios,G.A., Hamodrakas,J.S. and Hamodrakas,S.J., 1999) in a
new application package called PRED-TMR2. A WWW server running the PRED-TMR2
software is available at http://o2.db.uoa.gr/PRED-TMR2

We propose two ways of estimating the current source density (CSD) from
measurements of voltage on a Cartesian grid with missing recording points using
the inverse CSD method. The simplest approach is to substitute local averages
(LA) in place of missing data. A more elaborate alternative is to estimate a
smaller number of CSD parameters than the actual number of recordings and to
take the least-squares fit (LS). We compare the two approaches in the three
dimensional case on several sets of surrogate and experimental data, for
varying numbers of missing data points, and discuss their advantages and
drawbacks. One can construct CSD distributions for which one or the other
approach is better. However, in general, LA method is to be recommended being
more stable and more robust to variations in the recorded fields.

Graphical analysis methods are widely used in positron emission tomography
quantification because of their simplicity and model independence. But they
may, particularly for reversible kinetics, lead to bias in the estimated
parameters. The source of the bias is commonly attributed to noise in the data.
Assuming a two-tissue compartmental model, we investigate the bias that
originates from model error. This bias is an intrinsic property of the
simplified linear models used for limited scan durations, and it is exaggerated
by random noise and numerical quadrature error. Conditions are derived under
which Logan's graphical method either over- or under-estimates the distribution
volume in the noise-free case. The bias caused by model error is quantified
analytically. The presented analysis shows that the bias of graphical methods
is inversely proportional to the dissociation rate. Furthermore, visual
examination of the linearity of the Logan plot is not sufficient for
guaranteeing that equilibrium has been reached. A new model which retains the
elegant properties of graphical analysis methods is presented, along with a
numerical algorithm for its solution. We perform simulations with the fibrillar
amyloid-beta radioligand [11C] benzothiazole-aniline using published data from
the University of Pittsburgh and Rotterdam groups. The results show that the
proposed method significantly reduces the bias due to model error. Moreover,
the results for data acquired over a 70 minutes scan duration are at least as
good as those obtained using existing methods for data acquired over a 90
minutes scan duration.

Logan's graphical analysis (LGA) is a widely-used approach for quantification
of biochemical and physiological processes from Positron emission tomography
(PET) image data. A well-noted problem associated with the LGA method is the
bias in the estimated parameters. We recently systematically evaluated the bias
associated with the linear model approximation and developed an alternative to
minimize the bias due to model error. In this study, we examined the noise
structure in the equations defining linear quantification methods, including
LGA. The noise structure conflicts with the conditions given by the
Gauss-Markov theorem for the least squares (LS) solution to generate the best
linear unbiased estimator. By carefully taking care of the data error
structure, we propose to use structured total least squares (STLS) to obtain
the solution using a one-dimensional optimization problem. Simulations of PET
data for [11C] benzothiazole-aniline (Pittsburgh Compound-B [PIB]) show that
the proposed method significantly reduces the bias. We conclude that the bias
associated with noise is primarily due to the unusual structure of he
correlated noise and it can be reduced with the proposed STLS method.

Parametric imaging of the cerebral metabolic rate for glucose (CMRGlc) using
[18F]-fluorodeoxyglucose positron emission tomography is considered.
Traditional imaging is hindered due to low signal to noise ratios at individual
voxels. We propose to minimize the total variation of the tracer uptake rates
while requiring good fit of traditional Patlak equations. This minimization
guarantees spatial homogeneity within brain regions and good distinction
between brain regions. Brain phantom simulations demonstrate significant
improvement in quality of images by the proposed method as compared to Patlak
images with post-filtering using Gaussian or median filters.

Knowing which mode of combinatorial regulation (typically, AND or OR logic
operation) that a gene employs is important for determining its function in
regulatory networks. Here, we introduce a dynamic cross-correlation function
between the output of a gene and its upstream regulator concentrations for
signatures of combinatorial regulation in gene expression noise. We find that
the correlation function is always upwards convex for the AND operation whereas
downwards convex for the OR operation, whichever sources of noise (intrinsic or
extrinsic or both). In turn, this fact implies a means for inferring regulatory
synergies from available experimental data. The extensions and applications are
discussed.

The social networks that infectious diseases spread along are typically
clustered. Because of the close relation between percolation and epidemic
spread, the behavior of percolation in such networks gives insight into
infectious disease dynamics. A number of authors have studied clustered
networks, but the networks often contain preferential mixing between high
degree nodes. We introduce a class of random clustered networks and another
class of random unclustered networks with the same preferential mixing. We
analytically show that percolation in the clustered networks reduces the
component sizes and increases the epidemic threshold compared to the
unclustered networks.

Experimental data regarding auxin and venation formation exist at both
macroscopic and molecular scales, and we attempt to unify them into a
comprehensive model for venation formation. We begin with a set of principles
to guide an abstract model of venation formation, from which we show how
patterns in plant development are related to the representation of global
distance information locally as cellular-level signals. Venation formation, in
particular, is a function of distances between cells and their locations. The
first principle, that auxin is produced at a constant rate in all cells, leads
to a (Poisson) reaction-diffusion equation. Equilibrium solutions uniquely
codify information about distances, thereby providing cells with the signal to
begin differentiation from ground to vascular. A uniform destruction hypothesis
and scaling by cell size leads to a more biologically-relevant (Helmholtz)
model, and simulations demonstrate its capability to predict leaf and root
auxin distributions and venation patterns. The mathematical development is
centered on properties of the distance map, and provides a mechanism by which
global information about shape can be presented locally to individual cells.
The principles provide the foundation for an elaboration of these models in a
companion paper \cite{plos-paper2}, and together they provide a framework for
understanding organ- and plant-scale organization.

To support and guide an extensive experimental research into systems biology
of signaling pathways, increasingly more mechanistic models are being developed
with hopes of gaining further insight into biological processes. In order to
analyse these models, computational and statistical techniques are needed to
estimate the unknown kinetic parameters. This chapter reviews methods from
frequentist and Bayesian statistics for estimation of parameters and for
choosing which model is best for modeling the underlying system. Approximate
Bayesian Computation (ABC) techniques are introduced and employed to explore
different hypothesis about the JAK-STAT signaling pathway.

The principles underlying plant development are extended to allow a more
molecular mechanism to elaborate the schema by which ground cells differentiate
into vascular cells. Biophysical considerations dictate that linear dynamics
are not sufficent to capture facilitated auxin transport (e.g., through PIN).
We group these transport facilitators into a non-linear model under the
assumption that they attempt to minimize certain {\em differences} of auxin
concentration. This Constant Gradient Hypothesis greatly increases the
descriptive power of our model to include complex dynamical behaviour.
Specifically, we show how the early pattern of PIN1 expression appears in the
embryo, how the leaf primordium emerges, how convergence points arise on the
leaf margin, how the first loop is formed, and how the intricate pattern of PIN
shifts during the early establishment of vein patterns in incipient leaves of
Arabidopsis. Given our results, we submit that the model provides evidence that
many of the salient structural characteristics that have been described at
various stages of plant development can arise from the uniform application of a
small number of abstract principles.

We introduce an alternative formulation of the exact stochastic simulation
algorithm (SSA) for sampling trajectories of the chemical master equation for a
well-stirred system of coupled chemical reactions. Our formulation is based on
factored-out, partial reaction propensities. This novel exact SSA, called the
partial propensity direct method (PDM), is highly efficient and has a
computational cost that scales at most linearly with the number of chemical
species, irrespective of the degree of coupling of the reaction network. In
addition, we propose a sorting variant, SPDM, which is especially efficient for
multiscale reaction networks.

New technologies and equipment allow for mass treatment of samples and
research teams share acquired data on an always larger scale. In this context
scientists are facing a major data exploitation problem. More precisely, using
these data sets through data mining tools or introducing them in a classical
experimental approach require a preliminary understanding of the information
space, in order to direct the process. But acquiring this grasp on the data is
a complex activity, which is seldom supported by current software tools. The
goal of this paper is to introduce a solution to this scientific data grasp
problem. Illustrated in the Tissue MicroArrays application domain, the proposal
is based on the synthesis notion, which is inspired by Information Retrieval
paradigms. The envisioned synthesis model gives a central role to the study the
researcher wants to conduct, through the task notion. It allows for the
implementation of a task-oriented Information Retrieval prototype system. Cases
studies and user studies were used to validate this prototype system. It opens
interesting prospects for the extension of the model or extensions towards
other application domains.

Fluorescent and luminescent gene reporters allow us to dynamically quantify
changes in molecular species concentration over time on the single cell level.
The mathematical modeling of their interaction through multivariate dynamical
models requires the development of effective statistical methods to calibrate
such models against available data. Given the prevalence of stochasticity and
noise in biochemical systems inference for stochastic models is of special
interest. In this paper we present a simple and computationally efficient
algorithm for the estimation of biochemical kinetic parameters from gene
reporter data. We use the linear noise approximation to model biochemical
reactions through a stochastic dynamic model which essentially approximates a
diffusion model by an ordinary differential equation model with an
appropriately defined noise process. An explicit formula for the likelihood
function can be derived allowing for computationally efficient parameter
estimation. The proposed algorithm is embedded in a Bayesian framework and
inference is performed using Markov chain Monte Carlo. The major advantage of
the method is that in contrast to the more established diffusion approximation
based methods the computationally costly methods of data augmentation are not
necessary. Our approach also allows for unobserved variables and measurement
error. The application of the method to both simulated and experimental data
shows that the proposed methodology provides a useful alternative to diffusion
approximation based methods.

We study the correlation of the occurrence of coronary heart disease (CHD)
with the presence of the single-nucleotide polymorphism (SNP) at the -308
position of the tumor necrosis factor alpha (TNF-$\alpha$) gene. We also
consider the influence of the occurrence of type 2 diabetes (t2DM). Using
Bayesian inference, we first pursue a bottom-up approach to compute the working
hypothesis and the probabilities derivable from the data. We then pursue a
top-down approach by modelling the signal pathway that causally connects the
SNP with the emergence of CHD. We compute the functional form of the
probability of CHD conditional on the presence of the SNP in terms of both the
statistical and biochemical properties of the system. From the probability of
occurrence of a disease conditional on a given risk factor, we explore the
possibility of extracting information on the pathways involved in the
occurrence of the disease. This is a first study that we want to systematise
into a comprehensive formalism to be applied to the inference of the mechanism
connecting the risk factors to the disease.

The animals, in particular insects (Drosophila melanogaster), response
towards odor stimuli in nature can be established by measuring the dynamic of
the odor response. Such an approach is innovative since responses to odors were
tested only at certain time point so far, not in hour intervals for several
hours. The odor attraction to 14 natural and ecologically relevant odor stimuli
such as fruits- ripe and rotten, yeast and vinegar was tested. A mathematical
model to evaluate obtained data is proposed in this study in which the number
of flies caught over several time points is presented as one simple parameter
showing trapping potential of the trap housing particular odor stimuli. The
knowledge concerning dynamic of the odor response in Drosophila melanogaster
may enlighten the principles of flies behavior in context of exposure towards
odor stimulus.

A real-time recording setup combining exhaled breath VOC measurements by
proton transfer reaction mass spectrometry (PTR-MS) with hemodynamic and
respiratory data is presented. Continuous automatic sampling of exhaled breath
is implemented on the basis of measured respiratory flow: a flow-controlled
shutter mechanism guarantees that only end-tidal exhalation segments are drawn
into the mass spectrometer for analysis.
  Exhaled breath concentration profiles of two prototypic compounds, isoprene
and acetone, during several exercise regimes were acquired, reaffirming and
complementing earlier experimental findings regarding the dynamic response of
these compounds reported by Senthilmohan et al. [1] and Karl et al. [2]. While
isoprene tends to react very sensitively to changes in pulmonary ventilation
and perfusion due to its lipophilic behavior and low Henry constant,
hydrophilic acetone shows a rather stable behavior. Characteristic (median)
values for breath isoprene concentration and molar flow, i.e., the amount of
isoprene exhaled per minute are 100 ppb and 29 nmol/min, respectively, with
some intra-individual day-to-day variation. At the onset of exercise breath
isoprene concentration increases drastically, usually by a factor of ~3-4
within about one minute. Due to a simultaneous increase in ventilation, the
associated rise in molar flow is even more pronounced, leading to a ratio
between peak molar flow and molar flow at rest of ~11.
  Our setup holds great potential in capturing continuous dynamics of
non-polar, low-soluble VOCs over a wide measurement range with simultaneous
appraisal of decisive physiological factors affecting exhalation kinetics.

The purpose of this Note is twofold: First, we introduce the general
formalism of evolutionary genetics dynamics involving fitnesses, under both the
deterministic and stochastic setups, and chiefly in discrete-time. In the
process, we particularize it to a one-parameter model where only a selection
parameter is unknown. Then and in a parallel manner, we discuss the estimation
problems of the selection parameter based on a single-generation frequency
distribution shift under both deterministic and stochastic evolutionary
dynamics. In the stochastics, we consider both the celebrated Wright-Fisher and
Moran models.

Calibration of the self-thinning frontier in even-aged monocultures is
hampered by scarce data and by subjective decisions about the proximity of data
to the frontier. We present a simple model that applies to observations of the
full trajectory of stand mean diameter across a range of densities not close to
the frontier. Development of the model is based on a consideration of the slope
s=ln(Nt/Nt 1)/ln(Dt/Dt 1) of a log-transformed plot of stocking Nt and mean
stem diameter Dt at time t. This avoids the need for subjective decisions about
limiting density and allows the use of abundant data further from the
self-thinning frontier. The model can be solved analytically and yields
equations for the stocking and the stand basal area as an explicit function of
stem diameter. It predicts that self-thinning may be regulated by the maximum
basal area with a slope of -2. The significance of other predictor variables
offers an effective test of competing self-thinning theories such Yoda's -3/2
power rule and Reineke's stand density index.

Array-Based Comparative Genomic Hybridization (aCGH) is a method used to
search for genomic regions with copy numbers variations. For a given aCGH
profile, one challenge is to accurately segment it into regions of constant
copy number. Subjects sharing the same disease status, for example a type of
cancer, often have aCGH profiles with similar copy number variations, due to
duplications and deletions relevant to that particular disease. We introduce a
constrained optimization algorithm that jointly segments aCGH profiles of many
subjects. It simultaneously penalizes the amount of freedom the set of profiles
have to jump from one level of constant copy number to another, at genomic
locations known as breakpoints. We show that breakpoints shared by many
different profiles tend to be found first by the algorithm, even in the
presence of significant amounts of noise. The algorithm can be formulated as a
group LARS problem. We propose an extremely fast way to find the solution path,
i.e., a sequence of shared breakpoints in order of importance. For no extra
cost the algorithm smoothes all of the aCGH profiles into piecewise-constant
regions of equal copy number, giving low-dimensional versions of the original
data. These can be shown for all profiles on a single graph, allowing for
intuitive visual interpretation. Simulations and an implementation of the
algorithm on bladder cancer aCGH profiles are provided.

Gene regulatory circuits show significant stochastic fluctuations in their
circuit signals due to the low copy number of transcription factors. When a
gene circuit component is connected to an existing circuit, the dynamic
properties of the existing circuit can be affected by the connected component.
In this paper, we investigate modularity in the dynamics of the gene circuit
based on stochastic fluctuations in the circuit signals. We show that the noise
in the output signal of the existing circuit can be affected significantly when
the output is connected to the input of another circuit component. More
specifically, the output signal noise can show significantly longer
correlations when the two components are connected. This equivalently means
that the noise power spectral density becomes narrower. We define the relative
change in the correlation time or the spectrum bandwidth by stochastic
retroactivity, which is shown to be directly related to the retroactivity
defined in the deterministic framework by del Vecchio et al. This provides an
insight on how to measure retroactivity, by investigating stochastic
fluctuations in gene expression levels, more specifically, by obtaining an
autocorrelation function of the fluctuations. We also provide an interesting
aspect of the frequency response of the circuit. We show that depending on the
magnitude of operating frequencies, different kinds of signals need to be
preferably chosen for circuit description in a modular fashion: at low enough
frequency, expression level of transcription factor that are not bound to their
specific promoter region needs to be chosen, and at high enough frequency, that
of the total transcription factor, both bound and unbound, does.

We define new profiles based on hydropathy properties and point out specific
profiles for regions surrounding splice sites. We built a set T of flanking
regions of genes with 1-3 introns from 21st and 22nd chromosomes. These genes
contained 313 introns and 385 exons and were extracted from GenBank. They were
used in order to define hydropathy profiles. Most human introns, around 99.66%,
are likely to be U2- type introns. They have highly degenerate sequence motifs
and many different sequences can function as U2-type splice sites. Our new
profiles allow to identify regions which have conservative biochemical features
that are essential for recognition by spliceosome. We have also found
differences between hydropathy profiles for U2 or U12-types of introns on sets
of spice sites extracted from SpliceRack database in order to distinguish GT?AG
introns belonging to U2 and U12-types. Indeed, intron type cannot be simply
determined by the dinucleotide termini. We show that there is a similarity of
hydropathy profiles inside intron types. On the one hand, GT?AG and GC?AG
introns belonging to U2-type have resembling hydropathy profiles as well as
AT?AC and GT?AG introns belonging to U12-type. On the other hand, hydropathy
profiles of U2 and U12-types GT?AG introns are completely different. Finally,
we define and compute a pvalue; we compare our profiles with the profiles
provided by a classical method, Pictogram.

One important preprocessing step in the analysis of microarray data is
background subtraction. In high-density oligonucleotide arrays this is
recognized as a crucial step for the global performance of the data analysis
from raw intensities to expression values.
  We propose here an algorithm for background estimation based on a model in
which the cost function is quadratic in a set of fitting parameters such that
minimization can be performed through linear algebra. The model incorporates
two effects: 1) Correlated intensities between neighboring features in the chip
and 2) sequence-dependent affinities for non-specific hybridization fitted by
an extended nearest-neighbor model.
  The algorithm has been tested on 360 GeneChips from publicly available data
of recent expression experiments. The algorithm is fast and accurate. Strong
correlations between the fitted values for different experiments as well as
between the free-energy parameters and their counterparts in aqueous solution
indicate that the model captures a significant part of the underlying physical
chemistry.

We present an approach to computing spatial information based on Fourier
coefficient distributions. The Fourier transform (FT) of an image contains a
complete description of the image, and the values of the FT coefficients are
uniquely associated with that image. For an image where the distribution of
pixels is uncorrelated, the FT coefficients are normally distributed and
uncorrelated. Further, the probability distribution for the FT coefficients of
such an image can readily be obtained by Parseval's theorem. We take advantage
of these properties to compute the spatial information in an image by
determining the probability of each coefficient (both real and imaginary parts)
in the FT, then using the Shannon formalism to calculate information. By using
the probability distribution obtained from Parseval's theorem, an effective
distance from the completely uncorrelated or most uncertain case is obtained.
The resulting quantity is an information computed in k-space (kSI). This
approach provides a robust, facile and highly flexible framework for
quantifying spatial information in images and other types of data (of arbitrary
dimensions). The kSI metric is tested on a 2D Ising ferromagnet, and the
temperature-dependent phase transition is accurately determined from the
spatial information in configurations of the system.

In this work, we introduce the novel technique of in-chip drop on demand,
which consists in dispensing picoliter to nanoliter drops on demand directly in
the liquid-filled channels of a polymer microfluidic chip, at frequencies up to
2.5 kHz and with precise volume control. The technique involves a PDMS chip
with one or several microliter-size chambers driven by piezoelectric actuators.
Individual aqueous microdrops are dispensed from the chamber to a main
transport channel filled with an immiscible fluid, in a process analogous to
atmospheric drop on demand dispensing. In this article, the drop formation
process is characterized with respect to critical dispense parameters such as
the shape and duration of the driving pulse, and the size of both the fluid
chamber and the nozzle. Several features of the in-chip drop on demand
technique with direct relevance to lab on a chip applications are presented and
discussed, such as the precise control of the dispensed volume, the ability to
merge drops of different reagents and the ability to move a drop from the
shooting area of one nozzle to another for multi-step reactions. The
possibility to drive the microfluidic chip with inexpensive audio electronics
instead of research-grade equipment is also examined and verified. Finally, we
show that the same piezoelectric technique can be used to generate a single gas
bubble on demand in a microfluidic chip.

Risk stratification is most directly and informatively summarized as a risk
distribution curve. From this curve the ROC curve, predictiveness curve, and
other curves depicting risk stratification can be derived, demonstrating that
they present similar information. A mathematical expression for the ROC curve
AUC is derived which clarifies how this measure of discrimination quantifies
the overlap between patients who have and don't have events. This expression is
used to define the positive correlation between the dispersion of the risk
distribution curve and the ROC curve AUC. As more disperse risk distributions
and greater separation between patients with and without events characterize
superior risk stratification, the ROC curve AUC provides useful information.

High-throughput data analyses are becoming common in biology, communications,
economics and sociology. The vast amounts of data are usually represented in
the form of matrices and can be considered as knowledge networks. Spectra-based
approaches have proved useful in extracting hidden information within such
networks and for estimating missing data, but these methods are based
essentially on linear assumptions. The physical models of matching, when
applicable, often suggest non-linear mechanisms, that may sometimes be
identified as noise. The use of non-linear models in data analysis, however,
may require the introduction of many parameters, which lowers the statistical
weight of the model. According to the quality of data, a simpler linear
analysis may be more convenient than more complex approaches.
  In this paper, we show how a simple non-parametric Bayesian model may be used
to explore the role of non-linearities and noise in synthetic and experimental
data sets.

Multivariate methods that relate outcomes to risk factors have been adopted
clinically to individualize treatment. This has promoted the belief that
individuals have a true or unique risk.
  The logic of assigning an individual a single risk value has been criticized
since 1866. The reason is that any individual can be simultaneously considered
a member of different groups, with each group having its own risk level (the
reference class problem).
  Lemeshow et al. provided well-documented examples of remarkable discordance
between predictions for an individual by different valid predictive methods
utilizing different risk factors. The prevalence of such discordance is unknown
as it is rarely evaluated, but must be substantial due to the abundance of risk
factors.
  Lemeshow et al. cautioned against using ICU mortality predictions for the
provision of care to individual patients. If individual risk estimates are used
clinically, users should be aware that valid methods may give very different
results.

Frameshift mutations in protein-coding DNA sequences produce a drastic change
in the resulting protein sequence, which prevents classic protein alignment
methods from revealing the proteins' common origin. Moreover, when a large
number of substitutions are additionally involved in the divergence, the
homology detection becomes difficult even at the DNA level. To cope with this
situation, we propose a novel method to infer distant homology relations of two
proteins, that accounts for frameshift and point mutations that may have
affected the coding sequences. We design a dynamic programming alignment
algorithm over memory-efficient graph representations of the complete set of
putative DNA sequences of each protein, with the goal of determining the two
putative DNA sequences which have the best scoring alignment under a powerful
scoring system designed to reflect the most probable evolutionary process. This
allows us to uncover evolutionary information that is not captured by
traditional alignment methods, which is confirmed by biologically significant
examples.

We combine stroboscopic laser excitation with stochastic photoactivation and
super-resolution fluorescence imaging. This makes it possible to record
hundreds of diffusion trajectories of small protein molecules in single
bacterial cells with millisecond time resolution and sub-diffraction limited
spatial precision. We conclude that the small protein mEos2 exhibits normal
diffusion in the bacterial cytoplasm with a diffusion coefficient of 13.1 -+
1.2 \mu m^2 s^(-1). This investigation lays the groundwork for studying
single-molecule binding and dissociation events for a wide range of
intracellular processes.

We describe the fundamental difference between the nature of problems in
traditional physics and that of many problems arising today in systems biology
and other complex settings. The difference hinges on the much larger number of
a priori plausible alternative laws for explaining the phenomena at hand in the
latter case. An approach and a mathematical framework for prediction in this
hypothesis-rich regime are introduced.

Recommended standardized procedures for determining exhaled lower respiratory
nitric oxide and nasal nitric oxide have been developed by task forces of the
European Respiratory Society and the American Thoracic Society. These
recommendations have paved the way for the measurement of nitric oxide to
become a diagnostic tool for specific clinical applications. It would be
desirable to develop similar guidelines for the sampling of other trace gases
in exhaled breath, especially volatile organic compounds (VOCs) which reflect
ongoing metabolism. The concentrations of water-soluble, blood-borne substances
in exhaled breath are influenced by: (i) breathing patterns affecting gas
exchange in the conducting airways; (ii) the concentrations in the
tracheo-bronchial lining fluid; (iii) the alveolar and systemic concentrations
of the compound. The classical Farhi equation takes only the alveolar
concentrations into account. Real-time measurements of acetone in end-tidal
breath under an ergometer challenge show characteristics which cannot be
explained within the Farhi setting. Here we develop a compartment model that
reliably captures these profiles and is capable of relating breath to the
systemic concentrations of acetone. By comparison with experimental data it is
inferred that the major part of variability in breath acetone concentrations
(e.g., in response to moderate exercise or altered breathing patterns) can be
attributed to airway gas exchange, with minimal changes of the underlying blood
and tissue concentrations. Moreover, it is deduced that measured end-tidal
breath concentrations of acetone determined during resting conditions and free
breathing will be rather poor indicators for endogenous levels. Particularly,
the current formulation includes the classical Farhi and the Scheid series
inhomogeneity model as special limiting cases.

In multicellular organisms, patterns of gene expression are established in
response to gradients of signaling molecules. During fly development in early
Drosophila embryos, the Bicoid (Bcd) morphogen gradient is established within
the first hour after fertilization. Bcd acts as a transcription factor,
initiating the expression of a cascade of genes that determine the segmentation
pattern of the embryo, which serves as a blueprint for the future adult
organism. A robust understanding of the mechanisms that govern this
segmentation cascade is still lacking, and a new generation of quantitative
measurements of the spatio-temporal concentration dynamics of the individual
players of this cascade are necessary for further progress. Here we describe a
series of methods that are meant to represent a start of such a quantification
using Bcd as an example. We describe the generation of a transgenic fly line
expressing a Bcd-eGFP fusion protein, and we use this line to carefully analyze
the Bcd concentration dynamics and to measure absolute Bcd expression levels in
living fly embryos using two-photon microscopy. These experiments have proven
to be a fruitful tool generating new insights into the mechanisms that lead to
the establishment and the readout of the Bcd gradient. Generalization of these
methods to other genes in the Drosophila segmentation cascade is
straightforward and should further our understanding of the early patterning
processes and the architecture of the underlying genetic network structure.

Term enrichment analysis facilitates biological interpretation by assigning
to experimentally/computationally obtained data annotation associated with
terms from controlled vocabularies. This process usually involves obtaining
statistical significance for each vocabulary term and using the most
significant terms to describe a given set of biological entities, often
associated with weights. Many existing enrichment methods require selections of
(arbitrary number of) the most significant entities and/or do not account for
weights of entities. Others either mandate extensive simulations to obtain
statistics or assume normal weight distribution. In addition, most methods have
difficulty assigning correct statistical significance to terms with few
entities. Implementing the well-known Lugananni-Rice formula, we have developed
a novel approach, called SaddleSum, that is free from all the aforementioned
constraints and evaluated it against several existing methods. With entity
weights properly taken into account, SaddleSum is internally consistent and
stable with respect to the choice of number of most significant entities
selected. Making few assumptions on the input data, the proposed method is
universal and can thus be applied to areas beyond analysis of microarrays.
Employing asymptotic approximation, SaddleSum provides a term-size dependent
score distribution function that gives rise to accurate statistical
significance even for terms with few entities. As a consequence, SaddleSum
enables researchers to place confidence in its significance assignments to
small terms that are often biologically most specific.

Investigating the relation between the structure and behavior of complex
biological networks often involves posing the following two questions: Is a
hypothesized structure of a regulatory network consistent with the observed
behavior? And can a proposed structure generate a desired behavior? Answering
these questions presupposes that we are able to test the compatibility of
network structure and behavior. We cast these questions into a parameter search
problem for qualitative models of regulatory networks, in particular
piecewise-affine differential equation models. We develop a method based on
symbolic model checking that avoids enumerating all possible parametrizations,
and show that this method performs well on real biological problems, using the
IRMA synthetic network and benchmark experimental data sets. We test the
consistency between the IRMA network structure and the time-series data, and
search for parameter modifications that would improve the robustness of the
external control of the system behavior.

Simulated evolution of biological networks can be used to generate functional
networks as well as investigate hypotheses regarding natural evolution. A
handful of studies have shown how simulated evolution can be used for studying
the functional space spanned by biochemical networks, studying natural
evolution, or designing new synthetic networks. If there was a method for
easily performing such studies, it can allow the community to further
experiment with simulated evolution and explore all of its uses. As a result,
we have developed a library written in the C language that performs all the
basic functions needed to carry out simulated evolution of biological networks.
The library comes with a generic genetic algorithm as well as genetic
algorithms for specifically evolving genetic networks, protein networks, or
mass-action networks. The library also comes with functions for simulating
these networks. A user needs to specify a desired function. A GUI is provided
for users to become oriented with all the options available in the library. The
library is free and open source under the BSD lisence and can be obtained at
evolvenetworks.sourceforge.net. It can be built on all major platforms. The
code can be most conveniently compiled using cross-platform make (CMake).

We introduce the software tool NTRFinder to find the complex repetitive
structure in DNA we call a nested tandem repeat (NTR). An NTR is a recurrence
of two or more distinct tandem motifs interspersed with each other. We propose
that nested tandem repeats can be used as phylogenetic and population markers.
We have tested our algorithm on both real and simulated data, and present some
real nested tandem repeats of interest. We discuss how the NTR found in the
ribosomal DNA of taro (Colocasia esculenta) may assist in determining the
cultivation prehistory of this ancient staple food crop. NTRFinder can be
downloaded from http://www.maths.otago.ac.nz/? aamatroud/.

Although computationally aligning sequence is a crucial step in the vast
majority of comparative genomics studies our understanding of alignment biases
still needs to be improved. To infer true structural or homologous regions
computational alignments need further evaluation. It has been shown that the
accuracy of aligned positions can drop substantially in particular around gaps.
Here we focus on re-evaluation of score-based alignments with affine gap
penalty costs. We exploit their relationships with pair hidden Markov models
and develop efficient algorithms by which to identify gaps which are
significant in terms of length and multiplicity. We evaluate our statistics
with respect to the well-established structural alignments from SABmark and
find that indel reliability substantially increases with their significance in
particular in worst-case twilight zone alignments. This points out that our
statistics can reliably complement other methods which mostly focus on the
reliability of match positions.

We consider here the problem of chaining seeds in ordered trees. Seeds are
mappings between two trees Q and T and a chain is a subset of non overlapping
seeds that is consistent with respect to postfix order and ancestrality. This
problem is a natural extension of a similar problem for sequences, and has
applications in computational biology, such as mining a database of RNA
secondary structures. For the chaining problem with a set of m constant size
seeds, we describe an algorithm with complexity O(m2 log(m)) in time and O(m2)
in space.

We consider the optimal strategy for laboratory testing of biological samples
when we wish to know the results for each sample rather than the average
prevalence of positive samples. If the proportion of positive samples is low
considerable resources may be devoted to testing samples most of which are
negative. An attractive strategy is to pool samples. If the pooled samples test
positive one must then test the individual samples, otherwise they can all be
assumed to be negative. The pool should be big enough to reduce the number of
tests but not so big that the pooled samples are almost all positive. We show
that if the prevalence of positive samples is greater than 30% it is never
worth pooling. From 30% down to 1% pools of size 4 are close to optimal. Below
1% substantial gains can be made by pooling, especially if the samples are
pooled twice. However, with large pools the sensitivity of the test will fall
correspondingly and this must be taken into consideration. We derive simple
expressions for the optimal pool size and for the corresponding proportion of
samples tested.

We consider novel phylogenetic models with rate matrices that arise via the
embedding of a progenitor model on a small number of character states, into a
target model on a larger number of character states. Adapting
representation-theoretic results from recent investigations of Markov
invariants for the general rate matrix model, we give a prescription for
identifying and counting Markov invariants for such `symmetric embedded'
models, and we provide enumerations of these for low-dimensional cases. The
simplest example is a target model on 3 states, constructed from a general 2
state model; the `2->3' embedding. We show that for 2 taxa, there exist two
invariants of quadratic degree, that can be used to directly infer pairwise
distances from observed sequences under this model. A simple simulation study
verifies their theoretical expected values, and suggests that, given the
appropriateness of the model class, they have greater statistical power than
the standard (log) Det invariant (which is of cubic degree for this case).

Yeast glycolysis is considered the prototype of dissipative biochemical
oscillators. In cellular conditions, under sinusoidal source of glucose, the
activity of glycolytic enzymes can display either periodic, quasiperiodic or
chaotic behavior.
  In order to quantify the functional connectivity for the glycolytic enzymes
in dissipative conditions we have analyzed different catalytic patterns using
the non-linear statistical tool of Transfer Entropy. The data were obtained by
means of a yeast glycolytic model formed by three delay differential equations
where the enzymatic speed functions of the irreversible stages have been
explicitly considered. These enzymatic activity functions were previously
modeled and tested experimentally by other different groups. In agreement with
experimental conditions, the studied time series corresponded to a
quasi-periodic route to chaos. The results of the analysis are three-fold:
first, in addition to the classical topological structure characterized by the
specific location of enzymes, substrates, products and feedback regulatory
metabolites, an effective functional structure emerges in the modeled
glycolytic system, which is dynamical and characterized by notable variations
of the functional interactions. Second, the dynamical structure exhibits a
metabolic invariant which constrains the functional attributes of the enzymes.
Finally, in accordance with the classical biochemical studies, our numerical
analysis reveals in a quantitative manner that the enzyme phosphofructokinase
is the key-core of the metabolic system, behaving for all conditions as the
main source of the effective causal flows in yeast glycolysis.

The goal of the work is to implement molecular phylogenetic calculations
using the Grid paradigm by means of the MrBayes software using Directed Acyclic
Graphs (DAG) jobs. In this method, a set of jobs depends on the input or the
output of other jobs. Once the runs have been successfully done, all the
results can be collected by a specific Perl script inside the defined DAG job.
For testing this methodology, we calculate the evolution of papillomavirus with
121 sequences.

This document is an introduction to the use of the point-centered quarter
method. It briefly outlines its history, its methodology, and some of the
practical issues (and modifications) that inevitably arise with its use in the
field. Additionally this paper shows how data collected using point-centered
quarter method sampling may be used to determine importance values of different
species of trees and describes and derives several methods of estimating plant
density and corresponding confidence intervals. New to this revision is an
appendix of R functions to carry out these calculations.

We present a new method for inferring hidden Markov models from noisy time
sequences without the necessity of assuming a model architecture, thus allowing
for the detection of degenerate states. This is based on the statistical
prediction techniques developed by Crutchfield et al., and generates so called
causal state models, equivalent to hidden Markov models. This method is
applicable to any continuous data which clusters around discrete values and
exhibits multiple transitions between these values such as tethered particle
motion data or Fluorescence Resonance Energy Transfer (FRET) spectra. The
algorithms developed have been shown to perform well on simulated data,
demonstrating the ability to recover the model used to generate the data under
high noise, sparse data conditions and the ability to infer the existence of
degenerate states. They have also been applied to new experimental FRET data of
Holliday Junction dynamics, extracting the expected two state model and
providing values for the transition rates in good agreement with previous
results and with results obtained using existing maximum likelihood based
methods.

Background: The vast computational resources that became available during the
past decade enabled the development and simulation of increasingly complex
mathematical models of cancer growth. These models typically involve many free
parameters whose determination is a substantial obstacle to model development.
Direct measurement of biochemical parameters in vivo is often difficult and
sometimes impracticable, while fitting them under data-poor conditions may
result in biologically implausible values.
  Results: We discuss different methodological approaches to estimate
parameters in complex biological models. We make use of the high computational
power of the Blue Gene technology to perform an extensive study of the
parameter space in a model of avascular tumor growth. We explicitly show that
the landscape of the cost function used to optimize the model to the data has a
very rugged surface in parameter space. This cost function has many local
minima with unrealistic solutions, including the global minimum corresponding
to the best fit.
  Conclusions: The case studied in this paper shows one example in which model
parameters that optimally fit the data are not necessarily the best ones from a
biological point of view. To avoid force-fitting a model to a dataset, we
propose that the best model parameters should be found by choosing, among
suboptimal parameters, those that match criteria other than the ones used to
fit the model. We also conclude that the model, data and optimization approach
form a new complex system, and point to the need of a theory that addresses
this problem more generally.

In this Chapter, we ask questions (1) What is the right way to measure the
quality of information processing in a biological system? and (2) What can
real-life organisms do in order to improve their performance in
information-processing tasks? We then review the body of work that investigates
these questions experimentally, computationally, and theoretically in
biological domains as diverse as cell biology, population biology, and
computational neuroscience

Conan is a C++ library created for the accurate and efficient modelling,
inference and analysis of complex networks. It implements the generation and
modification of graphs according to several published models, as well as the
unexpensive computation of global and local network properties. Other features
include network inference and community detection. Furthermore, Conan provides
a Python interface to facilitate the use of the library and its integration in
currently existing applications.
  Conan is available at http://github.com/rhz/conan/.

We have shown elsewhere that the presence of mixed-culture growth of
microbial species in fermentation processes can be detected with high accuracy
by employing the wavelet transform. This is achieved because the crosses in the
different growth processes contributing to the total biomass signal appear as
singularities that are very well evidenced through their singularity cones in
the wavelet transform. However, we used very simple two-species cases. In this
work, we extend the wavelet method to a more complicated illustrative
fermentation case of three microbial species for which we employ several
wavelets of different number of vanishing moments in order to eliminate
possible numerical artifacts. Working in this way allows to filter in a more
precise way the numerical values of the H\"older exponents. Therefore, we were
able to determine the characteristic H\"older exponents for the corresponding
crossing singularities of the microbial growth processes and their stability
logarithmic scale ranges up to the first decimal in the value of the
characteristic exponents. Since calibrating the mixed microbial growth by means
of their H\"older exponents could have potential industrial applications, the
dependence of the H\"older exponents on the kinetic and physical parameters of
the growth models remains as a future experimental task

The microscopic green alga Ostreococcus tauri is rapidly emerging as a
promising model organism in the green lineage. In particular, recent results by
Corellou et al. [Plant Cell, 21, 3436 (2009)] and Thommen et al. [PLoS Comput.
Biol. 6, e1000990 (2010)] strongly suggest that its circadian clock is a
simplified version of Arabidopsis thaliana clock, and that it is architectured
so as to be robust to natural daylight fluctuations. In this work, we analyze
time series data from luminescent reporters for the two central clock genes
TOC1 and CCA1 and correlate them with microarray data previously analyzed. Our
mathematical analysis strongly supports both the existence of a simple two-gene
oscillator at the core of Ostreococcus tauri clock and the fact that its
dynamics is not affected by light in normal entrainment conditions, a signature
of its robustness.

This work emphasizes the assets of implementing the distributed computing for
the intensive use in computational science devoted to the search of new
medicines that could be applied in public healthy problems.

There are many instances in genetics in which we wish to determine whether
two candidate populations are distinguishable on the basis of their genetic
structure. Examples include populations which are geographically separated,
case--control studies and quality control (when participants in a study have
been genotyped at different laboratories). This latter application is of
particular importance in the era of large scale genome wide association
studies, when collections of individuals genotyped at different locations are
being merged to provide increased power. The traditional method for detecting
structure within a population is some form of exploratory technique such as
principal components analysis. Such methods, which do not utilise our prior
knowledge of the membership of the candidate populations. are termed
\emph{unsupervised}. Supervised methods, on the other hand are able to utilise
this prior knowledge when it is available.
  In this paper we demonstrate that in such cases modern supervised approaches
are a more appropriate tool for detecting genetic differences between
populations. We apply two such methods, (neural networks and support vector
machines) to the classification of three populations (two from Scotland and one
from Bulgaria). The sensitivity exhibited by both these methods is considerably
higher than that attained by principal components analysis and in fact
comfortably exceeds a recently conjectured theoretical limit on the sensitivity
of unsupervised methods. In particular, our methods can distinguish between the
two Scottish populations, where principal components analysis cannot. We
suggest, on the basis of our results that a supervised learning approach should
be the method of choice when classifying individuals into pre-defined
populations, particularly in quality control for large scale genome wide
association studies.

Nonlinear mixed effects models represent a powerful tool to simultaneously
analyze data from several individuals. In this study a compartmental model of
leucine kinetics is examined and extended with a stochastic differential
equation to model non-steady state concentrations of free leucine in the
plasma. Data obtained from tracer/tracee experiments for a group of healthy
control individuals and a group of individuals suffering from diabetes mellitus
type 2 are analyzed. We find that the interindividual variation of the model
parameters is much smaller for the nonlinear mixed effects models, compared to
traditional estimates obtained from each individual separately. Using the mixed
effects approach, the population parameters are estimated well also when only
half of the data are used for each individual. For a typical individual the
amount of free leucine is predicted to vary with a standard deviation of 8.9%
around a mean value during the experiment. Moreover, leucine degradation and
protein uptake of leucine is smaller, proteolysis larger, and the amount of
free leucine in the body is much larger for the diabetic individuals than the
control individuals. In conclusion nonlinear mixed effects models offers
improved estimates for model parameters in complex models based on
tracer/tracee data and may be a suitable tool to reduce data sampling in
clinical studies.

We report the key findings from numerical solutions of a model of transport
within an established perfusion bioreactor design. The model includes a
complete formulation of transport with fully coupled convection-diffusion and
scaffold cell attachment. It also includes the experimentally determined
internal (Poly-L-Lactic Acid (PLLA)) scaffold boundary, together with the
external vessel and flow-port boundaries. Our findings, obtained using parallel
lattice Boltzmann equation method, relate to (i) whole-device, steady-state
flow and species distribution and (ii) the properties of the scaffold. In
particular the results identify which elements of the problem may be addressed
by coarse grained methods such as the Darcy approximation and those which
require a more complete description. The work demonstrates that appropriate
numerical modelling will make a key contribution to the design and development
of large scale bioreactors.

We propose a novel two-stage Gene Set Gibbs Sampling (GSGS) framework, to
reverse engineer signaling pathways from gene sets inferred from molecular
profiling data. We hypothesize that signaling pathways are structurally an
ensemble of overlapping linear signal transduction events which we encode as
Information Flow Gene Sets (IFGS's). We infer pathways from gene sets
corresponding to these events subjected to a random permutation of genes within
each set. In Stage I, we use a source separation algorithm to derive unordered
and overlapping IFGS's from molecular profiling data, allowing cross talk among
IFGS's. In Stage II, we develop a Gibbs sampling like algorithm, Gene Set Gibbs
Sampler, to reconstruct signaling pathways from the latent IFGS's derived in
Stage I. The novelty of this framework lies in the seamless integration of the
two stages and the hypothesis of IFGS's as the basic building blocks for signal
pathways. In the proof-of-concept studies, our approach is shown to outperform
the existing Bayesian network approaches using both continuous and discrete
data generated from benchmark networks in the DREAM initiative. We perform a
comprehensive sensitivity analysis to assess the robustness of the approach.
Finally, we implement the GSGS framework to reconstruct signaling pathways in
breast cancer cells.

I derive formulas for the electrostatic potential of a charge in or near a
membrane modeled as one or more dielectric slabs lying between two
semi-infinite dielectrics. One can use these formulas in Monte Carlo codes to
compute the distribution of ions near cell membranes more accurately than by
using Poisson-Boltzmann theory or its linearized version. Here I use them to
discuss the electric field of a uniformly charged membrane, the image charges
of an ion, the distribution of salt ions near a charged membrane, the energy of
a zwitterion near a lipid slab, and the effect of including the phosphate head
groups as thin layers of high electric permittivity.

The quasi-steady state assumption (QSSA) forms the basis for rigorous
mathematical justification of the Michaelis-Menten formalism commonly used in
modeling a broad range of intracellular phenomena. A critical supposition of
QSSA-based analyses is that the underlying biochemical reaction is
enzymatically "closed," so that free enzyme is neither added to nor removed
from the reaction over the relevant time period. Yet there are multiple
circumstances in living cells under which this assumption may not hold, e.g.
during translation of genetic elements or metabolic regulatory events. Here we
consider a modified version of the most basic enzyme-catalyzed reaction which
incorporates enzyme input and removal. We extend the QSSA to this enzymatically
"open" system, computing inner approximations to its dynamics, and we compare
the behavior of the full open system, our approximations, and the closed system
under broad range of kinetic parameters. We also derive conditions under which
our new approximations are provably valid; numerical simulations demonstrate
that our approximations remain quite accurate even when these conditions are
not satisfied. Finally, we investigate the possibility of damped oscillatory
behavior in the enzymatically open reaction.

The overwhelming amount of available scholarly literature in the life
sciences poses significant challenges to scientists wishing to keep up with
important developments related to their research, but also provides a useful
resource for the discovery of recent information concerning genes, diseases,
compounds and the interactions between them. In this paper, we describe an
algorithm called Bio-LDA that uses extracted biological terminology to
automatically identify latent topics, and provides a variety of measures to
uncover putative relations among topics and bio-terms. Relationships identified
using those approaches are combined with existing data in life science datasets
to provide additional insight. Three case studies demonstrate the utility of
the Bio-LDA model, including association predication, association search and
connectivity map generation. This combined approach offers new opportunities
for knowledge discovery in many areas of biology including target
identification, lead hopping and drug repurposing.

Synthetic Biology is the new engineering-based approach to biology that
includes applications of designing complex biological devices. At present, it
is not yet clear what will emerge as the defining principles of Synthetic
Biology. One proposed approach is to build Synthetic Biology around the
classical engineering principles of standardization, modularity/decoupling and
abstraction/modeling to facilitate component-based design. In this article we
suggest and discuss an alternative paradigm, which we call High-throughput
Biologically Optimized Search Engineering (HT-BOSE). Stemming from directed
evolution, in HT-BOSE the focal point is a biological knowledge based rational
optimization of the search process in the space of device design possibilities.
The HT-BOSE approach may also be relevant in other contexts and we briefly
highlight how it could be applicable to the development of multi-drug cocktails
in a biomedical setting.

Periodic patterns play the important regulatory and structural roles in
genomic DNA sequences. Commonly, the underlying periodicities should be
understood in a broad statistical sense, since the corresponding periodic
patterns have been strongly distorted by the random point mutations and
insertions/deletions during molecular evolution. The latent periodicities in
DNA sequences can be efficiently displayed by Fourier transform. The criteria
of significance for observed periodicities are obtained via the comparison
versus the counterpart characteristics of the reference random sequences. We
show that the restrictions imposed on the significance criteria by the rigorous
spectral sum rules can be rationally described with De Finetti distribution.
This distribution provides the convenient intermediate asymptotic form between
Rayleigh distribution and exact combinatoric theory.

We examine how the shape of cells and the geometry of experiment affect the
reaction-diffusion kinetics at the binding between target and probe molecules
on molecular biochips. In particular, we compare the binding kinetics for the
probes immobilized on surface of the semispherical and flat circular cells, the
limit of thin slab of analyte solution over probe cell as well as hemispherical
gel pads and cells printed in gel slab over a substrate. It is shown that
hemispherical geometry provides significantly faster binding kinetics and
ensures more spatially homogeneous distribution of local (from a pixel) signals
over a cell in the transient regime. The advantage of using thin slabs with
small volume of analyte solution may be hampered by the much longer binding
kinetics needing the auxiliary mixing devices. Our analysis proves that the
shape of cells and the geometry of experiment should be included to the list of
essential factors at biochip designing.

Protein structural alignment is an important problem in computational
biology. In this paper, we present first successes on provably optimal pairwise
alignment of protein inter-residue distance matrices, using the popular Dali
scoring function. We introduce the structural alignment problem formally, which
enables us to express a variety of scoring functions used in previous work as
special cases in a unified framework. Further, we propose the first
mathematical model for computing optimal structural alignments based on dense
inter-residue distance matrices. We therefore reformulate the problem as a
special graph problem and give a tight integer linear programming model. We
then present algorithm engineering techniques to handle the huge integer linear
programs of real-life distance matrix alignment problems. Applying these
techniques, we can compute provably optimal Dali alignments for the very first
time.

Irregular bone remodeling is associated with a number of bone diseases such
as osteoporosis and multiple myeloma.
  Computational and mathematical modeling can aid in therapy and treatment as
well as understanding fundamental biology. Different approaches to modeling
give insight into different aspects of a phenomena so it is useful to have an
arsenal of various computational and mathematical models.
  Here we develop a mathematical representation of bone remodeling that can
effectively describe many aspects of the complicated geometries and spatial
behavior observed.
  There is a sharp interface between bone and marrow regions. Also the surface
of bone moves in and out, i.e. in the normal direction, due to remodeling.
Based on these observations we employ the use of a level-set function to
represent the spatial behavior of remodeling. We elaborate on a temporal model
for osteoclast and osteoblast population dynamics to determine the change in
bone mass which influences how the interface between bone and marrow changes.
  We exhibit simulations based on our computational model that show the motion
of the interface between bone and marrow as a consequence of bone remodeling.
The simulations show that it is possible to capture spatial behavior of bone
remodeling in complicated geometries as they occur \emph{in vitro} and \emph{in
vivo}.
  By employing the level set approach it is possible to develop computational
and mathematical representations of the spatial behavior of bone remodeling. By
including in this formalism further details, such as more complex cytokine
interactions and accurate parameter values, it is possible to obtain
simulations of phenomena related to bone remodeling with spatial behavior much
as \emph{in vitro} and \emph{in vivo}. This makes it possible to perform
\emph{in silica} experiments more closely resembling experimental observations.

Motivation: Capillary electrophoresis (CE) of nucleic acids is a workhorse
technology underlying high-throughput genome analysis and large-scale chemical
mapping for nucleic acid structural inference. Despite the wide availability of
CE-based instruments, there remain challenges in leveraging their full power
for quantitative analysis of RNA and DNA structure, thermodynamics, and
kinetics. In particular, the slow rate and poor automation of available
analysis tools have bottlenecked a new generation of studies involving hundreds
of CE profiles per experiment.
  Results: We propose a computational method called high-throughput robust
analysis for capillary electrophoresis (HiTRACE) to automate the key tasks in
large-scale nucleic acid CE analysis, including the profile alignment that has
heretofore been a rate-limiting step in the highest throughput experiments. We
illustrate the application of HiTRACE on thirteen data sets representing 4
different RNAs, three chemical modification strategies, and up to 480 single
mutant variants; the largest data sets each include 87,360 bands. By applying a
series of robust dynamic programming algorithms, HiTRACE outperforms prior
tools in terms of alignment and fitting quality, as assessed by measures
including the correlation between quantified band intensities between replicate
data sets. Furthermore, while the smallest of these data sets required 7 to 10
hours of manual intervention using prior approaches, HiTRACE quantitation of
even the largest data sets herein was achieved in 3 to 12 minutes. The HiTRACE
method therefore resolves a critical barrier to the efficient and accurate
analysis of nucleic acid structure in experiments involving tens of thousands
of electrophoretic bands.

Many cellular behaviors are regulated by gene regulation networks, kinetics
of which is one of the main subjects in the study of systems biology. Because
of the low number molecules in these reacting systems, stochastic effects are
significant. In recent years, stochasticity in modeling the kinetics of gene
regulation networks have been drawing the attention of many researchers. This
paper is a self contained review trying to provide an overview of stochastic
modeling. I will introduce the derivation of the main equations in modeling the
biochemical systems with intrinsic noise (chemical master equation, Fokker-Plan
equation, reaction rate equation, chemical Langevin equation), and will discuss
the relations between these formulations. The mathematical formulations for
systems with fluctuations in kinetic parameters are also discussed. Finally, I
will introduce the exact stochastic simulation algorithm and the approximate
explicit tau-leaping method for making numerical simulations.

A theory for direct quantitative analysis of an antigen is proposed. It is
based on a potential homogenous immunoreaction system. It establishes an
equation to describe the concentration change of the antigen and antibody
complex. A maximum point is found in the concentration profile of the complex
which can be used to calculate the concentration of the antigen. An
experimental scheme was designed for a commercial time-resolved
fluoroimmunoassay kit for HBsAg, which is based heterogeneous immunoreaction.
The results showed that the theory is practically applicable.

To investigate possible errors, length-weight parameters from FishBase.org
were used to graph length-weight curves for six different species: channel
catfish, black crappie, largemouth bass, rainbow trout, flathead catfish, and
lake trout along with the standard weight curves (Anderson and Neumann 1996,
Bister et al. 2000). Parameters noted as doubtful by FishBase were excluded.
For each species, variations in curves were noted, and the minimum and maximum
predicted weights for a 30 cm long fish were compared with each other and with
the standard weight for that length. For lake trout, additional comparisons
were made between the parameters and study details reported in FishBase.org for
6 of 8 length-weight relationships and those reported in the reference
(Carlander 1969) for those 6 relationships. In all species studied, minimum and
maximum curves produced with the length-weight parameters at FishBase.org are
notably different from each other, and in many cases predict weights that are
clearly absurd. For example, one set of parameters predicts a 30 cm rainbow
trout weighing 44 g. For 30 cm length, the range of weights (relative to the
standard weight) for each species are: channel catfish (31.4% to 193.1%), black
crappie (54.0% to 149.0%), largemouth bass (28.8% to 130.4%), rainbow trout
(14.9% to 113.4%), flathead catfish (29.3% to 250.7%), and lake trout (44.0% to
152.7%). Length-weight tables at FishBase.org are not generally reliable and
the on-line database contains dubious parameters. Assurance of quality probably
will require a systematic review with more careful and comprehensive methods
than those currently employed.

Cellular populations are typically heterogenous collections of cells at
different points in their respective cell cycles, each with a cell cycle time
that varies from individual to individual. As a result, true single-cell
behavior, particularly that which is cell-cycle--dependent, is often obscured
in population-level (averaged) measurements. We have developed a simple
deconvolution method that can be used to remove the effects of asynchronous
variability from population-level time-series data. In this paper, we summarize
some recent progress in the development and application of our approach, and
provide technical updates that result in increased biological fidelity. We also
explore several preliminary validation results and discuss several ongoing
applications that highlight the method's usefulness for estimating parameters
in differential equation models of single-cell gene regulation.

In a study of the heterogeneity in malaria infection rates among children
Smith et al.1 fitted several mathematical models to data from community studies
in Africa. They concluded that 20% of children receive 80% of infections, that
infections last about six months on average, that children who clear infections
are not immune to new infections, and that the sensitivity and specificity of
microscopy for the detection of malaria parasites are 95.8% and 88.4%,
respectively. These findings would have important implications for disease
control, but we show here that the statistical analysis is unsound and that the
data do not support their conclusions.

At present, the best hope for eliminating HIV transmission and bringing the
epidemic of HIV to an end lies in the use of anti-retroviral therapy for
prevention, a strategy referred to variously as Test and Treat (T&T), Treatment
as Prevention (TasP) or Treatment centred Prevention (TcP). One of the key
objections to the use of T&T to stop transmission concerns the role of the
acute phase in HIV transmission. The acute phase of infection lasts for one to
three months after HIV-seroconversion during which time the risk of
transmission may be ten to twenty times higher, per sexual encounter, than it
is during the chronic phase which lasts for the next ten years. Regular testing
for HIV is more likely to miss people who are in the acute phase than in the
chronic phase and it is essential to determine the extent to which this might
compromise the impact of T&T on HIV-transmission.
  Here we show that 1) provided the initial epidemic doubling time is about 1.0
to 1.5 years, as observed in South Africa, random testing with an average test
interval of one year will still bring the epidemic close to elimination even if
the acute phase lasts for 3 months during which time transmission is 26 times
higher than in the chronic phase; 2) testing people regularly at yearly
intervals is significantly more effective then testing them randomly; 3)
testing people regularly at six monthly intervals and starting them on ART
immediately, will almost certainly guarantee elimination.
  In general it seems unlikely that elevated transmission during the acute
phase is likely to change predictions of the impact of treatment on
transmission significantly. Other factors, in particular age structure, the
structure of sexual networks and variation in set-point viral load are likely
to be more important and should be given priority in further analyses.

We extend an hypergraph representation, introduced by Finkelstein and
Roytberg, to unify dynamic programming algorithms in the context of RNA folding
with pseudoknots. Classic applications of RNA dynamic programming energy
minimization, partition function, base-pair probabilities...) are reformulated
within this framework, giving rise to very simple algorithms. This
reformulation allows one to conceptually detach the conformation space/energy
model -- captured by the hypergraph model -- from the specific application,
assuming unambiguity of the decomposition. To ensure the latter property, we
propose a new combinatorial methodology based on generating functions. We
extend the set of generic applications by proposing an exact algorithm for
extracting generalized moments in weighted distribution, generalizing a prior
contribution by Miklos and al. Finally, we illustrate our full-fledged
programme on three exemplary conformation spaces (secondary structures,
Akutsu's simple type pseudoknots and kissing hairpins). This readily gives sets
of algorithms that are either novel or have complexity comparable to classic
implementations for minimization and Boltzmann ensemble applications of dynamic
programming.

Wood-decay fungi decompose their substrate by extracellular, degradative
enzymes and play an important role in natural ecosystems by recycling carbon
and minerals fixed in plants. Thereby, they cause significant damage to the
wood structure and limit the use of wood as building material. Besides their
role as biodeteriorators wood-decay fungi can be used for biotechnological
purposes, e.g. the white-rot fungus Physisporinus vitreus for improving the
uptake of preservatives and wood-modification substances of refractory wood.
Therefore, the visualization and the quantification of microscopic decay
patterns are important for the study of the impact of wood-decay fungi in
general, as well as for wood-decay fungi and microorganisms with possible
applications in biotechnology. In the present work, we developed a method for
the automated localization and quantification of microscopic cell wall elements
(CWE) of Norway spruce wood such as bordered pits, intrinsic defects, hyphae or
alterations induced by P. vitreus using high resolution X-ray computed
tomographic microscopy. In addition to classical destructive wood anatomical
methods such as light or laser scanning microscopy, our method allows for the
first time to compute the properties (e.g. area, orientation and
size-distribution) of CWE of the tracheids in a sample. This is essential for
modeling the influence of microscopic CWE to macroscopic properties such as
wood strength and permeability.

We consider a mathematical model comprising of four coupled ordinary
differential equations (ODEs) for studying the hepatitis C (HCV) viral
dynamics. The model embodies the efficacies of a combination therapy of
interferon and ribavirin. A condition for the stability of the uninfected and
the infected steady states is presented. A large number of sample points for
the model parameters (which were physiologically feasible) were generated using
Latin hypercube sampling. Analysis of our simulated values indicated
approximately 24% cases as having an uninfected steady state. Statistical tests
like the chi-square-test and the Spearman's test were also done on the sample
values. The results of these tests indicate a distinctly differently
distribution of certain parameter values and not in case of others, vis-a-vis,
the stability of the uninfected and the infected steady states.

The traditional power law model, W(L) = aL^b, is widely applied to describe
weight (W) vs. length (L) in fish. The model, W(L) = (L/L1)^b, is proposed as
an improvement. The Levenberg-Marquardt non-linear least squares technique is
used to determine the best-fit parameters L1 and b. This model has the
advantages that L1 has the same units (length) independent of the value of the
exponent and has an easily interpreted physical meaning as the typical length
of a fish with one unit of weight. This proposed model is compared with the
traditional model on length-weight data sets for black crappie, largemouth
bass, chain pickerel, yellow perch, and brown bullhead obtained from Stilwell
Reservoir, West Point, New York. The resulting best-fit parameters, parameter
standard errors, and covariances are compared between the two models. The
average relative weight for these species is determined, along with typical
meat yields for four species. For the five species, using the logarithmic
approach and a linear least-squares, standard errors in the coefficient, a,
range from 60.2% to 136.5% for the traditional model. Using a non-linear least
squares technique to determine best fit parameters, the standard errors for the
coefficient, a, range from 68.5% to 164.0% in the traditional model. In the
improved model, standard errors in the parameter L1 range from 0.94% to 15.0%.
The covariance between a and b in the traditional model has a magnitude between
0.999 and 1.000 in both linear and non-linear parameter estimation methods. In
the improved model, the covariances between L1 and b are smaller. The improved
model, W(L) = (L/L1)^b, is preferable for weight vs. length in fish, because
the estimated parameter uncertainties and covariances are smaller in magnitude.
Furthermore, the parameters both have consistent units and an easily
interpreted physical meaning.

Mathematical models of stem cell differentiation are commonly based upon the
concept of subsequent cell fate decisions, each controlled by a gene regulatory
network. These networks exhibit a multistable behavior and cause the system to
switch between qualitatively distinct stable steady states. However, the
network structure of such a switching module is often uncertain, and there is
lack of knowledge about the exact reaction kinetics. In this paper, we
therefore perform an elementary study of small networks consisting of three
interacting transcriptional regulators responsible for cell differentiation: We
investigate which network structures can reproduce a certain multistable
behavior, and how robustly this behavior is realized by each network. In order
to approach these questions, we use a modeling framework which only uses
qualitative information about the network, yet allows model discrimination as
well as to evaluate the robustness of the desired multistability properties. We
reveal structural network properties which are necessary and sufficient to
realize distinct steady state patterns required for cell differentiation. Our
results also show that structural and robustness properties of the networks are
related to each other.

Motivated by the biologically important and complex phenomena of A\beta\
peptide aggregation in Alzheimer's disease, we introduce a model and simulation
methodology for studying protein aggregation that includes extra-cellular
aggregation, aggregation on the cell-surface assisted by a membrane bound
protein, and in addition, supply, clearance, production and sequestration of
peptides and proteins. The model is used to produce equilibrium and
kinetic-aggregation phase diagrams for aggregation onset and of reduced stable
A\beta\ monomer concentrations due to aggregation. The methodology we
implemented permits modeling of a phenomenon involving orders of magnitude
differences in time scales and concentrations which can be retained in the
simulation. We demonstrate how to identify ranges of parameter values that give
monomer concentration depletion upon aggregation similar to that observed in
Alzheimer's disease. We show how very different behavior can be obtained as
reaction parameters and protein concentrations vary, and discuss the difficulty
reconciling results of experiments from two vastly different concentration
regimes. The latter is an important general issue in relating in-vitro and mice
based experiments to humans.

Carlander's Handbook of Freshwater Fishery Biology (1969) contains life
history data from many species of freshwater fish found in North America. It
has been cited over 1200 times and used to produce standard-weight curves for
some species. Recent work (Cole-Fletcher et al. 2011) suggests Carlander (1969)
contains numerous errors in listed weight-length equations. This paper assesses
the weight-length relationships listed in Carlander for muskellunge, northern
pike, and chain pickerel by comparing graphs of the weight vs. length equations
with other data listed and with standard weight curves published by independent
sources. A number of discrepancies are identified through this analysis and new
weight-length relationships are produced from listed data.

We analyzed the periodic patterns in E. coli promoters and compared the
distributions of the corresponding patterns in promoters and in the complete
genome to elucidate their function. Except the three-base periodicity,
coincident with that in the coding regions and growing stronger in the region
downstream from the transcriptions start (TS), all other salient periodicities
are peaked upstream of TS. We found that helical periodicities with the lengths
about B-helix pitch ~10.2-10.5 bp and A-helix pitch ~10.8-11.1 bp coexist in
the genomic sequences. We mapped the distributions of stretches with A-, B-,
and Z- like DNA periodicities onto E.coli genome. All three periodicities tend
to concentrate within non-coding regions when their intensity becomes stronger
and prevail in the promoter sequences. The comparison with available
experimental data indicates that promoters with the most pronounced
periodicities may be related to the supercoiling-sensitive genes.

For velocity-jump Markov processes with equivariant internal dynamics, we
remark that population distributions are invariant. This provides a
formalization of the fact that FCD (scale) and other symmetry invariant systems
perform identical spatial searches under input transformations.

We propose an Individual-Based Model of ant-trail formation. The ants are
modeled as self-propelled particles which deposit directed pheromones and
interact with them through alignment interaction. The directed pheromones
intend to model pieces of trails, while the alignment interaction translates
the tendency for an ant to follow a trail when it meets it. Thanks to adequate
quantitative descriptors of the trail patterns, the existence of a phase
transition as the ant-pheromone interaction frequency is increased can be
evidenced. Finally, we propose both kinetic and fluid descriptions of this
model and analyze the capabilities of the fluid model to develop trail
patterns. We observe that the development of patterns by fluid models require
extra trail amplification mechanisms that are not needed at the
Individual-Based Model level.

We examine two models for hepatitis C viral (HCV) dynamics, one for
monotherapy with interferon (IFN) and the other for combination therapy with
IFN and ribavirin. Optimal therapy for both the models is determined using the
steepest gradient method, by defining an objective functional which minimizes
the infected hepatocyte levels, virion population and the side-effects of the
drug(s). The optimal therapy for both the models shows an initial period of
high efficacy, followed by a gradual decline. The period of high efficacy
coincides with a significant decrease in the infected hepatocyte levels as well
as viral load, whereas the efficacy drops after liver regeneration through
restored hepatocyte levels. The period of high efficacy is not altered
significantly when the cost coefficients are varied, as long as the side
effects are relatively small. This suggests a higher dependence of the optimal
therapy on the model parameters in case of drugs with minimal side effects.
  We use the Latin hypercube sampling technique to randomly generate a large
number of patient scenarios (i.e, model parameter sets) and study the dynamics
of each set under the optimal therapy already determined. Results show an
increase in the percentage of responders (as indicated by drop in viral load
below detection levels) in case of combination therapy as compared to
monotherapy. Statistical tests performed to study the correlations between
sample parameters and the time required for the viral load to fall below
detection level, show a strong monotonic correlation with the death rate of
infected hepatocytes, identifying it to be an important factor in deciding
individual drug regimens.

Within the preprocessing pipeline of a Next Generation Sequencing sample, its
set of Single-Base Mismatches is one of the first outcomes, together with the
number of correctly aligned reads. The union of these two sets provides a 4x4
matrix (called Single Base Indicator, SBI in what follows) representing a
blueprint of the sample and its preprocessing ingredients such as the
sequencer, the alignment software, the pipeline parameters. In this note we
show that, under the same technological conditions, there is a strong relation
between the SBI and the biological nature of the sample. To reach this goal we
need to introduce a similarity measure between SBIs: we also show how two
measures commonly used in machine learning can be of help in this context.

In several countries in southern Africa, including South Africa, the
prevalence of HIV remains stubbornly high in spite of considerable efforts to
reduce transmission and to provide anti-retroviral therapy (ART). It is
important to know the extent to which the high prevalence of HIV reflects the
increasing number of people on ART in which case the prevalence of those not on
ART may be falling. Unfortunately, direct measures of the proportion of
HIV-positive people who are on ART are lacking in most countries and we need to
use dynamical models to estimate the impact of ART on the prevalence of HIV. In
this paper we show that the current level of ART provision in South Africa has
probably reduced the prevalence of HIV among those not on ART by 1.9 million,
averted 259 thousand new infections and 428 thousand deaths.

The magnitude of traction forces exerted by living animal cells on their
environment is a monotonically increasing and approximately sigmoidal function
of the stiffness of the external medium. This observation is rationalized using
active matter theory: adaptation to substrate rigidity results from an
interplay between passive elasticity and active contractility.

Genotyping errors are known to influence the power of both family-based and
case-control studies in the genetics of complex disease. Estimating genotyping
error rate in a given dataset can be complex, but when family information is
available error rates can be inferred from the patterns of Mendelian
inheritance between parents and offspring. I introduce a novel likelihood-based
method for calculating error rates from family data, given known allele
frequencies. I apply this to an example dataset, demonstrating a low genotyping
error rate in genotyping data from a personal genomics company.

We derive an exact Green's function of the diffusion equation for a pair of
spherical interacting particles in 2D subject to a back-reaction boundary
condition.

The paper presents an algorithm for syndromic surveillance of an epidemic
outbreak formulated in the context of stochastic nonlinear filtering. The
dynamics of the epidemic is modeled using a generalized compartmental
epidemiological model with inhomogeneous mixing. The syndromic (typically
non-medical) observations of the number of infected people (e.g. visits to
pharmacies, sale of certain products, absenteeism from work/study etc.) are
used for estimation. The state of the epidemic, including the number of
infected people and the unknown parameters of the model, are estimated via a
particle filter. The numerical results indicate that the proposed framework can
provide useful early prediction of the epidemic peak if the uncertainty in
prior knowledge of model parameters is not excessive.

Summary: CytoSaddleSum provides Cytoscape users with access to the
functionality of SaddleSum, a functional enrichment tool based on sum-of-weight
scores. It operates by querying SaddleSum locally (using the standalone
version) or remotely (through an HTTP request to a web server). The functional
enrichment results are shown as a term relationship network, where nodes
represent terms and edges show term relationships. Furthermore, query results
are written as Cytoscape attributes allowing easy saving, retrieval and
integration into network-based data analysis workflows.
  Availability: www.ncbi.nlm.nih.gov/CBBresearch/Yu/downloads The source code
is placed in Public Domain.

The toxins associated with infectious diseases are potential targets for
inhibitors which have the potential for prophylactic or therapeutic use. Many
antibodies have been generated for this purpose, and the objective of this
study was to develop a simple mathematical model that may be used to evaluate
the potential protective effect of antibodies. This model was used to evaluate
the contributions of antibody affinity and concentration to reducing
antibody-receptor complex formation and internalization. The model also enables
prediction of the antibody kinetic constants and concentration required to
provide a specified degree of protection. We hope that this model, once
validated experimentally, will be a useful tool for in vitro selection of
potentially protective antibodies for progression to in vivo evaluation.

Several authors have hypothesized that ecological systems are subject to
thermodynamic optimization, which, if proven correct, could represent a long
sought general principle of organization in ecology. Although there have been
recent advances, this still remains as an unresolved topic, and ecologists lack
a general method to test thermodynamic optimization hypotheses in specific
systems. Here we present a general, novel approach that allows generating a
null model for testing thermodynamic optimization on ecological systems. We
first describe the general methodology, which is based in the analysis of a
parametrized mathematical model of the system and the explicit consideration of
constraints. Next we present an application example to an animal population
using a general age-structured population model and physiological parameters
from the literature. We finalize discussing the relevance of this work in the
context of the current state of ecology, and implications for the further
development of a thermodynamic ecological theory.

We present a numerically efficient method to reconstruct a disordered network
of thin biopolymers, such as collagen gels, from three-dimensional (3D) image
stacks recorded with a confocal microscope. Our method is based on a template
matching algorithm that simultaneously performs a binarization and
skeletonization of the network. The size and intensity pattern of the template
is automatically adapted to the input data so that the method is scale
invariant and generic. Furthermore, the template matching threshold is
iteratively optimized to ensure that the final skeletonized network obeys a
universal property of voxelized random line networks, namely, solid-phase
voxels have most likely three solid-phase neighbors in a $3\times3$
neighborhood. This optimization criterion makes our method free of user-defined
parameters and the output exceptionally robust against imaging noise.

We propose a quantitative method to estimate the statistical properties of
sets of genes for which expression data are available and co-registered to a
reference atlas of the brain. It is based on graph-theoretic properties of
co-expression coefficients between pairs of genes. We apply this method to
mouse genes from the Allen Gene Expression Atlas. Co-expression patterns of a
list of several hundreds of genes related to addiction are analyzed, using ISH
data produced for the mouse brain at the Allen Institute. It appears that large
subsets of this set of genes are much more highly co-expressed than expected by
chance.

We consider a mathematical model that describes the release of
heparin-binding growth factors from an affinity-based delivery system. In the
delivery system, heparin binds to a peptide which has been covalently
cross-linked to a fibrin matrix. Growth factor in turn binds to the heparin,
and growth factor release is governed by both binding and diffusion mechanisms,
the purpose of the binding being to slow growth factor release. The governing
mathematical model, which in its original formulation consists of five partial
differential equations, is reduced to a system of just two equations. We
identify the governing non-dimensional parameters that can be varied to tune
the growth factor release rate. In particular, we identify a parameter regime
that ensures slow passive release (usually desirable) of at least a fraction of
the growth factor. It is found that slow release is assured if the matrix is
prepared with the concentration of cross-linked peptide greatly exceeding the
dissociation constant of heparin from the peptide, and with the concentration
of heparin greatly exceeding the dissociation constant of the growth factor
from heparin. Also, for the first time, in vitro experimental release data is
directly compared with theoretical release profiles generated by the model. We
propose that the two stage release behaviour frequently seen in experiments is
due to an initial rapid out-diffusion of free growth factor over a diffusion
time scale (typically days), followed by a much slower release of the bound
fraction over a time scale depending on both diffusion and binding parameters
(frequently months).

Randomising networks using a naive `accept-all' edge-swap algorithm is
generally biased. Building on recent results for nondirected graphs, we
construct an ergodic detailed balance Markov chain with non-trivial acceptance
probabilities for directed graphs, which converges to a strictly uniform
measure and is based on edge swaps that conserve all in- and out-degrees. The
acceptance probabilities can also be generalized to define Markov chains that
target any alternative desired measure on the space of directed graphs, in
order to generate graphs with more sophisticated topological features. This is
demonstrated by defining a process tailored to the production of directed
graphs with specified degree-degree correlation functions. The theory is
implemented numerically and tested on synthetic and biological network
examples.

Cre-lox and other systems are used as genetic tools to control site-specific
recombination (SSR) events in genomic DNA. If multiple recombination sites are
organized in a compact cluster within the same genome, a series of random
recombination events may generate substantial cell specific genomic diversity.
This diversity is used, for example, to distinguish neurons in the brain of the
same multicellular mosaic organism, within the brainbow approach to neuronal
connectome. In this paper we study an exactly solvable statistical model for
SSR operating on a cluster of recombination sites. We consider two types of
recombination events: inversions and excisions. Both of these events are
available in the Cre-lox system. We derive three properties of the sequences
generated by multiple recombination events. First, we describe the set of
sequences that can in principle be generated by multiple inversions operating
on the given initial sequence. We call this description the ergodicity theorem.
On the basis of this description we calculate the number of sequences that can
be generated from an initial sequence. This number of sequences is
experimentally testable. Second, we demonstrate that after a large number of
random inversions every sequence that can be generated is generated with equal
probability. Lastly, we derive the equations for the probability to find a
sequence as a function of time in the limit when excisions are much less
frequent than inversions, such as in shufflon sequences.

CSA is a web server for the comprehensive comparison of pairwise protein
structure alignments. Its exact alignment engine computes either optimal,
top-scoring alignments or heuristic alignments with quality guarantee for the
inter-residue distance based scorings of contact map overlap, PAUL, DALI and
MATRAS. These and additional, uploaded alignments are compared using a number
of quality measures and intuitive visualizations. CSA brings new insight into
the structural relationship of the protein pairs under investigation and is a
valuable tool for studying structural similarities. It is available at
http://csa.project.cwi.nl

It is well known that individuals who abuse drugs usually use more than one
substance. Toxic consequences of single and multiple drug use are well
documented in the Treatment Episodes Data Set that lists combinations that
result in hospital admissions. Using this list as a guide, we focused our
attention on combinations that result in the most hospital admissions and
searched the PubMed database to determine the number of publications dealing
with these toxic combinations. Of special interest were those publications that
looked for or used the term synergism in their titles or abstracts, a search
that produced an extensive list of published articles. However, a further
intersection of these with the term isobole revealed a surprisingly small
number of literature reports. Because the method of isoboles is the most common
quantitative method for distinguishing between drug synergism and simple
additivity, the small number of investigations that actually employed this
quantitation suggests that the term synergism is not properly documented in
describing the toxicity among these abused substances. The possible reasons for
this lack of quantitation may be related to a misunderstanding of the modeling
equations. The theory and modeling are discussed here.

Euclidean distance geometry is the study of Euclidean geometry based on the
concept of distance. This is useful in several applications where the input
data consists of an incomplete set of distances, and the output is a set of
points in Euclidean space that realizes the given distances. We survey some of
the theory of Euclidean distance geometry and some of the most important
applications: molecular conformation, localization of sensor networks and
statics.

Here are presenting the blank based time-alignment (BBTA) as a strong
analytical approach for treatment of non-linear shift in time occurring in
HPLC-MS data. Need of such tool in recent large dataset produced by analytical
chemistry and so-called omics studies is evident. Proposed approach is based on
measurement and comparison of blank and analyzed sample evident features. In
the first step of BBTA procedure, the number of compounds is reduced by
max-to-mean ratio thresholding, which extensively reduce the computational
time. Simple thresholding is followed by selection of time markers defined from
blank inflex points which are then used for the transformation function,
polynomial of second degree, in the example. BBTA approach was compared on real
HPLC-MS measurement with Correlation Optimized Warping (COW) method. It was
proved to have distinctively shorter computational time as well as lower level
of mathematical presumptions. The BBTA is computationally much easier, quicker
(more then 1000x) and accurate in comparison with warping. Moreover, markers
selection works efficiently without any peak detection. It is sufficient to
analyze only baseline contribution in the analyte measurement with sparse
knowledge of blank behavior. Finally, BBTA does not required usage of extra
internal standards and due to its simplicity it has a potential to be
widespread tool in HPLC-MS data treatment.

A/H1N1 epidemic data from Istanbul, Turkey during the period June
2009-February 2010 is analyzed with SEIR (Susceptible-Exposed-Infected-Removed)
model. The data consist of the daily adult hospitalization numbers and
fatalities recorded in various state hospitals serving an adult population of
about 1.5-2 million. June 2009-August 2009 period corresponds to the initial
stage of the epidemic where the hospitalization rate is nearly %100 and it is
excluded from further consideration. The analysis covers the September
2009-February 2010 period, the total number of hospitalizations and fatalities
being respectively 869 and 46. It is shown that the maximum correlation between
the number of fatalities and hospitalizations occur with a time shift of 9 days
and the proportionality constant is {\delta}=0.0537. The SEIR epidemic model is
applied to the data by back-shifting the number of fatalities. The
determination of the best fitting model is based on the L2 norms of errors
between the model and the data and the errors are around %10 and %2.6 for the
number of hospitalizations and fatalities, respectively. The parameters in the
model are I0, {\eta}, {\epsilon} and {\beta}, where I0 is the percentage of
people infected initially, {\eta} and {\epsilon} are related to the inverses of
the infection and incubation periods and {\beta}/{\eta} is the representative
of the basic reproduction number. These parameters are determined as
{\eta}=0.09 (1/{\eta} =11.11days), I0=10^-7.4, {\epsilon}=0.32 (1/{\epsilon}
=3.125 days), {\beta}=0.585, {\beta}/{\eta}=6.5.

The three-dimensional data-driven Allen Gene Expression Atlas of the adult
mouse brain consists of numerized in-situ hybridization data for thousands of
genes, co-registered to the Allen Reference Atlas. We propose quantitative
criteria to rank genes as markers of a brain region, based on the localization
of the gene expression and on its functional fitting to the shape of the
region. These criteria lead to natural generalizations to sets of genes. We
find sets of genes weighted with coefficients of both signs with almost perfect
localization in all major regions of the left hemisphere of the brain, except
the pallidum. Generalization of the fitting criterion with positivity
constraint provides a lesser improvement of the markers, but requires sparser
sets of genes.

Amphiphilic peptide conjugation affords a significant increase in sensitivity
with protein quantification by electrospray-ionization mass spectrometry. This
has been demonstrated here for human growth hormone in serum using
N-(3-iodopropyl)-N,N,N-dimethyloctylammonium iodide (IPDOA-iodide) as
derivatizing reagent. The signal enhancement achieved in comparison to the
method without derivatization enables extension of the applicable concentration
range down to the very low concentrations as encountered with clinical glucose
suppression tests for patients with acromegaly. The method has been validated
using a set of serum samples spiked with known amounts of recombinant 22 kDa
growth hormone in the range of 0.48 to 7.65 \mug/L. The coefficient of
variation (CV) calculated, based on the deviation of results from the expected
concentrations, was 3.5% and the limit of quantification (LoQ) was determined
as 0.4 \mug/L. The potential of the method as a tool in clinical practice has
been demonstrated with patient samples of about 1 \mug/L.

Background: Psychedelic drugs facilitate profound changes in consciousness
and have potential to provide insights into the nature of human mental
processes and their relation to brain physiology. Yet published scientific
literature reflects a very limited understanding of the effects of these drugs,
especially for newer synthetic compounds. The number of clinical trials and
range of drugs formally studied is dwarfed by the number of written
descriptions of the many drugs taken by people. Analysis of these descriptions
using machine-learning techniques can provide a framework for learning about
these drug use experiences. Methods: We collected 1000 reports of 10 drugs from
the drug information website Erowid.org and formed a term-document frequency
matrix. Using variable selection and a random-forest classifier, we identified
a subset of words that differentiated between drugs. Results: A random forest
using a subset of 110 predictor variables classified with accuracy comparable
to a random forest using the full set of 3934 predictors. Our estimated
accuracy was 51.1%, which compares favorably to the 10% expected from chance.
Reports of MDMA had the highest accuracy at 86.9%; those describing DPT had the
lowest at 20.1%. Hierarchical clustering suggested similarities between certain
drugs, such as DMT and Salvia divinorum. Conclusion: Machine-learning
techniques can reveal consistencies in descriptions of drug use experiences
that vary by drug class. This may be useful for developing hypotheses about the
pharmacology and toxicity of new and poorly characterized drugs.

Many stochastic systems in physics and biology are investigated by recording
the two-dimensional (2D) positions of a moving test particle in regular time
intervals. The resulting sample trajectories are then used to induce the
properties of the underlying stochastic process. Often, it can be assumed a
priori that the underlying discrete-time random walk model is independent from
absolute position (homogeneity), direction (isotropy) and time (stationarity),
as well as ergodic. In this article we first review some common statistical
methods for analyzing 2D trajectories, based on quantities with built-in
rotational invariance. We then discuss an alternative approach in which the
two-dimensional trajectories are reduced to one dimension by projection onto an
arbitrary axis and rotational averaging. Each step of the resulting 1D
trajectory is further factorized into sign and magnitude. The statistical
properties of the signs and magnitudes are mathematically related to those of
the step lengths and turning angles of the original 2D trajectories,
demonstrating that no essential information is lost by this data reduction. The
resulting binary sequence of signs lends itself for a pattern counting
analysis, revealing temporal properties of the random process that are not
easily deduced from conventional measures such as the velocity autocorrelation
function. In order to highlight this simplified 1D description, we apply it to
a 2D random walk with restricted turning angles (RTA model), defined by a
finite-variance distribution $p(L)$ of step length and a narrow turning angle
distribution $p(\phi)$, assuming that the lengths and directions of the steps
are independent.

Many stochastic time series can be modelled by discrete random walks in which
a step of random sign but constant length $\delta x$ is performed after each
time interval $\delta t$. In correlated discrete time random walks (CDTRWs),
the probability $q$ for two successive steps having the same sign is unequal
1/2. The resulting probability distribution $P(\Delta x,\Delta t)$ that a
displacement $\Delta x$ is observed after a lagtime $\Delta t$ is known
analytically for arbitrary persistence parameters $q$. In this short note we
show how a CDTRW with parameters $[\delta t, \delta x, q]$ can be mapped onto
another CDTRW with rescaled parameters $[\delta t/s, \delta x\cdot g(q,s),
q^{\prime}(q,s)]$, for arbitrary scaling parameters $s$, so that both walks
have the same displacement distributions $P(\Delta x,\Delta t)$ on long time
scales. The nonlinear scaling functions $g(q,s)$ and $q^{\prime}(q,s)$ and
derived explicitely. This scaling method can be used to model time series
measured at discrete sample intervals $\delta t$ but actually corresponding to
continuum processes with variations occuring on a much shorter time scale
$\delta t/s$.

We have developed a method combining microfluidics, time-lapsed
single-molecule microscopy and automated image analysis allowing for the
observation of an excess of 3000 complete cell cycles of exponentially growing
Escherichia coli cells per experiment. The method makes it possible to analyze
the rate of gene expression at the level of single proteins over the bacterial
cell cycle. We also demonstrate that it is possible to count the number of
non-specifically DNA binding LacI-Venus molecules using short excitation light
pulses. The transcription factors are localized on the nucleoids in the cell
and appear to be uniformly distributed on chromosomal DNA. An increase of the
expression of LacI is observed at the beginning of the cell cycle, possibly
because some gene copies are de-repressed as a result of partitioning
inequalities at cell division. Finally, observe a size-growth rate uncertainty
relation where cells living in rich media vary more in the length at birth than
in generation time and the opposite is true for cells living in poorer media.

The evolution of a continuous time Markov process with a finite number of
states is usually calculated by the Master equation - a linear differential
equations with a singular generator matrix. We derive a general method for
reducing the dimensionality of the Master equation by one by using the
probability normalization constraint, thus obtaining a affine differential
equation with a (non-singular) stable generator matrix. Additionally, the
reduced form yields a simple explicit expression for the stationary probability
distribution, which is usually derived implicitly. Finally, we discuss the
application of this method to stochastic differential equations.

We study kinetic models of reversible enzyme reactions and compare two
techniques for analytic approximate solutions of the model. Analytic
approximate solutions of non-linear reaction equations for reversible enzyme
reactions are calculated using the Homotopy Perturbation Method (HPM) and the
Simple Iteration Method (SIM). The results of the approximations are similar.
The Matlab programs are included in appendices.

Stochastic rearrangement of germline DNA by VDJ recombination is at the
origin of immune system diversity. This process is implemented via a series of
stochastic molecular events involving gene choices and random nucleotide
insertions between, and deletions from, genes. We use large sequence
repertoires of the variable CDR3 region of human CD4+ T-cell receptor beta
chains to infer the statistical properties of these basic biochemical events.
Since any given CDR3 sequence can be produced in multiple ways, the probability
distribution of hidden recombination events cannot be inferred directly from
the observed sequences; we therefore develop a maximum likelihood inference
method to achieve this end. To separate the properties of the molecular
rearrangement mechanism from the effects of selection, we focus on
non-productive CDR3 sequences in T-cell DNA. We infer the joint distribution of
the various generative events that occur when a new T-cell receptor gene is
created. We find a rich picture of correlation (and absence thereof), providing
insight into the molecular mechanisms involved. The generative event statistics
are consistent between individuals, suggesting a universal biochemical process.
Our distribution predicts the generation probability of any specific CDR3
sequence by the primitive recombination process, allowing us to quantify the
potential diversity of the T-cell repertoire and to understand why some
sequences are shared between individuals. We argue that the use of formal
statistical inference methods, of the kind presented in this paper, will be
essential for quantitative understanding of the generation and evolution of
diversity in the adaptive immune system.

Dynamical systems are used to model a variety of phenomena in which the
bifurcation structure is a fundamental characteristic. Here we propose a
statistical machine-learning approach to derive lowdimensional models that
automatically integrate information in noisy time-series data from partial
observations. The method is tested using artificial data generated from two
cell-cycle control system models that exhibit different bifurcations, and the
learned systems are shown to robustly inherit the bifurcation structure.

This short article presents a mathematical formula required for metric
corrections in image extraction and processing when using different length
scale factors in three-dimensional space which is normally encountered in
cryomicrotome image construction techniques.

A lot of criticism against the standard formulation of pharmacokinetics has
been raised by several authors. It seems that the natural reaction for that
criticism is to comment it from the point of view of the theory of conservation
laws. Simple example of balance equations for the intravenous administration of
drug has been given in 2011 and the corresponding equations for extravasal
administration are in the text. In principle, the equations of that kind allow
one to describe in the self consistent manner different processes of
administration, distribution, metabolism and elimination of drugs. Moreover, it
is possible to model different pharmacokinetic parameters of the
non-compartmental pharmacokinetics and therefore to comment criticism of
Rosigno. However, for practical purposes one needs approximate methods, in
particular, those based on separation of the time scales. In this text, such
method is described and its effectiveness is discussed. Basic equations are in
the next chapter. Final remarks are at the end of the text.

The standard genetic code is known to be much more efficient in minimizing
adverse effects of misreading errors and one-point mutations in comparison with
a random code having the same structure, i.e. the same number of codons coding
for each particular amino acid. We study the inverse problem, how the code
structure affects the optimal physico-chemical parameters of amino acids
ensuring the highest stability of the genetic code. It is shown that the choice
of two or more amino acids with given properties determines unambiguously all
the others. In this sense the code structure determines strictly the optimal
parameters of amino acids. In the code with the structure of the standard
genetic code the resulting values for hydrophobicity obtained in the scheme
leave one out and in the scheme with fixed maximum and minimum parameters
correlate significantly with the natural scale. This indicates the co-evolution
of the genetic code and physico-chemical properties of amino acids.

Embryonic stem cells (ESCs) and induced pluripotent stem cells (iPSCs)
derived from somatic cells (SCs) provide promising resources for regenerative
medicine and medical research, leading to a daily identification of new cell
lines. However, an efficient system to discriminate the cell lines is lacking.
Here, we developed a quantitative system to discriminate the three cell types,
iPSCs, ESCs and SCs. The system contains DNA-methylation biomarkers and
mathematical models, including an artificial neural network and support vector
machines. All biomarkers were unbiasedly selected by calculating an eigengene
score derived from analysis of genome-wide DNA methylations. With 30
biomarkers, or even with as few as 3 top biomarkers, this system can
discriminate SCs from ESCs and iPSCs with almost 100% accuracy, and with
approximately 100 biomarkers, the system can distinguish ESCs from iPSCs with
an accuracy of 95%. This robust system performs precisely with raw data without
normalization as well as with converted data in which the continuous
methylation levels are accounted. Strikingly, this system can even accurately
predict new samples generated from different microarray platforms and the
next-generation sequencing. The subtypes of cells, such as female and male
iPSCs and fetal and adult SCs, can also be discriminated with this system.
Thus, this quantitative system works as a novel general and accurate framework
for discriminating the three cell types, iPSCs, ESCs, and SCs and this strategy
supports the notion that DNA-methylation generally varies among the three cell
types.

In recent years, we are seeing the formulation and use of elaborate and
complex models in ecological studies. The questions related to the efficient,
systematic and error-proof exploration of parameter spaces are of great
importance to better understand, estimate confidences and make use of the
output from these models. In this work, we investigate some of the relevant
questions related to parameter space exploration, in particular using the
technique known as Latin Hypercube Sampling and focusing in quantitative output
analysis. We present the analysis of a structured population growth model and
contrast our findings with results from previously used techniques, known as
sensitivity and elasticity analyses. We also assess how are the questions
related to parameter space analysis being currently addressed in the ecological
literature.

Multiple sequence alignment (MSA) is a fundamental and ubiquitous technique
in bioinformatics used to infer related residues among biological sequences.
Thus alignment accuracy is crucial to a vast range of analyses, often in ways
difficult to assess in those analyses. To compare the performance of different
aligners and help detect systematic errors in alignments, a number of
benchmarking strategies have been pursued. Here we present an overview of the
main strategies--based on simulation, consistency, protein structure, and
phylogeny--and discuss their different advantages and associated risks. We
outline a set of desirable characteristics for effective benchmarking, and
evaluate each strategy in light of them. We conclude that there is currently no
universally applicable means of benchmarking MSA, and that developers and users
of alignment tools should base their choice of benchmark depending on the
context of application--with a keen awareness of the assumptions underlying
each benchmarking strategy.

The concentration of CD4 T-lymphocytes (CD4 count), in a person's plasma is
widely used to decide when to start HIV-positive people on anti-retroviral
therapy (ART) and to predict the impact of ART on the future course of HIV and
tuberculosis (TB). However, CD4 cell-counts vary widely within and among
populations and depend on many factors besides HIV-infection. The way in which
CD4 counts decline over the course of HIV infection is neither well understood
nor widely agreed. We review what is known about CD4 counts in relation to HIV
and TB and discuss areas in which more research is needed to build a consensus
on how to interpret and use CD4 counts in clinical practice and to develop a
better understanding of the dynamics and control of HIV and HIV-related TB.

Background: Animals from the same litter are often more alike compared with
animals from different litters. This litter-to-litter variation, or "litter
effects", can influence the results in addition to the experimental factors of
interest. Furthermore, an experimental treatment can be applied to whole
litters rather than to individual offspring. For example, in the valproic acid
(VPA) model of autism, VPA is administered to pregnant females thereby inducing
the disease phenotype in the offspring. With this type of experiment the sample
size is the number of litters and not the total number of offspring. If such
experiments are not appropriately designed and analysed, the results can be
severely biased as well as extremely underpowered.
  Results: A review of the VPA literature showed that only 9% (3/34) of studies
correctly determined that the experimental unit (n) was the litter and
therefore made valid statistical inferences. In addition, litter effects
accounted for up to 61% (p <0.001) of the variation in behavioural outcomes,
which was larger than the treatment effects. In addition, few studies reported
using randomisation (12%) or blinding (18%), and none indicated that a sample
size calculation or power analysis had been conducted.
  Conclusions: Litter effects are common, large, and ignoring them can make
replication of findings difficult and can contribute to the low rate of
translating preclinical in vivo studies into successful therapies. Only a
minority of studies reported using rigorous experimental methods, which is
consistent with much of the preclinical in vivo literature.

We investigate to what extent the interaction dynamics of a population of
wild house mouse (Mus musculus domesticus) in their environment can be
explained by a simple stochastic model. We use a Markov chain model to describe
the transitions of mice in a discrete space of nestboxes, and implement a
multi-agent simulation of the model. We find that some important features of
our behavioural dataset can be reproduced using this simplified stochastic
representation, and discuss the improvements that could be made to our model in
order to increase the accuracy of its predictions. Our findings have
implications for the understanding of the complexity underlying social
behaviour in the animal kingdom and the cognitive requirements of such
behaviour.

Leaf area LA, is a plant biometric index important to agroforestry and crop
production. Previous works have demonstrated the conservativeness of the
inverse of the product of the fresh leaf density and thickness, the so-called
Hughes constant, K. We use this fact to develop LAMM, an absolute method of LA
measurement, i.e. no regression fits or prior calibrations with planimeters.
Nor does it require drying the leaves. The concept involves the in situ
determination of K using geometrical shapes and their weights obtained from a
subset of fresh leaves of the set whose areas are desired. Subsequently the
LAs, at any desired stratification level, are derived by utilizing K and the
previously measured masses of the fresh leaves. The concept was first tested in
the simulated ideal case of complete planarity and uniform thickness by using
plastic film covered card-paper sheets. Next the species-specific
conservativeness of K over individual leaf zones and different leaf types from
leaves of plants from two species, Mandevilla splendens and Spathiphyllum
wallisii, was quantitatively validated. Using the global average K values, the
LA of these and additional plants, were obtained. LAMM was found to be a rapid,
simple, economic technique with accuracies, as measured for the geometrical
shapes, that were comparable to those obtained by the planimetric method that
utilizes digital image analysis, DIA. For the leaves themselves, there were no
statistically significant differences between the LAs measured by LAMM and by
the DIA and the linear correlation between the two methods was excellent.

In the text S.Piekarski, M.Rewekant,(arXiv:1208.3847)it has been mentioned
that some information on bioavailability and bioequivalence of drugs can be
obtained from simulations based on the conservation laws. Here we shortly
discuss that possibility starting from the fundamental pharmacokinetic
parameter called AUC (Area Under the Curve). The curve is is the profile shape
of plasma drug concentration in time intervals after drug administration into
organism. Our aim here is to give some information on the subject for the
reader with no experience in pharmacokinetics.

HIV increases the likelihood that a person will develop TB. Starting them on
anti-retroviral therapy (ART) reduces their risk of TB but not to the level in
HIV negative people. Since HIV-positive people who are on ART can expect to
live a normal life for several decades this raises the possibility that their
elevated risk of infection, lasting for a long time, could lead to an increase
in the population level incidence of TB. Here we investigate the conditions
under which this could happen and show that provided HIV-positive people start
ART when their CD4+ cell count is greater than 350/microL and that there is
high coverage, ART will not lead to a long-term increase in HIV. Only if people
start ART very late and there is low coverage of ART might starting people on
ART increase the population level incidence of TB.

Parameter estimation in ordinary differential equations, although applied and
refined in various fields of the quantitative sciences, is still confronted
with a variety of difficulties. One major challenge is finding the global
optimum of a log-likelihood function that has several local optima, e.g. in
oscillatory systems. In this publication, we introduce a formulation based on
continuation of the log-likelihood function that allows to restate the
parameter estimation problem as a boundary value problem. By construction, the
ordinary differential equations are solved and the parameters are estimated
both in one step. The formulation as a boundary value problem enables an
optimal transfer of information given by the measurement time courses to the
solution of the estimation problem, thus favoring convergence to the global
optimum. This is demonstrated explicitly for the fully as well as the partially
observed Lotka-Volterra system.

Light sheet microscopy promises to revolutionize developmental biology by
enabling live in toto imaging of entire embryos with minimal phototoxicity. We
present detailed instructions for building a compact and customizable Selective
Plane Illumination Microscopy (SPIM) system. The integrated OpenSPIM hardware
and software platform is shared with the scientific community through a public
website, thereby making light sheet microscopy accessible for widespread use
and optimization to various applications.

This note works out an advection-diffusion approximation to the density of a
population of E. coli bacteria undergoing chemotaxis in a one-dimensional
space. Simulations show the high quality of predictions under a
shallow-gradient regime.

Mathematical methods together with measurements of single-cell dynamics
provide unprecedented means to reconstruct intracellular processes that are
only partly or indirectly accessible experimentally. To obtain reliable
reconstructions the pooling of measurements from several cells of a clonal
population is mandatory. The population's considerable cell-to-cell variability
originating from diverse sources poses novel computational challenges for
process reconstruction. We introduce an exact Bayesian inference framework that
properly accounts for the population heterogeneity but also retains scalability
with respect to the number of pooled cells. The key ingredient is a stochastic
process that captures the heterogeneous kinetics of a population. The method
allows to infer inaccessible molecular states, kinetic parameters, compute
Bayes factors and to dissect intrinsic, extrinsic and technical contributions
to the variability in the data. We also show how additional single-cell
readouts such as morphological features can be included into the analysis. We
then reconstruct the expression dynamics of a gene under an inducible GAL1
promoter in yeast from time-lapse microscopy data. Based on Bayesian model
selection the data yields no evidence of a refractory period for this promoter.

Responsible for many complex human diseases including cancers, disrupted or
abnormal gene interactions can be identified through their expression changes
correlating with the progression of a disease. However, the examination of all
possible combinatorial interactions between gene features in a genome-wide
case-control study is computationally infeasible as the search space is
exponential in nature.
  In this paper, we propose a novel computational approach, QUIRE, to identify
discriminative complex interactions among informative gene features for cancer
diagnosis. QUIRE works in two stages, where it first identifies functionally
relevant feature groups for the disease and, then explores the search space
capturing the combinatorial relationships among the genes from the selected
informative groups. Using QUIRE, we explore the differential patterns and the
interactions among informative gene features in three different types of
cancers, Renal Cell Carcinoma(RCC), Ovarian Cancer(OVC) and Colorectal Cancer
(CRC). Our experimental results show that QUIRE identifies gene-gene
interactions that can better identify the different cancer stages of samples
and can predict CRC recurrence and death from CRC more successfully, as
compared to other state-of-the-art feature selection methods. A literature
survey shows that many of the interactions identified by QUIRE play important
roles in the development of cancer.

In this paper, we give an overview of the differential algebra approach to
identifiability, and then note a very simple observation about input-output
equivalence and identifiability, that describes the identifiability equivalence
between input-output equivalent models. We then give several simple
consequences of this observation that can be useful in showing identifiability,
including examining non-first order ODE models, nondimensionalization and
rescaling, model reducibility, and a modular approach to evaluating
identifiability. We also examine how input-output equivalence can allow us to
generate input output equations in the differential algebra approach through a
wider range of methods (e.g. substitution and differential or standard Groebner
basis approaches).

We briefly outline an algorithm for accurate quantification of specific
binding of gold particles to fixed biological tissue samples prepared for
immuno-transmission electron microscopy (TEM). The algorithm is based on
existing protocols for rational accounting of colloidal gold particles used in
secondary antibodies for immuno-gold labeling.

Although microarrays are routine analysis tools in biomedical research, they
still yield noisy output that often requires experimental confirmation. Many
studies have aimed at optimizing probe design and statistical analysis to
tackle this problem. However, less emphasis has been placed on controlling the
noise inherent to the experimental approach. To address this problem, we
investigate here a procedure that controls for such experimental variance and
combine it with an assessment of probe performance. Two custom arrays were used
to evaluate the procedure: one based on 25mer probes from an Affymetrix design
and the other based on 60mer probes from an Agilent design. To assess
experimental variance, all probes were replicated ten times. To assess probe
performance, the probes were calibrated using a dilution series of target
molecules and the signal response was fitted to an absorption model. We found
that significant variance of the signal could be controlled by averaging across
probes and removing probes that are nonresponsive. Thus, a more reliable signal
could be obtained using our procedure than conventional approaches. We suggest
that once an array is properly calibrated, absolute quantification of signals
becomes straight forward, alleviating the need for normalization and reference
hybridizations.

In the XYZ color space, the subset of the tri-stimuli corresponding to
spike-type (monochromatic) impingement of energy is the chromaticity cone, CC.
Using a family of concentric spheres, we describe a nonlinear transformation
over the CC and construct a bijection from the CC onto the flat plane. In the
process, we open up the CC and view it as a chart on the plane. Because the map
is a bijection, the color perception information is preserved (invariant)
through the transformation. We discuss stereographic projection of the
chromaticity chart and some examples.

Exploiting the information provided by the molecular noise of a biological
process has proven to be valuable in extracting knowledge about the underlying
kinetic parameters and sources of variability from single cell measurements.
However, quantifying this additional information a priori, to decide whether a
single cell experiment might be beneficial, is currently only possibly in very
simple systems where either the chemical master equation is computationally
tractable or a Gaussian approximation is appropriate. Here we show how the
information provided by distribution measurements can be approximated from the
first four moments of the underlying process. The derived formulas are
generally valid for any stochastic kinetic model including models that comprise
both intrinsic and extrinsic noise. This allows us to propose an optimal
experimental design framework for heterogeneous cell populations which we
employ to compare the utility of dual reporter and perturbation experiments for
separating extrinsic and intrinsic noise in a simple model of gene expression.
Subsequently, we compare the information content of different experiments which
have been performed in an engineered light-switch gene expression system in
yeast and show that well chosen gene induction patterns may allow one to
identify features of the system which remain hidden in unplanned experiments.

Pharmacological challenge imaging has mapped, but rarely quantified, the
sensitivity of a biological system to a given drug. We describe a novel method
called rapid quantitative pharmacodynamic imaging. This method combines
pharmacokinetic-pharmacodynamic modeling, repeated small doses of a challenge
drug over a short time scale, and functional imaging to rapidly provide
quantitative estimates of drug sensitivity including EC50 (the concentration of
drug that produces half the maximum possible effect). We first test the method
with simulated data, assuming a typical sigmoidal dose-response curve and
assuming imperfect imaging that includes artifactual baseline signal drift and
random error. With these few assumptions, rapid quantitative pharmacodynamic
imaging reliably estimates EC50 from the simulated data, except when noise
overwhelms the drug effect or when the effect occurs only at high doses. In
preliminary fMRI studies of primate brain using a dopamine agonist, the
observed noise level is modest compared with observed drug effects, and a
quantitative EC50 can be obtained from some regional time-signal curves. Taken
together, these results suggest that research and clinical applications for
rapid quantitative pharmacodynamic imaging are realistic.

The most established method of reconstructing neural circuits from animals
involves slicing tissue very thin, then taking mosaics of electron microscope
(EM) images. To trace neurons across different images and through different
sections, these images must be accurately aligned, both with the others in the
same section and to the sections above and below. Unfortunately, sectioning and
imaging are not ideal processes - some of the problems that make alignment
difficult include lens distortion, tissue shrinkage during imaging, tears and
folds in the sectioned tissue, and dust and other artifacts. In addition the
data sets are large (hundreds of thousands of images) and each image must be
aligned with many neighbors, so the process must be automated and reliable.
This paper discusses methods of dealing with these problems, with numeric
results describing the accuracy of the resulting alignments.

Our view of the microbial world and its impact on human health is changing
radically with the ability to sequence uncultured or unculturable microbes
sampled directly from their habitats, ability made possible by fast and cheap
next generation sequencing technologies. Such recent developments represents a
paradigmatic shift in the analysis of habitat biodiversity, be it the human,
soil or ocean microbiome. We review here some research examples and results
that indicate the importance of the microbiome in our lives and then discus
some of the challenges faced by metagenomic experiments and the subsequent
analysis of the generated data. We then analyze the economic and social impact
on genomic-medicine and research in both developing and developed countries. We
support the idea that there are significant benefits in building capacities for
developing high-level scientific research in metagenomics in developing
countries. Indeed, the notion that developing countries should wait for
developed countries to make advances in science and technology that they later
import at great cost has recently been challenged.

To facilitate the analysis of large-scale high-throughput capillary
electrophoresis data, we previously proposed a suite of efficient analysis
software named HiTRACE (High Throughput Robust Analysis of Capillary
Electrophoresis). HiTRACE has been used extensively for quantitating data from
RNA and DNA structure mapping experiments, including mutate-and-map contact
inference, chromatin footprinting, the EteRNA RNA design project and other
high-throughput applications. However, HiTRACE is based on a suite of
command-line MATLAB scripts that requires nontrivial efforts to learn, use, and
extend. Here we present HiTRACE-Web, an online version of HiTRACE that includes
standard features previously available in the command-line version as well as
additional features such as automated band annotation and flexible adjustment
of annotations, all via a user-friendly environment. By making use of
parallelization, the on-line workflow is also faster than software
implementations available to most users on their local computers. Free access:
http://hitrace.org

The problem of RNA secondary structure design (also called inverse folding)
is the following: given a target secondary structure, one aims to create a
sequence that folds into, or is compatible with, a given structure. In several
practical applications in biology, additional constraints must be taken into
account, such as the presence/absence of regulatory motifs, either at a
specific location or anywhere in the sequence. In this study, we investigate
the design of RNA sequences from their targeted secondary structure, given
these additional sequence constraints. To this purpose, we develop a general
framework based on concepts of language theory, namely context-free grammars
and finite automata. We efficiently combine a comprehensive set of constraints
into a unifying context-free grammar of moderate size. From there, we use
generic generic algorithms to perform a (weighted) random generation, or an
exhaustive enumeration, of candidate sequences. The resulting method, whose
complexity scales linearly with the length of the RNA, was implemented as a
standalone program. The resulting software was embedded into a publicly
available dedicated web server. The applicability demonstrated of the method on
a concrete case study dedicated to Exon Splicing Enhancers, in which our
approach was successfully used in the design of \emph{in vitro} experiments.

This paper presents a non-parametric classification technique for identifying
a candidate bi-allelic genetic marker set that best describes disease
susceptibility in gene-gene interaction studies. The developed technique
functions by creating a mapping between inferred haplotypes and case/control
status. The technique cycles through all possible marker combination models
generated from the available marker set where the best interaction model is
determined from prediction accuracy and two auxiliary criteria including
low-to-high order haplotype propagation capability and model parsimony. Since
variable-length haplotypes are created during the best model identification,
the developed technique is referred to as a variable-length haplotype
construction for gene-gene interaction (VarHAP) technique. VarHAP has been
benchmarked against a multifactor dimensionality reduction (MDR) program and a
haplotype interaction technique embedded in a FAMHAP program in various
two-locus interaction problems. The results reveal that VarHAP is suitable for
all interaction situations with the presence of weak and strong linkage
disequilibrium among genetic markers.

Analysis of the sequence-structure relationship in RNA molecules are
essential to evolutionary studies but also to concrete applications such as
error-correction methodologies in sequencing technologies. The prohibitive
sizes of the mutational and conformational landscapes combined with the volume
of data to proceed require efficient algorithms to compute sequence-structure
properties. More specifically, here we aim to calculate which mutations
increase the most the likelihood of a sequence to a given structure and RNA
family. In this paper, we introduce RNApyro, an efficient linear-time and space
inside-outside algorithm that computes exact mutational probabilities under
secondary structure and evolutionary constraints given as a multiple sequence
alignment with a consensus structure. We develop a scoring scheme combining
classical stacking base pair energies to novel isostericity scales, and apply
our techniques to correct point-wise errors in 5s and 16s rRNA sequences. Our
results suggest that RNApyro is a promising algorithm to complement existing
tools in the NGS error-correction pipeline.

Although cancer is known to be characterized by several unifying biological
hallmarks, systems biology has had limited success in identifying molecular
signatures present in in all types of cancer. The current availability of rich
data sets from many different cancer types provides an opportunity for thorough
computational data mining in search of such common patterns. Here we report the
identification of 18 "pan-cancer" molecular signatures resulting from analysis
of data sets containing values from mRNA expression, microRNA expression, DNA
methylation, and protein activity, from twelve different cancer types. The
membership of many of these signatures points to particular biological
mechanisms related to cancer progression, suggesting that they represent
important attributes of cancer in need of being elucidated for potential
applications in diagnostic, prognostic and therapeutic products applicable to
multiple cancer types.

We study collective behavior of Brodmann regions of human cerebral cortex
using functional Magnetic Resonance Imaging (fMRI) and Random Matrix Theory
(RMT). The raw fMRI data is mapped onto the cortex regions corresponding to the
Brodmann areas with the aid of the Talairach coordinates. Principal Component
Analysis (PCA) of the Pearson correlation matrix for 41 different Brodmann
regions is carried out to determine their collective activity in the idle state
and in the active state stimulated by tapping. The collective brain activity is
identified through the statistical analysis of the eigenvectors to the largest
eigenvalues of the Pearson correlation matrix. The leading eigenvectors have a
large participation ratio. This indicates that several Broadmann regions
collectively give rise to the brain activity associated with these
eigenvectors. We apply random matrix theory to interpret the underlying
multivariate data.

Photosynthetic starch reserves that accumulate in Arabidopsis leaves during
the day decrease approximately linearly with time at night to support
metabolism and growth. We find that the rate of decrease is adjusted to
accommodate variation in the time of onset of darkness and starch content, such
that reserves last almost precisely until dawn. Generation of these dynamics
therefore requires an arithmetic division computation between the starch
content and expected time to dawn. We introduce two novel chemical kinetic
models capable of implementing analog arithmetic division. Predictions from the
models are successfully tested in plants perturbed by a night-time light period
or by mutations in starch degradation pathways. Our experiments indicate which
components of the starch degradation apparatus may be important for appropriate
arithmetic division. Our results are potentially relevant for any biological
system dependent on a food reserve for survival over a predictable time period.

Boolean networks are special types of finite state time-discrete dynamical
systems. A Boolean network can be described by a function from an n-dimensional
vector space over the field of two elements to itself. A fundamental problem in
studying these dynamical systems is to link their long term behaviors to the
structures of the functions that define them. In this paper, a method for
deriving a Boolean network's dynamical information via its disjunctive normal
form is explained. For a given Boolean network, a matrix with entries 0 and 1
is associated with the polynomial function that represents the network, then
the information on the fixed points and the limit cycles is derived by
analyzing the matrix. The described method provides an algorithm for the
determination of the fixed points from the polynomial expression of a Boolean
network. The method can also be used to construct Boolean networks with
prescribed limit cycles and fixed points. Examples are provided to explain the
algorithm.

Identifiability is a necessary condition for successful parameter estimation
of dynamic system models. A major component of identifiability analysis is
determining the identifiable parameter combinations, the functional forms for
the dependencies between unidentifiable parameters. Identifiable combinations
can help in model reparameterization and also in determining which parameters
may be experimentally measured to recover model identifiability. Several
numerical approaches to determining identifiability of differential equation
models have been developed, however the question of determining identifiable
combinations remains incompletely addressed. In this paper, we present a new
approach which uses parameter subset selection methods based on the Fisher
Information Matrix, together with the profile likelihood, to effectively
estimate identifiable combinations. We demonstrate this approach on several
example models in pharmacokinetics, cellular biology, and physiology.

The development of potent drugs for the control of viraemia in people living
with HIV means that infected people may live a normal, healthy life and offers
the prospect of eliminating HIV transmission in the short term and HIV
infection in the long term. Other interventions, including the use of condoms,
pre-exposure prophylaxis, treatment of sexually transmitted infections and
behaviour change programmes, may also be effective in reducing HIV transmission
to varying degrees.
  Here we examine recommendations for when to start treatment with
anti-retroviral drugs, estimate the impact that treatment may have on HIV
transmission in the short and in the long term, and compare the impact and cost
of treatment with that of other methods of control. We focus on generalized HIV
epidemics in sub-Saharan Africa. We show that universal access to ART combined
with early treatment is the most effective and, in the long term, the most
cost-effective intervention. Elimination will require effective coverage of
about 80% or more but treatment is effective and cost effective even at low
levels of coverage.
  Other interventions may provide important support to a programme of early
treatment in particular groups. Condoms provide protection for both men and
women and should be readily available whenever they are needed. Medical male
circumcision will provide a degree of immediate protection for men and
microbicides will do the same for women. Behaviour change programmes in
themselves are unlikely to have a significant impact on overall transmission
but may play a critical role in supporting early treatment through helping to
avoid stigma and discrimination, ensuring the acceptability of testing and
early treatment as well as compliance.

Accurate measurements of kinetic rate constants for interacting biomolecules
is crucial for understanding the mechanisms underlying intracellular signalling
pathways. The magnitude of binding rates plays a very important molecular
regulatory role which can lead to very different cellular physiological
responses under different conditions. Here, we extend the k-space image
correlation spectroscopy (kICS) technique to study the kinetic binding rates of
systems wherein: (a) fluorescently labelled, free ligands in solution interact
with unlabelled, diffusing receptors in the plasma membrane and (b) systems
where labelled, diffusing receptors are allowed to bind/unbind and interconvert
between two different diffusing states on the plasma membrane. We develop the
necessary mathematical framework for the kICS analysis and demonstrate how to
extract the elevant kinetic binding parameters of the underlying molecular
system from fluorescence video-microscopy image time-series. Finally, by
examining real data for two model experimental systems, we demonstrate how kICS
can be a powerful tool to measure molecular transport coefficients and binding
kinetics.

A variety of methods have been proposed for structure similarity calculation,
which are called structure alignment or superposition. One major shortcoming in
current structure alignment algorithms is in their inherent design, which is
based on local structure similarity. In this work, we propose a method to
incorporate global information in obtaining optimal alignments and
superpositions. Our method, when applied to optimizing the TM-score and the GDT
score, produces significantly better results than current state-of-the-art
protein structure alignment tools. Specifically, if the highest TM-score found
by TMalign is lower than (0.6) and the highest TM-score found by one of the
tested methods is higher than (0.5), there is a probability of (42%) that
TMalign failed to find TM-scores higher than (0.5), while the same probability
is reduced to (2%) if our method is used. This could significantly improve the
accuracy of fold detection if the cutoff TM-score of (0.5) is used.
  In addition, existing structure alignment algorithms focus on structure
similarity alone and simply ignore other important similarities, such as
sequence similarity. Our approach has the capacity to incorporate multiple
similarities into the scoring function. Results show that sequence similarity
aids in finding high quality protein structure alignments that are more
consistent with eye-examined alignments in HOMSTRAD. Even when structure
similarity itself fails to find alignments with any consistency with
eye-examined alignments, our method remains capable of finding alignments
highly similar to, or even identical to, eye-examined alignments.

Metabolomics complements investigation of the genome, transcriptome, and
proteome of an organism. Today, the vast majority of metabolites remain
unknown, in particular for non-model organisms. Mass spectrometry is one of the
predominant techniques for analyzing small molecules such as metabolites. A
fundamental step for identifying a small molecule is to determine its molecular
formula.
  Here, we present and evaluate three algorithm engineering techniques that
speed up the molecular formula determination. For that, we modify an existing
algorithm for decomposing the monoisotopic mass of a molecule. These techniques
lead to a four-fold reduction of running times, and reduce memory consumption
by up to 94%. In comparison to the classical search tree algorithm, our
algorithm reaches a 1000-fold speedup.

Progressive methods offer efficient and reasonably good solutions to the
multiple sequence alignment problem. However, resulting alignments are biased
by guide-trees, especially for relatively distant sequences.
  We propose MSARC, a new graph-clustering based algorithm that aligns sequence
sets without guide-trees. Experiments on the BAliBASE dataset show that MSARC
achieves alignment quality similar to best progressive methods and
substantially higher than the quality of other non-progressive algorithms.
Furthermore, MSARC outperforms all other methods on sequence sets with the
similarity structure hardly represented by a phylogenetic tree. Furthermore,
MSARC outperforms all other methods on sequence sets whose evolutionary
distances are hardly representable by a phylogenetic tree. These datasets are
most exposed to the guide-tree bias of alignments.
  MSARC is available at http://bioputer.mimuw.edu.pl/msarc

Statistics in ranked lists is important in analyzing molecular biology
measurement data, such as ChIP-seq, which yields ranked lists of genomic
sequences. State of the art methods study fixed motifs in ranked lists. More
flexible models such as position weight matrix (PWM) motifs are not addressed
in this context. To assess the enrichment of a PWM motif in a ranked list we
use a PWM induced second ranking on the same set of elements. Possible orders
of one ranked list relative to the other are modeled by permutations. Due to
sample space complexity, it is difficult to characterize tail distributions in
the group of permutations. In this paper we develop tight upper bounds on tail
distributions of the size of the intersection of the top of two uniformly and
independently drawn permutations and demonstrate advantages of this approach
using our software implementation, mmHG-Finder, to study PWMs in several
datasets.

Tumorigenesis is an evolutionary process which involves a significant number
of genomic rearrangements typically coupled with changes in the gene copy
number profiles of numerous cells. Fluorescence in situ hybridization (FISH) is
a cytogenetic technique which allows counting copy numbers of genes in single
cells. The study of cancer progression using FISH data has received
considerably less attention compared to other types of cancer datasets.
  In this work we focus on inferring likely tumor progression pathways using
publicly available FISH data. We model the evolutionary process as a Markov
chain in the positive integer cone Z_+^g where g is the number of genes
examined with FISH. Compared to existing work which oversimplifies reality by
assuming independence of copy number changes, our model is able to capture
dependencies. We model the probability distribution of a dataset with
hierarchical log-linear models, a popular probabilistic model of count data.
Our choice provides an attractive trade-off between parsimony and good data
fit. We prove a theorem of independent interest which provides necessary and
sufficient conditions for reconstructing oncogenetic trees. Using this theorem
we are able to capitalize on the wealth of inter-tumor phylogenetic methods. We
show how to produce tumor phylogenetic trees which capture the dynamics of
cancer progression. We validate our proposed method on a breast tumor dataset.

In cell differentiation, a cell of a less specialized type becomes one of a
more specialized type, even though all cells have the same genome.
Transcription factors and epigenetic marks like histone modifications can play
a significant role in the differentiation process. In this paper, we present a
simple analysis of cell types and differentiation paths using phylogenetic
inference based on ChIP-Seq histone modification data. We propose new data
representation techniques and new distance measures for ChIP-Seq data and use
these together with standard phylogenetic inference methods to build
biologically meaningful trees that indicate how diverse types of cells are
related. We demonstrate our approach on H3K4me3 and H3K27me3 data for 37 and 13
types of cells respectively, using the dataset to explore various issues
surrounding replicate data, variability between cells of the same type, and
robustness. The promising results we obtain point the way to a new approach to
the study of cell differentiation.

Cell heterogeneity and the inherent complexity due to the interplay of
multiple molecular processes within the cell pose difficult challenges for
current single-cell biology. We introduce an approach that identifies a disease
phenotype from multiparameter single-cell measurements, which is based on the
concept of "supercell statistics", a single-cell-based averaging procedure
followed by a machine learning classification scheme. We are able to assess the
optimal tradeoff between the number of single cells averaged and the number of
measurements needed to capture phenotypic differences between healthy and
diseased patients, as well as between different diseases that are difficult to
diagnose otherwise. We apply our approach to two kinds of single-cell datasets,
addressing the diagnosis of a premature aging disorder using images of cell
nuclei, as well as the phenotypes of two non-infectious uveitides (the ocular
manifestations of Beh\c{c}et's disease and sarcoidosis) based on multicolor
flow cytometry. In the former case, one nuclear shape measurement taken over a
group of 30 cells is sufficient to classify samples as healthy or diseased, in
agreement with usual laboratory practice. In the latter, our method is able to
identify a minimal set of 5 markers that accurately predict Beh\c{c}et's
disease and sarcoidosis. This is the first time that a quantitative phenotypic
distinction between these two diseases has been achieved. To obtain this clear
phenotypic signature, about one hundred CD8+ T cells need to be measured.
Beyond these specific cases, the approach proposed here is applicable to
datasets generated by other kinds of state-of-the-art and forthcoming
single-cell technologies, such as multidimensional mass cytometry, single-cell
gene expression, and single-cell full genome sequencing techniques.

We utilized abundant transcriptomic data for the primary classes of brain
cancers to study the feasibility of separating all of these diseases
simultaneously based on molecular data alone. These signatures were based on a
new method reported herein that resulted in a brain cancer marker panel of 44
unique genes. Many of these genes have established relevance to the brain
cancers examined, with others having known roles in cancer biology. Analyses on
large-scale data from multiple sources must deal with significant challenges
associated with heterogeneity between different published studies, for it was
observed that the variation among individual studies often had a larger effect
on the transcriptome than did phenotype differences, as is typical. We found
that learning signatures across multiple datasets greatly enhanced
reproducibility and accuracy in predictive performance on truly independent
validation sets, even when keeping the size of the training set the same. This
was most likely due to the meta-signature encompassing more of the
heterogeneity across different sources and conditions, while amplifying signal
from the repeated global characteristics of the phenotype. When molecular
signatures of brain cancers were constructed from all currently available
microarray data, 90 percent phenotype prediction accuracy, or the accuracy of
identifying a particular brain cancer from the background of all phenotypes,
was found. Looking forward, we discuss our approach in the context of the
eventual development of organ-specific molecular signatures from peripheral
fluids such as the blood.

Light sheet fluorescence microscopy is able to image large specimen with high
resolution by imaging the sam- ples from multiple angles. Multi-view
deconvolution can significantly improve the resolution and contrast of the
images, but its application has been limited due to the large size of the
datasets. Here we present a Bayesian- based derivation of multi-view
deconvolution that drastically improves the convergence time and provide a fast
implementation utilizing graphics hardware.

These are the proceedings of the 13th Workshop on Algorithms in
Bioinformatics, WABI2013, which was held September 2-4 2013 in Sophia
Antipolis, France. All manuscripts were peer reviewed by the WABI2013 program
committee and external reviewers.

We apply a force-directed spring embedding graph layout approach to
electronic health records in order to visualise population-wide associations
between human disorders as presented in an individual biological organism. The
introduced visualisation is implemented on the basis of the Google maps
platform and can be found at http://disease-map.net . We argue that the
suggested method of visualisation can both validate already known specifics of
associations between disorders and identify novel never noticed association
patterns.

Numerous metrics of heart rate variability (HRV) have been described,
analyzed, and compared in the literature. However, they rarely cover the actual
metrics used in a class of HRV data acquisition devices - those designed
primarily to produce real-time metrics. This paper characterizes a class of
metrics that we term dynamic metrics. We also report the results of a pilot
study which compares one such dynamic metric, based on photoplethysmographic
data using a moving sampling window set to the length of an estimated breath
cycle (EBC), with established HRV metrics. The results show high correlation
coefficients between the dynamic EBC metrics and the established static SDNN
metric (standard deviation of Normal-to-Normal) based on electrocardiography.
These results demonstrate the usefulness of data acquisition devices designed
for real-time metrics.

We study the statistical properties of melanoma cell colonies grown in vitro
by analyzing the results of crystal violet assays at different concentrations
of initial plated cells and for different growth times. The distribution of
colony sizes is described well by a continuous time branching process. To
characterize the shape fluctuations of the colonies, we compute the
distribution of eccentricities. The experimental results are compared with
numerical results for models of random division of elastic cells, showing that
experimental results are best reproduced by restricting cell division to the
outer rim of the colony. Our results serve to illustrate the wealth of
information that can be extracted by a standard experimental method such as the
crystal violet assay.

RNA-seq has become a de facto standard for measuring gene expression.
Traditionally, RNA-seq experiments are mathematically averaged -- they sequence
the mRNA of individuals from different treatment groups, hoping to correlate
phenotype with differences in arithmetic read count averages at shared loci of
interest. Alternatively, the tissue from the same individuals may be pooled
prior to sequencing in what we refer to as a biologically averaged design. As
mathematical averaging sequences all individuals it controls for both
biological and technical variation; however, is the statistical resolution
gained always worth the additional cost? To compare biological and mathematical
averaging, we examined theoretical and empirical estimates of statistical
efficiency and relative cost efficiency. Though less efficient at a fixed
sample size, we found that biological averaging can be more cost efficient than
mathematical averaging. With this motivation, we developed a differential
expression classifier, ICRBC, that can detect alternatively expressed genes
between biologically averaged samples. In simulation studies, we found that
biological averaging and subsequent analysis with our classifier performed
comparably to existing methods, such as ASC, edgeR, and DESeq, especially when
individuals were pooled evenly and less than 20% of the regulome was expected
to be differentially regulated. In two technically distinct mouse datasets and
one plant dataset, we found that our method was over 87% concordant with edgeR
for the 100 most significant features. We therefore conclude biological
averaging may sufficiently control biological variation to a level that
differences in gene expression may be detectable. In such situations, ICRBC can
enable reliable exploratory analysis at a fraction of the cost, especially when
interest lies in the most differentially expressed loci.

Identifying reliable domain-domain interactions (DDIs) will increase our
ability to predict novel protein-protein interactions (PPIs), to unravel
interactions in protein complexes, and thus gain more information about the
function and behavior of genes. One of the challenges of identifying reliable
DDIs is domain promiscuity. Promiscuous domains are domains that can occur in
many domain architectures and are therefore found in many proteins. This
becomes a problem for a method where the score of a domain-pair is the ratio
between observed and expected frequencies because the PPI network is sparse. As
such, many protein-pairs will be non-interacting and domain-pairs with
promiscuous domains will be penalized. This domain promiscuity challenge to the
problem of inferring reliable DDIs from PPIs has been recognized, and a number
of work-arounds have been proposed. In this paper, we report an application of
Formal Concept Analysis (FCA) to this problem. We find that the relationship
between formal concepts provide a natural way for rare domains to elevate the
rank of promiscuous domains, and enrich highly ranked domain-pairs with
reliable DDIs. This piggy-backing of promiscuous domains onto rare domains is
possible due to the domain architecture of proteins which mixes promiscuous
with rare domains.

Weak-signal detection and single-particle selection from low-contrast
micrographs of frozen hydrated biomolecules by cryo-electron microscopy
(cryo-EM) presents a practical challenge. Cryo-EM image contrast degrades as
the size of biomolecules of structural interest decreases. When the image
contrast falls into a range where the location or presence of single particles
becomes ambiguous, a need arises for objective computational approaches to
detect weak signal and to select and verify particles from these low-contrast
micrographs. Here we propose an objective validation scheme for low-contrast
particle selection using a combination of two different target functions. In an
implementation of this dual-target function (DTF) validation, a first target
function of fast local correlation was used to select particles through
template matching, followed by signal validation through a second target
function of maximum likelihood. By a systematic study of simulated data, we
found that such an implementation of DTF validation is capable of selecting and
verifying particles from cryo-EM micrographs with a signal-to-noise ratio as
low as 0.002. Importantly, we demonstrated that DTF validation can robustly
evade over-fitting or reference bias from the particle-picking template,
allowing true signal to emerge from amidst heavy noise in an objective fashion.
The DTF approach allows efficient assembly of a large number of single-particle
cryo-EM images of smaller biomolecules or specimens containing
contrast-degrading agents like detergents in a semi-automatic manner.

Plant or soil water status are required in many scientific fields to
understand plant responses to drought. Because the transcriptomic response to
abiotic conditions, such as water deficit, reflects plant water status, genomic
tools could be used to develop a new type of molecular biomarker. Using the
sunflower (Helianthus annuus L.) as a model species to study the transcriptomic
response to water deficit both in greenhouse and field conditions, we
specifically identified three genes that showed an expression pattern highly
correlated to plant water status as estimated by the pre-dawn leaf water
potential, fraction of transpirable soil water, soil water content or fraction
of total soil water in controlled conditions. We developed a generalized linear
model to estimate these classical water status indicators from the expression
levels of the three selected genes under controlled conditions. This estimation
was independent of the four tested genotypes and the stage (pre- or
post-flowering) of the plant. We further validated this gene expression
biomarker under field conditions for four genotypes in three different trials,
over a large range of water status, and we were able to correct their
expression values for a large diurnal sampling period.

Extrinsic environmental factors influence the distribution and population
dynamics of many organisms, including insects that are of concern for human
health and agriculture. This is particularly true for vector-borne infectious
diseases, like malaria, which is a major source of morbidity and mortality in
humans. Understanding the mechanistic links between environment and population
processes for these diseases is key to predicting the consequences of climate
change on transmission and for developing effective interventions. An important
measure of the intensity of disease transmission is the reproductive number
$R_0$. However, understanding the mechanisms linking $R_0$ and temperature, an
environmental factor driving disease risk, can be challenging because the data
available for parameterization are often poor. To address this we show how a
Bayesian approach can help identify critical uncertainties in components of
$R_0$ and how this uncertainty is propagated into the estimate of $R_0$. Most
notably, we find that different parameters dominate the uncertainty at
different temperature regimes: bite rate from 15-25$^\circ$ C; fecundity across
all temperatures, but especially $\sim$25-32$^\circ$ C; mortality from
20-30$^\circ$ C; parasite development rate at $\sim$15-16$^\circ$C and again at
$\sim$33-35$^\circ$C. Focusing empirical studies on these parameters and
corresponding temperature ranges would be the most efficient way to improve
estimates of $R_0$. While we focus on malaria, our methods apply to improving
process-based models more generally, including epidemiological, physiological
niche, and species distribution models.

We developed a novel method based on the Fourier analysis of protein
molecular surfaces to speed up the analysis of the vast structural data
generated in the post-genomic era. This method computes the power spectrum of
surfaces of the molecular electrostatic potential, whose three-dimensional
coordinates have been either experimentally or theoretically determined. Thus
we achieve a reduction of the initial three-dimensional information on the
molecular surface to the one-dimensional information on pairs of points at a
fixed scale apart. Consequently, the similarity search in our method is
computationally less demanding and significantly faster than shape comparison
methods. As proof of principle, we applied our method to a training set of
viral proteins that are involved in major diseases such as Hepatitis C, Dengue
fever, Yellow fever, Bovine viral diarrhea and West Nile fever. The training
set contains proteins of four different protein families, as well as a
mammalian representative enzyme. We found that the power spectrum successfully
assigns a unique signature to each protein included in our training set, thus
providing a direct probe of functional similarity among proteins. The results
agree with established biological data from conventional structural
biochemistry analyses.

Cell functional diversity is a significant determinant on how biological
processes unfold. Most accounts of diversity involve a search for sequence or
expression differences. Perhaps there are more subtle mechanisms at work. Using
the metaphor of information processing and decision-making might provide a
clearer view of these subtleties. Understanding adaptive and transformative
processes (such as cellular reprogramming) as a series of simple decisions
allows us to use a technique called cellular signal detection theory (cellular
SDT) to detect potential bias in mechanisms that favor one outcome over
another. We can apply method of detecting cellular reprogramming bias to
cellular reprogramming and other complex molecular processes. To demonstrate
scope of this method, we will critically examine differences between cell
phenotypes reprogrammed to muscle fiber and neuron phenotypes. In cases where
the signature of phenotypic bias is cryptic, signatures of genomic bias
(pre-existing and induced) may provide an alternative. The examination of these
alternates will be explored using data from a series of fibroblast cell lines
before cellular reprogramming (pre-existing) and differences between fractions
of cellular RNA for individual genes after drug treatment (induced). In
conclusion, the usefulness and limitations of this method and associated
analogies will be discussed.

The measurement of information flows within moving animal groups has recently
been a topic of considerable interest, and it has become clear that the
individual(s) that drive collective movement may change over time, and that
such individuals may not necessarily always lead from the front. However,
methods to quantify the influence of specific individuals on the behaviour of
other group members and the direction of information flow in moving group, are
lacking on the level of empirical studies and theoretical models. Using high
spatio-temporal resolution GPS trajectories of foraging meerkats, Suricata
suricatta, we provide an information-theoretic framework to identify dynamical
coupling between animals independent of their relative spatial positions. Based
on this identification, we then compare designations of individuals as either
drivers or responders against designations provided by the relative spatial
position. We find that not only does coupling occur both from the frontal to
the trailing individuals and vice versa, but also that the coupling direction
is a non-linear function of the relative position. This provides evidence for
(i) intermittent fluctuation of the coupling strength and (ii) alternation in
the coupling direction within foraging meerkat pairs. The framework we
introduce allows for a detailed description of the dynamical patterns of mutual
influence between all pairs of individuals within moving animal groups. We
argue that applying an information-theoretic perspective to the study of
coordinated phenomena in animal groups will eventually help to understand cause
and effect in collective behaviour.

Current efforts in the biomedical sciences and related interdisciplinary
fields are focused on gaining a molecular understanding of health and disease,
which is a problem of daunting complexity that spans many orders of magnitude
in characteristic length scales, from small molecules that regulate cell
function to cell ensembles that form tissues and organs working together as an
organism. In order to uncover the molecular nature of the emergent properties
of a cell, it is essential to measure multiple cell components simultaneously
in the same cell. In turn, cell heterogeneity requires multiple cells to be
measured in order to understand health and disease in the organism. This review
summarizes current efforts towards a data-driven framework that leverages
single-cell technologies to build robust signatures of healthy and diseased
phenotypes. While some approaches focus on multicolor flow cytometry data and
other methods are designed to analyze high-content image-based screens, we
emphasize the so-called Supercell/SVM paradigm (recently developed by the
authors of this review and collaborators) as a unified framework that captures
mesoscopic-scale emergence to build reliable phenotypes. Beyond their specific
contributions to basic and translational biomedical research, these efforts
illustrate, from a larger perspective, the powerful synergy that might be
achieved from bringing together methods and ideas from statistical physics,
data mining, and mathematics to solve the most pressing problems currently
facing the life sciences.

South Africa has more people infected with HIV but, by providing access to
anti-retroviral therapy (ART), has kept more people alive than any other
country. The effectiveness, availability and affordability of potent
anti-retroviral therapy (ART) make it possible to contemplate ending the
epidemic of HIV/AIDS. We consider what would have happened without ART, the
impact of the current roll-out of ART, what might be possible if early
treatment becomes available to all, and what could have happened if ART had
been provided much earlier in the epidemic. In 2013 the provision of ART has
reduced the prevalence of HIV from an estimated 15% to 9% among adults not on
ART, the annual incidence from 2% to 0.9%, and the AIDS related deaths from
0.9% to 0.3% p.a. saving 1.5 million lives and USD727M. Regular testing and
universal access to ART could reduce the prevalence among adults not on ART in
2023 to 0.06%, annual incidence to 0.05%, and eliminate AIDS deaths. Cumulative
costs between 2013 ands 2023 would increase by USD692M only 4% of the total
cost of USD17Bn. If a universal testing and early treatment had started in 1998
the prevalence of HIV among adults not on ART in 2013 would have fallen to
0.03%, annual incidence to 0.03%, and saved 2.5 million lives. The cost up to
2013 would have increased by USD18Bn but this would have been cost effective at
US$7,200 per life saved. Future surveys of HIV among women attending ante-natal
clinics should include testing women for the presence of anti-retroviral drugs,
measuring their viral loads, and using appropriate assays for estimating HIV
incidence. These data would make it possible to develop better and more
reliable estimates of the current state of the epidemic, the success of the
current ART programme, levels of viral load suppression for those on ART and
the incidence of infection.

A number of biological systems can be modeled by Markov chains. Recently,
there has been an increasing concern about when biological systems modeled by
Markov chains will perform a dynamic phenomenon called overshoot. In this
article, we found that the steady-state behavior of the system will have a
great effect on the occurrence of overshoot. We confirmed that overshoot in
general cannot occur in systems which will finally approach an equilibrium
steady state. We further classified overshoot into two types, named as simple
overshoot and oscillating overshoot. We showed that except for extreme cases,
oscillating overshoot will occur if the system is far from equilibrium. All
these results clearly show that overshoot is a nonequilibrium dynamic
phenomenon with energy consumption. In addition, the main result in this
article is validated with real experimental data.

We address the problem of using nonlinear models to design experiments to
characterize the dynamics of cellular processes by using the approach of the
Maximally Informative Next Experiment (MINE), which was introduced in [W. Dong,
et al. Systems biology of the clock in neurospora crassa. {\em PLoS ONE}, page
e3105, 2008] and independently in [M. M. Donahue, et al. Experiment design
through dynamical characterization of non-linear systems biology models
utilising sparse grids. {\em IET System Biology}, 4:249--262, 2010]. In this
approach, existing data is used to define a probability distribution on the
parameters; the next measurement point is the one that yields the largest model
output variance with this distribution. Building upon this approach, we
introduce the Expected Dynamics Estimator (EDE), which is the expected value
using this distribution of the output as a function of time. We prove the
consistency of this estimator (uniform convergence to true dynamics) even when
the chosen experiments cluster in a finite set of points. We extend this proof
of consistency to various practical assumptions on noisy data and moderate
levels of model mismatch. Through the derivation and proof, we develop a
relaxed version of MINE that is more computationally tractable and robust than
the original formulation. The results are illustrated with numerical examples
on two nonlinear ordinary differential equation models of biomolecular and
cellular processes.

The discovery of peptides having high biological activity is very challenging
mainly because there is an enormous diversity of compounds and only a minority
have the desired properties. To lower cost and reduce the time to obtain
promising compounds, machine learning approaches can greatly assist in the
process and even replace expensive laboratory experiments by learning a
predictor with existing data. Unfortunately, selecting ligands having the
greatest predicted bioactivity requires a prohibitive amount of computational
time. For this combinatorial problem, heuristics and stochastic optimization
methods are not guaranteed to find adequate compounds.
  We propose an efficient algorithm based on De Bruijn graphs, guaranteed to
find the peptides of maximal predicted bioactivity. We demonstrate how this
algorithm can be part of an iterative combinatorial chemistry procedure to
speed up the discovery and the validation of peptide leads. Moreover, the
proposed approach does not require the use of known ligands for the target
protein since it can leverage recent multi-target machine learning predictors
where ligands for similar targets can serve as initial training data. Finally,
we validated the proposed approach in vitro with the discovery of new cationic
anti-microbial peptides.
  Source code is freely available at
http://graal.ift.ulaval.ca/peptide-design/.

Optical mapping by direct visualization of individual DNA molecules,
stretched in nanochannels with sequence-specific fluorescent labeling,
represents a promising tool for disease diagnostics and genomics. An important
challenge for this technique is thermal motion of the DNA as it undergoes
imaging; this blurs fluorescent patterns along the DNA and results in
information loss. Correcting for this effect (a process referred to as
kymograph alignment) is a common preprocessing step in nanochannel-based
optical mapping workflows, and we present here a highly efficient algorithm to
accomplish this via pattern recognition. We compare our method with the one
previous approach, and we find that our method is orders of magnitude faster
while producing data of similar quality. We demonstrate proof of principle of
our approach on experimental data consisting of melt mapped bacteriophage DNA.

Portable low-cost sensors and sensing systems for the identification and
quantitative measurement of bacteria in field water are critical in preventing
drinking water from being contaminated by bacteria. In this article, we
reported the design, fabrication and testing of a low-cost, miniaturized and
sensitive bacteria sensor based on electrical impedance spectroscopy method
using a smartphone as the platform. Our design of microfluidics enabled the
pre-concentration of the bacteria which lowered the detection limit to 10
bacterial cells per milliliter. We envision that our demonstrated
smartphone-based sensing system will realize highly-sensitive and rapid
in-field quantification of multiple species of bacteria and pathogens.

The ultimate target of proteomics identification is to identify and quantify
the protein in the organism. Mass spectrometry (MS) based on label-free protein
quantitation has mainly focused on analysis of peptide spectral counts and ion
peak heights. Using several observed peptides (proteotypic) can identify the
origin protein. However, each peptide's possibility to be detected was severely
influenced by the peptide physicochemical properties, which confounded the
results of MS accounting. Using about a million peptide identification
generated by four different kinds of proteomic platforms, we successfully
identified >16,000 proteotypic peptides. We used machine learning
classification to derive peptide detection probabilities that are used to
predict the number of trypic peptides to be observed, which can serve to
estimate the absolutely abundance of protein with highly accuracy. We used the
data of peptides (provides by CAS lab) to derive the best model from different
kinds of methods. We first employed SVM and Random Forest classifier to
identify the proteotypic and unobserved peptides, and then searched the best
parameter for better prediction results. Considering the excellent performance
of our model, we can calculate the absolutely estimation of protein abundance.

A nucleotide sequence 35 base pairs long can take
1,180,591,620,717,411,303,424 possible values. An example of systems biology
datasets, protein binding microarrays, contain activity data from about 40000
such sequences. The discrepancy between the number of possible configurations
and the available activities is enormous. Thus, albeit that systems biology
datasets are large in absolute terms, they oftentimes require methods developed
for rare events due to the combinatorial increase in the number of possible
configurations of biological systems. A plethora of techniques for handling
large datasets, such as Empirical Bayes, or rare events, such as importance
sampling, have been developed in the literature, but these cannot always be
simultaneously utilized. Here we introduce a principled approach to Empirical
Bayes based on importance sampling, information theory, and theoretical physics
in the general context of sequence phenotype model induction. We present the
analytical calculations that underlie our approach. We demonstrate the
computational efficiency of the approach on concrete examples, and demonstrate
its efficacy by applying the theory to publicly available protein binding
microarray transcription factor datasets and to data on synthetic
cAMP-regulated enhancer sequences. As further demonstrations, we find
transcription factor binding motifs, predict the activity of new sequences and
extract the locations of transcription factor binding sites. In summary, we
present a novel method that is efficient (requiring minimal computational time
and reasonable amounts of memory), has high predictive power that is comparable
with that of models with hundreds of parameters, and has a limited number of
optimized parameters, proportional to the sequence length.

The goal of animal movement analysis is to understand how organisms explore
and exploit the complex and varying environment. Animals usually exhibit varied
and complicated movements, from apparently deterministic behaviors to highly
random ones. This is critical for assessing movement efficiency and strategies
that are used to quantify and analyze movement trajectories. Here we introduce
a tortuosity entropy (TorEn) based on comparison of parameters, e.g. heading,
bearing, speed, of consecutive points in movement trajectory, which is a simple
measure for quantifying the behavioral change in animal movement data in a fine
scale. In our approach, the differences between pairwise successive track
points are transformed inot symbolic sequences, then we map these symbols into
a group of pattern vectors and calculate the information entropy of pattern
vector. Tortuosity entropy can be easily applied to arbitrary real-world
data-deterministic or stochastic, stationary or non-stationary. We test the
algorithm on both simulated trajectories and real trajectories and show that
both mixed segments in synthetic data and different phases in real movement
data are identified accurately. The results show that the algorithm is
applicable to various situations, indicating that our approach is a promising
tool to reveal the behavioral pattern in movement data.

Computational discovery of microRNAs (miRNA) is based on pre-determined sets
of features from miRNA precursors (pre-miRNA). These feature sets used by
current tools for pre-miRNA recognition differ in construction and dimension.
Some feature sets are composed of sequence-structure patterns commonly found in
pre-miRNAs, while others are a combination of more sophisticated RNA features.
Current tools achieve similar predictive performance even though the feature
sets used - and their computational cost - differ widely. In this work, we
analyze the discriminant power of seven feature sets, which are used in six
pre-miRNA prediction tools. The analysis is based on the classification
performance achieved with these feature sets for the training algorithms used
in these tools. We also evaluate feature discrimination through the F-score and
feature importance in the induction of random forests. More diverse feature
sets produce classifiers with significantly higher classification performance
compared to feature sets composed only of sequence-structure patterns. However,
small or non-significant differences were found among the estimated
classification performances of classifiers induced using sets with
diversification of features, despite the wide differences in their dimension.
Based on these results, we applied a feature selection method to reduce the
computational cost of computing the feature set, while maintaining discriminant
power. We obtained a lower-dimensional feature set, which achieved a
sensitivity of 90% and a specificity of 95%. Our feature set achieves a
sensitivity and specificity within 0.1% of the maximal values obtained with any
feature set while it is 34x faster to compute. Even compared to another feature
set, which is the computationally least expensive feature set of those from the
literature which perform within 0.1% of the maximal values, it is 34x faster to
compute.

Antibody-functionalized silicon nanowire field-effect transistors have been
shown to exhibit excellent analyte detection sensitivity enabling sensing of
analyte concentrations at levels not readily accessible by other methods. One
example where accurate measurement of small concentrations is necessary is
detection of serum biomarkers, such as the recently discovered tumor necrosis
factor receptor superfamily member TROY (TNFRSF19), which may serve as a
biomarker for melanoma. TROY is normally only present in brain but it is
aberrantly expressed in primary and metastatic melanoma cells and shed into the
surrounding environment. In this study, we show the detection of different
concentrations of TROY in buffer solution using top-down fabricated silicon
nanowires. We demonstrate the selectivity of our sensors by comparing the
signal with that obtained from bovine serum albumin in buffer solution. Both
the signal size and the reaction kinetics serve to distinguish the two signals.
Using a fast-mixing two-compartment reaction model, we are able to extract the
association and dissociation rate constants for the reaction of TROY with the
antibody immobilized on the sensor surface.

Estimating the required dose in radiotherapy is of crucial importance since
the administrated dose should be sufficient to eradicate the tumor and at the
same time should inflict minimal damage on normal cells. The probability that a
given dose and schedule of ionizing radiation eradicates all the tumor cells in
a given tissue is called the tumor control probability (TCP), and is often used
to compare various treatment strategies used in radiation therapy. In this
paper, we aim to investigate the effects of including cell-cycle phase on the
TCP by analyzing a stochastic model of a tumor comprised of actively dividing
cells and quiescent cells with different radiation sensitivities. We derive an
exact phase-diagram for the steady-state TCP of the model and show that at
high, clinically-relevant doses of radiation, the distinction between active
and quiescent tumor cells (i.e. accounting for cell-cycle effects) becomes of
negligible importance in terms of its effect on the TCP curve. However, for
very low doses of radiation, these proportions become significant determinants
of the TCP. Moreover, we use a novel numerical approach based on the method of
characteristics for partial differential equations, validated by the Gillespie
algorithm, to compute the TCP as a function of time. We observe that our
results differ from the results in the literature using similar existing
models, even though similar parameters values are used, and the reasons for
this are discussed.

Algorithms that detect covariance between pairs of columns in multiple
sequence alignments are commonly employed to predict functionally important
residues and structural contacts. However, the assumption that co-variance only
occurs between individual residues in the protein is more driven by
computational convenience rather than fundamental protein architecture. Here we
develop a novel algorithm that defines a covariance score across two groups of
columns where each group represents a stretch of contiguous columns in the
alignment. We define a test set that consists of secondary structure elements
({\alpha}-helixes and {\beta}-strands) across more than 1,100 PFAM families.
Using these alignments to predict segments that are physically close in
structure, we show that our method substantially out-performs approaches that
aggregate the results of algorithms that operate on individual column pairs.
Our approach demonstrates that considering units of proteins beyond pairs of
columns can improve the power and utility of covariance algorithms.

Silicon nanochannel field effect transistor (FET) biosensors are one of the
most promising technologies in the development of highly sensitive and
label-free analyte detection for cancer diagnostics. With their exceptional
electrical properties and small dimensions, silicon nanochannels are ideally
suited for extraordinarily high sensitivity. In fact, the high
surface-to-volume ratios of these systems make single molecule detection
possible. Further, FET biosensors offer the benefits of high speed, low cost,
and high yield manufacturing, without sacrificing the sensitivity typical for
traditional optical methods in diagnostics. Top down manufacturing methods
leverage advantages in Complementary Metal Oxide Semiconductor (CMOS)
technologies, making richly multiplexed sensor arrays a reality. Here, we
discuss the fabrication and use of silicon nanochannel FET devices as
biosensors for breast cancer diagnosis and monitoring.

Complex phenotypic differences among different acute leukemias cannot be
fully captured by analyzing the expression levels of one single molecule, such
as a miR, at a time, but requires systematic analysis of large sets of miRs.
While a popular approach for analysis of such datasets is principal component
analysis (PCA), this method is not designed to optimally discriminate different
phenotypes. Moreover, PCA and other low-dimensional representation methods
yield linear or non-linear combinations of all measured miRs. Global human miR
expression was measured in AML, B-ALL, and T-ALL cell lines and patient RNA
samples. By systematically applying support vector machines to all measured
miRs taken in dyad and triad groups, we built miR networks using cell line data
and validated our findings with primary patient samples. All the coordinately
transcribed members of the miR-23a cluster (which includes also miR-24 and
miR-27a), known to function as tumor suppressors of acute leukemias, appeared
in the AML, B-ALL and T-ALL centric networks. Subsequent qRT-PCR analysis
showed that the most connected miR in the B-ALL-centric network, miR-708, is
highly and specifically expressed in B-ALLs, suggesting that miR-708 might
serve as a biomarker for B-ALL. This approach is systematic, quantitative,
scalable, and unbiased. Rather than a single signature, our approach yields a
network of signatures reflecting the redundant nature of biological signaling
pathways. The network representation allows for visual analysis of all
signatures by an expert and for future integration of additional information.
Furthermore, each signature involves only small sets of miRs, such as dyads and
triads, which are well suited for in depth validation through laboratory
experiments such as loss- and gain-of-function assays designed to drive changes
in leukemia cell survival, proliferation and differentiation.

Glioblastoma multiforme, the most frequent type of primary brain tumor, is a
rapidly evolving and spatially heterogeneous high-grade astrocytoma that
presents areas of necrosis, hypercellularity and microvascular hyperplasia. The
aberrant vasculature leads to hypoxic areas and results in an increase of the
oxidative stress selecting for more invasive tumor cell phenotypes. In our
study we assay in silico different therapeutic approaches which combine
antithrombotics, antioxidants and standard radiotherapy. To do so, we have
developed a biocomputational model of glioblastoma multiforme that incorporates
the spatio-temporal interplay among two glioma cell phenotypes corresponding to
oxygenated and hypoxic cells, a necrotic core and the local vasculature whose
response evolves with tumor progression. Our numerical simulations predict that
suitable combinations of antithrombotics and antioxidants may diminish, in a
synergetic way, oxidative stress and the subsequent hypoxic response. This
novel therapeutical strategy, with potentially low or no toxicity, might reduce
tumor invasion and further sensitize glioblastoma multiforme to conventional
radiotherapy or other cytotoxic agents, hopefully increasing median patient
overall survival time.

Extended systems governed by partial differential equations can, under
suitable conditions, be approximated by means of sets of ordinary differential
equations for global quantities capturing the essential features of the systems
dynamics. Here we obtain a small number of effective equations describing the
dynamics of single-front and localized solutions of Fisher-Kolmogorov type
equations. These solutions are parametrized by means of a minimal set of
time-dependent quantities for which ordinary differential equations ruling
their dynamics are found. A comparison of the finite dimensional equations and
the dynamics of the full partial differential equation is made showing a very
good quantitative agreement with the dynamics of the partial differential
equation. We also discuss some implications of our findings for the
understanding of the growth progression of certain types of primary brain
tumors and discuss possible extensions of our results to related equations
arising in different modelling scenarios.

Low grade gliomas (LGGs) are a group of primary brain tumors usually
encountered in young patient populations. These tumors represent a difficult
challenge because many patients survive a decade or more and may be at a higher
risk for treatment-related complications. Specifically, radiation therapy is
known to have a relevant effect on survival but in many cases it can be
deferred to avoid side effects while maintaining its beneficial effect.
However, a subset of low-grade gliomas manifests more aggressive clinical
behavior and requires earlier intervention. Moreover, the effectiveness of
radiotherapy depends on the tumor characteristics. Recently Pallud et al.,
[Neuro-oncology, 14(4):1-10, 2012], studied patients with LGGs treated with
radiation therapy as a first line therapy. and found the counterintuitive
result that tumors with a fast response to the therapy had a worse prognosis
than those responding late. In this paper we construct a mathematical model
describing the basic facts of glioma progression and response to radiotherapy.
The model provides also an explanation to the observations of Pallud et al.
Using the model we propose radiation fractionation schemes that might be
therapeutically useful by helping to evaluate the tumor malignancy while at the
same time reducing the toxicity associated to the treatment.

The Multistage Differential Transform Method (MDTM) is employed to solve the
model for HIV infection of CD4+T cells. Comparing the numerical results to
those obtained by the classical fourth order Runge-Kutta method showed the
preciseness and efficacy of the multistep differential transform method. The
study shows that the method is a powerful and promising tool for solving
coupled systems of differential equations.

Markovian population models are suitable abstractions to describe well-mixed
interacting particle systems in situation where stochastic fluctuations are
significant due to the involvement of low copy particles. In molecular biology,
measurements on the single-cell level attest to this stochasticity and one is
tempted to interpret such measurements across an isogenic cell population as
different sample paths of one and the same Markov model. Over recent years
evidence built up against this interpretation due to the presence of
cell-to-cell variability stemming from factors other than intrinsic
fluctuations. To account for this extrinsic variability, Markovian models in
random environments need to be considered and a key emerging question is how to
perform inference for such models. We model extrinsic variability by a random
parametrization of all propensity functions. To detect which of those
propensities have significant variability, we lay out a sparse learning
procedure captured by a hierarchical Bayesian model whose evidence function is
iteratively maximized using a variational Bayesian expectation-maximization
algorithm.

This pilot study explored physiological responses to playing and listening to
the Native American flute. Autonomic, electroencephalographic (EEG), and heart
rate variability (HRV) metrics were recorded while participants (N = 15) played
flutes and listened to several styles of music. Flute playing was accompanied
by an 84% increase in HRV (p < .001). EEG theta (4-8 Hz) activity increased
while playing flutes (p = .007) and alpha (8-12 Hz) increased while playing
lower-pitched flutes (p = .009). Increase in alpha from baseline to the flute
playing conditions strongly correlated with experience playing Native American
flutes (r = +.700). Wide-band beta (12-25 Hz) decreased from the silence
conditions when listening to solo Native American flute music (p = .013). The
findings of increased HRV, increasing slow-wave rhythms, and decreased beta
support the hypothesis that Native American flutes, particularly those with
lower pitches, may have a role in music therapy contexts. We conclude that the
Native American flute may merit a more prominent role in music therapy and that
a study of the effects of flute playing on clinical conditions, such as
post-traumatic stress disorder (PTSD), asthma, chronic obstructive pulmonary
disease (COPD), hypertension, anxiety, and major depressive disorder, is
warranted.

This paper aims at predicting the next maxima values of the state variables
of the seasonal SEIR epidemic model and their in-between time intervals.
Lorenz's method of analogues is applied on the attractor formed by the maxima
of the corresponding state variables. It is found that both quantities are
characterized by a high degree of predictability in the case of the chaotic
regime of the parameter space.

1. Complex systems of moving and interacting objects are ubiquitous in the
natural and social sciences. Predicting their behavior often requires models
that mimic these systems with sufficient accuracy, while accounting for their
inherent stochasticity. Though tools exist to determine which of a set of
candidate models is best relative to the others, there is currently no generic
goodness-of-fit framework for testing how close the best model is to the real
complex stochastic system.
  2. We propose such a framework, using a novel application of the Earth
mover's distance, also known as the Wasserstein metric. It is applicable to any
stochastic process where the probability of the model's state at time $t$ is a
function of the state at previous times. It generalizes the concept of a
residual, often used to analyze 1D summary statistics, to situations where the
complexity of the underlying model's probability distribution makes standard
residual analysis too imprecise for practical use.
  3. We give a scheme for testing the hypothesis that a model is an accurate
description of a data set. We demonstrate the tractability and usefulness of
our approach by application to animal movement models in complex, heterogeneous
environments. We detail methods for visualizing results and extracting a
variety of information on a given model's quality, such as whether there is any
inherent bias in the model, or in which situations it is most accurate. We
demonstrate our techniques by application to data on multi-species flocks of
insectivore birds in the Amazon rainforest.
  4. This work provides a usable toolkit to assess the quality of generic
movement models of complex systems, in an absolute rather than a relative
sense.

A hypercomplex representation of DNA is proposed to facilitate comparing DNA
sequences with fuzzy composition. With the hypercomplex number representation,
the conventional sequence analysis method, such as, dot matrix analysis,
dynamic programming, and cross-correlation method have been extended and
improved to align DNA sequences with fuzzy composition. The hypercomplex dot
matrix analysis can provide more control over the degree of alignment desired.
A new scoring system has been proposed to accommodate the hypercomplex number
representation of DNA and integrated with dynamic programming alignment method.
By using hypercomplex cross-correlation, the match and mismatch alignment
information between two aligned DNA sequences are separately stored in the
resultant real part and imaginary parts respectively. The mismatch alignment
information is very useful to refine consensus sequence based motif scanning.

Background: The identification of transcription factor binding sites (TFBSs)
and cis-regulatory modules (CRMs) is a crucial step in studying gene
expression, but the computational method attempting to distinguish CRMs from
NCNRs still remains a challenging problem due to the limited knowledge of
specific interactions involved. Methods: The statistical properties of
cis-regulatory modules (CRMs) are explored by estimating the similar-word set
distribution with overrepresentation (Z-score). It is observed that CRMs tend
to have a thin-tail Z-score distribution. A new statistical thin-tail test with
two thinness coefficients is proposed to distinguish CRMs from non-coding
non-regulatory regions (NCNRs). Results: As compared with the existing
fluffy-tail test, the first thinness coefficient is designed to reduce
computational time, making the novel thin-tail test very suitable for long
sequences and large database analysis in the post-genome time and the second
one to improve the separation accuracy between CRMs and NCNRs. These two
thinness coefficients may serve as valuable filtering indexes to predict CRMs
experimentally. Conclusions: The novel thin-tail test provides an efficient and
effective means for distinguishing CRMs from NCNRs based on the specific
statistical properties of CRMs and can guide future experiments aimed at
finding new CRMs in the post-genome time.

Familiarity with a simulation platform can seduce modellers into accepting
untested assumptions for convenience of implementation. These assumptions may
have consequences greater than commonly suspected, and it is important that
modellers remain mindful of assumptions and remain diligent with sensitivity
testing. Familiarity with a technique can lead to complacency, and alternative
approaches and software can reveal untested assumptions. Visual modelling
environments based on system dynamics may help to make critical assumptions
more evident by offering an accessible visual overview and empowering a focus
on representational rather than computational efficiency. This capacity is
illustrated using a cohort-based forest growth model developed for mixed
species forest. The alternative model implementation revealed that untested
assumptions in the original model could have substantial influence on simulated
outcomes. An important implication is that modellers should remain conscious of
all assumptions, consider alternative implementations that reveal assumptions
more clearly, and conduct sensitivity tests to inform decisions.

After more than a century of research the typical growth pattern of a tree
was thought to be fairly well understood. Following germination height growth
accelerates for some time, then increment peaks and the added height each year
becomes less and less. The cross sectional area (basal area) of the tree
follows a similar pattern, but the maximum basal area increment occurs at some
time after the maximum height increment. An increase in basal area in a tall
tree will add more volume to the stem than the same increase in a short tree,
so the increment in stem volume (or mass) peaks very late. Stephenson et al.
challenge this paradigm, and suggest that mass increment increases
continuously. Their analysis methods however are a textbook example of the
ecological fallacy, and their conclusions therefore unsupported.

We have, using spin-echo nuclear magnetic resonance spectroscopy, measured
the relaxation times and diffusion coefficient of water protons in primary
mammary adenocarcinomas of mice. In our biological model, three morphological
stages were defined: (a) mammary gland tissue from pregnant mice, (b)
preneoplastic nodules, and (c) neoplastic tissue. It was found that neoplastic
tissues could be distinguished from normal and prenoeplastic tissue. Spin-spin
and spin-lattice relaxation times and the diffusion coefficient of water
protons are increased in the neoplastic tissue relative to mammary gland tissue
from pregnant mice and preneoplastic nodule tissue. These results suggested
that one can use a pulsed NMR method to detect and even predict breast cancer.

Biologically inspired pressure actuated cellular structures can alter their
shape through pressure variations. Previous work introduced a computational
framework for pressure actuated cellular structures which was limited to two
cell rows and central cell corner hinges. This article rigorously extends these
results by taking into account an arbitrary number of cell rows, a more
complicated cell kinematics that includes hinge eccentricities and varying side
lengths as well as rotational and axial cell side springs. The nonlinear
effects of arbitrary cell deformations are fully considered. Furthermore, the
optimization is considerably improved by using a second-order approach. The
presented framework enables the design of compliant pressure actuated cellular
structures that can change their form from one shape to another within a set of
one-dimensional C1 continuous functions.

Motivation: Although principal component analysis (PCA) is widely used for
the dimensional reduction of biomedical data, interpretation of PCA results
remains daunting. Most existing methods attempt to explain each principal
component (PC) in terms of a small number of variables by generating
approximate PCs with few non-zero loadings. Although useful when just a few
variables dominate the population PCs, these methods are often inadequate for
characterizing the PCs of high-dimensional genomic data. For genomic data,
reproducible and biologically meaningful PC interpretation requires methods
based on the combined signal of functionally related sets of genes. While gene
set testing methods have been widely used in supervised settings to quantify
the association of groups of genes with clinical outcomes, these methods have
seen only limited application for testing the enrichment of gene sets relative
to sample PCs. Results: We describe a novel approach, principal component gene
set enrichment (PCGSE), for computing the statistical association between gene
sets and the PCs of genomic data. The PCGSE method performs a two-stage
competitive gene set test using the correlation between each gene and each PC
as the gene-level test statistic with flexible choice of both the gene set test
statistic and the method used to compute the null distribution of the gene set
statistic. Using simulated data with simulated gene sets and real gene
expression data with curated gene sets, we demonstrate that biologically
meaningful and computationally efficient results can be obtained from a simple
parametric version of the PCGSE method that performs a correlation-adjusted
two-sample t-test between the gene-level test statistics for gene set members
and genes not in the set. Availability:
http://cran.r-project.org/web/packages/PCGSE/index.html Contact:
rob.frost@dartmouth.edu or jason.h.moore@dartmouth.edu

The prevalence of HIV in West Africa is lower than elsewhere in Africa but
Gabon has one of the highest rates of HIV in that region. Gabon has a small
population and a high per capita gross domestic product making it an ideal
place to carry out a programme of early treatment for HIV. The effectiveness,
availability and affordability of triple combination therapy make it possible
to contemplate ending AIDS deaths and HIV transmission in the short term and
HIV prevalence in the long term. Here we consider what would have happened in
Gabon without the development of potent anti-retroviral therapy (ART), the
impact that the current roll-out of ART has had on HIV, and what might be
possible if early treatment with ART becomes available to all. We fit a dynamic
transmission model to trends in the adult prevalence of HIV and infer trends in
incidence, mortality and the impact of ART. The availability of ART has reduced
the prevalence of HIV among adults not on ART from 4.2% to 2.9%, annual
incidence from 0.43% to 0.27%, and the proportion of adults dying from AIDS
illnesses each year from 0.36% to 0.13% saving the lives of 2.3 thousand people
in 2013 alone. The provision of ART has been highly cost effective saving the
country at least $18 million up to 2013.

In a recent article Hontelez and colleagues investigate the prospects for
elimination of HIV in South Africa through expanded access to antiretroviral
therapy (ART) using STDSIM, a micro-simulation model. One of the first
published models to suggest that expanded access to ART could lead to the
elimination of HIV, referred to by the authors as the Granich Model, was
developed and implemented by the present author. The notion that expanded
access to ART could lead to the end of the AIDS epidemic gave rise to
considerable interest and debate and remains contentious. In considering this
notion Hontelez et al. start by stripping down STDSIM to a simple model that is
equivalent to the model developed by the present author3 but is a stochastic
event driven model. Hontelez and colleagues then reintroduce levels of
complexity to explore ways in which the model structure affects the results. In
contrast to our earlier conclusions Hontelez and colleagues conclude that
universal voluntary counselling and testing with immediate ART at 90% coverage
should result in the elimination of HIV but would take three times longer than
predicted by the model developed by the present author. Hontelez et al. suggest
that the current scale-up of ART at CD4 cell counts less than 350 cells/microL
will lead to elimination of HIV in 30 years. I disagree with both claims and
believe that their more complex models rely on unwarranted and unsubstantiated
assumptions.

The past decade has witnessed a dramatic increase in the size and scope of
biological and behavioral experiments. These experiments are providing an
unprecedented level of detail and depth of data. However, this increase in data
presents substantial statistical and graphical hurdles to overcome, namely how
to distinguish signal from noise and how to visualize multidimensional results.
Here we present a series of tools designed to support a research project from
inception to publication. We provide implementation of dimension reduction
techniques and visualizations that function well with the types of data often
seen in animal behavior studies. This package is designed to be used with
experimental data but can also be used for experimental design and sample
justification. The goal for this project is to create a package that will
evolve over time, thereby remaining relevant and reflective of current methods
and techniques.

Cryopreservation is beset with the challenge of protocol alignment across a
wide range of cell types and process variables. By taking a cross-sectional
assessment of previously published cryopreservation data (sample means and
standard errors) as preliminary meta-data, a decision tree learning analysis
(DTLA) was performed to develop an understanding of target survival and
optimized pruning methods based on different approaches. Briefly, a clear
direction on the decision process for selection of methods was developed with
key choices being the cooling rate, plunge temperature on the one hand and
biomaterial choice, use of composites (sugars and proteins), loading procedure
and cell location in 3D scaffold on the other. Secondly, using machine learning
and generalized approaches via the Na\"ive Bayes Classification (NBC) approach,
these metadata were used to develop posterior probabilities for combinatorial
approaches that were implicitly recorded in the metadata. These latter results
showed that newer protocol choices developed using probability elicitation
techniques can unearth improved protocols consistent with multiple
unidimensional optimized physical protocols. In conclusion, this article
proposes the use of DTLA models and subsequently NBC for the improvement of
modern cryopreservation techniques through an integrative approach.
  Keywords: 3D cryopreservation, decision-tree learning (DTL), sugars, mouse
embryonic stem cells, meta-data, Na\"ive Bayes Classifier (NBC)

Four chapters of the synthesis represent four major areas of my research
interests: 1) data analysis in molecular biology, 2) mathematical modeling of
biological networks, 3) genome evolution, and 4) cancer systems biology. The
first chapter is devoted to my work in developing non-linear methods of
dimension reduction (methods of elastic maps and principal trees) which extends
the classical method of principal components. Also I present application of
matrix factorization techniques to analysis of cancer data. The second chapter
is devoted to the complexity of mathematical models in molecular biology. I
describe the basic ideas of asymptotology of chemical reaction networks aiming
at dissecting and simplifying complex chemical kinetics models. Two
applications of this approach are presented: to modeling NFkB and apoptosis
pathways, and to modeling mechanisms of miRNA action on protein translation.
The third chapter briefly describes my investigations of the genome structure
in different organisms (from microbes to human cancer genomes). Unsupervised
data analysis approaches are used to investigate the patterns in genomic
sequences shaped by genome evolution and influenced by the basic properties of
the environment. The fourth chapter summarizes my experience in studying cancer
by computational methods (through combining integrative data analysis and
mathematical modeling approaches). In particular, I describe the on-going
research projects such as mathematical modeling of cell fate decisions and
synthetic lethal interactions in DNA repair network. The synthesis is concluded
by listing major challenges in computational systems biology, connected to the
topics of this text, i.e. dealing with complexity of biological systems.

An accurate method for enumerating pathogen indicators, such as Escherichia
coli (E. coli), and Salmonella spp. is important for assessing the safety of
compost samples. This study aimed to determine the occurrence of pathogen
indicators in compost samples by using a molecular approach known as Polymerase
Chain Reaction (PCR). The DNA sample was extracted from sewage sludge compost.
The specificity of the probes and primers at the species level were verified by
performing NCBI-BLAST2 (Basic Local Alignment Search Tool). Primers that target
the gadAB gene for E.coli and invA gene for Salmonella spp. were selected which
produce fragment lengths around 670bp and 285bp respectively. The primers were
tested against bacterial cultures of both species and produced a strong signal
band of the expected fragment length. It provided results within 6 hours which
is relatively rapid compared to conventional culturing techniques. The other
advantages of PCR are shown to be its high sensitivity, and high specificity.

Accurate enumeration of Salmonella spp. is important for assessing whether
this pathogen has survived composting. Recent literature has reported that
enumeration of Salmonella spp. using standard microbiological methods has a
numbers of disadvantages, particularly the time taken to obtain a result. This
research is an attempt to develop a rapid, low-cost detection method that is
quantitative, highly sensitive and target specific. This paper reports the
development of a DNA fragment that can be used to quantify Salmonella spp. by
competitive polymerase chain reaction (cPCR) targeted at the invA gene of
Salmonella (PCR primers that target the invA gene are reported to have very
high specificity for Salmonella strains). It is shown that cPCR, which could be
completed in 5 hours, could quantify the number of copies of the Salmonella
invA gene in a sample solution.

Composting is defined as the biological decomposition and stabilization of
organic substrates under aerobic conditions to allow the development of
thermophilic temperatures. This thermophilic temperature is a result of
biologically produced heat. Composting produces the final product which is
sufficiently stable for storage and application to land without adverse
environmental effects. There are many factors which affect the decomposition of
organic matter in the composting process. Since the composting process is very
intricate, it is not easy to estimate the effect of a single factor on the rate
of organic matter decomposition. This paper looked at the main factors
affecting the composting process. Problems regarding the controlling,
inactivation and regrowth of pathogen in compost material are also discussed.

The survival of Salmonella spp. as pathogen indicator in composting was
studied. The inoculums technique was used to gives the known amounts of
Salmonella spp. involved in composting. The inoculums of Salmonella spp.
solution was added directly into the compost material. The direct inoculum was
compared with inoculums in vial technique. The Salmonella spp. solution placed
into a vial and inserted into the middle of compost material before starting
the composting process. The conventional method that is used for the
enumeration of Salmonella spp. is serial dilution followed by standard membrane
filtration as recommended in the compost quality standard method PAS 100 and
the British Standard (BS EN ISO 6579:2002). This study was designed to
investigate the relationship of temperature and contact material that may also
involve in pathogen activation specifically to Salmonella spp. The exposure to
an average temperature during composting of about 55-60{\deg}C was kept for at
least 3 days as it was reported sufficiently kills the vast majority of enteric
pathogen (Deportes et al., 1995). The amount of Salmonella spp. and temperature
for both samples was set as indicator to determine the survival of Salmonella
spp. in direct and non-direct inoculums. This study gives the figures of
die-off rate for Salmonella spp. during composting. The differentiation between
direct contact (Sample A) and non-contact of Salmonella spp. with compost
material (Sample B) during composting was also revealed. The results from
laboratory scale of composting study has been showed that after 8 days (which
included at least at 66{\deg}C) the numbers of Salmonella spp. in Sample A were
below the limits in UK compost standard (known as PAS 100)(BSI, 2005) which
required the compost to be free of Salmonella spp. Meanwhile, Sample B still
gives high amount of Salmonella spp. in even after composting for 20 days.

The mathematical models used to capture features of complex, biological
systems are typically non-linear, meaning that there are no generally valid
simple relationships between their outputs and the data that might be used to
validate them. This invalidates the assumptions behind standard statistical
methods such as linear regression, and often the methods used to parameterise
biological models from data are ad hoc. In this perspective, I will argue for
an approach to model fitting in mathematical biology that incorporates modern
statistical methodology without losing the insights gained through non-linear
dynamic models, and will call such an approach principled model fitting.
Principled model fitting therefore involves defining likelihoods of observing
real data on the basis of models that capture key biological mechanisms.

Recent studies show that the microbial communities inhabiting the human
intestine can have profound impact on our well-being and health. However, we
have limited understanding of the mechanisms that control this complex
ecosystem. Based on a deep phylogenetic analysis of the intestinal microbiota
in a thousand western adults we identified groups of bacteria that tend to be
either nearly absent, or abundant in most individuals. The abundances of these
bimodally distributed bacteria vary independently, and their contrasting
alternative states are associated with host factors such as ageing and
overweight. We propose that such bimodal groups represent independent tipping
elements of the intestinal microbiota. These reflect the overall state of the
intestinal ecosystem whose critical transitions can have profound health
implications and diagnostic potential.

The binding of transcription factors (TFs) is essential for gene expression.
One important characteristic is the actual occupancy of a putative binding site
in the genome. In this study, we propose an analytical model to predict genomic
occupancy that incorporates the preferred target sequence of a TF in the form
of a position weight matrix (PWM), DNA accessibility data (in case of
eukaryotes), the number of TF molecules expected to be bound specifically to
the DNA and a parameter that modulates the specificity of the TF. Given actual
occupancy data in form of ChIP-seq profiles, we backwards inferred copy number
and specificity for five Drosophila TFs during early embryonic development:
Bicoid, Caudal, Giant, Hunchback and Kruppel. Our results suggest that these
TFs display thousands of molecules that are specifically bound to the DNA and
that, while Bicoid and Caudal display a higher specificity, the other three
transcription factors (Giant, Hunchback and Kruppel) display lower specificity
in their binding (despite having PWMs with higher information content). This
study gives further weight to earlier investigations into TF copy numbers that
suggest a significant proportion of molecules are not bound specifically to the
DNA.

Preterm infants with very low birth weight suffer from a high risk of
intra-ventricular hemorrhage(IVH) and other serious diseases. To improve the
clinical risk assessment of preterm infants and develop potential clinically
makers for the adverse outcome, the first part of the paper develops the
frequency spectral analysis on the non-invasively measured heart rate
variability, blood pressure variability and cerebral near-infrared spectroscopy
measures. Moderate and high correlations with the clinical risk index for
babies were identified from various spectral measures of arterial baroreflex
and cerebral autoregulation functions. It was also observed that the
cross-spectral transfer function analysis of cerebral NIRS and arterial blood
pressure was able to provide a number of parameters that were potentially
useful for distinguishing between preterm infants with or without IVH.
Furthermore, the detrended fluctuation analysis that quantifies the fractal
correlation properties of physiological signals has been examined, to determine
whether it could derive markers for the identification of preterm infants with
IVH. Cardiac output(CO) and total peripheral resistance(TPR) are two important
parameters of the cardiovascular system. Measurement of these two parameters
can provide valuable information for the assessment and management of patients
needing intensive care, including preterm infants in the neonatal intensive
care unit. To further assess the changes in CO and TPR in the preterm infants,
the multivariate regression model based on the useful features from ABP method
was used to improve the accuracy and robustness of the estimation. The
combination of signal analysis and multivariate regression model in estimation
of CO has produced some outcomes, and in the future, more effort should be
involved in this kind of research to improve the prediction of serious diseases
in preterm infants.

A remarkable property of nastic, shape changing plants is their complete
fusion between actuators and structure. This is achieved by combining a large
number of cells whose geometry, internal pressures and material properties are
optimized for a given set of target shapes and stiffness requirements. An
advantage of such a fusion is that cell walls are prestressed by cell pressures
which increases, decreases the overall structural stiffness, weight. Inspired
by the nastic movement of plants, Pagitz et al. 2012 Bioinspir. Biomim. 7
published a novel concept for pressure actuated cellular structures. This
article extends previous work by introducing a modular approach to adaptive
structures. An algorithm that breaks down any continuous target shapes into a
small number of standardized modules is presented. Furthermore it is shown how
cytoskeletons within each cell enhance the properties of adaptive modules. An
adaptive passenger seat and an aircrafts leading, trailing edge is used to
demonstrate the potential of a modular approach.

Several methods are available for the detection of covarying positions from a
multiple sequence alignment (MSA). If the MSA contains a large number of
sequences, information about the proximities between residues derived from
covariation maps can be sufficient to predict a protein fold. If the structure
is already known, information on the covarying positions can be valuable to
understand the protein mechanism.
  In this study we have sought to determine whether a multivariate extension of
traditional mutual information (MI) can be an additional tool to study
covariation. The performance of two multidimensional MI (mdMI) methods,
designed to remove the effect of ternary/quaternary interdependencies, was
tested with a set of 9 MSAs each containing <400 sequences, and was shown to be
comparable to that of methods based on maximum entropy/pseudolikelyhood
statistical models of protein sequences. However, while all the methods tested
detected a similar number of covarying pairs among the residues separated by <
8 {\AA} in the reference X-ray structures, there was on average less than 65%
overlap between the top scoring pairs detected by methods that are based on
different principles.
  We have also attempted to identify whether the difference in performance
among methods is due to different efficiency in removing covariation
originating from chains of structural contacts. We found that the reason why
methods that derive partial correlation between the columns of a MSA provide a
better recognition of close contacts is not because they remove chaining
effects, but because they filter out the correlation between distant residues
that originates from general fitness constraints. In contrast we found that
true chaining effects are expression of real physical perturbations that
propagate inside proteins, and therefore are not removed by the derivation of
partial correlation between variables.

Clonal structure of the human peripheral T-cell repertoire is shaped by a
number of homeostatic mechanisms, including antigen presentation, cytokine and
cell regulation. Its accurate tuning leads to a remarkable ability to combat
pathogens in all their variety, while systemic failures may lead to severe
consequences like autoimmune diseases. Here we develop and make use of a
non-parametric statistical approach to assess T cell clonal size distributions
from recent next generation sequencing data. For 41 healthy individuals and a
patient with ankylosing spondylitis, who undergone treatment, we invariably
find power law scaling over several decades and for the first time calculate
quantitatively meaningful values of decay exponent. It has proved to be much
the same among healthy donors, significantly different for an autoimmune
patient before the therapy, and converging towards a typical value afterwards.
We discuss implications of the findings for theoretical understanding and
mathematical modeling of adaptive immunity.

Background: Recent research in animal behaviour has contributed to determine
how alignment, turning responses, and changes of speed mediate flocking and
schooling interactions in different animal species. Here, we address
specifically the problem of what interaction responses support different
nearest neighbour configurations in terms of mutual position and distance.
Results: We find that the different interaction rules observed in different
animal species may be a simple consequence of the relative positions that
individuals assume when they move together, and of the noise inherent with the
movement of animals, or associated with tracking inaccuracy. Conclusions: The
anisotropic positioning of individuals with respect to their neighbours, in
combination with noise, can explain several aspects of the movement responses
observed in real animal groups, and should be considered explicitly in future
models of flocking and schooling. By making a distinction between interaction
responses involved in maintaining a preferred flock configuration, and
interaction responses directed at changing it, we provide a frame to
discriminate movement interactions that signal directional conflict from those
underlying consensual group motion.

In this paper we study a reduced continuous model describing the local
evolution of high grade gliomas - a lethal type of primary brain tumor -
through the interplay of different cellular phenotypes. We show how hypoxic
events, even sporadic and/or limited in space may have a crucial role on the
acceleration of the growth speed of high grade gliomas. Our modeling approach
is based on two cellular phenotypes one of them being more migratory and the
second one more proliferative with transitions between them being driven by the
local oxygen values, assumed in this simple model to be uniform. Surprisingly
even acute hypoxia events (i.e. very localized in time) leading to the
appearance of migratory populations have the potential of accelerating the
invasion speed of the proliferative phenotype up to speeds close to those of
the migratory phenotype. The high invasion speed of the tumor persists for
times much longer than the lifetime of the hypoxic event and the phenomenon is
observed both when the migratory cells form a persistent wave of cells located
on the invasion front and when they form a evanecent wave dissapearing after a
short time by decay into the more proliferative phenotype.
  Our findings are obtained through numerical simulations of the model
equations. We also provide a deeper mathematical analysis of some aspects of
the problem such as the conditions for the existence of persistent waves of
cells with a more migratory phenotype.

One of the promising frontiers of bioengineering is the controlled release of
a therapeuticdrug from a vehicle across the skin (transdermal drug delivery).
In order to study the complete process, a two-phase mathematical model
describing the dynamics of a substance between two coupled media of different
properties and dimensions is presented. A system of partial differential
equations describes the diffusion and the binding/unbinding processes in both
layers. Additional flux continuity at the interface and clearance conditions
into systemic circulation are imposed. An eigenvalue problem with discontinuous
coefficients is solved and an analytical solution is given in the form of an
infinite series expansion. The model points out the role of the diffusion and
reaction parameters, which control the complex transfer mechanism and the drug
kinetics across the two layers. Drug masses are given and their dependence on
the physical parameters is discussed.

In the autoregressive process of first order AR(1), a homogeneous correlated
time series $u_t$ is recursively constructed as $u_t = q\; u_{t-1} + \sigma
\;\epsilon_t$, using random Gaussian deviates $\epsilon_t$ and fixed values for
the correlation coefficient $q$ and for the noise amplitude $\sigma$. To model
temporally heterogeneous time series, the coefficients $q_t$ and $\sigma_t$ can
be regarded as time-dependend variables by themselves, leading to the
time-varying autoregressive processes TVAR(1). We assume here that the time
series $u_t$ is known and attempt to infer the temporal evolution of the
'superstatistical' parameters $q_t$ and $\sigma_t$. We present a sequential
Bayesian method of inference, which is conceptually related to the Hidden
Markov model, but takes into account the direct statistical dependence of
successively measured variables $u_t$. The method requires almost no prior
knowledge about the temporal dynamics of $q_t$ and $\sigma_t$ and can handle
gradual and abrupt changes of these superparameters simultaneously. We compare
our method with a Maximum Likelihood estimate based on a sliding window and
show that it is superior for a wide range of window sizes.

Measurement of serum growth hormone by mass spectrometry is demonstrated to
be unaffected by interferences with growth hormone binding protein as
frequently encountered with antibody-based routine test methods and provides an
alternative approach, therefore, to acquisition of accurate results.

A novel portable fluorometer combining the attributes of a smartphone with an
easy fit, simple and compact sample chamber fabricated using 3D printing has
been developed for pH measurements of environmental water in the field. The
results were then compared directly with those obtained using conventional
electrode based measurements.

Size-structured population models provide a popular means to mathematically
describe phenomena such as bacterial aggregation, schooling fish, and
planetesimal evolution. For parameter estimation, generalized sensitivity
functions (GSFs) provide a tool that quantifies the impact of data from
specific regions of the experimental domain. These functions help identify the
most relevant data subdomains, which enhances the optimization of experimental
design. To our knowledge, GSFs have not been used in the partial differential
equation (PDE) realm, so we provide a novel PDE extension of the discrete and
continuous ordinary differential equation (ODE) concepts of Thomaseth and
Cobelli and Banks et al. respectively. We analyze the GSFs in the context of
size-structured population models, and specifically analyze the Smoluchowski
coagulation equation to determine the most relevant time and volume domains for
three, distinct aggregation kernels. Finally, we provide evidence that
parameter estimation for the Smoluchowski coagulation equation does not require
post-gelation data.

Imposing a minimum principle in the framework of the so called crystal basis
model of the genetic code, we determine the structure of the minimum set of 22
anticodons which allows the translational-transcription for animal
mitochondrial code. The results are in very good agreement with the observed
anticodons.
  Then, we analyze the evolution of the genetic code, with 20 amino acids
encoded from the beginning, from the viewpoint of codon-anticodon interaction.
Following the same spirit as above, we determine the structure of the
anticodons in the Ancient, Archetypal and Early Genetic codes. Most of our
results agree with the generally accepted scheme.

Localizing the sources of electrical activity in the brain from
Electroencephalographic (EEG) data is an important tool for non-invasive study
of brain dynamics. Generally, the source localization process involves a
high-dimensional inverse problem that has an infinite number of solutions and
thus requires additional constraints to be considered to have a unique
solution. In the context of EEG source localization, we propose a novel
approach that is based on dividing the cerebral cortex of the brain into a
finite number of Functional Zones which correspond to unitary functional areas
in the brain. In this paper we investigate the use of Brodmanns areas as the
Functional Zones. This approach allows us to apply a sparsity constraint to
find a unique solution for the inverse EEG problem. Compared to previously
published algorithms which use different sparsity constraints to solve this
problem, the proposed method is potentially more consistent with the known
sparsity profile of the human brain activity and thus may be able to ensure
better localization. Numerical experiments are conducted on a realistic head
model obtained from segmentation of MRI images of the head and includes four
major compartments namely scalp, skull, cerebrospinal fluid (CSF) and brain
with relative conductivity values. Three different electrode setups are tested
in the numerical experiments.

Thermodynamic aspects of chemical reactions have a long history in the
Physical Chemistry literature. In particular, biochemical cycles - the
building-blocks of biochemical systems - require a source of energy to
function. However, although fundamental, the role of chemical potential and
Gibb's free energy in the analysis of biochemical systems is often overlooked
leading to models which are physically impossible. The bond graph approach was
developed for modelling engineering systems where energy generation, storage
and transmission are fundamental. The method focuses on how power flows between
components and how energy is stored, transmitted or dissipated within
components. Based on early ideas of network thermodynamics, we have applied
this approach to biochemical systems to generate models which automatically
obey the laws of thermodynamics. We illustrate the method with examples of
biochemical cycles. We have found that thermodynamically compliant models of
simple biochemical cycles can easily be developed using this approach. In
particular, both stoichiometric information and simulation models can be
developed directly from the bond graph. Furthermore, model reduction and
approximation while retaining structural and thermodynamic properties is
facilitated. Because the bond graph approach is also modular and scaleable, we
believe that it provides a secure foundation for building thermodynamically
compliant models of large biochemical networks.

The nonlinearity of dynamics in systems biology makes it hard to infer them
from experimental data. Simple linear models are computationally efficient, but
cannot incorporate these important nonlinearities. An adaptive method based on
the S-system formalism, which is a sensible representation of nonlinear
mass-action kinetics typically found in cellular dynamics, maintains the
efficiency of linear regression. We combine this approach with adaptive model
selection to obtain efficient and parsimonious representations of cellular
dynamics. The approach is tested by inferring the dynamics of yeast glycolysis
from simulated data. With little computing time, it produces dynamical models
with high predictive power and with structural complexity adapted to the
difficulty of the inference problem.

1. Understanding how to find targets with very limited information is a topic
of interest in many disciplines. In ecology, such research has often focused on
the development of two movement models: i) the L\'evy walk and; ii) the
composite correlated random walk and its associated area-restricted search
behaviour. Although the processes underlying these models differ, they can
produce similar movement patterns. Due to this similarity and because of their
disparate formulation, current methods cannot reliably differentiate between
these two models.
  2. Here, we present a method that differentiates between the two models. It
consists of likelihood functions, including one for a hidden Markov model, and
associated statistical measures that assess the relative support for and
absolute fit of each model.
  3. Using a simulation study, we show that our method can differentiate
between the two search models over a range of parameter values. Using the
movement data of two polar bears (\textit{Ursus maritimus}), we show that the
method can be applied to complex, real-world movement paths.
  4. By providing the means to differentiate between the two most prominent
search models in the literature, and a framework that could be extended to
include other models, we facilitate further research into the strategies
animals use to find resources.

Persistent homology computes topological invariants from point cloud data.
Recent work has focused on developing statistical methods for data analysis in
this framework. We show that, in certain models, parametric inference can be
performed using statistics defined on the computed invariants. We develop this
idea with a model from population genetics, the coalescent with recombination.
We apply our model to an influenza dataset, identifying two scales of
topological structure which have a distinct biological interpretation.

We identify fundamental issues with discretization when estimating
information-theoretic quantities in the analysis of data. These difficulties
are theoretical in nature and arise with discrete datasets carrying significant
implications for the corresponding claims and results. Here we describe the
origins of the methodological problems, and provide a clear illustration of
their impact with the example of biological network reconstruction. We propose
an algorithm (shared information metric) that corrects for the biases and the
resulting improved performance of the algorithm demonstrates the need to take
due consideration of this issue in different contexts.

Metagenomics is an approach for characterizing environmental microbial
communities in situ, it allows their functional and taxonomic characterization
and to recover sequences from uncultured taxa. For communities of up to medium
diversity, e.g. excluding environments such as soil, this is often achieved by
a combination of sequence assembly and binning, where sequences are grouped
into 'bins' representing taxa of the underlying microbial community from which
they originate. Assignment to low-ranking taxonomic bins is an important
challenge for binning methods as is scalability to Gb-sized datasets generated
with deep sequencing techniques. One of the best available methods for the
recovery of species bins from an individual metagenome sample is the
expert-trained PhyloPythiaS package, where a human expert decides on the taxa
to incorporate in a composition-based taxonomic metagenome classifier and
identifies the 'training' sequences using marker genes directly from the
sample. Due to the manual effort involved, this approach does not scale to
multiple metagenome samples and requires substantial expertise, which
researchers who are new to the area may not have. With these challenges in
mind, we have developed PhyloPythiaS+, a successor to our previously described
method PhyloPythia(S). The newly developed + component performs the work
previously done by the human expert. PhyloPythiaS+ also includes a new k-mer
counting algorithm, which accelerated k-mer counting 100-fold and reduced the
overall execution time of the software by a factor of three. Our software
allows to analyze Gb-sized metagenomes with inexpensive hardware, and to
recover species or genera-level bins with low error rates in a fully automated
fashion.

Growth hormone (GH) constitutes a set of closely related protein isoforms. In
clinical practice, the disagreement of test results between commercially
available ligand-binding assays is still an ongoing issue, and incomplete
knowledge about the particular function of the different forms leaves an
uncertainty of what should be the appropriate measurand. Mass spectrometry is
promising to be a way forward. Not only is it capable of providing SI-traceable
reference values for the calibration of current GH-tests, but it also offers an
independent approach to highly reliable mass-selective quantification of
individual GH-isoforms. This capability may add to reliability in doping
control too. The article points out why and how.

Radiotherapy is a commonly used treatment for cancer and is usually given in
varying doses. At low radiation doses relatively few cells die as a direct
response to radiation but secondary radiation effects such as DNA mutation or
bystander effects affect many cells. Consequently it is at low radiation levels
where an understanding of bystander effects is essential in designing novel
therapies with superior clinical outcomes. In this article, we use a hybrid
multiscale mathematical model to study the direct effects of radiation as well
as radiation-induced bystander effects on both tumour cells and normal cells.
We show that bystander responses may play a major role in mediating radiation
damage to cells at low-doses of radiotherapy, doing more damage than that due
to direct radiation. The survival curves derived from our computational
simulations showed an area of hyper-radiosensitivity at low-doses that are not
obtained using a traditional radiobiological model.

Grade II gliomas are slowly growing primary brain tumors that affect mostly
young patients and become fatal after a few years. Current clinical handling
includes surgery as first line treatment. Cytotoxic therapies (radiotherapy RT
or chemotherapy QT) are used initially only for patients having a bad
prognosis. Therapies are administered following the 'maximum dose in minimum
time' principle, what is the same schedule used for high grade brain tumors.
Using mathematical models describing the growth of these tumors in response to
radiotherapy, we find that a extreme protraction therapeutical strategy, i.e.
enlarging substantially the time interval between RT fractions, may lead to a
better tumor control. Explicit formulas are found providing the optimal spacing
between doses in a very good agreement with the simulations of the full
three-dimensional mathematical model approximating the tumor spatio-temporal
dynamics. This idea, although breaking the well-stablished paradigm, has
biological meaning since in these slowly growing tumors it may be more
favourable to treat the tumor as the different tumor subpopulations move to
more sensitive phases of the cell cycle.

Using very precise (up to 0.05%) measurements of the growth parameters for
bacteria E. coli grown on minimal media, we aimed to determine the lowest
deuterium concentration at which the adverse effects that are prominent at
higher enrichments start to become noticeable. Such a threshold was found at
0.5% D, a surprisingly high value, while the ultralow deuterium concentrations
(up to 0.25% D) showed signs of the opposite trend. Bacterial adaptation for
400 generations in isotopically different environment confirmed preference for
ultralow (up to 0.25% D) enrichment. This effect appears to be similar to those
described in sporadic but multiple earlier reports. Possible explanations
include hormesis and isotopic resonance phenomena, with the latter explanation
being favored.

We illustrate shape mode analysis as a simple, yet powerful technique to
concisely describe complex biological shapes and their dynamics. We
characterize undulatory bending waves of beating flagella and reconstruct a
limit cycle of flagellar oscillations, paying particular attention to the
periodicity of angular data. As a second example, we analyze non-convex
boundary outlines of gliding flatworms, which allows us to expose stereotypic
body postures that can be related to two different locomotion mechanisms.
Further, shape mode analysis based on principal component analysis allows to
discriminate different flatworm species, despite large motion-associated shape
variability. Thus, complex shape dynamics is characterized by a small number of
shape scores that change in time. We present this method using descriptive
examples, explaining abstract mathematics in a graphic way.

Motivation: Assigning statistical significance accurately has become
increasingly important as meta data of many types, often assembled in
hierarchies, are constructed and combined for further biological analyses.
Statistical inaccuracy of meta data at any level may propagate to downstream
analyses, undermining the validity of scientific conclusions thus drawn. From
the perspective of mass spectrometry based proteomics, even though accurate
statistics for peptide identification can now be achieved, accurate protein
level statistics remain challenging.
  Results: We have constructed a protein ID method that combines peptide
evidences of a candidate protein based on a rigorous formula derived earlier;
in this formula the database $P$-value of every peptide is weighted, prior to
the final combination, according to the number of proteins it maps to. We have
also shown that this protein ID method provides accurate protein level
$E$-value, eliminating the need of using empirical post-processing methods for
type-I error control. Using a known protein mixture, we find that this protein
ID method, when combined with the Soric formula, yields accurate values for the
proportion of false discoveries. In terms of retrieval efficacy, the results
from our method are comparable with other methods tested.
  Availability: The source code, implemented in C++ on a linux system, is
available for download at
ftp://ftp.ncbi.nlm.nih.gov/pub/qmbp/qmbp_ms/RAId/RAId_Linux_64Bit

Locomotion and gross morphology have been important phenotypes for C. elegans
genetics since the inception of the field and remain relevant. In parallel with
developments in genome sequencing and editing, phenotyping methods have become
more automated and quantitative, making it possible to detect subtle
differences between mutants and wild-type animals. In this chapter, we describe
how to calibrate a single worm tracker consisting of a USB microscope mounted
on a motorized stage and how to record and analyze movies of worms crawling on
food. The resulting quantitative phenotypic fingerprint can sensitively
identify differences between mutant and wild type worms.

We mapped current and future temperature suitability for malaria transmission
in Africa using a published model that incorporates nonlinear physiological
responses to temperature of the mosquito vector Anopheles gambiae and the
malaria parasite Plasmodium falciparum. We found that a larger area of Africa
currently experiences the ideal temperature for transmission than previously
supposed. Under future climate projections, we predicted a modest increase in
the overall area suitable for malaria transmission, but a net decrease in the
most suitable area. Combined with population density projections, our maps
suggest that areas with temperatures suitable for year-round, highest risk
transmission will shift from coastal West Africa to the Albertine Rift between
Democratic Republic of Congo and Uganda, while areas with seasonal transmission
suitability will shift toward sub-Saharan coastal areas. Mapping temperature
suitability places important bounds on malaria transmissibility and, along with
local level demographic, socioeconomic, and ecological factors, can indicate
where resources may be best spent on malaria control.

UNAIDS has embraced an ambitious global target for the implementation of
treatment for people living with HIV. This 90-90-90 target would mean that, by
2020, 90% of all those living with HIV should know their status, 90% of these
would be on treatment and 90% of these would have fully suppressed plasma viral
loads. To reach this target in the next five years presents a major logistical
challenge. However, the prevalence of HIV varies greatly by risk groups, age,
gender, geography and social conditions. For reasons of effectiveness and
impact the focus must first be on those who are most likely to be infected with
HIV and therefore most likely to infect others. In Kenya the prevalence of HIV
in adults varies by two orders of magnitude among the counties. The effective
implementation of 90-90-90 will depend on first providing ART where the
prevalence of infection is greatest, then to those that are most easily reached
in large numbers and finally to the whole population. Here we use routine data
from ante-natal clinics and national survey data to assess the variation of the
prevalence of HIV among counties in Kenya; we suggest reasons for this
variation, and estimate the effectiveness of targeting the role out of ART. The
highest prevalence occurs in some of the counties bordering Lake Victoria and
these are most in need of ART. These districts in Nyanza Province, account for
31% of all cases in Kenya but make up 10% of the population and cover 1.8% of
the land-area. The highest concentrations of HIV cases are in Nairobi and
Mombasa which account for a further 18% of all cases in Kenya but make up 12%
of the population and cover 0.1% of the land-area. Providing ART in these two
cities will be relatively straightforward given their small geographical area.

There are various cases of animal movement where behaviour broadly switches
between two modes of operation, corresponding to a long distance movement state
and a resting or local movement state. Here a mathematical description of this
process is formulated, adapted from Friedrich et. al. (2006). The approach
allows the specification any running or waiting time distribution along with
any angular and speed distributions. The resulting system of partial
integro-differential equations are tumultuous and therefore it is necessary to
both simplify and derive summary statistics. An expression for the mean squared
displacement is derived which shows good agreement with experimental data from
the bacterium Escherichia coli and the gull Larus fuscus. Finally a large time
diffusive approximation is considered via a Cattaneo approximation (Hillen,
2004). This leads to the novel result that the effective diffusion constant is
dependent on the mean and variance of the running time distribution but only on
the mean of the waiting time distribution.

Multiple-scale and broad-scale assessments often require rescaling the
original data to a consistent grain size for analysis. Rescaling categorical
raster data by spatial aggregation is common in large area ecological
assessments. However, distortion and loss of information are associated with
aggregation. Using a majority rule generally results in dominant classes
becoming more pronounced and rare classes becoming less pronounced. Using
nearest neighbor techniques generally maintains the global proportion of each
category in the original map but can lead to disaggregation. In this paper we
implement the spatial scan statistic for spatial aggregation of categorical
raster maps and describe the behavior of the technique at the local level
(aggregation unit) and global level (map). We also contrast the spatial scan
statistic technique with the majority rule and nearest neighbor approaches. In
general, the scan statistic technique behaved inverse the majority rule
approach in that rare classes rather than abundant classes were preserved. We
suggest the scan statistic techniques should be used for spatial aggregation of
categorical maps when preserving heterogeneity and information from rare
classes are important goals of the study or assessment.

Motivation: The study of diverse enzyme superfamilies can provide important
insight into the relationships between protein sequence, structure and
function. It is often challenging, however, to discover these relationships
across a large and diverse superfamily. Contemporary similarity network
visualization techniques allow researchers to aggregate sequence similarity
information into a single global view. Network visualization provides a
qualitative estimate of functional diversity within a superfamily, but is
unable to quantitate explicit boundaries, when present, between neighboring
families in sequence space. This limits the potential of existing
sequence-based algorithms to generate functional predictions from superfamily
datasets.
  Results: By building on current network analysis tools, we have developed a
new algorithm for elucidating pairs of homologous families within a sequence
dataset. Our algorithm is able to filter through a dense similarity network in
order to estimate both the boundaries of individual families and also how the
families neighbor one another. Globally, these neighboring families define a
topology across the entire superfamily. The topology is simple to interpret by
visualizing the network output generated by our filtration protocol. We have
compared the network topology within the kinase superfamily against available
phylogenetic data. Our results suggest that neighbors within the filtered
kinase network are more likely to share structural and functional properties
than more distant network clusters.

We present a calculation technique for modeling inhomogeneous DNA replication
kinetics, where replication factors such as initiation rates or fork speeds can
change with both position and time. We can use our model to simulate data sets
obtained by molecular combing, a widely used experimental technique for probing
replication. We can also infer information about the replication program by
fitting our model to experimental data sets and also test the efficacy of
planned experiments by fitting our model to simulated data sets. We consider
asynchronous data sets and illustrate how a lack of synchrony affects
replication profiles. In addition to combing data, our technique is
well-adapted to microarray-based studies of replication.

Metabonomics, the measure of the fingerprint of biochemical perturbations
caused by disease, drugs or toxins, recently has become a major focus of
research in various areas especially indications of drug toxicity. Two types of
technology (known by the initials NMR and MS) are employed and both produce
massive data in form of spectra. Sophisticated statistical models, known as
pattern recognition techniques, are commonly applied for summarizing and
analyzing these multidimensional data. However, strong signals from compounds
that are administered during toxicological trials interfere with these models.
So called 'spectral replacement' is a method to eliminate these signals by
replacing them with the signals in their corresponding regions in control
spectrum. The replaced regions are subsequently scaled. However, this scaling
is not accurately measured and often results in overestimation of integrated
intensity of the replaced signals. Here, a novel protocol is proposed which
provides an accurate estimation of the replaced regions.

Solving the chemical master equation exactly is typically not possible, so
instead we must rely on simulation based methods. Unfortunately, drawing exact
realisations, results in simulating every reaction that occurs. This will
preclude the use of exact simulators for models of any realistic size and so
approximate algorithms become important. In this paper we describe a general
framework for assessing the accuracy of the linear noise and two moment
approximations. By constructing an efficient space filling design over the
parameter region of interest, we present a number of useful diagnostic tools
that aids modellers in assessing whether the approximation is suitable. In
particular, we leverage the normality assumption of the linear noise and moment
closure approximations.

The epidemic of HIV in Malawi started early and at its peak 15% of all adults
were infected with HIV. Malawi is a low-income country and the cost of putting
all HIV-positive people in Malawi onto ART, expressed as a percentage of the
gross domestic product, is the highest in the world. Nevertheless, Malawi has
made great progress and the greatly reduced cost of potent anti-retroviral
therapy (ART) makes it possible to contemplate ending the epidemic of HIV/AIDS.
Here we consider what would have happened without ART, the No ART
counterfactual, the impact if the current level of roll-out of ART is
maintained, the Current Programme, and the likely impact if treatment is made
available to everyone who is eligible under the 2013 guidelines of the World
Health Organization reaching full coverage by 2020, the Expanded Programme.
  The Current Programme has substantially reduced the epidemic of HIV and the
number of people dying of AIDS. The Expanded Programme has the potential to
avert more infections, save more lives and end the epidemic. The annual cost of
managing HIV will increase from about US$132 million in 2014 to about US$155
million in 2020 but will fall after that. If the Expanded Programme is
implemented several key areas must be addressed. Testing services will need to
be expanded and supported by mass testing campaigns, so as to diagnose people
with HIV and enrol them in treatment and care as early as possible. A regular
and uninterrupted supply of drugs will have to be assured. The quantity and
quality of existing health staff will need to be strengthened. Community health
workers will need to be mobilized and trained to encourage people to be tested
and accept treatment, to monitor progress and to support people on treatment;
this in turn will help to reduce stigma and discrimination, loss to follow up
of people diagnosed with HIV, and improve adherence for those on treatment.

A unified mathematical language for medicine and science will be presented.
Using this language, models for DNA replication, protein synthesis, chemical
reactions, neurons and a cardiac cycle of a heart have been built. Models for
Turing machines, cellular automaton, fractals and physical systems are also
represented with the use of this language. Interestingly, the language comes
with a way to represent probability theory concepts and also programming
statements. With this language, questions and processes in medicine can be
represented as systems of equations; and solutions to these equations are
viewed as treatments or previously unknown processes. This language can serve
as the framework for the creation of a large interactive open-access scientific
database that allows extensive mathematical medicine computations. It can also
serve as a basis for exploring ideas related to what could be called
metascience.

The effects of different levels of maltose on feed pellet water stability and
nutrient leaching were studied. Five treatments, including control with three
replicates with setup (0.0, 20, 25, 30 and 35%). Pellet leaching rates were
used to indicate pellet water stability. The results show that the presence of
maltose in the diets significantly improved pellet water stability (p<0.05),
but the leaching rates of the feed (35% maltose) observed higher than other
feeds. Increased maltose resulted in the corresponding decrease in pellet
stability. The protein leaching rate of control feed and feed (20% maltose) was
significantly (p < 0.05) lower than the rates of other diets The lipid leaching
rate of control feed was lower than the rates of other diets, while the feed
(35% maltose) was more leaching rate. It improved feeds water stability is one
important reason why maltose enhances fish growth.

We present the results of an experiment with light microscopy performed to
capture the trajectories of live Nitzschia sp. diatoms. The time series
corresponding to the motility of this kind of cells along ninety-five
circular-like trajectories have been obtained and analyzed with the scaling
statistical method of detrended fluctuation analysis optimized via a wavelet
transform. In this way, we determined the Hurst parameters, in two orthogonal
directions, which characterize the nature of the motion of live diatoms in
light microscopy experiments. We have found mean values of these directional
Hurst parameters between 0.70 and 0.63 with overall standard errors below 0.15.
These numerical values give evidence that the motion of Nitzschia sp. diatoms
is of persistent type and suggest an active cell motility with a kind of memory
associated with long-range correlations on the path of their trajectories. For
the collected statistics, we also find that the values of the Hurst exponents
depend on the number of abrupt turns that occur in the diatom trajectory and on
the type of wavelet, although their mean values do not change much

Natural and man-made transport webs are frequently dominated by dense sets of
nested cycles. The architecture of these networks, as defined by the topology
and edge weights, determines how efficiently the networks perform their
function. Yet, the set of tools that can characterize such a weighted
cycle-rich architecture in a physically relevant, mathematically compact way is
sparse. In order to fill this void, we have developed a new algorithm that
rests on an abstraction of the physical `tiling' in the case of a two
dimensional network to an effective tiling of an abstract surface in space that
the network may be thought to sit in. Generically these abstract surfaces are
richer than the flat plane and as a result there are now two families of
fundamental units that may aggregate upon cutting weakest links -- the
plaquettes of the tiling and the longer `topological' cycles associated with
the abstract surface itself. Upon sequential removal of the weakest links, as
determined by the edge weight, neighboring plaquettes merge and a tree
characterizing this merging process results. The properties of this
characteristic tree can provide the physical and topological data required to
describe the architecture of the network and to build physical models. The new
algorithm can be used for automated phenotypic characterization of any weighted
network whose structure is dominated by cycles, such as mammalian vasculature
in the organs, the root networks of clonal colonies like quaking aspen, or the
force networks in jammed granular matter.

Finding the underlying relationships among multiple imaging modalities in a
coherent fashion is one of challenging problems in the multimodal analysis. In
this study, we propose a novel multimodal network approach based on multidi-
mensional persistent homology. In this extension of the previous threshold-free
method of persistent homology, we visualize and discriminate the topological
change of integrated brain networks by varying not only threshold but also
mixing ratios between two different imaging modalities. Moreover, we also pro-
pose an integration method for multimodal networks, called one-dimensional
projection, with a specific mixing ratio between modalities. We applied the
proposed methods to PET and MRI data from 21 autism spectrum disorder (ASD)
children and 10 pediatric control subjects. From the results, we found that the
brain networks of ASD children and controls differ significantly, with ASD
showing asymmetrical changes of connected structures between PET and MRI. The
integrated MRI and PET networks showed that ASD children had weaker connections
than controls within the visual cortex, between dorsal and ventral parts of the
temporal pole, between frontal and parietal regions, and between the left
perisylvian and other brain regions. These results provide a multidimensional
homological understanding of disease-related PET and MRI networks that
discloses the network association with ASD.

Besides humans, several marine mammal species exhibit prerequisites to evolve
language: high cognitive abilities, flexibility in vocal production and
advanced social interactions. Here, we describe and analyse the vocal
repertoire of long-finned pilot whales (Globicephalus melas) recorded in
northern Norway. Observer based analysis reveals a complex vocal repertoire
with 140 different call types, call sequences, call repetitions and
group-specific differences in the usage of call types. Developing and applying
a new automated analysis method, the bag-of-calls approach, we find that groups
of pilot whales can be distinguished purely by statistical properties of their
vocalisations. Comparing inter-and intra-group differences of ensembles of
calls allows to identify and quantify group-specificity. Consequently, the
bag-of-calls approach is a valid method to specify difference and concordance
in acoustic communication in the absence of exact knowledge about signalers,
which is common observing marine mammals under natural conditions.

In this paper we develop a simple two compartment model which extends the
Farhi equation to the case when the inhaled concentration of a volatile organic
compound (VOC) is not zero. The model connects the exhaled breath concentration
of systemic VOCs with physiological parameters such as endogenous production
rates and metabolic rates. Its validity is tested with data obtained for
isoprene and inhaled deuterated isoprene-D5.

Chemical reactions inside cells are generally considered to happen within
fixed-size compartments. Needless to say, cells and their compartments are
highly dynamic. Thus, such stringent assumptions may not reflect biochemical
reality, and can highly bias conclusions from simulation studies. In this work,
we present an intuitive algorithm for particle-based diffusion in and on moving
boundaries, for both point particles and spherical particles. We first
benchmark in appropriate scenarios our proposed stochastic method against
solutions of partial differential equations, and further demonstrate that
moving boundaries can give rise to super diffusive motion as well as
time-inhomogeneous reaction rates. Finally, we conduct a numerical experiment
representing photobleaching of diffusing fluorescent proteins in dividing
Saccharomyces cerevisiae cells to demonstrate that moving boundaries might
cause important effects neglected in previously published studies.

Rhizoctonia solani anastomosis group AG2-2 IIIB is a severe sugar beet and
maize pathogen. It causes crown and root rot disease which leads to yield
losses world-wide. The soil-borne pathogen is difficult to detect and quantify
by conventional methods. We developed a real-time PCR (qPCR) assay for the
quantification of genomic DNA of Rhizoctonia solani AG2-2 IIIB based on the ITS
region of rDNA genes. The limit of quantification of the assay is 1.8 pg
genomic DNA. The amplification efficiency was 96.4. The assay will be helpful
in the diagnoses of Rhizoctonia solani infection of sugar beet and maize roots
and in the quantification of R. solani AG2-2 IIIB inoculum in plant debris and
soil.

Background: Gene expression studies on non-model organisms require open-end
strategies for transcription profiling. Gel-based analysis of cDNA fragments
allows to detect alterations in gene expression for genes which have neither
been sequenced yet nor are available in cDNA libraries. Commonly used protocols
are cDNA Differential Display (DDRT-PCR) and cDNA-AFLP. Both methods have been
used merely as qualitative gene discovery tools so far. Results: We developed
procedures for the conversion of DDRT-PCR data into quantitative transcription
profiles. Amplified cDNA fragments are separated on a DNA sequencer. Data
processing consists of four steps: (i) cDNA bands in lanes corresponding to
samples treated with the same primer combination are matched in order to
identify fragments originating from the same transcript, (ii) intensity of
bands is determined by densitometry, (iii) densitometric values are normalized,
and (iv) intensity ratio is calculated for each pair of corresponding bands.
Transcription profiles are represented by sets of intensity ratios (control vs.
treatment) for cDNA fragments defined by primer combination and DNA mobility.
We demonstrated the procedure by analyzing DDRT-PCR data on the effect of
secondary metabolites of oilseed rape Brassica napus on the transcriptome of
the pathogenic fungus Leptosphaeria maculans. Conclusion: We developed a data
processing procedure for quantitative analysis of amplified cDNA fragments. The
system utilizes common software and provides an open-end alternative to
microarray analysis. The processing is expected to work equally well with
DDRT-PCR and cDNA-AFLP data and be useful in research on organisms for which
microarray analysis is not available or economical.

Phenotypes are the observable characteristics of an organism arising from its
response to the environment. Phenotypes associated with engineered and natural
genetic variation are widely recorded using phenotype ontologies in model
organisms, as are signs and symptoms of human Mendelian diseases in databases
such as OMIM and Orphanet. Exploiting these resources, several computational
methods have been developed for integration and analysis of phenotype data to
identify the genetic etiology of diseases or suggest plausible interventions. A
similar resource would be highly useful not only for rare and Mendelian
diseases, but also for common, complex and infectious diseases. We apply a
semantic text- mining approach to identify the phenotypes (signs and symptoms)
associated with over 8,000 diseases. We demonstrate that our method generates
phenotypes that correctly identify known disease-associated genes in mice and
humans with high accuracy. Using a phenotypic similarity measure, we generate a
human disease network in which diseases that share signs and symptoms cluster
together, and we use this network to identify phenotypic disease modules.

Models of diffusion MRI within a voxel are useful for making inferences about
the properties of the tissue and inferring fiber orientation distribution used
by tractography algorithms. A useful model must fit the data accurately.
However, evaluations of model-accuracy of some of the models that are commonly
used in analyzing human white matter have not been published before. Here, we
evaluate model-accuracy of the two main classes of diffusion MRI models. The
diffusion tensor model (DTM) summarizes diffusion as a 3-dimensional Gaussian
distribution. Sparse fascicle models (SFM) summarize the signal as a linear sum
of signals originating from a collection of fascicles oriented in different
directions. We use cross-validation to assess model-accuracy at different
gradient amplitudes (b-values) throughout the white matter. Specifically, we
fit each model to all the white matter voxels in one data set and then use the
model to predict a second, independent data set. This is the first evaluation
of model-accuracy of these models. In most of the white matter the DTM predicts
the data more accurately than test-retest reliability; SFM model-accuracy is
higher than test-retest reliability and also higher than the DTM, particularly
for measurements with (a) a b-value above 1000 in locations containing fiber
crossings, and (b) in the regions of the brain surrounding the optic
radiations. The SFM also has better parameter-validity: it more accurately
estimates the fiber orientation distribution function (fODF) in each voxel,
which is useful for fiber tracking.

The production processes of proteins in prokaryotic cells are investigated.
Most of the mathematical models in the literature study the production of {\em
one} fixed type of proteins. When several classes of proteins are considered,
an important additional aspect has to be taken into account, the limited common
resources of the cell (polymerases and ribosomes) used by the production
process. Understanding the impact of this limitation is a key issue in this
domain. In this paper we focus on the allocation of ribosomes in the case of
the production of multiple proteins. The cytoplasm of the cell being a
disorganized medium subject to thermal noise, the protein production process
has an important stochastic component. For this reason, a Markovian model of
this process is introduced. Asymptotic results of the equilibrium are obtained
under a scaling procedure and a realistic biological assumption of saturation
of the ribosomes available in the cell. It is shown in particular that, in the
limit, the number of non-allocated ribosomes at equilibrium converges in
distribution to a Poisson distribution whose parameter satisfies a fixed point
equation. It is also shown that the production process of different types of
proteins can be seen as independent production processes but with modified
parameters.

It has been a common practice to place electrodes based on external
landmarks, rather than locating the appropriate organ first by imaging
technuiqes such as CT scan, ultrasound, etc. Therefore, aside from abiding the
cutaneous EGG (electrogastrography) electrodes placement rule, identification
of its waveform should be performed to ease the validation of one's EGG
recording method. This research focused on the assembly of EGG instrument, its
performance testing, and its usage on local white rabbit (O. cuniculus). A
total of 72 recordings obtained and processed. Data procession implies EGG
parameterization based on segmentation and time domain analysis. Therefore this
research gives an insight of an EGG recording method that could be applied on
another preclinical, veterinary, and even for clinical examination.

Many microbes associate with higher eukaryotes and impact their vitality. In
order to engineer microbiomes for host benefit, we must understand the rules of
community assembly and maintenence, which in large part, demands an
understanding of the direct interactions between community members. Toward this
end, we've developed a Poisson-multivariate normal hierarchical model to learn
direct interactions from the count-based output of standard metagenomics
sequencing experiments. Our model controls for confounding predictors at the
Poisson layer, and captures direct taxon-taxon interactions at the multivariate
normal layer using an $\ell_1$ penalized precision matrix. We show in a
synthetic experiment that our method handily outperforms state-of-the-art
methods such as SparCC and the graphical lasso (glasso). In a real, in planta
perturbation experiment of a nine member bacterial community, we show our
model, but not SparCC or glasso, correctly resolves a direct interaction
structure among three community members that associate with Arabidopsis
thaliana roots. We conclude that our method provides a structured, accurate,
and distributionally reasonable way of modeling correlated count based random
variables and capturing direct interactions among them.

EGG recordings performed on 13 local white rabbits (O. cuniculus) which
divided into 3 groups; acetosal 35 mg/kgBM receiver, reserpine 37.5 mg/kgBM
receiver, and control group. A total of 72 EGG recordings obtained from them,
which divided furthermore into 9 datasets based on prepandrial state,
postpandrial state, and post 1 hour drug administration state. EGG parameters
such as the number of cycle per minute ($cpm$), average voltage of action
potential segment ($\bar{V_a}$), root mean square voltage of action potential
segment ($A_{rms}$), root mean square voltage of all EGG segment ($V_{rms}$),
average period of action potential segment ($\bar{T_a}$), average period of
resting plateau ($\bar{T_i}$), average period difference among action potential
segment and resting plateau ($\bar{T_a - T_i}$), and dominant frequency ($f_d$)
are obtained. Insignificant difference of $f_d$ ($P$ = 0.9112993) and cpm ($P$
= 0.9382463) among 9 EGG datasets were found. These findings contrasted the
common practice of EGG assessment, which $f_d$ and $cpm$ are the main
parameters for diagnosis base. In other hand, significant difference between 9
EGG datasets found for $\bar{V_a}$, $A_{rms}$, and $V_{rms}$ parameter with $P$
= 0.0007346, 0.0039191, and 0.0000559 respectively. In conclusion, EGG
parameterization should not be limited to $f_d$ and $cpm$ only.

Pulse-type weakly electric fishes communicate through electrical discharges
with a stereotyped waveform, varying solely the interval between pulses
according to the information being transmitted. This simple codification
mechanism is similar to the one found in various known neuronal circuits, which
renders these animals as good models for the study of natural communication
systems, allowing experiments involving behavioral and neuroethological
aspects. Performing analysis of data collected from more than one freely
swimming fish is a challenge since the detected electric organ discharge (EOD)
patterns are dependent on each animal's position and orientation relative to
the electrodes. However, since each fish emits a characteristic EOD waveform,
computational tools can be employed to match each EOD to the respective fish.
In this paper we describe a computational method able to recognize fish EODs
from dyads using normalized feature vectors obtained by applying Fourier and
dual-tree complex wavelet packet transforms. We employ support vector machines
as classifiers, and a continuity constraint algorithm allows us to solve issues
caused by overlapping EODs and signal saturation. Extensive validation
procedures with Gymnotus sp. showed that EODs can be assigned correctly to each
fish with only two errors per million discharges.

Spaced seeds have been recently shown to not only detect more alignments, but
also to give a more accurate measure of phylogenetic distances (Boden et al.,
2013, Horwege et al., 2014, Leimeister et al., 2014), and to provide a lower
misclassification rate when used with Support Vector Machines (SVMs) (On-odera
and Shibuya, 2013), We confirm by independent experiments these two results,
and propose in this article to use a coverage criterion (Benson and Mak, 2008,
Martin, 2013, Martin and No{\'e}, 2014), to measure the seed efficiency in both
cases in order to design better seed patterns. We show first how this coverage
criterion can be directly measured by a full automaton-based approach. We then
illustrate how this criterion performs when compared with two other criteria
frequently used, namely the single-hit and multiple-hit criteria, through
correlation coefficients with the correct classification/the true distance. At
the end, for alignment-free distances, we propose an extension by adopting the
coverage criterion, show how it performs, and indicate how it can be
efficiently computed.

The purpose of this study was to enhance the existing time dependent flux
model for the transdermal iontophoretic transport of drugs. This study
evaluated the flux data as influenced by time and current density. In vitro
iontophoresis performed on the piglet (Sus scrofa) necropsy-taken medial scapha
pinneal skin that mounted in the U shaped sink chamber. Iontophoresis of
atenolol with a constant dose of 1000 ppm was implemented for 3 hours with
acceptor phase sampling every 30 minutes. Data were analised based on
exponential fitting of each current density value to produce a current density
dependent flux model. This model then combined with the time differential model
of flux to produce a flux model that takes account of both current density and
time.

Understanding historical trends in the epidemic of HIV is important for
assessing current and projecting future trends in prevalence, incidence and
mortality and for evaluating the impact and cost-effectiveness of control
measures. In generalized epidemics the available data are of variable quality
among countries and limited mainly to trends in the prevalence of HIV among
women attending ante-natal clinics. In concentrated epidemics one needs, at the
very least, time trends in the prevalence of HIV among different risk groups,
including intravenous drug users, men-who-have-sex-with-men, and commercial sex
workers as well as the size of each group and the degree of overlap between
them. Here we focus on the comparatively straight forward problems presented by
generalized epidemics. We fit data from Kenya to a susceptible-infected model
and then successively add structure to the model, drawing on our knowledge of
the natural history of HIV, to explore the effect that different structural
aspects of the model have on the fits and the projections.
  Both heterogeneity in risk and changes in behaviour over time are important
but easily confounded. Using a Weibull rather than exponential survival
function for people infected with HIV, in the absence of treatment, makes a
significant difference to the estimated trends in incidence and mortality and
to the projected trends. Allowing for population growth has a small effect on
the fits and the projections but is easy to include. Including details of the
demography adds substantially to the complexity of the model, increases the run
time by several orders of magnitude, but changes the fits and projections only
slightly and to an extent that is less than the uncertainty inherent in the
data. We make specific recommendations for the kind of model that would be
suitable for understanding and managing HIV epidemics in east and southern
Africa.

Systems that evolve over time and follow mathematical laws as they do so, are
called dynamical systems. Lymphocyte recovery and clinical outcomes in 41
allograft recipients conditioned using anti-thymocyte globulin (ATG) and 4.5
Gray total-body-irradiation were studied to determine if immune reconstitution
could be described as a dynamical system. Survival, relapse, and graft vs. host
disease (GVHD) were not significantly different in two cohorts of patients
receiving different doses of ATG. However, donor-derived CD3+ (ddCD3) cell
reconstitution was superior in the lower ATG dose cohort, and there were fewer
instances of donor lymphocyte infusion (DLI). Lymphoid recovery was plotted in
each individual over time and demonstrated one of three sigmoid growth
patterns; Pattern A (n=15), had rapid growth with high lymphocyte counts,
pattern B (n=14), slower growth with intermediate recovery and pattern C, poor
lymphocyte reconstitution (n=10). There was a significant association between
lymphocyte recovery patterns and both the rate of change of ddCD3 at day 30
post-SCT and the clinical outcomes. GVHD was observed more frequently with
pattern A; relapse and DLI more so with pattern C, with a consequent survival
advantage in patients with patterns A and B. We conclude that evaluating immune
reconstitution following SCT as a dynamical system may differentiate patients
at risk of adverse outcomes and allow early intervention to modulate that risk.

This paper presents a novel model for wine fermentation including a death
phase for yeast and the influence of oxygen on the process. A model for the
inclusion of the yeast dying phase is derived and compared to a model taken
from the literature. The modeling ability of the several models is analyzed by
comparing their simulation results.

Geometry of the metabolic trajectories is characteristic of the biological
response (Keun, Ebbels et al. 2004). Yet, due to unavoidable inter-individual
variations, the exact trajectories characterising the biological responses
differ. We examined whether the differences seen between metabolic trajectories
of a specific treatment, correspond to the variations seen in the other
biological manifestations of the same treatment. Differences in trajectories
were measured via alignment procedures which introduced and implemented in this
study. Our study revealed strong correlation between the scales of the aligned
trajectories of metabolic responses and the severity of the hepatocelluar
lesions induced after administration of hydrazine. Thus the results confirm
that aligned trajectories are characteristic of a specific treatment. They then
can be used for comparison with other treatment specific or unknown metabolic
trajectories and can have many metabonomic applications such as preclinical
toxicological screening

We use a large single particle tracking data set to analyze the short time
and small spatial scale motion of quantum dots labeling proteins in cell
membranes. Our analysis focuses on the jumps which are the changes in the
position of the quantum dots between frames in a movie of their motion.
Previously we have shown that the directions of the jumps are uniformly
distributed and the jump lengths can be characterized by a double power law
distribution.
  Here we show that the jumps over a small number of time steps can be
described by scalings of a {\em single} double power law distribution. This
provides additional strong evidence that the double power law provides an
accurate description of the fine scale motion. This more extensive analysis
provides strong evidence that the double power law is a novel stable
distribution for the motion. This analysis provides strong evidence that an
earlier result that the motion can be modeled as diffusion in a space of
fractional dimension roughly 3/2 is correct. The form of the power law
distribution quantifies the excess of short jumps in the data and provides an
accurate characterization of the fine scale diffusion and, in fact, this
distribution gives an accurate description of the jump lengths up to a few
hundred nanometers. Our results complement of the usual mean squared
displacement analysis used to study diffusion at larger scales where the
proteins are more likely to strongly interact with larger membrane structures.

A network is a set of nodes that are linked together by a set of edges.
Networks can represent any set of objects that have relations among themselves.
Communities are sets of nodes that are related in an important way, probably
sharing common properties and/or playing similar roles within a network. When
network analysis is applied to study the livestock movement patterns, the
epidemiological units of interest (farm premises, counties, states, countries,
etc.) are represented as nodes, and animal movements between the nodes are
represented as the edges of a network. Unraveling a network structure, and
hence the trade preferences and pathways, could be very useful to a researcher
or a decision-maker. We implemented a community detection algorithm to find
livestock communities that is consistent with the definition of a livestock
production zone, assuming that a community is a group of farm premises in which
an animal is more likely to stay during its life time than expected by chance.
We applied this algorithm to the network of within animal movements made inside
the State of Mato Grosso, for the year of 2007. This database holds information
about 87,899 premises and 521,431 movements throughout the year, totalizing
15,844,779 animals moved. The community detection algorithm achieved a network
partition that shows a clear geographical and commercial pattern, two crucial
features to preventive veterinary medicine applications, and also has a
meaningful interpretation in trade networks where links emerge from the choice
of trader nodes.

Objectives: In the United States, 25% of people with type 2 diabetes are
undiagnosed. Conventional screening models use limited demographic information
to assess risk. We evaluated whether electronic health record (EHR) phenotyping
could improve diabetes screening, even when records are incomplete and data are
not recorded systematically across patients and practice locations. Methods: In
this cross-sectional, retrospective study, data from 9,948 US patients between
2009 and 2012 were used to develop a pre-screening tool to predict current type
2 diabetes, using multivariate logistic regression. We compared (1) a full EHR
model containing prescribed medications, diagnoses, and traditional predictive
information, (2) a restricted EHR model where medication information was
removed, and (3) a conventional model containing only traditional predictive
information (BMI, age, gender, hypertensive and smoking status). We
additionally used a random-forests classification model to judge whether
including additional EHR information could increase the ability to detect
patients with Type 2 diabetes on new patient samples. Results: Using a
patient's full or restricted EHR to detect diabetes was superior to using basic
covariates alone (p<0.001). The random forests model replicated on out-of-bag
data. Migraines and cardiac dysrhythmias were negatively associated with type 2
diabetes, while acute bronchitis and herpes zoster were positively associated,
among other factors. Conclusions: EHR phenotyping resulted in markedly superior
detection of type 2 diabetes in a general US population, could increase the
efficiency and accuracy of disease screening, and are capable of picking up
signals in real-world records.

Spatial reaction-diffusion models have been employed to describe many
emergent phenomena in biological systems. The modelling technique most commonly
adopted in the literature implements systems of partial differential equations
(PDEs), which assumes there are sufficient densities of particles that a
continuum approximation is valid. However, due to recent advances in
computational power, the simulation, and therefore postulation, of
computationally intensive individual-based models has become a popular way to
investigate the effects of noise in reaction-diffusion systems in which regions
of low copy numbers exist.
  The stochastic models with which we shall be concerned in this manuscript are
referred to as `compartment-based'. These models are characterised by a
discretisation of the computational domain into a grid/lattice of
`compartments'. Within each compartment particles are assumed to be well-mixed
and are permitted to react with other particles within their compartment or to
transfer between neighbouring compartments.
  We develop two hybrid algorithms in which a PDE is coupled to a
compartment-based model. Rather than attempting to balance average fluxes, our
algorithms answer a more fundamental question: `how are individual particles
transported between the vastly different model descriptions?' First, we present
an algorithm derived by carefully re-defining the continuous PDE concentration
as a probability distribution. Whilst this first algorithm shows strong
convergence to analytic solutions of test problems, it can be cumbersome to
simulate. Our second algorithm is a simplified and more efficient
implementation of the first, it is derived in the continuum limit over the PDE
region alone. We test our hybrid methods for functionality and accuracy in a
variety of different scenarios by comparing the averaged simulations to
analytic solutions of PDEs for mean concentrations.

Recent studies have revealed that for the majority of species the length
distributions of duplicated sequences in natural DNA follow a power-law tail.
We study duplication-mutation models for processes in natural DNA sequences and
the length distributions of exact matches computed from both synthetic and
natural sequences. Here we present a hierarchy of equations for various number
of exact matches for these models. The reduction of these equations to one
equation for pairs of exact repeats is found. Quantitative correspondence of
solutions of the equation to simulations is demonstrated.

Models accounting for imperfect detection are important. Single-visit methods
have been proposed as an alternative to multiple-visits methods to relax the
assumption of closed population. Knape and Korner-Nievergelt (2015) showed that
under certain models of probability of detection single-visit methods are
statistically non-identifiable leading to biased population estimates. There is
a close relationship between estimation of the resource selection probability
function (RSPF) using weighted distributions and single-visit methods for
occupancy and abundance estimation. We explain the precise mathematical
conditions needed for RSPF estimation as stated in Lele and Keim (2006). The
identical conditions, that remained unstated in our papers on single-visit
methodology, are needed for single-visit methodology to work. We show that the
class of admissible models is quite broad and does not excessively restrict the
application of the RSPF or the single-visit methodology. To complement the work
by Knape and Korner-Nievergelt, we study the performance of multiple-visit
methods under the scaled logistic detection function and a much wider set of
situations. In general, under the scaled logistic detection function
multiple-visits methods also lead to biased estimates. As a solution to this
problem, we extend the single-visit methodology to a class of models that
allows use of scaled probability function. We propose a Multinomial extension
of single visit methodology that can be used to check whether the detection
function satisfies the RSPF condition or not. Furthermore, we show that if the
scaling factor depends on covariates, then it can also be estimated.

Auto-logistic and related auto-models, implemented approximately as
autocovariate regression, provide simple and direct modelling of spatial
dependence. The autologistic model has been widely applied in ecology since
Augustin, Mugglestone and Buckland (J. Appl. Ecol., 1996, 33, 339) analysed red
deer census data using a hybrid estimation approach, combining maximum
pseudo-likelihood estimation with Gibbs sampling of missing data. However
Dormann (Ecol. Model., 2007, 207, 234) questioned the validity of auto-logistic
regression, giving examples of apparent underestimation of covariate parameters
in analysis of simulated "snouter" data. Dormann et al. (Ecography, 2007, 30,
609) extended this analysis to auto-Poisson and auto-normal models, reporting
similar anomalies. All the above studies employ neighbourhood weighting schemes
inconsistent with conditions (Besag, J. R. Stat. Soc., Ser. B, 1974, 36, 192)
required for auto-model validity; furthermore the auto-Poisson analysis fails
to exclude cooperative interactions. We show that all "snouter" anomalies are
resolved by correct auto-model implementation. Re-analysis of the red deer data
shows that invalid neighbourhood weightings generate only small estimation
errors for the full dataset, but larger errors occur on geographic subsamples.
A substantial fraction of papers applying auto-logistic regression to
ecological data use these invalid weightings, which are default options in the
widely used "spdep" spatial dependence package for R. Auto-logistic analyses
using invalid neighbourhood weightings will be erroneous to an extent that can
vary widely. These analyses can easily be corrected by using valid
neighbourhood weightings available in "spdep". The hybrid estimation approach
for missing data is readily adapted for valid neighbourhood weighting schemes
and is implemented here in R for application to sparse presence-absence data.

The delay difference model was implemented to fit 21 years of brown tiger
prawn (Penaeus esculentus) catch in Moreton Bay by maximum likelihood to assess
the status of this stock. Monte Carlo simulations testing of the stock
assessment software coded in C++ showed that the model could estimate
simultaneously natural mortality in addition to catchability, recruitment and
initial biomasses. Applied to logbooks data collected from 1990 to 2010, this
implementation of the delay difference provided for the first time an estimate
of natural mortality for brown tiger prawn in Moreton Bay, equal to $0.031 \pm
0.002$ week$^{-1}$. This estimate is approximately 30\% lower than the value of
natural mortality (0.045 week$^{-1}$) used in previous stock assessments of
this species.

Individuals traversing challenging obstacles are faced with a decision: they
can adopt traversal strategies that minimally disrupt their normal locomotion
patterns or they can adopt strategies that substantially alter their gait,
conferring new advantages and disadvantages. We flew pigeons (Columba livia)
through an array of vertical obstacles in a flight arena, presenting them with
this choice. The pigeons selected either a strategy involving only a slight
pause in the normal wingbeat cycle, or a wings folded posture granting reduced
efficiency but greater stability should a misjudgment lead to collision. The
more stable but less efficient flight strategy was not employed to traverse
easy obstacles with wide gaps for passage, but came to dominate the postures
used as obstacle challenge increased with narrower gaps and there was a greater
chance of a collision. These results indicate that birds weigh potential
obstacle negotiation strategies and estimate task difficulty during locomotor
pattern selection.

Carey's Equality pertaining to stationary models is well known. In this
paper, we have stated and proved a fundamental theorem related to the formation
of this Equality. This theorem will provide an in-depth understanding of the
role of each captive subject, and their corresponding follow-up duration in a
stationary population. We have demonstrated a numerical example of a captive
cohort and the survival pattern of medfly populations. These results can be
adopted to understand age-structure and aging process in stationary and
non-stationary population population models. Key words: Captive cohort, life
expectancy, symmetric patterns.

Respiration measurements of whole tree plants have been reported that give
evidence that the relative per volume/mass unit respiration decreases with
increase of tree body size. In this study, based on the available data
published a question was explored if the relative per area unit respiration in
trees can be a constant, independent of the surface area size. There is a
definite gap in the published data when the allometric studies of tree body
structure do not intercept with studies on trees respiration. Thus the question
was studied with the help of indirect comparison between various data. The
comparison showed that the scaling exponents, volume vs. surface area and
respiration vs. stem volume, are slightly larger than they should be for the
hypothesis of the relative respiration constancy to hold. The data studied give
evidence that the relative per area unit respiration slightly increases with
the increase in tree surface area. Possible explanations of the relationship
include a different distribution of metabolically active parts of stem and
higher nitrogen content in larger trees. Also, the published datasets might
include large fast growing trees, which imply that larger trees grow faster and
hence have higher per unit surface area growth respiration. A crucial
experiment is required in which the respiration measurements were performed for
the same data as the measurements of scaling between stem volume and surface
area.

We describe the current state and future plans for a set of tools for
scientific data management (SDM) designed to support scientific transparency
and reproducible research. SDM has been in active use at our MRI Center for
more than two years. We designed the system to be used from the beginning of a
research project, which contrasts with conventional end-state databases that
accept data as a project concludes. A number of benefits accrue from using
scientific data management tools early and throughout the project, including
data integrity as well as reuse of the data and of computational methods.

Colony Collapse Disorder has become a global problem for beekeepers and for
the crops which depend on bee polination. Multiple factors are known to
increase the risk of colony colapse, and the ectoparasitic mite Varroa
destructor that parasitizes honey bees is among the main threats to colony
health. Although this mite is unlikely to, by itself, cause the collapse of
hives, it plays an important role as it is a vector for many viral diseases.
Such diseases are among the likely causes for Colony Collapse Disorder.
  The effects of V. destructor infestation are disparate in different parts of
the world. Greater morbidity - in the form of colony losses - has been reported
in colonies of European honey bees (EHB) in Europe, Asia and North America.
However, this mite has been present in Brasil for many years and yet there are
no reports of Africanized honey bee (AHB) colonies losses.
  Studies carried out in Mexico showed that some resistance behaviors to the
mite - especially grooming and hygienic behavior - appear to be different in
each subspecies. Could those difference in behaviors explain why the AHB are
less susceptible to Colony Collapse Disorder?
  In order to answer this question, we propose a mathematical model of the
coexistence dynamics of these two species, the bee and the mite, to analyze the
role of resistance behaviors in the overall health of the colony, and, as a
consequence, its ability to face epidemiological challenges.

The escape trajectories animals take following a predatory attack appear to
show high degrees of apparent 'randomness' - a property that has been described
as 'protean behaviour'. Here we present a method of quantifying the escape
trajectories of individual animals using a path complexity approach. When fish
(Pseudomugil signifer) were attacked either on their own or in groups, we find
that an individual's path rapidly increases in entropy (our measure of
complexity) following the attack. For individuals on their own, this entropy
remains elevated (indicating a more random path) for a sustained period (10
seconds) after the attack, whilst it falls more quickly for individuals in
groups. The entropy of the path is context dependent. When attacks towards
single fish come from greater distances, a fish's path shows less complexity
compared to attacks that come from short range. This context dependency effect
did not exist, however, when individuals were in groups. Nor did the path
complexity of individuals in groups depend on a fish's local density of
neighbours. We separate out the components of speed and direction changes to
determine which of these components contributes to the overall increase in path
complexity following an attack. We found that both speed and direction measures
contribute similarly to an individual's path's complexity in absolute terms.
Our work highlights the adaptive behavioural tactics that animals use to avoid
predators and also provides a novel method for quantifying the escape
trajectories of animals.

A total of 50 patients were enrolled in the study, and MRI brain with MR
spectroscopy was done. Tuberculosis was the most common neurologic disease
found in the HIV positive group, consisting of 9 patients. Seven of these
patients had tuberculous meningitis amongst which a further 2 had vasculitic
infarcts.PML was seen in 6 patients. NAA to Cr ratio was found to be reduced in
all the patients, and in fact the value was further reduced compared to the HIV
positive group as a whole. Raised choline and myoInositol peaks were also found
in all the patients. MR Spectroscopy showed lipid lactate peaks confirming the
diagnosis. 2 patients had HIV encephalopathy on the imaging study. Their
spectra also revealed lowered NAA peaks along with raised choline peaks. 2
patients with cryptococcosis showed characteristic imaging finding of enlarged
Virchow Robin (perivascular) spaces. They revealed elevated choline peaks in
addition to reduced NAA. The values of NAA to Cr ratio were determined after
duly processing the spectroscopic data from both cases and controls. Each group
(Cases and controls) were divided on the basis of age into two age groups:
Lesser than or equal to 40 years, and greater than 40 years. In all three
groups the values of the mean NAA to Cr ratio ratio was significantly (p-value
less than 0.05) reduced in comparison to controls. An ancillary finding was the
reduction of NAA to Cr ratio further in cases of PML. Combined use of both the
conventional and advanced MRI sequences is advisable as spectroscopy helps in
confirming the diagnosis of opportunistic infection of the CNS in HIV positive
patients. NAA to Cr ratio ratio is reduced in HIV positive patients and is a
marker for HIV infection of the brain even in the absence of imaging findings
of HIV encephalopathy or when the patient is symptomatic due to neurological
disease of other etiologies.

An analysis of breast cancer incidences in women and the relationship between
ethnicity and survival rate has been an ongoing study with recorded incidences
of missing values in the secondary data. In this paper, we study and report the
results of breast cancer survival rate by ethnicity, age and income groups from
the dataset collected for 53593 patients in South East England between the
years 1998 and 2003. In addition to this, we also predict the missing values
for the ethnic groups in the dataset. The principle findings in our study
suggest that: 1) women of white ethnicity in South East England have a highest
percentage of survival rate when compared to the black ethnicity, 2) High
income groups have higher survival rates to that of lower income groups and 3)
Age groups between 80-95 have lower percentage of survival rate.

We present a variant of the well sounded Expectation-Maximization Clustering
algorithm that is constrained to generate partitions of the input space into
high and low values. The motivation of splitting input variables into high and
low values is to favour the semantic interpretation of the final clustering.
The Expectation-Maximization binary Clustering is specially useful when a
bimodal conditional distribution of the variables is expected or at least when
a binary discretization of the input space is deemed meaningful. Furthermore,
the algorithm deals with the reliability of the input data such that the larger
their uncertainty the less their role in the final clustering. We show here its
suitability for behavioural annotation of movement trajectories. However, it
can be considered as a general purpose algorithm for the clustering or
segmentation of multivariate data or temporal series.

In metastatic castration-resistant prostate cancer (mCRPC) clinical trials,
the assessment of treatment efficacy essentially relies on the time-to-death
and the kinetics of prostate-specific antigen (PSA). Joint modelling has been
increasingly used to characterize the relationship between a time-to-event and
a biomarker kinetics but numerical difficulties often limit this approach to
linear models. Here we evaluated by simulation the capability of a new feature
of the Stochastic Approximation Expectation-Maximization algorithm in Monolix
to estimate the parameters of a joint model where PSA kinetics was defined by a
mechanistic nonlinear mixed-effect model. The design of the study and the
parameter values were inspired from one arm of a clinical trial. Increasingly
high levels of association between PSA and survival were considered and results
were compared with those found using two simplified alternatives to joint
model, a two-stage and a joint sequential model. We found that joint model
allowed for a precise estimation of all longitudinal and survival parameters.
In particular the effect of PSA kinetics on survival could be precisely
estimated, regardless of the strength of the association. In contrast, both
simplified approaches led to bias on longitudinal parameters and two-stage
model systematically underestimated the effect of PSA kinetics on survival. In
summary we showed that joint model can be used to characterize the relationship
between a nonlinear kinetics and survival. This opens the way for the use of
more complex and physiological models to improve treatment evaluation and
prediction in oncology.

Clinicians need to predict patient outcomes with high accuracy as early as
possible after disease inception. In this manuscript, we show that
patient-to-patient variability sets a fundamental limit on outcome prediction
accuracy for a general class of mathematical models for the immune response to
infection. However, accuracy can be increased at the expense of delayed
prognosis. We investigate several systems of ordinary differential equations
(ODEs) that model the host immune response to a pathogen load. Advantages of
systems of ODEs for investigating the immune response to infection include the
ability to collect data on large numbers of `virtual patients', each with a
given set of model parameters, and obtain many time points during the course of
the infection. We implement patient-to-patient variability $v$ in the ODE
models by randomly selecting the model parameters from Gaussian distributions
with variance $v$ that are centered on physiological values. We use logistic
regression with one-versus-all classification to predict the discrete
steady-state outcomes of the system. We find that the prediction algorithm
achieves near $100\%$ accuracy for $v=0$, and the accuracy decreases with
increasing $v$ for all ODE models studied. The fact that multiple steady-state
outcomes can be obtained for a given initial condition, i.e. the basins of
attraction overlap in the space of initial conditions, limits the prediction
accuracy for $v>0$. Increasing the elapsed time of the variables used to train
and test the classifier, increases the prediction accuracy, while adding
explicit external noise to the ODE models decreases the prediction accuracy.
Our results quantify the competition between early prognosis and high
prediction accuracy that is frequently encountered by clinicians.

Photoactivatable ribonucleoside-enhanced cross-linking and
immunoprecipitation (PAR-CLIP) is an experimental method based on
next-generation sequencing for identifying the RNA interaction sites of a given
protein. The method deliberately inserts T-to-C substitutions at the
RNA-protein interaction sites, which provides a second layer of evidence
compared to other CLIP methods. However, the experiment includes several
sources of noise which cause both low-frequency errors and spurious
high-frequency alterations. Therefore, rigorous statistical analysis is
required in order to separate true T-to-C base changes, following
cross-linking, from noise. So far, most of the existing PAR-CLIP data analysis
methods focus on discarding the low-frequency errors and rely on high-frequency
substitutions to report binding sites, not taking into account the possibility
of high-frequency false positive substitutions. Here, we introduce BMix, a new
probabilistic method which explicitly accounts for the sources of noise in PAR-
CLIP data and distinguishes cross-link induced T-to-C substitutions from low
and high-frequency erroneous alterations. We demonstrate the superior speed and
accuracy of our method compared to existing approaches on both simulated and
real, publicly available human datasets. The model is implemented in the Matlab
toolbox BMix, freely available at www.cbg.bsse.ethz.ch/software/BMix.

Diverse classes of proteins function through large-scale conformational
changes; sophisticated enhanced sampling methods have been proposed to generate
these macromolecular transition paths. As such paths are curves in a
high-dimensional space, they have been difficult to compare quantitatively, a
prerequisite to, for instance, assess the quality of different sampling
algorithms. The Path Similarity Analysis (PSA) approach alleviates these
difficulties by utilizing the full information in 3N-dimensional trajectories
in configuration space. PSA employs the Hausdorff or Fr\'echet path
metrics---adopted from computational geometry---enabling us to quantify path
(dis)similarity, while the new concept of a Hausdorff-pair map permits the
extraction of atomic-scale determinants responsible for path differences.
Combined with clustering techniques, PSA facilitates the comparison of many
paths, including collections of transition ensembles. We use the closed-to-open
transition of the enzyme adenylate kinase (AdK)---a commonly used testbed for
the assessment enhanced sampling algorithms---to examine multiple microsecond
equilibrium molecular dynamics (MD) transitions of AdK in its substrate-free
form alongside transition ensembles from the MD-based dynamic importance
sampling (DIMS-MD) and targeted MD (TMD) methods, and a geometrical targeting
algorithm (FRODA). A Hausdorff pairs analysis of these ensembles revealed, for
instance, that differences in DIMS-MD and FRODA paths were mediated by a set of
conserved salt bridges whose charge-charge interactions are fully modeled in
DIMS-MD but not in FRODA. We also demonstrate how existing trajectory analysis
methods relying on pre-defined collective variables, such as native contacts or
geometric quantities, can be used synergistically with PSA, as well as the
application of PSA to more complex systems such as membrane transporter
proteins.

In this paper we consider a model based on branching process theory for the
proliferation and the dissemination network of T cells in the adaptive immune
response. A multi-type Galton Watson branching process is assumed as the basic
proliferation mechanism, associated to the migration of T cells of the
different generations from the draining lymph node to the spleen and other
lymphoid organs. Time recursion equations for the mean values and the
covariance matrices of the the cell population counts are derived in all the
compartments of the network model. Moreover, a normal approximation of the
log-likelihood function of the cell relative frequencies is derived, which
allows one to obtain estimates of both the probability parameters of the
branching process and the migration rates in the various compartments of the
network.

Unicellular organisms are open metabolic systems that need to process
information about their external environment in order to survive. In most types
of tissues and organisms, cells use calcium signaling to carry information from
the extracellular side of the plasma membrane to the different metabolic
targets of their internal medium. This information might be encoded in the
amplitude, frequency, duration, waveform or timing of the calcium oscillations.
Thus, specific information coming from extracellular stimuli can be encoded in
the calcium signal and decoded again later in different locations within the
cell. Despite its cellular importance, little is known about the quantitative
informative properties of the calcium concentration dynamics inside the cell.
In order to understand some of these informational properties, we have studied
experimental Ca2+ series of Xenopus laevis oocytes under different external pH
stimulus. The data has been analyzed by means of information-theoretic
approaches such as Conditional Entropy, Information Retention, and other
non-linear dynamics tools such as the power spectra, the Largest Lyapunov
exponent and the bridge detrended Scaled Window Variance analysis. We have
quantified the biomolecular information flows of the experimental data in bits,
and essential aspects of the information contained in the experimental calcium
fluxes have been exhaustively analyzed. Our main result shows that inside all
the studied intracellular Ca2+ flows a highly organized informational structure
emerge, which exhibit deterministic chaotic behavior, long term memory and
complex oscillations of the uncertainty reduction based on past values. The
understanding of the informational properties of calcium signals is one of the
key elements to elucidate the physiological functional coupling of the cell and
the integrative dynamics of cellular life.

NeXML is a powerful and extensible exchange standard recently proposed to
better meet the expanding needs for phylogenetic data and metadata sharing.
Here we present the RNeXML package, which provides users of the R programming
language with easy-to-use tools for reading and writing NeXML documents,
including rich metadata, in a way that interfaces seamlessly with the extensive
library of phylogenetic tools already available in the R ecosystem.

The success of metabolomics studies depends upon the "fitness" of each
biological sample used for analysis: it is critical that metabolite levels
reported for a biological sample represent an accurate snapshot of the studied
organism's metabolite profile at time of sample collection. Numerous factors
may compromise metabolite sample fitness, including chemical and biological
factors which intervene during sample collection, handling, storage, and
preparation for analysis. We propose a probabilistic model for the quantitative
assessment of metabolite sample fitness. Collection and processing of nuclear
magnetic resonance (NMR) and ultra-performance liquid chromatography (UPLC-MS)
metabolomics data is discussed. Feature selection methods utilized for
multivariate data analysis are briefly reviewed, including feature clustering
and computation of latent vectors using spectral methods. We propose that the
time-course of metabolite changes in samples stored at different temperatures
may be utilized to identify changing-metabolite-to-stable-metabolite ratios as
markers of sample fitness. Tolerance intervals may be computed to characterize
these ratios among fresh samples. In order to discover additional structure in
the data relevant to sample fitness, we propose using data labeled according to
these ratios to train a Dirichlet process mixture model (DPMM) for assessing
sample fitness. DPMMs are highly intuitive since they model the metabolite
levels in a sample as arising from a combination of processes including, e.g.,
normal biological processes and degradation- or contamination-inducing
processes. The outputs of a DPMM are probabilities that a sample is associated
with a given process, and these probabilities may be incorporated into a final
classifier for sample fitness.

Engineering genetic networks to be both predictable and robust is a key
challenge in synthetic biology. Synthetic circuits must reliably function in
dynamic, stochastic and heterogeneous environments, and simple circuits can be
studied to refine complex gene-regulation models. Although robust behaviours
such as genetic oscillators have been designed and implemented in prokaryotic
and eukaryotic organisms, a priori genetic engineering of even simple networks
remains difficult, and many aspects of cell and molecular biology critical to
engineering robust networks are still inadequately characterized. Particularly,
periodic processes such as gene doubling and cell division are rarely
considered in gene regulatory models, which may become more important as
synthetic biologists utilize new tools for chromosome integration. We studied a
chromosome-integrated, negative-feedback circuit based upon the bacteriophage
{\lambda} transcriptional repressor Cro and observed strong, feedback-dependent
oscillations in single-cell time traces. This finding was surprising due to a
lack of cooperativity, long delays or fast protein degradation. We further show
that oscillations are synchronized to the cell cycle by gene duplication, with
phase shifts predictably correlating with estimated gene doubling times.
Furthermore, we characterized the influence of negative feedback on the
magnitude and dynamics of noise in gene expression. Our results show that
cell-cycle effects must be accounted for in accurate, predictive models for
even simple gene circuits. Cell-cycle-periodic expression of {\lambda} Cro also
suggests an explanation for cell-size dependence in lysis probability and an
evolutionary basis for site-specific {\lambda} integration.

Tomasetti and Vogelstein recently proposed that the majority of variation in
cancer risk among tissues is due to "bad luck," that is, random mutations
arising during DNA replication in normal noncancerous stem cells. They
generalize this finding to cancer overall, claiming that "the stochastic
effects of DNA replication appear to be the major contributor to cancer in
humans." We show that this conclusion results from a logical fallacy based on
ignoring the influence of population heterogeneity in correlations exhibited at
the level of the whole population. Because environmental and genetic factors
cannot explain the huge differences in cancer rates between different organs,
it is wrong to conclude that these factors play a minor role in cancer rates.
In contrast, we show that one can indeed measure huge differences in cancer
rates between different organs and, at the same time, observe a strong effect
of environmental and genetic factors in cancer rates.

Diffusive transport is a universal phenomenon, throughout both biological and
physical sciences, and models of diffusion are routinely used to interrogate
diffusion-driven processes. However, most models neglect to take into account
the role of volume exclusion, which can significantly alter diffusive
transport, particularly within biological systems where the diffusing particles
might occupy a significant fraction of the available space. In this work we use
a random walk approach to provide a means to reconcile models that incorporate
crowding effects on different spatial scales. Our work demonstrates that
coarse-grained models incorporating simplified descriptions of excluded volume
can be used in many circumstances, but that care must be taken in pushing the
coarse-graining process too far.

Automated analysis of imaged phenotypes enables fast and reproducible
quantification of biologically relevant features. Despite recent developments,
recordings of complex, networked structures, such as: leaf venation patterns,
cytoskeletal structures, or traffic networks, remain challenging to analyze.
Here we illustrate the applicability of img2net to automatedly analyze such
structures by reconstructing the underlying network, computing relevant network
properties, and statistically comparing networks of different types or under
different conditions. The software can be readily used for analyzing image data
of arbitrary 2D and 3D network-like structures. img2net is open-source software
under the GPL and can be downloaded from
http://mathbiol.mpimp-golm.mpg.de/img2net/, where supplementary information and
data sets for testing are provided.

The leaves of angiosperms contain highly complex venation networks consisting
of recursively nested, hierarchically organized loops. We describe a new
phenotypic trait of reticulate vascular networks based on the topology of the
nested loops. This phenotypic trait encodes information orthogonal to widely
used geometric phenotypic traits, and thus constitutes a new dimension in the
leaf venation phenotypic space. We apply our metric to a database of 186 leaves
and leaflets representing 137 species, predominantly from the Burseraceae
family, revealing diverse topological network traits even within this single
family. We show that topological information significantly improves
identification of leaves from fragments by calculating a "leaf venation
fingerprint" from topology and geometry. Further, we present a phenomenological
model suggesting that the topological traits can be explained by noise effects
unique to specimen during development of each leaf which leave their imprint on
the final network. This work opens the path to new quantitative identification
techniques for leaves which go beyond simple geometric traits such as vein
density and is directly applicable to other planar or sub-planar networks such
as blood vessels in the brain.

The origin of allometric scaling patterns that are multiples of 1/4 has long
fascinated biologists. While not universal, scaling relationships with
exponents that are close to multiples of 1/4 are common and have been described
in all major clades. Foremost among these relationships is the 3/4 scaling of
metabolism with mass which underpins the 1/4 power dependence of biological
rates and times. Several models have been advanced to explain the underlying
mechanistic drivers of such patterns, but questions regarding a disconnect
between model structures and empirical data have limited their widespread
acceptance. Notable among these is a fractal branching model which predicts
power law scaling of both metabolism and physical dimensions. While a power law
is a useful first approximation to many datasets, non-linearity in some large
data compilations suggest the possibility of more complex or alternative
mechanisms. Here, we first show that quarter power scaling can be derived using
only the preservation of volume flow rate and velocity as model constraints.
Applying our model to the specific case of land plants, we show that
incorporating biomechanical principles and allowing different parts of plant
branching networks to be optimized to serve different functions predicts
non-linearity in allometric relationships, and helps explain why interspecific
scaling exponents covary along a fractal continuum. We also demonstrate that
while branching may be a stochastic process, due to the conservation of volume,
data may still be consistent with the expectations for a fractal network when
one examines subtrees within a tree. Data from numerous sources at the level of
plant shoots, stems, petioles, and leaves show strong agreement with our model
predictions. This novel theoretical framework provides an easily testable
alternative to current general models of plant metabolic allometry.

Multiview light sheet fluorescence microscopy (LSFM) allows to image
developing organisms in 3D at unprecedented temporal resolution over long
periods of time. The resulting massive amounts of raw image data requires
extensive processing interactively via dedicated graphical user interface (GUI)
applications. The consecutive processing steps can be easily automated and the
individual time points can be processed independently, which lends itself to
trivial parallelization on a high performance cluster (HPC). Here we introduce
an automated workflow for processing large multiview, multi-channel,
multi-illumination time-lapse LSFM data on a single workstation or in parallel
on a HPC. The pipeline relies on snakemake to resolve dependencies among
consecutive processing steps and can be easily adapted to any cluster
environment for processing LSFM data in a fraction of the time required to
collect it.

In medical sciences, a biomarker is "a characteristic that is objectively
measured and evaluated as an indicator of normal biological processes,
pathogenic processes, or pharmacologic responses to a therapeutic
intervention". Molecular experiments are providing rapid and systematic
approaches to search for biomarkers, but because single-molecule biomarkers
have shown a disappointing lack of robustness for clinical diagnosis,
researchers have begun searching for distinctive sets of molecules, called
"biosignatures". However, the most popular statistics are not appropriate for
their identification, and the number of possible biosignatures to be tested is
frequently intractable. In the present work, we developed a "multivariate
filter" using genetic algorithms (GA) as a feature (gene) selector to optimize
a measure of intra-group cohesion and inter-group dispersion. This method was
implemented using Python and R (pyBioSig, available at
https://github.com/fredgca/pybiosig under LGPL) and can be manipulated via
graphical interface or Python scripts. Using it, we were able to identify
putative biosignatures composed by just a few genes and capable of recovering
multiple groups simultaneously in a hierarchical clustering, even ones that
were not recovered using the whole transcriptome, within a feasible length of
time using a personal computer. Our results allowed us to conclude that using
GA to optimize our new intra-group cohesion and inter-group dispersion measure
is a clear, effective, and computationally feasible strategy for the
identification of putative "omical" biosignatures that could support
discrimination among multiple groups simultaneously.

Social foraging shows unexpected features such as the existence of a group
size threshold to accomplish a successful hunt. Above this threshold,
additional individuals do not increase the probability of capturing the prey.
Recent direct observations of wolves in Yellowstone Park show that the group
size threshold when hunting its most formidable prey, bison, is nearly three
times greater than when hunting elk, a prey that is considerably less
challenging to capture than bison. These observations provide empirical support
to a computational particle model of group hunting which was previously shown
to be effective in explaining why hunting success peaks at apparently small
pack sizes when hunting elk. The model is based on considering two critical
distances between wolves and prey: the minimal safe distance at which wolves
stand from the prey, and the avoidance distance at which wolves move away from
each other when they approach the prey. The minimal safe distance is longer
when the prey is more dangerous to hunt. We show that the model explains
effectively that the group size threshold is greater when the minimal safe
distance is longer. Although both distances are longer when the prey is more
dangerous, they contribute oppositely to the value of the group size threshold:
the group size threshold is smaller when the avoidance distance is longer. This
unexpected mechanism gives rise to a global increase of the group size
threshold when considering bison with respect to elk, but other prey more
dangerous than elk can lead to specific critical distances that can give rise
to the same group size threshold. Our results show that the computational model
can guide further research on group size effects, suggesting that more
experimental observations should be obtained for other kind of prey as e.g.
moose.

Land cover has been evaluated and classified on the basis of general features
using reflectance or digital levels of photographic or satellite data. One of
the most common methodologies based on CORINE land cover (Coordination of
Information on the Environment) data, which classifies natural cover according
to a small number of categories. This method produces generalizations about the
inventoried areas, resulting in the loss of important floristic and structural
information about vegetation types present (such as palm groves, tall dense
mangroves, and dense forests). This classification forfeits relevant
information on sites with high heterogeneity and diversity. Especially in the
tropics, simplification of coverage types reaches its maximum level with the
use of deforestation analysis, particularly when it is reduced to the two
classes of forests and nonforests. As this paper demonstrates, these results
have considerable consequences for political efforts to conserve the
biodiversity of megadiverse countries. We designed a new methodological
approach that incorporates biological distinctiveness combined with
phytosociological classification of vegetation and its relation to physical
features. This approach is based on parameters obtained through canonical
correspondence analysis on a fuzzy logic model, which are used to construct
multiple coverage maps. This tool is useful for monitoring and analyzing
vegetation dynamics, since it maintains the typological integrity of a
cartographic series. The methodology creates cartographic series congruent in
time and scale, can be applied to multiple and varied satellite inputs, and
always evaluates the same model parameters. We tested this new method in the
southwestern Colombian Caribbean region and compared our results with those
from what we believe are outdated tools used in other analyses of deforestation
around the world.

A nondestructive method for estimating the amount of carbon stored by
individuals, communities, vegetation types, and coverages, as well as their
volume and aboveground biomass, is presented. This methodology is based on
information on carbon stocks obtained through three-dimensional analysis of
tree architecture and artificial neural networks. This technique accurately
incorporates the diversity of plant forms measured in plots, transects, and
relev\'es. Stored carbon in any vegetation type is usually calculated as half
the biomass of sampled individuals, estimated with allometric formulas. The
most complete of these formulas incorporate diameter, height, and specific
gravity of wood but do not consider the variation in carbon stored in different
organs or different species, nor do they include information on the wide array
of architectures present in different plant communities. To develop these
allometric models, many individuals of different species must be sacrificed to
identify and validate samples and to minimize error. It is common to find
cutting-edge studies that encourage logging to improve estimates of carbon. In
our approach we replace this destructive methodology with a new technique for
quantifying global aboveground carbon. We demonstrate that carbon content in
forest aboveground biomass in the pantropics could rise to 723.97 Pg C. This
study shows that a reevaluation of climatic and ecological models is needed to
move toward a better understanding of the adverse effects of climate change,
deforestation, and degradation of tropical vegetation.

The asymmetric simple exclusion process (ASEP) is an important model from
statistical physics describing particles that hop randomly from one site to the
next along an ordered lattice of sites, but only if the next site is empty.
ASEP has been used to model and analyze numerous multiagent systems with local
interactions including the flow of ribosomes along the mRNA strand.
  In ASEP with periodic boundary conditions a particle that hops from the last
site returns to the first one. The mean field approximation of this model is
referred to as the ribosome flow model on a ring (RFMR). The RFMR may be used
to model both synthetic and endogenous gene expression regimes.
  We analyze the RFMR using the theory of monotone dynamical systems. We show
that it admits a continuum of equilibrium points and that every trajectory
converges to an equilibrium point. Furthermore, we show that it entrains to
periodic transition rates between the sites. We describe the implications of
the analysis results to understanding and engineering cyclic mRNA translation
in-vitro and in-vivo.

In many chemical and biological applications, systems of differential
equations containing unknown parameters are used to explain empirical
observations and experimental data. The DEs are typically nonlinear and
difficult to analyze, requiring numerical methods to approximate the solutions.
Compounding this difficulty are the unknown parameters in the DE system, which
must be given specific numerical values in order for simulations to be run.
  Estrogen receptor protein dimerization is used as an example to demonstrate
model construction, reduction, simulation, and parameter estimation.
Mathematical, computational, and statistical methods are applied to empirical
data to deduce kinetic parameter estimates and guide decisions regarding future
experiments and modeling. The process demonstrated serves as a pedagogical
example of quantitative methods being used to extract parameter values from
biochemical data models.

The promise of extracting connectomes and performing useful analysis on large
electron microscopy (EM) datasets has been an elusive dream for many years.
Tracing in even the smallest portions of neuropil requires copious human
annotation, the rate-limiting step for generating a connectome. While a
combination of improved imaging and automatic segmentation will lead to the
analysis of increasingly large volumes, machines still fail to reach the
quality of human tracers. Unfortunately, small errors in image segmentation can
lead to catastrophic distortions of the connectome.
  In this paper, to analyze very large datasets, we explore different
mechanisms that are less sensitive to errors in automation. Namely, we advocate
and deploy extensive synapse detection on the entire antennal lobe (AL)
neuropil in the brain of the fruit fly Drosophila, a region much larger than
any densely annotated to date. The resulting synapse point cloud produced is
invaluable for determining compartment boundaries in the AL and choosing
specific regions for subsequent analysis. We introduce our methodology in this
paper for region selection and show both manual and automatic synapse
annotation results. Finally, we note the correspondence between image datasets
obtained using the synaptic marker, antibody nc82, and our datasets enabling
registration between light and EM image modalities.

This article details the results of analyses we conducted on the discourse of
schizophrenic patients, at the oral production (disfluences) and lexical
(part-of-speech and lemmas) levels. This study is part of a larger project,
which includes other levels of analyses (syntax and discourse). The obtained
results should help us rebut or identify new linguistic evidence participating
in the manifestation of a dysfunction at these different levels. The corpus
contains more than 375,000 words, its analysis therefore required that we use
Natural Language Processing (NLP) and lexicometric tools. In particular, we
processed disfluencies and parts-of-speech separately, which allowed us to
demonstrate that if schizophrenic patients do produce more disfluencies than
control, their lexical richness is not significatively different.

There is an urgent need for economical blood based, noninvasive molecular
biomarkers to assist in the detection and diagnosis of cancers in a cost
effective manner at an early stage, when curative interventions are still
possible. Serum autoantibodies are attractive biomarkers for early cancer
detection, but their development has been hindered by the punctuated genetic
nature of the ten million known cancer mutations. A recent study of 50,000
patients (Pedersen et al., 2013) showed p53 15mer epitopes are much more
sensitive colon cancer biomarkers than p53, which in turn is a more sensitive
cancer biomarker than any other protein. The function of p53 as a nearly
universal tumor suppressor is well established, because of its strong
immunogenicity in terms of not only antibody recruitment, but also stimulation
of autoantibodies. Here we examine bioinformatic fractal scaling analysis for
identifying sensitive epitopes from the p53 amino acid sequence, and show how
it could be used for early cancer detection (ECD). We trim 15mers to 7mers, and
identify specific 7mers from other species that could be more sensitive to
aggressive human cancers, such as liver cancer.

We present a meta-analysis of independent studies on the potential
implication in the occurrence of coronary heart disease (CHD) of the
single-nucleotide polymorphism (SNP) at the -308 position of the tumor necrosis
factor alpha (TNF-alpha) gene. We use Bayesian analysis to integrate
independent data sets and to infer statistically robust measurements of
correlation. Bayesian hypothesis testing indicates that there is no preference
for the hypothesis that the -308 TNF-alpha SNP is related to the occurrence of
CHD, in the Caucasian or in the Asian population, over the null hypothesis. As
a measure of correlation, we use the probability of occurrence of CHD
conditional on the presence of the SNP, derived as the posterior probability of
the Bayesian meta-analysis. The conditional probability indicates that CHD is
not more likely to occur when the SNP is present, which suggests that the -308
TNF-alpha SNP is not implicated in the occurrence of CHD.

Alisporivir is a cyclophilin inhibitor with demonstrated in vitro and in vivo
activity against hepatitis C 11 virus (HCV). We estimated antiviral
effectiveness of alisporivir alone or in combination with 12
pegylated-Inteferon (peg-IFN) in 88 patients infected with different HCV
genotypes treated for four 13 weeks. The pharmacokinetics of both drugs were
modeled and used as driving functions for the viral 14 kinetic model. Genotype
was found to significantly affect pegylated-Inteferon effectiveness
($\epsilon$= 86.3% 15 and 99.1% in genotype-1/4 and genotype-2/3, respectively,
p\textless{}10 -7) and infected cells loss rate ($\delta$= 16 0.22 vs 0.39 day
-1 in genotype-1/4 and genotype-2/3, respectively, p\textless{}10 -6).
Alisporivir effectiveness 17 was not significantly different across genotype
and was high for doses $\ge$600 mg QD. We simulated 18 virologic responses with
other alisporivir dosing regimens in HCV genotype-2/3 patients using the 19
model. Our predictions consistently matched the observed responses,
demonstrating that this model 20 could be a useful tool for anticipating
virologic response and optimize alisporivir-based therapies.

We develop a simple three compartment model based on mass balance equations
which quantitatively describes the dynamics of breath methane concentration
profiles during exercise on an ergometer. With the help of this model it is
possible to estimate the endogenous production rate of methane in the large
intestine by measuring breath gas concentrations of methane.

We present a simulation algorithm that accurately propagates a molecule pair
using large time steps without the need to invoke the full exact analytical
solutions of the Smoluchowski diffusion equation. Because the proposed method
only uses uniform and Gaussian random numbers, it allows for position updates
that are two to three orders of magnitude faster than those of a corresponding
scheme based on full solutions, while mantaining the same degree of accuracy.
Neither simplifying nor ad hoc assumptions that are foreign to the underlying
Smoluchowski theory are employed, instead, the algorithm faithfully
incorporates the individual elements of the theoretical model. The method is
flexible and applicable in 1, 2 and 3 dimensions, suggesting that it may find
broad usage in various stochastic simulation algorithms. We demonstrate the
algorithm for the case of a non-reactive, irreversible and reversible reacting
molecule pair.

In the analysis of frozen hydrated biomolecules by single-particle
cryo-electron microscopy, template-based particle picking by a target function
called fast local correlation (FLC) allows a large number of particle images to
be automatically picked from micrographs. A second, independent target function
based on maximum likelihood (ML) can be used to align the images and verify the
presence of signal in the picked particles. Although the paradigm of this
dual-target-function (DTF) evaluation of single-particle selection has been
practiced in recent years, it remains unclear how the performance of this DTF
approach is affected by the signal-to-noise ratio of the images and by the
choice of references for FLC and ML. Here we examine this problem through a
systematic study of simulated data, followed by experimental substantiation. We
quantitatively pinpoint the critical signal-to-noise ratio (SNR), at which the
DTF approach starts losing its ability to select and verify particles from
cryo-EM micrographs. A Gaussian model is shown to be as effective in picking
particles as a single projection view of the imaged molecule in the tested
cases. For both simulated micrographs and real cryo-EM data of the 173-kDa
glucose isomerase complex, we found that the use of a Gaussian model to
initialize the target functions suppressed the detrimental effect of reference
bias in template-based particle selection. Given a sufficient signal-to-noise
ratio in the images and the appropriate choice of references, the DTF approach
can expedite the automated assembly of single-particle data sets.

While the use of technology to provide accurate and objective measurements of
human movement performance is presently an area of great interest, efforts to
quantify the performance of movement are hampered by the lack of a principled
model that describes how a subject goes about making a movement. We put forward
a principled mathematical formalism that describes human movements using an
optimal control model in which the subject controls the jerk of the movement.
We construct the formalism by assuming that the movement a subject chooses to
make is better than the alternatives. We quantify the relative quality of
movements mathematically by specifying a cost functional that assigns a
numerical value to every possible movement; the subject makes the movement that
minimizes the cost functional. We develop the mathematical structure of
movements that minimize a cost functional, and observe that this development
parallels the development of analytical mechanics from the Principle of Least
Action. We derive a constant of the motion for human movements that plays a
role that is analogous to the role that the energy plays in classical
mechanics. We apply the formalism to the description of two movements: (1)
rapid, targeted movements of a computer mouse, and (2) finger-tapping, and show
that the constant of the motion that we have derived provides a useful value
with which we can characterize the performance of the movements. In the case of
rapid, targeted movements of a computer mouse, we show how the model of human
movement that we have developed can be made to agree with Fitts' law, and we
show how Fitts' law is related to the constant of the motion that we have
derived. We finally show that solutions exist within the model of human
movements that exhibit an oscillatory character reminiscent of tremor.

The widening gap between known proteins and their functions has encouraged
the development of methods to automatically infer annotations. Automatic
functional annotation of proteins is expected to meet the conflicting
requirements of maximizing annotation coverage, while minimizing erroneous
functional assignments. This trade-off imposes a great challenge in designing
intelligent systems to tackle the problem of automatic protein annotation. In
this work, we present a system that utilizes rule mining techniques to predict
metabolic pathways in prokaryotes. The resulting knowledge represents
predictive models that assign pathway involvement to UniProtKB entries. We
carried out an evaluation study of our system performance using
cross-validation technique. We found that it achieved very promising results in
pathway identification with an F1-measure of 0.982 and an AUC of 0.987. Our
prediction models were then successfully applied to 6.2 million
UniProtKB/TrEMBL reference proteome entries of prokaryotes. As a result,
663,724 entries were covered, where 436,510 of them lacked any previous pathway
annotations.

Partial differential equations are a convenient way to describe reaction-
advection-diffusion processes of signalling models. If only one cell type is
present, and tissue dynamics can be neglected, the equations can be solved
directly. However, in case of multiple cell types it is not always straight
forward to integrate a continuous description of the tissue dynamics. Here, we
discuss (delayed) differentiation of cells into different cell types and
hypertrophic cell volume change upon differentiation.

Allosteric (long-range) interactions can be surprisingly strong in proteins
of biomedical interest. Here we use bioinformatic scaling to connect prior
results on nonsteroidal anti-inflammatory drugs to promising new drugs that
inhibit cancer cell metabolism. Many parallel features are apparent, which
explain how even one amino acid mutation, remote from active sites, can alter
medical results. The enzyme twins involved are cyclooxygenase (aspirin) and
isocitrate dehydrogenase (IDH). The IDH results are accurate to 1% and are
overdetermined by adjusting a single bioinformatic scaling parameter. It
appears that the final stage in optimizing protein functionality may involve
leveling of the hydrophobic cutoffs of the arms of conformational hydrophilic
hinges.

The risk of false positive results in noninvasive prenatal diagnosis focused
on fetal gender and RhD status determination could be a problem in clinical
routine. This is because these tests are based on detection of presence of DNA
sequences with high population frequency and so there is the risk of sample
contamination during sample collection and processing. In our study the
different fragmentation of fetal and maternal DNA molecules present in maternal
circulation was utilized in identification of contaminated samples.
Amplification of Y-chromosome specific assays different in size was tested on
circulating DNA samples.
  Of the four tested assays two shorter (84 and 177 bp) showed expected qPCR
efficiency and have comparable amplification profiles. The difference in Ct
values between these two assays was found to be statistically significant in
comparison of fetal male and normal male samples (p<0.0001) as well as in
blinded pilot study performed on 10 artificially contaminated and 10
non-contaminated samples (F=34.4, p<0.0001) that were all identified correctly.
  Our results showed that differently sized assays performed well in detection
of external contamination of samples in noninvasive prenatal fetal gender test
and could be of help in clinical laboratories to minimize the risk of false
positive results.

Urban vegetation is of key importance because a large proportion of the human
population lives in cities. Nevertheless, urban vegetation is understudied
outside central Europe and particularly, little is known about the flora of
tropical Asian, African and Latin American cities. We present an estimate of
how the vegetation has changed in the city of San Jos\'e, Costa Rica, after
about one century, with the repeat photography technique (based on a collection
of 19th and early 20th century photographs by Jos\'e Fidel Trist\'an and
others) and with data from the Costa Rican National Herbarium. We found little
vegetation change in the landscape of San Jos\'e during the 20th century, where
a total of 95 families and 458 species were collected in the late 19th and
early 20th century. The families with most species were Asteraceae, Fabaceae,
Poaceae, Lamiaceae, Euphorbiaceae, Solanaceae, Cyperaceae, Acanthaceae,
Malvaceae, Piperaceae and Verbenaceae. Similar results have been found in
Europe, where the number of plant species often is stable for long periods even
when the individual species vary.

Lichens are good bio-indicators of air pollution, but in most tropical
countries there are few studies on the subject; however, in the city of San
Jos\'e, Costa Rica, the relationship between air pollution and lichens has been
studied for decades. In this article we evaluate the hypothesis that air
pollution is lower where the wind enters the urban area (Northeast) and higher
where it exits San Jos\'e (Southwest). We identified the urban parks with a
minimum area of approximately 5 000m2 and randomly selected a sample of 40
parks located along the passage of wind through the city. To measure lichen
coverage, we applied a previously validated 10 x 20cm template with 50 random
points to five trees per park (1.5m above ground, to the side with most
lichens). Our results (years 2008 and 2009) fully agree with the generally
accepted view that lichens reflect air pollution carried by circulating air
masses. The practical implication is that the air enters the city relatively
clean by the semi-rural and economically middle class area of Coronado, and
leaves through the developed neighborhoods of Escaz\'u and Santa Ana with a
significant amount of pollutants. In the dry season, the live lichen coverage
of this tropical city was lower than in the May to December rainy season, a
pattern that contrasts with temperate habitats; but regardless of the season,
pollution follows the pattern of wind movement through the city.

Impedance Spectroscopy resolves electrical properties into uncorrelated
variables, as a function of frequency, with exquisite resolution. Separation is
robust and most useful when the system is linear. Impedance spectroscopy
combined with appropriate structural knowledge provides insight into pathways
for current flow, with more success than other methods. Biological applications
of impedance spectroscopy are often not useful since so much of biology is
strongly nonlinear in its essential features, and impedance spectroscopy is
fundamentally a linear analysis. All cells and tissues have cell membranes and
its capacitance is both linear and important to cell function. Measurements
proved straightforward in skeletal muscle, cardiac muscle, and lens of the eye.
In skeletal muscle, measurements provided the best estimates of the predominant
(cell) membrane system that dominates electrical properties. In cardiac muscle,
measurements showed definitively that classical microelectrode voltage clamp
could not control the potential of the predominant membranes, that were in the
tubular system separated from the extracellular space by substantial
distributed resistance. In the lens of the eye, impedance spectroscopy changed
the basis of all recording and interpretation of electrical measurements and
laid the basis for Rae and Mathias extensive later experimental work. Many
tissues are riddled with extracellular space as clefts and tubules, for
example, cardiac muscle, the lens of the eye, most epithelia, and of course
frog muscle. These tissues are best analyzed with a bidomain theory that arose
from the work on electrical structure described here. There has been a great
deal of work since then on the bi-domain and this represents the most important
contribution to biology of the analysis of electrical structure in my view.

Reticulate evolutionary processes result in phylogenetic histories that
cannot be modeled using a tree topology. Here, we apply methods from
topological data analysis to molecular sequence data with reticulations. Using
a simple example, we demonstrate the correspondence between nontrivial higher
homology and reticulate evolution. We discuss the sensitivity of the standard
filtration and show cases where reticulate evolution can fail to be detected.
We introduce an extension of the standard framework and define the median
complex as a construction to recover signal of the frequency and scale of
reticulate evolution by inferring and imputing putative ancestral states.
Finally, we apply our methods to two datasets from phylogenetics. Our work
expands on earlier ideas of using topology to extract important evolutionary
features from genomic data.

Over the past few decades, magnetoreception has been discovered in several
species of teleost and elasmobranch fishes by employing varied experimental
methods including conditioning experiments, observations of alignment with
external fields, and experiments with magnetic deterrents. Biogenic magnetite
has been confirmed to be an important receptor mechanism in some species, but
there is ongoing debate regarding whether other mechanisms are at work. This
paper presents evidence for magnetoreception in three additional species, red
drum (Sciaenops ocellatus), black drum (Pogonias cromis), and sea catfish
(Ariopsis felis), by employing experiments to test whether fish respond
differently to bait on a magnetic hook than on a control. In red drum, the
control hook outcaught the magnetic hook by 32 - 18 for chi-squared = 3.92 and
a P-value of 0.048. Black drum showed a significant attraction for the magnetic
hook, which prevailed over the control hook by 11 - 3 for chi-squared = 4.57
and a P-value of 0.033. Gafftopsail catfish (Bagre marinus) showed no
preference with a 31 - 35 split between magnetic hook and control for
chi-squared = 0.242 and a P-value of 0.623. In a sample of 100 sea catfish in
an analogous experiment using smaller hooks, the control hook was preferred
62-38 for chi-squared = 5.76 and a P-value of < 0.001. Such a simple method for
identifying magnetoreceptive species may quickly expand the number of known
magnetoreceptive species and allow for easier access to magnetoreceptive
species and thus facilitate testing of magnetoreceptive hypotheses.

Biogeographical regions (bioregions) reveal how different sets of species are
spatially grouped and therefore are important units for conservation,
historical biogeography, ecology and evolution. Several methods have been
developed to identify bioregions based on species distribution data rather than
expert opinion. One approach successfully applies network theory to simplify
and highlight the underlying structure in species distributions. However, this
method lacks tools for simple and efficient analysis. Here we present Infomap
Bioregions, an interactive web application that inputs species distribution
data and generates bioregion maps. Species distributions may be provided as
georeferenced point occurrences or range maps, and can be of local, regional or
global scale. The application uses a novel adaptive resolution method to make
best use of often incomplete species distribution data. The results can be
downloaded as vector graphics, shapefiles or in table format. We validate the
tool by processing large datasets of publicly available species distribution
data of the world's amphibians using species ranges, and mammals using point
occurrences. We then calculate the fit between the inferred bioregions and WWF
ecoregions. As examples of applications, researchers can reconstruct ancestral
ranges in historical biogeography or identify indicator species for targeted
conservation.

Cryo-electron tomography enables 3D visualization of cells in a near native
state at molecular resolution. The produced cellular tomograms contain detailed
information about all macromolecular complexes, their structures, their
abundances and their specific spatial locations in the cell. However,
extracting this information is very challenging and current methods usually
rely on templates of known structure. Here, we formulate a template-free visual
proteomics analysis as a de novo pattern mining problem and propose a new
framework called "Multi Pattern Pursuit" for supporting proteome-scale de novo
discovery of macromolecular complexes in cellular tomograms without using
templates of known structures. Our tests on simulated and experimental
tomograms show that our method is a promising tool for template-free visual
proteomics analysis.

Participant needs to achieve a given power are frequently underestimated.
This is particularly problematic when effect sizes are small, such as is common
in neuroscience and psychology. We provide tools to make these demands
immediately obvious in the form of a powerscape visualization.

Infectious diseases are notorious for their complex dynamics, which make it
difficult to fit models to test hypotheses. Methods based on state-space
reconstruction have been proposed to infer causal interactions in noisy,
nonlinear dynamical systems. These "model-free" methods are collectively known
as convergent cross-mapping (CCM). Although CCM has theoretical support,
natural systems routinely violate its assumptions. To identify the practical
limits of causal inference under CCM, we simulated the dynamics of two pathogen
strains with varying interaction strengths. The original method of CCM is
extremely sensitive to periodic fluctuations, inferring interactions between
independent strains that oscillate with similar frequencies. This sensitivity
vanishes with alternative criteria for inferring causality. However, CCM
remains sensitive to high levels of process noise and changes to the
deterministic attractor. This sensitivity is problematic because it remains
challenging to gauge noise and dynamical changes in natural systems, including
the quality of reconstructed attractors that underlie cross-mapping. We
illustrate these challenges by analyzing time series of reportable childhood
infections in New York City and Chicago during the pre-vaccine era. We comment
on the statistical and conceptual challenges that currently limit the use of
state-space reconstruction in causal inference.

Time course measurement of single molecules on a cell surface provides
detailed information on the dynamics of the molecules, which is otherwise
inaccessible. To extract the quantitative information, single particle tracking
(SPT) is typically performed. However, trajectories extracted by SPT inevitably
have linking errors when the diffusion speed of single molecules is high
compared to the scale of the particle density. To circumvent this problem we
developed an algorithm to estimate diffusion constants without relying on SPT.
We demonstrated that the proposed algorithm provides reasonable estimation of
diffusion constants even when other methods fail due to high particle density
or inhomogeneous particle distribution. In addition, our algorithm can be used
for visualization of time course data from single molecular measurements.

Background: The increasing volume and variety of genotypic and phenotypic
data is a major defining characteristic of modern biomedical sciences. At the
same time, the limitations in technology for generating data and the inherently
stochastic nature of biomolecular events have led to the discrepancy between
the volume of data and the amount of knowledge gleaned from it. A major
bottleneck in our ability to understand the molecular underpinnings of life is
the assignment of function to biological macromolecules, especially proteins.
While molecular experiments provide the most reliable annotation of proteins,
their relatively low throughput and restricted purview have led to an
increasing role for computational function prediction. However, accurately
assessing methods for protein function prediction and tracking progress in the
field remain challenging. Methodology: We have conducted the second Critical
Assessment of Functional Annotation (CAFA), a timed challenge to assess
computational methods that automatically assign protein function. One hundred
twenty-six methods from 56 research groups were evaluated for their ability to
predict biological functions using the Gene Ontology and gene-disease
associations using the Human Phenotype Ontology on a set of 3,681 proteins from
18 species. CAFA2 featured significantly expanded analysis compared with CAFA1,
with regards to data set size, variety, and assessment metrics. To review
progress in the field, the analysis also compared the best methods
participating in CAFA1 to those of CAFA2. Conclusions: The top performing
methods in CAFA2 outperformed the best methods from CAFA1, demonstrating that
computational function prediction is improving. This increased accuracy can be
attributed to the combined effect of the growing number of experimental
annotations and improved methods for function prediction.

A biological experiment is the most reliable way of assigning function to a
protein. However, in the era of high-throughput sequencing, scientists are
unable to carry out experiments to determine the function of every single gene
product. Therefore, to gain insights into the activity of these molecules and
guide experiments, we must rely on computational means to functionally annotate
the majority of sequence data. To understand how well these algorithms perform,
we have established a challenge involving a broad scientific community in which
we evaluate different annotation methods according to their ability to predict
the associations between previously unannotated protein sequences and Gene
Ontology terms. Here we discuss the rationale, benefits and issues associated
with evaluating computational methods in an ongoing community-wide challenge.

Mast fruiting represents a synchronous population behaviour which can spread
on large landscape areas. This reproductive pattern is generally perceived as a
synchronous periodic production of large seed crops and has a significant
practical importance to forest natural regeneration in order to synchronize
cuttings. The mechanisms of masting are still argued and models of this
phenomenon are uncommon, so a stochastic approach can cast significant light on
some particular aspects. Trees manage to get synchronized and coordinate their
reproductive routines. But is it possible that trees get synchronized by
chance, absolutely random? Using a Monte Carlo simulation of seeding years and
a theoretical masting pattern, a stochastic analysis is performed in order to
assess the chance of random mast fruiting. Two populations of 100 trees, with
different fruiting periodicity of 2-3 years and 4-6 years, were set and the
fruition dynamic was simulated for 100 years. The results show that periodicity
itself cannot induce by chance the masting effect, but periodicity
mathematically influences the reproductive pattern.

How long people live depends on their health, and how it changes with age.
Individual health can be tracked by the accumulation of age-related health
deficits. The fraction of age-related deficits is a simple quantitative measure
of human aging. This quantitative frailty index (F) is as good as chronological
age in predicting mortality. In this paper, we use a dynamical network model of
deficits to explore the effects of interactions between deficits, deficit
damage and repair processes, and the connection between the F and mortality.
With our model, we qualitatively reproduce Gompertz's law of increasing human
mortality with age, the broadening of the F distribution with age, the
characteristic non-linear increase of the F with age, and the increased
mortality of high-frailty individuals. No explicit time-dependence in damage or
repair rates is needed in our model. Instead, implicit time-dependence arises
through deficit interactions -- so that the average deficit damage rates
increases, and deficit repair rates decreases, with age . We use a simple
mortality criterion, where mortality occurs when the most connected node is
damaged.

Development of several alternative mathematical models for the biological
system in question and discrimination between such models using experimental
data is the best way to robust conclusions. Models which challenge existing
theories are more valuable than models which support such theories.

Collective movement can be achieved when individuals respond to the local
movements and positions of their neighbours. Some individuals may
disproportionately influence group movement if they occupy particular spatial
positions in the group, for example, positions at the front of the group. We
asked, therefore, what led individuals in moving pairs of fish (Gambusia
holbrooki) to occupy a position in front of their partner. Individuals adjusted
their speed and direction differently in response to their partner's position,
resulting in individuals occupying different positions in the group.
Individuals that were found most often at the front of the pair had greater
mean changes in speed than their partner, and were less likely to turn towards
their partner, compared to those individuals most often found at the back of
the pair. The pair moved faster when led by the individual that was usually at
the front. Our results highlight how differences in the social responsiveness
between individuals can give rise to leadership in free moving groups. They
also demonstrate how the movement characteristics of groups depend on the
spatial configuration of individuals within them.

Time course data are often used to study the changes to a biological process
after perturbation. Statistical methods have been developed to determine
whether such a perturbation induces changes over time, e.g. comparing a
perturbed and unperturbed time course dataset to uncover differences. However,
existing methods do not provide a principled statistical approach to identify
the specific time when the two time course datasets first begin to diverge
after a perturbation; we call this the perturbation time. Estimation of the
perturbation time for different variables in a biological process allows us to
identify the sequence of events following a perturbation and therefore provides
valuable insights into likely causal relationships.
  In this paper, we propose a Bayesian method to infer the perturbation time
given time course data from a wild-type and perturbed system. We use a
non-parametric approach based on Gaussian Process regression. We derive a
probabilistic model of noise-corrupted and replicated time course data coming
from the same profile before the perturbation time and diverging after the
perturbation time. The likelihood function can be worked out exactly for this
model and the posterior distribution of the perturbation time is obtained by a
simple histogram approach, without recourse to complex approximate inference
algorithms. We validate the method on simulated data and apply it to study the
transcriptional change occurring in Arabidopsis following inoculation with P.
syringae pv. tomato DC3000 versus the disarmed strain DC3000hrpA.
  An R package, DEtime, implementing the method is available at
https://github.com/ManchesterBioinference/DEtime along with the data and code
required to reproduce all the results.

We consider a class of biologically-motivated stochastic processes in which a
unicellular organism divides its resources (volume or damaged proteins, in
particular) symmetrically or asymmetrically between its progeny. Assuming the
final amount of the resource is controlled by a growth policy and subject to
additive and multiplicative noise, we derive the "master equation" describing
how the resource distribution evolves over subsequent generations and use it to
study the properties of stable resource distributions. We find conditions under
which a unique stable resource distribution exists and calculate its moments
for the class of affine linear growth policies. Moreover, we apply an
asymptotic analysis to elucidate the conditions under which the stable
distribution (when it exists) has a power-law tail. Finally, we use the results
of this asymptotic analysis along with the moment equations to draw a stability
phase diagram for the system that reveals the counterintuitive result that
asymmetry serves to increase stability while at the same time widening the
stable distribution. We also briefly discuss how cells can divide damaged
proteins asymmetrically between their progeny as a form of damage control. In
the appendix, motivated by the asymmetric division of cell volume in
Saccharomyces cerevisiae, we extend our results to the case wherein mother and
daughter cells follow different growth policies.

Diversity represents a key concept in ecology, and there are various methods
of assessing it. The multitude of diversity indices are quite puzzling and
sometimes difficult to compute for a large volume of data. This paper promotes
a computational tool used to assess the diversity of different entities. The
BIODIV software is a user-friendly tool, developed using Microsoft Visual
Basic. It is capable to compute several diversity indices such as: Shannon,
Simpson, Pielou, Brillouin, Berger-Parker, McIntosh, Margalef, Menhinick and
Gleason. The software tool was tested using real data sets and the results were
analysed in order to make assumption on the indices behaviour. The results
showed a clear segregation of indices in two major groups with similar
expressivity.

We explore the relationship among model fidelity, experimental design, and
parameter estimation in sloppy models. We show that the approximate nature of
mathematical models poses challenges for experimental design in sloppy models.
In many models of complex biological processes it is unknown what are the
relevant physics that must be included to explain collective behaviors. As a
consequence, models are often overly complex, with many practically
unidentifiable parameters. Furthermore, which details are relevant/irrelevant
vary among potential experiments. By selecting complementary experiments,
experimental design may inadvertently make details that were ommitted from the
model become relevant. When this occurs, the model will fail to give a good fit
to the data. We use a simple hyper-model of model error to quantify a model's
inadequacy and apply it to two models of complex biological processes (EGFR
signaling and DNA repair) with optimally selected experiments. We find that
although parameters may be accurately estimated, the error in the model renders
it less predictive than it was in the sloppy regime where model error is small.
We introduce the concept of a \emph{sloppy system}--a sequence of models of
increasing complexity that become sloppy in the limit of microscopic accuracy.
We explore the limits of accurate parameter estimation in sloppy systems and
argue that system identification better approached by considering a hierarchy
of models of varying detail rather than focusing parameter estimation in a
single model.

Use of accelerometers is now widespread within animal biotelemetry as they
provide a means of measuring an animal's activity in a meaningful and
quantitative way where direct observation is not possible. In sequential
acceleration data there is a natural dependence between observations of
movement or behaviour, a fact that has been largely ignored in most analyses.
Analyses of acceleration data where serial dependence has been explicitly
modelled have largely relied on hidden Markov models (HMMs). Depending on the
aim of an analysis, either a supervised or an unsupervised learning approach
can be applied. Under a supervised context, an HMM is trained to classify
unlabelled acceleration data into a finite set of pre-specified categories,
whereas we will demonstrate how an unsupervised learning approach can be used
to infer new aspects of animal behaviour. We will provide the details necessary
to implement and assess an HMM in both the supervised and unsupervised context,
and discuss the data requirements of each case. We outline two applications to
marine and aerial systems (sharks and eagles) taking the unsupervised approach,
which is more readily applicable to animal activity measured in the field. HMMs
were used to infer the effects of temporal, atmospheric and tidal inputs on
animal behaviour. Animal accelerometer data allow ecologists to identify
important correlates and drivers of animal activity (and hence behaviour). The
HMM framework is well suited to deal with the main features commonly observed
in accelerometer data. The ability to combine direct observations of animals
activity and combine it with statistical models which account for the features
of accelerometer data offer a new way to quantify animal behaviour, energetic
expenditure and deepen our insights into individual behaviour as a constituent
of populations and ecosystems.

Analytical ultracentrifugation (AUC) is a classical technique of physical
biochemistry providing information on size, shape, and interactions of
macromolecules from the analysis of their migration in centrifugal fields while
free in solution. A key mechanical element in AUC is the centerpiece, a
component of the sample cell assembly that is mounted between the optical
windows to allow imaging and to seal the sample solution column against high
vacuum while exposed to gravitational forces in excess of 300,000 g. For
sedimentation velocity it needs to be precisely sector-shaped to allow
unimpeded radial macromolecular migration. During the history of AUC a great
variety of centerpiece designs have been developed for different types of
experiments. Here, we report that centerpieces can now be readily fabricated by
3D printing at low cost, from a variety of materials, and with customized
designs. The new centerpieces can exhibit sufficient mechanical stability to
withstand the gravitational forces at the highest rotor speeds and be
sufficiently precise for sedimentation equilibrium and sedimentation velocity
experiments. Sedimentation velocity experiments with bovine serum albumin as a
reference molecule in 3D printed centerpieces with standard double-sector
design result in sedimentation boundaries virtually indistinguishable from
those in commercial double-sector epoxy centerpieces, with sedimentation
coefficients well within the range of published values. The statistical error
of the measurement is slightly above that obtained with commercial epoxy, but
still below 1%. Facilitated by modern open-source design and fabrication
paradigms, we believe 3D printed centerpieces and AUC accessories can spawn a
variety of improvements in AUC experimental design, efficiency and resource
allocation.

This paper describes methodological details used by WHO in 2015 to estimate
TB incidence, prevalence and mortality. Incidence and mortality are
disaggregated by HIV status, age and sex. Methods to derive MDR-TB burden
indicators are detailed. Four main methods were used to derive incidence: (i)
case notification data combined with expert opinion about case detection gaps
(120 countries representing 51% of global incidence); (ii) results from
national TB prevalence surveys (19 countries, 46% of global incidence); (iii)
notifications in high-income countries adjusted by a standard factor to account
for under-reporting and underdiagnosis (73 countries, 3% of global incidence)
and (iv) capture-recapture modelling (5 countries, 0.5% of global incidence).
Prevalence was obtained from results of national prevalence surveys in 21
countries, representing 69% of global prevalence). In other countries,
prevalence was estimated from incidence and disease duration. Mortality was
obtained from national vital registration systems of mortality surveys in 129
countries (43% of global HIV-negative TB mortality). In other countries,
mortality was derived indirectly from incidence and case fatality ratio.

Numerous processes across both the physical and biological sciences are
driven by diffusion. Partial differential equations (PDEs) are a popular tool
for modelling such phenomena deterministically, but it is often necessary to
use stochastic models to accurately capture the behaviour of a system,
especially when the number of diffusing particles is low. The stochastic models
we consider in this paper are `compartment-based': the domain is discretized
into compartments, and particles can jump between these compartments.
Volume-excluding effects (crowding) can be incorporated by blocking movement
with some probability.
  Recent work has established the connection between fine-grained models and
coarse-grained models incorporating volume exclusion, but only for uniform
lattices. In this paper we consider non-uniform, hybrid lattices that
incorporate both fine- and coarse-grained regions, and present two different
approaches to describing the interface of the regions. We test both techniques
in a range of scenarios to establish their accuracy, benchmarking against
fine-grained models, and show that the hybrid models developed in this paper
can be significantly faster to simulate than the fine-grained models in certain
situations, and are at least as fast otherwise.

Resting-state functional MRI (rs-fMRI) is widely used to noninvasively study
human brain networks. Network functional connectivity is often estimated by
calculating the timeseries correlation between blood-oxygen-level dependent
(BOLD) signal from different regions of interest. However, standard correlation
cannot characterize the direction of information flow between regions. In this
paper, we introduce and test a new concept, prediction correlation, to estimate
effective connectivity in functional brain networks from rs-fMRI. In this
approach, the correlation between two BOLD signals is replaced by a correlation
between one BOLD signal and a prediction of this signal via a causal system
driven by another BOLD signal. Three validations are described: (1) Prediction
correlation performed well on simulated data where the ground truth was known,
and outperformed four other methods. (2) On simulated data designed to display
the "common driver" problem, prediction correlation did not introduce false
connections between non-interacting driven ROIs. (3) On experimental data,
prediction correlation recovered the previously identified network organization
of human brain. Prediction correlation scales well to work with hundreds of
ROIs, enabling it to assess whole brain interregional connectivity at the
single subject level. These results provide an initial validation that
prediction correlation can capture the direction of information flow and
estimate the duration of extended temporal delays in information flow between
regions of interest based on BOLD signal. This approach not only maintains the
high sensitivity to network connectivity provided by the correlation analysis,
but also performs well in the estimation of causal information flow in the
brain.

In this study, we analyse a high-frequency movement dataset for a group of
grazing cattle and investigate their spatiotemporal patterns using a simple
two-state `stop-and-move' mobility model. We find that the dispersal kernel in
the moving state is best described by a mixture exponential distribution,
indicating the hierarchical nature of the movement. On the other hand, the
waiting time appears to be scale-invariant below a certain cut-off and is best
described by a truncated power-law distribution, suggesting heterogenous
dynamics in the non-moving state. We explore possible explanations for the
observed phenomena, covering factors that can play a role in the generation of
mobility patterns, such as the context of grazing environment, the intrinsic
decision-making mechanism or the energy status of different activities. In
particular, we propose a new hypothesis that the underlying movement pattern
can be attributed to the most probable observable energy status under the
maximum entropy configuration. These results are not only valuable for
modelling cattle movement but also provide new insights for understanding the
underlying biological basis of grazing behaviour.

Next-generation sequencing technologies allow the measurement of somatic
mutations in a large number of patients from the same cancer type. One of the
main goals in analyzing these mutations is the identification of mutations
associated with clinical parameters, such as survival time. This goal is
hindered by the genetic heterogeneity of mutations in cancer, due to the fact
that genes and mutations act in the context of pathways. To identify mutations
associated with survival time it is therefore crucial to study mutations in the
context of interaction networks.
  In this work we study the problem of identifying subnetworks of a large
gene-gene interaction network that have mutations associated with survival. We
formally define the associated computational problem by using a score for
subnetworks based on the test statistic of the log-rank test, a widely used
statistical test for comparing the survival of two populations. We show that
the computational problem is NP-hard and we propose a novel algorithm, called
Network of Mutations Associated with Survival (NoMAS), to solve it. NoMAS is
based on the color-coding technique, that has been previously used in other
applications to find the highest scoring subnetwork with high probability when
the subnetwork score is additive. In our case the score is not additive;
nonetheless, we prove that under a reasonable model for mutations in cancer
NoMAS does identify the optimal solution with high probability. We test NoMAS
on simulated and cancer data, comparing it to approaches based on single gene
tests and to various greedy approaches. We show that our method does indeed
find the optimal solution and performs better than the other approaches.
Moreover, on two cancer datasets our method identifies subnetworks with
significant association to survival when none of the genes has significant
association with survival when considered in isolation.

In cancer treatment, chemotherapy is administered according a constant
schedule. The chronotherapy approach, considering chronobiological drug
delivery, adapts the chemotherapy profile to the circadian rhythms of the human
organism. This reduces toxicity effects and at the same time enhances
efficiency of chemotherapy. To personalize cancer treatment, chemotherapy
profiles have to be further adapted to individual patients. Therefore, we
present a new model to represent cycle phenomena in circadian rhythms. The
model enables a more precise modelling of the underlying circadian rhythms. In
comparison with the standard model, our model delivers better results in all
defined quality indices. The new model can be used to adapt the chemotherapy
profile efficiently to individual patients. The adaption to individual patients
contributes to the aim of personalizing cancer therapy.

Communication and coordination play a major role in the ability of bacterial
cells to adapt to ever changing environments and conditions. Recent work has
shown that such coordination underlies several aspects of bacterial responses
including their ability to develop antibiotic resistance. Here we develop a new
distributed gradient descent method that helps explain how bacterial cells
collectively search for food in harsh environments using extremely limited
communication and computational complexity. This method can also be used for
computational tasks when agents are facing similarly restricted conditions. We
formalize the communication and computation assumptions required for successful
coordination and prove that the method we propose leads to convergence even
when using a dynamically changing interaction network. The proposed method
improves upon prior models suggested for bacterial foraging despite making
fewer assumptions. Simulation studies and analysis of experimental data
illustrate the ability of the method to explain and further predict several
aspects of bacterial swarm food search.

Biochemical reaction networks are often modelled using discrete-state,
continuous-time Markov chains. System statistics of these Markov chains usually
cannot be calculated analytically and therefore estimates must be generated via
simulation techniques. There is a well documented class of simulation
techniques known as exact stochastic simulation algorithms, an example of which
is Gillespie's direct method. These algorithms often come with high
computational costs, therefore approximate stochastic simulation algorithms
such as the tau-leap method are used. However, in order to minimise the bias in
the estimates generated using them, a relatively small value of tau is needed,
rendering the computational costs comparable to Gillespie's direct method.
  The multi-level Monte Carlo method (Anderson and Higham, Multiscale Model.
Simul. 10:146-179, 2012) provides a reduction in computational costs whilst
minimising or even eliminating the bias in the estimates of system statistics.
This is achieved by first crudely approximating required statistics with many
sample paths of low accuracy. Then correction terms are added until a required
level of accuracy is reached. Recent literature has primarily focussed on
implementing the multi-level method efficiently to estimate a single system
statistic. However, it is clearly also of interest to be able to approximate
entire probability distributions of species counts. We present two novel
methods that combine known techniques for distribution reconstruction with the
multi-level method. We demonstrate the potential of our methods using a number
of examples.

Systems biology approaches to the integrative study of cells, organs and
organisms offer the best means of understanding in a holistic manner the
diversity of molecular assays that can be now be implemented in a high
throughput manner. Such assays can sample the genome, epigenome, proteome,
metabolome and microbiome contemporaneously, allowing us for the first time to
perform a complete analysis of physiological activity. The central problem
remains empowering the scientific community to actually implement such an
integration, across seemingly diverse data types and measurements. One
promising solution is to apply semantic techniques on a self-consistent and
implicitly correct ontological representation of these data types. In this
paper we describe how we have applied one such solution, based around the
InterMine data warehouse platform which uses as its basis the Sequence
Ontology, to facilitate a systems biology analysis of virulence in the
apicomplexan pathogen $Toxoplasma~gondii$, a common parasite that infects up to
half the worlds population, with acute pathogenic risks for immuno-compromised
individuals or pregnant mothers. Our solution, which we named `toxoMine', has
provided both a platform for our collaborators to perform such integrative
analyses and also opportunities for such cyberinfrastructure to be further
developed, particularly to take advantage of possible semantic similarities of
value to knowledge discovery in the Omics enterprise. We discuss these
opportunities in the context of further enhancing the capabilities of this
powerful integrative platform.

This paper will detail the basis of our previously developed predictive model
for pigeon flight paths based on observations of the specific individual being
predicted. We will then describe how this model can be adapted to predict the
flight of a new, unobserved bird, based on observations of other individuals
from the same release site. We will test the accuracy of these predictions
relative to naive models with no previous flight information and those trained
on the focal bird's own previous flights, and discuss the implications of these
results for the nature of navigational cue use in the familiar area. Finally we
will discuss how visual cues may be explicitly encoded in the model in future
work.

PANDA (Passing Attributes between Networks for Data Assimilation) is a gene
regulatory network inference method that uses message-passing to integrate
multiple sources of 'omics data. PANDA was originally coded in C++. In this
application note we describe PyPanda, the Python version of PANDA. PyPanda runs
considerably faster than the C++ version and includes additional features for
network analysis. Availability: The open source PyPanda Python package is
freely available at https://github.com/davidvi/pypanda. Contact: d.g.p.van
ijzendoorn@lumc.nl

Stochastic simulation methods can be applied successfully to model exact
spatio-temporally resolved reaction-diffusion systems. However, in many cases,
these methods can quickly become extremely computationally intensive with
increasing particle numbers. An alternative description of many of these
systems can be derived in the diffusive limit as a deterministic, continuum
system of partial differential equations. Although the numerical solution of
such partial differential equations is, in general, much more efficient than
the full stochastic simulation, the deterministic continuum description is
generally not valid when copy numbers are low and stochastic effects dominate.
Therefore, to take advantage of the benefits of both of these types of models,
each of which may be appropriate in different parts of a spatial domain, we
have developed an algorithm that can be used to couple these two types of model
together. This hybrid coupling algorithm uses an overlap region between the two
modelling regimes. By coupling fluxes at one end of the interface and using a
concentration-matching condition at the other end, we ensure that mass is
appropriately transferred between PDE- and compartment-based regimes. Our
methodology gives notable reductions in simulation time in comparison with
using a fully stochastic model, whilst maintaining the important stochastic
features of the system and providing detail in appropriate areas of the domain.
We test our hybrid methodology robustly by applying it to several biologically
motivated problems including diffusion and morphogen gradient formation. Our
analysis shows that the resulting error is small, unbiased and does not grow
over time.

Despite extensive research during the last decades, coronary artery disease
(CAD) remains the number one cause of death, responsible for near 50% of global
mortality. A main reason for this is that CAD has a complex inheritance and
etiology that unlike rare single gene disorders cannot fully be understood from
studies of of genes one-by-one.In parallel, studies that simultaneously assess
multiple, functionally associated genes are warranted. For this reason we
undertook the Stockholm Atherosclerosis Gene Expression (STAGE) study that
besides careful clinical characterization and genome-wide DNA genotyping also
assessed the global gene expression profiles from seven CAD-relevant vascular
and metabolic tissues. In this thesis report we show that by integrating GWAS
with genetics of gene expression studies like STAGE, we can advance our
understanding from the perspective of multiple genes and gene variants acting
in conjunction to cause CAD in the form of regulatory gene networks. This is
done through developing new bioinformatics tools and applying them to
disease-specific, genetics of global gene expression studies like STAGE. These
tools are necessary to go beyond our current limited single-gene understanding
of complex traits, like CAD.

Computational systems biology has provided plenty of insights into cell
biology. Early on, the focus was on reaction networks between molecular
species. Spatial distribution only began to be considered mostly within the
last decade. However, calculations were restricted to small systems because of
tremendously high computational workloads. To date, application to the cell of
typical size with molecular resolution is still far from realization. In this
article, we present a new parallel stochastic method for particle
reaction-diffusion systems. The program called pSpatiocyte was created bearing
in mind reaction networks in biological cells operating in crowded
intracellular environments as the primary simulation target. pSpatiocyte
employs unique discretization and parallelization algorithms based on a
hexagonal close-packed lattice for efficient execution particularly on large
distributed memory parallel computers. For two-level parallelization, we
introduced isolated subdomain and tri-stage lockstep communication for
process-level, and voxel-locking techniques for thread-level. We performed a
series of parallel runs on RIKEN's K computer. For a fine lattice that had
relatively low occupancy, pSpatiocyte achieved 7686 times speedup with 663552
cores relative to 64 cores from the viewpoint of strong scaling and exhibited
74\% parallel efficiency. As for weak scaling, efficiencies at least 60% were
observed up to 663552 cores. In addition to computational performance,
diffusion and reaction rates were validated by theory and another
well-validated program and had good agreement. Lastly, as a preliminary example
of real-world applications, we present a calculation of the MAPK model, a
typical reaction network motif in cell signaling pathways.

We present a Bayesian methodology for infinite as well as finite dimensional
parameter identification for partial differential equation models. The Bayesian
framework provides a rigorous mathematical framework for incorporating prior
knowledge on uncertainty in the observations and the parameters themselves,
resulting in an approximation of the full probability distribution for the
parameters, given the data. Although the numerical approximation of the full
probability distribution is computationally expensive, parallelised algorithms
can make many practically relevant problems computationally feasible. The
probability distribution not only provides estimates for the values of the
parameters, but also provides information about the inferability of parameters
and the sensitivity of the model. This information is crucial when a
mathematical model is used to study the outcome of real-world experiments.
Keeping in mind the applicability of our approach to tackle real-world
practical problems with data from experiments, in this initial proof of concept
work, we apply this theoretical and computational framework to parameter
identification for a well studied semilinear reaction-diffusion system with
activator-depleted reaction kinetics, posed on evolving and stationary domains.

With the advance of experimental techniques such as time-lapse fluorescence
microscopy, the availability of single-cell trajectory data has vastly
increased, and so has the demand for computational methods suitable for
parameter inference with this type of data. However, most of the currently
available methods treat single-cell trajectories independently, ignoring the
mother-daughter relationships and the information provided by population
structure. This information is however essential if a process of interest
happens at cell division, or if it evolves slowly compared to the duration of
the cell cycle. In this work, we highlight the importance of tracking cell
lineage trees and propose a Bayesian framework for parameter inference on
tree-structured data. Our method relies on a combination of Sequential Monte
Carlo for likelihood approximation and Markov Chain Monte Carlo for parameter
sampling. We demonstrate the capabilities of our inference framework on two
simple examples in which the lineage tree information is necessary: one in
which the cell phenotype can only switch at cell division and another where the
cell type fluctuates randomly over timescales that extend well beyond the
lifetime of a single cell.

Motivation: Alternative splicing is an important mechanism in which the
regions of pre-mRNAs are differentially joined in order to form different
transcript isoforms. Alternative splicing is involved in the regulation of
normal physiological functions but also linked to the development of diseases
such as cancer. We analyse differential expression and splicing using RNA-seq
time series in three different settings: overall gene expression levels,
absolute transcript expression levels and relative transcript expression
levels.
  Results: Using estrogen receptor $\alpha$ signalling response as a model
system, our Gaussian process (GP)-based test identifies genes with differential
splicing and/or differentially expressed transcripts. We discover genes with
consistent changes in alternative splicing independent of changes in absolute
expression and genes where some transcripts change while others stay constant
in absolute level. The results suggest classes of genes with different modes of
alternative splicing regulation during the experiment.
  Availability: R and Matlab codes implementing the method are available at
https://github.com/PROBIC/diffsplicing . An interactive browser for viewing all
model fits is available at http://users.ics.aalto.fi/hande/splicingGP/ .

Animal cells use traction forces to sense the mechanics and geometry of their
environment. Measuring these traction forces requires a workflow combining cell
experiments, image processing and force reconstruction based on elasticity
theory. Such procedures have been established before mainly for planar
substrates, in which case one can use the Green's function formalism. Here we
introduce a worksflow to measure traction forces of cardiac myofibroblasts on
non-planar elastic substrates. Soft elastic substrates with a wave-like
topology were micromolded from polydimethylsiloxane (PDMS) and fluorescent
marker beads were distributed homogeneously in the substrate. Using feature
vector based tracking of these marker beads, we first constructed a hexahedral
mesh for the substrate. We then solved the direct elastic boundary volume
problem on this mesh using the finite element method (FEM). Using data
simulations, we show that the traction forces can be reconstructed from the
substrate deformations by solving the corresponding inverse problem with a
L1-norm for the residue and a L2-norm for 0th order Tikhonov regularization.
Applying this procedure to the experimental data, we find that cardiac
myofibroblast cells tend to align both their shapes and their forces with the
long axis of the deformable wavy substrate.

We address the need for affordable, rapid, and easy to use diagnostic
technologies by coupling an innovative thermocycling system that harnesses
natural convection to perform rapid DNA amplification via the polymerase chain
reaction (PCR) with smartphone-based detection. Our approach offers an
inherently simple design that enables PCR to be completed in 10-20 minutes.
Electrical power requirements are dramatically reduced by harnessing natural
convection to actuate the reaction, allowing the entire system to be operated
from a standard USB connection (5 V) via solar battery packs. Instantaneous
detection and analysis are enabled using an ordinary smartphone camera and
dedicated app interface.

Motivation: Clustering techniques are routinely applied to identify patterns
of co-expression in gene expression data. Co-regulation, and involvement of
genes in similar cellular function, is subsequently inferred from the clusters
which are obtained. Increasingly sophisticated algorithms have been applied to
microarray data, however, less attention has been given to the statistical
significance of the results of clustering studies. We present a technique for
the analysis of commonly used hierarchical linkage-based clustering called
Significance Analysis of Linkage Trees (SALT).
  Results: The statistical significance of pairwise similarity levels between
gene expression profiles, a measure of co-expression, is established using a
surrogate data analysis method. We find that a modified version of the standard
linkage technique, complete-linkage, must be used to generate hierarchical
linkage trees with the appropriate properties. The approach is illustrated
using synthetic data generated from a novel model of gene expression profiles
and is then applied to previously analysed microarray data on the
transcriptional response of human fibroblasts to serum stimulation.

The focus of pancreatic cancer research has been shifted from pancreatic
cancer cells towards their microenvironment, involving pancreatic stellate
cells that interact with cancer cells and influence tumor progression. To
quantitatively understand the pancreatic cancer microenvironment, we construct
a computational model for intracellular signaling networks of cancer cells and
stellate cells as well as their intercellular communication. We extend the
rule-based BioNetGen language to depict intra- and inter-cellular dynamics
using discrete and continuous variables respectively. Our framework also
enables a statistical model checking procedure for analyzing the system
behavior in response to various perturbations. The results demonstrate the
predictive power of our model by identifying important system properties that
are consistent with existing experimental observations. We also obtain
interesting insights into the development of novel therapeutic strategies for
pancreatic cancer.

The development of mechanistic models of biological systems is a central part
of Systems Biology. One major task in developing these models is the inference
of the correct model parameters. Due to the size of most realistic models and
their possibly complex dynamical behaviour one must usually rely on sample
based methods. In this paper we present a novel algorithm that reliably
estimates model parameters for deterministic as well as stochastic models from
trajectory data. Our algorithm samples iteratively independent particles from
the level sets of the likelihood and recovers the posterior from these level
sets. The presented approach is easily parallelizable and, by utilizing density
estimation through Dirichlet Process Gaussian Mixture Models, can deal with
high dimensional parameter spaces. We illustrate that our algorithm is
applicable to large, realistic deterministic and stochastic models and succeeds
in inferring the correct posterior from a given number of observed
trajectories. This algorithm presents a novel, computationally feasible
approach to identify parameters of large biochemical reaction models based on
sample path data.

Electric fishes modulate their electric organ discharges with a remarkable
variability. Some patterns can be easily identified, such as pulse rate
changes, offs and chirps, which are often associated with important behavioral
contexts, including aggression, hiding and mating. However, these behaviors are
only observed when at least two fish are freely interacting. Although their
electrical pulses can be easily recorded by non-invasive techniques,
discriminating the emitter of each pulse is challenging when physically similar
fish are allowed to freely move and interact. Here we optimized a custom-made
software recently designed to identify the emitter of pulses by using automated
chirp detection, adaptive threshold for pulse detection and slightly changing
how the recorded signals are integrated. With these optimizations, we performed
a quantitative analysis of the statistical changes throughout the dominance
contest with respect to Inter Pulse Intervals, Chirps and Offs dyads of freely
moving Gymnotus carapo. In all dyads, chirps were signatures of subsequent
submission, even when they occurred early in the contest. Although offs were
observed in both dominant and submissive fish, they were substantially more
frequent in submissive individuals, in agreement with the idea from previous
studies that offs are electric cues of submission. In general, after the
dominance is established the submissive fish significantly changes its average
pulse rate, while the pulse rate of the dominant remained unchanged.
Additionally, no chirps or offs were observed when two fish were manually kept
in direct physical contact, suggesting that these electric behaviors are not
automatic responses to physical contact.

FRET measurements can provide dynamic spatial information on length scales
smaller than the diffraction limit of light. Several methods exist to measure
FRET between fluorophores, including Fluorescence Lifetime Imaging Microscopy
(FLIM), which relies on the reduction of fluorescence lifetime when a
fluorophore is undergoing FRET. FLIM measurements take the form of histograms
of photon arrival times, containing contributions from a mixed population of
fluorophores both undergoing and not undergoing FRET, with the measured
distribution being a mixture of exponentials of different lifetimes. Here, we
present an analysis method based on Bayesian inference that rigorously takes
into account several experimental complications. We test the precision and
accuracy of our analysis on controlled experimental data and verify that we can
faithfully extract model parameters, both in the low-photon and low-fraction
regimes.

Protein quality assessment (QA) has played an important role in protein
structure prediction. We developed a novel single-model quality assessment
method - Qprob. Qprob calculates the absolute error for each protein feature
value against the true quality scores (i.e. GDT-TS scores) of protein
structural models, and uses them to estimate its probability density
distribution for quality assessment. Qprob has been blindly tested on the 11th
Critical Assessment of Techniques for Protein Structure Prediction (CASP11) as
MULTICOM-NOVEL server. The official CASP result shows that Qprob ranks as one
of the top single-model QA methods. In addition, Qprob makes contributions to
our protein tertiary structure predictor MULTICOM, which is officially ranked
3rd out of 143 predictors. The good performance shows that Qprob is good at
assessing the quality of models of hard targets. These results demonstrate that
this new probability density distribution based method is effective for protein
single-model quality assessment and is useful for protein structure prediction.
The webserver and software packages of Qprob are available at:
http://calla.rnet.missouri.edu/qprob/.

Cohort studies employ pairwise measures of association to quantify
dependencies among conditions and exposures. To reliably use these measures to
draw conclusions about the underlying association strengths requires that the
measures be robust and unbiased. These considerations assume greater
significance when applied to disease networks, where associations among
heterogeneous pairs of diseases are ranked. Using disease diagnoses data from a
large cohort of 5.5 million individuals, we develop a comprehensive methodology
to characterize the bias of standard association measures like relative risk
and $\phi$ correlation. To overcome these biases, we devise a novel measure
based on a stochastic model for disease development. The new measure is
demonstrated to have the least overall bias and hence would be most suitable
for application to heterogeneous disease cohorts.

These notes provide a short, focused introduction to modelling stochastic
gene expression, including a derivation of the master equation, the recovery of
deterministic dynamics, birth-and-death processes, and Langevin theory. The
notes were last updated around 2010 and written for lectures given at summer
schools held at McGill University's Centre for Non-linear Dynamics in 2004,
2006, and 2008.

Sigmoid semilogarithmic functions with shape of Boltzmann equations, have
become extremely popular to describe diverse biological situations. Part of the
popularity is due to the easy avail- ability of software which fits Boltzmann
functions to data, without much knowledge of the fitting procedure or the
statistical properties of the parameters derived from the procedure. The
purpose of this paper is to explore the plasticity of the Boltzmann function to
fit data, some aspects of the optimization procedure to fit the function to
data and how to use this plastic function to differentiate the effect of
treatment on data and to attest the statistical significance of treatment
effect on the data.

The biomechanics of the human body allow humans a range of possible ways of
executing movements to attain specific goals. Nevertheless, humans exhibit
significant patterns in how they execute movements. We propose that the
observed patterns of human movement arise because subjects select those ways to
execute movements that are, in a rigorous sense, optimal. In this project, we
show how this proposition can guide the development of computational models of
movement selection and thereby account for human movement patterns. We proceed
by first developing a movement utility formalism that operationalizes the
concept of a best or optimal way of executing a movement using a utility
function so that the problem of movement selection becomes the problem of
finding the movement that maximizes the utility function. Since the movement
utility formalism includes a contribution of the metabolic energy of the
movement (maximum utility movements try to minimize metabolic energy), we also
develop a metabolic energy formalism that we can use to construct estimators of
the metabolic energies of particular movements. We then show how we can
construct an estimator for the metabolic energies of normal walking gaits and
we use that estimator to construct a movement utility model of the selection of
normal walking gaits and show that the relationship between avg. walking speed
and avg. step length predicted by this model agrees with observation. We
conclude by proposing a physical mechanism that a subject might use to estimate
the metabolic energy of a movement in practice.

In this work, we propose a deep learning approach to improve docking-based
virtual screening. The introduced deep neural network, DeepVS, uses the output
of a docking program and learns how to extract relevant features from basic
data such as atom and residues types obtained from protein-ligand complexes.
Our approach introduces the use of atom and amino acid embeddings and
implements an effective way of creating distributed vector representations of
protein-ligand complexes by modeling the compound as a set of atom contexts
that is further processed by a convolutional layer. One of the main advantages
of the proposed method is that it does not require feature engineering. We
evaluate DeepVS on the Directory of Useful Decoys (DUD), using the output of
two docking programs: AutodockVina1.1.2 and Dock6.6. Using a strict evaluation
with leave-one-out cross-validation, DeepVS outperforms the docking programs in
both AUC ROC and enrichment factor. Moreover, using the output of
AutodockVina1.1.2, DeepVS achieves an AUC ROC of 0.81, which, to the best of
our knowledge, is the best AUC reported so far for virtual screening using the
40 receptors from DUD.

Independent computational reproducibility of scientific results is rapidly
becoming of pivotal importance in scientific progress as computation itself
plays a more and more central role in so many branches of science.
Historically, reproducibility has followed the familiar Popperian [38] model
whereby theory cannot be verified by scientific testing, it can only be
falsified. Ultimately, this implies that if an experiment cannot be reproduced
independently to some satisfactory level of precision, its value is essentially
unquantifiable; put brutally, it is impossible to determine its scientific
value. The burgeoning presence of software in most scientific work adds a new
and particularly opaque layer of complexity [29]. In spite of much recent
interest in many scientific areas, emphasis remains more on procedures,
strictures and discussion [12, 14, 16, 29, 30, 37, 41], reflecting the
inexperience of most scientific journals when it comes to software, rather than
the details of how computational reproducibility is actually achieved, for
which there appear to be relatively few guiding examples [6, 10, 17]. After
considering basic principles, here we show how full computational
reproducibility can be achieved in practice at every stage using a case study
of a multi-gigabyte protein study on the open SwissProt protein database, from
data download all the way to individual figure by figure reproduction as an
exemplar for general scientific computation.

Background: Species abundance distributions in chemical reaction network
models cannot usually be computed analytically. Instead, stochas- tic
simulation algorithms allow sample from the the system configuration. Although
many algorithms have been described, no fast implementation has been provided
for {\tau}-leaping which i) is Matlab-compatible, ii) adap- tively alternates
between SSA, implicit and explicit {\tau}-leaping, and iii) provides summary
statistics necessary for Bayesian inference. Results: We provide a
Matlab-compatible implementation of the adap- tive explicit-implicit
{\tau}-leaping algorithm to address the above-mentioned deficits. matLeap
provides equal or substantially faster results compared to two widely used
simulation packages while maintaining accuracy. Lastly, matLeap yields summary
statistics of the stochastic process unavailable with other methods, which are
indispensable for Bayesian inference. Conclusions: matLeap addresses
shortcomings in existing Matlab-compatible stochastic simulation software,
providing significant speedups and sum- mary statistics that are especially
useful for researchers utilizing particle- filter based methods for Bayesian
inference. Code is available for download at
https://github.com/claassengroup/matLeap. Contact:
justin.feigelman@imsb.biol.ethz.ch

Reaction-diffusion models are widely used to study spatially-extended
chemical reaction systems. In order to understand how the dynamics of a
reaction-diffusion model are affected by changes in its input parameters,
efficient methods for computing parametric sensitivities are required. In this
work, we focus on stochastic models of spatially-extended chemical reaction
systems that involve partitioning the computational domain into voxels.
Parametric sensitivities are often calculated using Monte Carlo techniques that
are typically computationally expensive; however, variance reduction techniques
can decrease the number of Monte Carlo simulations required. By exploiting the
characteristic dynamics of spatially-extended reaction networks, we are able to
adapt existing finite difference schemes to robustly estimate parametric
sensitivities in a spatially-extended network. We show that algorithmic
performance depends on the dynamics of the given network and the choice of
summary statistics. We then describe a hybrid technique that dynamically
chooses the most appropriate simulation method for the network of interest. Our
method is tested for functionality and accuracy in a range of different
scenarios.

Objective: Syncope is a sudden loss of consciousness with loss of postural
tone and spontaneous recovery; it is a common condition, albeit one that is
challenging to accurately diagnose. Uncertainties about the triggering
mechanisms and their underlying pathophysiology have led to various
classifications of patients exhibiting this symptom. This study presents a new
way to classify syncope types using machine learning. Method: we hypothesize
that syncope types can be characterized by analyzing blood pressure and heart
rate time series data obtained from the head-up tilt test procedure. By
optimizing classification rates, we identify a small number of determining
markers which enable data clustering. Results: We apply the proposed method to
clinical data from 157 subjects; each subject was identified by an expert as
being either healthy or suffering from one of three conditions:
cardioinhibitory syncope, vasodepressor syncope and postural orthostatic
tachycardia. Clustering confirms the three disease groups and identifies two
distinct subgroups within the healthy controls. Conclusion: The proposed method
provides evidence to question current syncope classifications; it also offers
means to refine them. Significance: Current syncope classifications are not
based on pathophysiology and have not led to significant improvements in
patient care. It is expected that a more faithful classification will
facilitate our understanding of the autonomic system for healthy subjects,
which is essential in analyzing pathophysiology of the disease groups.

In single-particle cryo-electron microscopy (cryo-EM), K-means clustering
algorithm is widely used in unsupervised 2D classification of projection images
of biological macromolecules. 3D ab initio reconstruction requires accurate
unsupervised classification in order to separate molecular projections of
distinct orientations. Due to background noise in single-particle images and
uncertainty of molecular orientations, traditional K-means clustering algorithm
may classify images into wrong classes and produce classes with a large
variation in membership. Overcoming these limitations requires further
development on clustering algorithms for cryo-EM data analysis. We propose a
novel unsupervised data clustering method building upon the traditional K-means
algorithm. By introducing an adaptive constraint term in the objective
function, our algorithm not only avoids a large variation in class sizes but
also produces more accurate data clustering. Applications of this approach to
both simulated and experimental cryo-EM data demonstrate that our algorithm is
a significantly improved alterative to the traditional K-means algorithm in
single-particle cryo-EM analysis.

In this paper we propose a workflow to detect and track mitotic cells in
time-lapse microscopy image sequences. In order to avoid the requirement for
cell lines expressing fluorescent markers and the associated phototoxicity,
phase contrast microscopy is often preferred over fluorescence microscopy in
live-cell imaging. However, common specific image characteristics complicate
image processing and impede use of standard methods. Nevertheless, automated
analysis is desirable due to manual analysis being subjective, biased and
extremely time-consuming for large data sets. Here, we present the following
workflow based on mathematical imaging methods. In the first step, mitosis
detection is performed by means of the circular Hough transform. The obtained
circular contour subsequently serves as an initialisation for the tracking
algorithm based on variational methods. It is sub-divided into two parts: in
order to determine the beginning of the whole mitosis cycle, a backwards
tracking procedure is performed. After that, the cell is tracked forwards in
time until the end of mitosis. As a result, the average of mitosis duration and
ratios of different cell fates (cell death, no division, division into two or
more daughter cells) can be measured and statistics on cell morphologies can be
obtained. All of the tools are featured in the user-friendly
MATLAB$^{\circledR}$ Graphical User Interface MitosisAnalyser.

Wild populations of Medicago ciliaris and Medicago polymorpha were subjected
to four salt treatments 0, 50, 100 and 150 mM NaCl, plant growth and proline
concentration in leaves were assessed. The analyzed data revealed significant
variability in salt response within and between the two species, depending on
the salinity level. It was found that high NaCl concentrations affected all the
growth parameters. However, the reduction was more important at higher NaCl
concentrations and the highest reduction was obtained for the populations of
Medicago polymorpha where it reached around 90% in root length at 150 mM NaCl
for Pmar. The Tunisian population of Medicago ciliaris, prospected on soils
affected by salinity, was the best tolerant in all ecotypes studied in this
work. This population, exhibits a particular adaptability to salt environment
at both germination and seedling stage. Furthermore, the correlation among the
studied plants sensitivity and leaf proline concentration showed that high
proline contents were related to their reactivity to salt. Consequently, it
appeared that proline biosynthesis occurred presumably as a consequence of
disturbance in cell homoeostasis and reflected poor performance and greater
damage in response to salt stress. These findings indicated that this osmolytes
content may be used as another useful criterion to differentiate salt-tolerant
from salt sensitive plant in annual medics.

The biomechanics of the human body allow humans a range of possible ways of
executing movements to attain specific goals. This range of movement is limited
by a number of mechanical, biomechanical, or cognitive constraints. Shifts in
these limits result in changes available possible movements from which a
subject can select and can affect which movements a subject selects. Therefore
by understanding the limits on the range of movement we can come to a better
understanding of declines in movement performance due to disease or aging. In
this project, we look at how models for the limits on the range of movement can
be derived in a principled manner from a model of the movement. Using the
example of normal walking gaits, we develop a lower limit on the avg. walking
speed by examining the process by which the body restores mechanical energy
lost during walking, and we develop an upper limit on the avg. step length by
examining the forces the body can exert doing external mechanical work, in this
case, pulling a cart. Making slight changes to the model for normal walking
gaits, we develop a model of very slow walking gaits with avg. walking speeds
below the lower limit on normal walking gaits but that also has a lower limit
on the avg. walking speed. We note that the lowest avg. walking speeds observed
clinically fall into the range of very slow walking gaits so defined, and argue
that forms of bipedal locomotion with still lower speeds should be considered
distinct from walking gaits.

STATCHECK is an R algorithm designed to scan papers automatically for
inconsistencies between test statistics and their associated p values (Nuijten
et al., 2016). The goal of this comment is to point out an important and
well-documented flaw in this busily applied algorithm: It cannot handle
corrected p values. As a result, statistical tests applying appropriate
corrections to the p value (e.g., for multiple tests, post-hoc tests,
violations of assumptions, etc.) are likely to be flagged as reporting
inconsistent statistics, whereas papers omitting necessary corrections are
certified as correct. The STATCHECK algorithm is thus valid for only a subset
of scientific papers, and conclusions about the quality or integrity of
statistical reports should never be based solely on this program.

Production of chemicals from engineered organisms in a batch culture involves
an inherent trade-off between productivity, yield, and titer. Existing
strategies for strain design typically focus on designing mutations that
achieve the highest yield possible while maintaining growth viability. While
these methods are computationally tractable, an optimum productivity could be
achieved by a dynamic strategy in which the intracellular division of resources
is permitted to change with time. New methods for the design and implementation
of dynamic microbial processes, both computational and experimental, have
therefore been explored to maximize productivity. However, solving for the
optimal metabolic behavior under the assumption that all fluxes in the cell are
free to vary is a challenging numerical task. This work presents an efficient
method for the calculation of a maximum theoretical productivity of a batch
culture system using a dynamic optimization framework. This metric is analogous
to the maximum theoretical yield, a measure that is well established in the
metabolic engineering literature and whose use helps guide strain and pathway
selection. The proposed method follows traditional assumptions of dynamic flux
balance analysis: (1) that internal metabolite fluxes are governed by a
pseudo-steady state, and (2) that external metabolite fluxes are dynamically
bounded. The optimization is achieved via collocation on finite elements, and
accounts explicitly for an arbitrary number of flux changes. The method can be
further extended to explicitly solve for the trade-off curve between maximum
productivity and yield. We demonstrate the method on succinate production in
two common microbial hosts, Escherichia coli and Actinobacillus succinogenes,
revealing that nearly optimal yields and productivities can be achieved with
only two discrete flux stages.

Thermodynamic scaling explains the dramatic successes of CTP fused human
growth proteins as regards lifetime in vivo and enhanced functionality compared
to their wild-type analogues, like Biogen. The theory is semi-quantitative and
contains no adjustable parameters. It shows how hydrophilic terminal spheres
orient fused proteins in the neighborhood of a membrane surface, extending
lifetimes and improving functionality.

Background: The huge quantity of data produced in Biomedical research needs
sophisticated algorithmic methodologies for its storage, analysis, and
processing. High Performance Computing (HPC) appears as a magic bullet in this
challenge. However, several hard to solve parallelization and load balancing
problems arise in this context. Here we discuss the HPC-oriented implementation
of a general purpose learning algorithm, originally conceived for DNA analysis
and recently extended to treat uncertainty on data (U BRAIN). The U-BRAIN
algorithm is a learning algorithm that finds a Boolean formula in disjunctive
normal form (DNF), of approximately minimum complexity, that is consistent with
a set of data (instances) which may have missing bits. The conjunctive terms of
the formula are computed in an iterative way by identifying, from the given
data, a family of sets of conditions that must be satisfied by all the positive
instances and violated by all the negative ones; such conditions allow the
computation of a set of coefficients (relevances) for each attribute (literal),
that form a probability distribution, allowing the selection of the term
literals. The great versatility that characterizes it, makes U-BRAIN applicable
in many of the fields in which there are data to be analyzed. However the
memory and the execution time required by the running are of O(n3) and of O(n5)
order, respectively, and so, the algorithm is unaffordable for huge data sets.

Modern technologies are enabling scientists to collect extraordinary amounts
of complex and sophisticated data across a huge range of scales like never
before. With this onslaught of data, we can allow the focal point to shift
towards answering the question of how we can analyze and understand the massive
amounts of data in front of us. Unfortunately, lack of standardized sharing
mechanisms and practices often make reproducing or extending scientific results
very difficult. With the creation of data organization structures and tools
which drastically improve code portability, we now have the opportunity to
design such a framework for communicating extensible scientific discoveries.
Our proposed solution leverages these existing technologies and standards, and
provides an accessible and extensible model for reproducible research, called
"science in the cloud" (sic). Exploiting scientific containers, cloud computing
and cloud data services, we show the capability to launch a computer in the
cloud and run a web service which enables intimate interaction with the tools
and data presented. We hope this model will inspire the community to produce
reproducible and, importantly, extensible results which will enable us to
collectively accelerate the rate at which scientific breakthroughs are
discovered, replicated, and extended.

Interpretability of machine learning models is critical for data-driven
precision medicine efforts. However, highly predictive models are generally
complex and are difficult to interpret. Here using Model-Agnostic Explanations
algorithm, we show that complex models such as random forest can be made
interpretable. Using MIMIC-II dataset, we successfully predicted ICU mortality
with 80% balanced accuracy and were also were able to interpret the relative
effect of the features on prediction at individual level.

In this paper, we study the "free exploration" of individual ants of the
species Atta insularis, i.e., their motion on a featureless flat, horizontal
surface. Two basic preliminary results emerge from our work (a) the free
exploration is super-diffusive and (b) ants tend to turn more frequently to the
left than to the right -so we call them "let-handed"-. More statistics is
needed, however, to confirm those findings.

Although somatic mutations are the main contributor to cancer, underlying
germline alterations may increase the risk of cancer, mold the somatic
alteration landscape and cooperate with acquired mutations to promote the tumor
onset and/or maintenance. Therefore, both tumor genome and germline sequence
data have to be analyzed to have a more complete picture of the overall genetic
foundation of the disease. To reinforce such notion we quantitatively assess
the bias of restricting the analysis to somatic mutation data using mutational
data from well-known cancer genes which displays both types of alterations,
inherited and somatically acquired mutations.

Metagenome, a mixture of different genomes (as a rule, bacterial), represents
a pattern, and the analysis of its composition is, currently, one of the
challenging problems of bioinformatics. In the present study, the possibility
of evaluating metagenome composition by DNA-marker methods is investigated.
These methods are based on using primers, short nucleic acid fragments. Each
primer picks out of the tested genome the fragment set specific just for this
genome, which is called its spectrum (for the given primer) and is used for
identifying the genome. The DNA-marker method, applied to a metagenome, also
gives its spectrum, which, obviously, represents the union of the spectra of
all genomes belonging to the metagenome. Thus each primer provides a projection
of the genomes and of the metagenome onto the corresponding spectra set. Here
we propose to apply the random projection (random primer) approach for
analyzing metagenome composition and present some estimates of the method
effectiveness for the case of Random Amplified Polymorphic DNA (RAPD)
technology.

Heparin is an important anticoagulant drug, about one billion doses are
produced annually. It is a polydisperse sulfated polysaccharide, and the
inherent heterogeneity makes the analysis of heparin difficult. The global
crisis resulting from adulterated heparin in 2008 has drawn renewed attention
to the challenges that are associated with the quality control and
characterization of this complex biological medicine from natural sources. The
present study addresses the need for simple and user-friendly analytical
methods for the fast and accurate quantification of heparin in complex
matrices. Direct quantification of heparin in the low microgram per mL range
was accomplished using a specific commercially available assay based on the
fluorescent molecular probe Heparin Red, simply by mixing the heparin
containing sample and a reagent solution in a 96-well microplate followed by
fluorescence readout. A screening of typical impurities in raw heparin
(selected other glycosaminoglycans, residual nucleic acids and proteins),
related to the extraction from animal tissues, as well as of components of the
urine matrix (inorganic salts, amino acids, trace proteins) revealed that these
compounds even in large excess have no or very little effect on the accuracy of
heparin determination. Heparin spike detection in urine, a biological
multicomponent matrix, also showed good accuracy. We envision applications of
this mix-and-read assay in the process and quality control in heparin
manufacturing, but also in pharmacokinetic studies as a convenient tool for
measuring of the urinary excretion of heparins.

Human movements are physical processes combining the classical mechanics of
the human body moving in space and the biomechanics of the muscles generating
the forces acting on the body under sophisticated sensory-motor control. One
way to characterize movement performance is through measures of energy
efficiency that relate the mechanical energy of the body and metabolic energy
expended by the muscles. We expect the practical utility of such measures to be
greater when human subjects execute movements that maximize energy efficiency.
We therefore seek to understand if and when subjects select movements with that
maximizing energy efficiency. We proceed using a model-based approach to
describe movements which perform a task requiring the body to add or remove
external mechanical work to or from an object. We use the specific example of
walking gaits doing external mechanical work by pulling a cart, and estimate
the relationship between the avg. walking speed and avg. step length. In the
limit where no external work is done, we find that the estimated maximum energy
efficiency walking gait is much slower than the walking gaits healthy adults
typically select. We then modify the situation of the walking gait by
introducing an idealized mechanical device that creates an adjustable
mechanical advantage. The walking gaits that maximize the energy efficiency
using the optimal mechanical advantage are again much slower than the walking
gaits healthy adults typically select. We finally modify the situation so that
the avg. walking speed is fixed and derive the pattern of the avg. step length
and mechanical advantage that maximize energy efficiency.

Both the weighted and unweighted Unifrac distances have been very
successfully employed to assess if two communities differ, but do not give any
information about how two communities differ. We take advantage of recent
observations that the Unifrac metric is equivalent to the so-called earth
mover's distance (also known as the Kantorovich-Rubinstein metric) to develop
an algorithm that not only computes the Unifrac distance in linear time and
space, but also simultaneously finds which operational taxonomic units are
responsible for the observed differences between samples. This allows the
algorithm, called EMDUnifrac, to determine why given samples are different, not
just if they are different, and with no added computational burden. EMDUnifrac
can be utilized on any distribution on a tree, and so is particularly suitable
to analyzing both operational taxonomic units derived from amplicon sequencing,
as well as community profiles resulting from classifying whole genome shotgun
metagenomes. The EMDUnifrac source code (written in python) is freely available
at: https://github.com/dkoslicki/EMDUnifrac.

Motivation:
  Flux balance analysis, and its variants, are widely used methods for
predicting steady-state reaction rates in biochemical reaction networks. The
exploration of high dimensional networks with such methods is currently
hampered by software performance limitations.
  Results:
  DistributedFBA.jl is a high-level, high-performance, open-source
implementation of flux balance analysis in Julia. It is tailored to solve
multiple flux balance analyses on a subset or all the reactions of large and
huge-scale networks, on any number of threads or nodes.
  Availability:
  The code and benchmark data are freely available on
http://github.com/opencobra/COBRA.jl. The documentation can be found at
http://opencobra.github.io/COBRA.jl

Dyslexia is a developmental learning disorder of single word reading accuracy
and/or fluency, with compelling research directed towards understanding the
contributions of the visual system. While dyslexia is not an oculomotor
disease, readers with dyslexia have shown different eye movements than
typically developing students during text reading. Readers with dyslexia
exhibit longer and more frequent fixations, shorter saccade lengths, more
backward refixations than typical readers. Furthermore, readers with dyslexia
are known to have difficulty in reading long words, lower skipping rate of
short words, and high gaze duration on many words. It is an open question
whether it is possible to harness these distinctive oculomotor scanning
patterns observed during reading in order to develop a screening tool that can
reliably identify struggling readers, who may be candidates for dyslexia. Here,
we introduce a novel, fast, objective, non-invasive method, named Rapid
Assessment of Difficulties and Abnormalities in Reading (RADAR) that screens
for features associated with the aberrant visual scanning of reading text seen
in dyslexia. Eye tracking parameter measurements that are stable under retest
and have high discriminative power, as indicated by their ROC curves, were
obtained during silent text reading. These parameters were combined to derive a
total reading score (TRS) that can reliably separate readers with dyslexia from
typical readers. We tested TRS in a group of school-age children ranging from
8.5 to 12.5 years of age. TRS achieved 94.2% correct classification of children
tested. Specifically, 35 out of 37 control (specificity 94.6%) and 30 out of 32
readers with dyslexia (sensitivity 93.8%) were classified correctly using
RADAR, under a circular validation condition where the individual evaluated was
not included in the test construction group.

Burkholderia is an important genus encompassing a variety of species,
including pathogenic strains as well as strains that promote plant growth. We
have carried out a global strategy, which combined two complementary
approaches. The first one is genome guided with deep analysis of genome
sequences and the second one is assay guided with experiments to support the
predictions obtained in silico. This efficient screening for new secondary
metabolites, performed on 48 gapless genomes of Burkholderia species, revealed
a total of 161 clusters containing nonribosomal peptide synthetases (NRPSs),
with the potential to synthesize at least 11 novel products. Most of them are
siderophores or lipopeptides, two classes of products with potential
application in biocontrol. The strategy led to the identification, for the
first time, of the cluster for cepaciachelin biosynthesis in the genome of
Burkholderia ambifaria AMMD and a cluster corresponding to a new
malleobactin-like siderophore, called phymabactin, was identified in
Burkholderia phymatum STM815 genome. In both cases, the siderophore was
produced when the strain was grown in iron-limited conditions. Elsewhere, the
cluster for the antifungal burkholdin was detected in the genome of B.
ambifaria AMMD and also Burkholderia sp. KJ006. Burkholderia pseudomallei
strains harbor the genetic potential to produce a novel lipopeptide called
burkhomycin, containing a peptidyl moiety of 12 monomers. A mixture of
lipopeptides produced by Burkholderia rhizoxinica lowered the surface tension
of the supernatant from 70 to 27 mN/m. The production of nonribosomal secondary
metabolites seems related to the three phylogenetic groups obtained from 16S
rRNA sequences. Moreover, the genome-mining approach gave new insights into the
nonribosomal synthesis exemplified by the identification of dual C/E domains in
lipopeptide NRPSs, up to now essentially found in Pseudomonas strains.

Metabarcoding on amplicons is rapidly expanding as a method to produce
molecular based inventories of microbial communities. Here, we work on
freshwater diatoms, which are microalgae possibly inventoried both on a
morphological and a molecular basis. We have developed an algorithm, in a
program called diagno-syst, based a the notion of informative read, which
carries out supervised clustering of reads by mapping them exactly one by one
on all reads of a well curated and taxonomically annotated reference database.
This program has been run on a HPC (and HTC) infrastructure to address
computation load. We compare optical and molecular based inventories on 10
samples from L\'eman lake, and 30 from Swedish rivers. We track all
possibilities of mismatches between both approaches, and compare the results
with standard pipelines (with heuristics) like Mothur. We find that the
comparison with optics is more accurate when using exact calculations, at the
price of a heavier computation load. It is crucial when studying the long tail
of biodiversity, which may be overestimated by pipelines or algorithms using
heuristics instead (more false positive). This work supports the analysis that
these methods will benefit from progress in, first, building an agreement
between molecular based and morphological based systematics and, second, having
as complete as possible publicly available reference databases.

The \emph{community}, the assemblage of organisms co-existing in a given
space and time, has the potential to become one of the unifying concepts of
biology, especially with the advent of high-throughput sequencing experiments
that reveal genetic diversity exhaustively. In this spirit we show that a tool
from community ecology, the Rank Abundance Distribution (RAD), can be turned by
the new MaxRank normalization method into a generic, expressive descriptor for
quantitative comparison of communities in many areas of biology. To illustrate
the versatility of the method, we analyze RADs from various \emph{generalized
communities}, i.e.\ assemblages of genetically diverse cells or organisms,
including human B cells, gut microbiomes under antibiotic treatment and of
different ages and countries of origin, and other human and environmental
microbial communities. We show that normalized RADs enable novel quantitative
approaches that help to understand structures and dynamics of complex
generalize communities.

Background. Wearable accelerometry devices allow collection of high-density
activity data in large epidemiological studies both in-the-lab as well as
in-the-wild (free-living). Such data can be used to detect and identify periods
of sustained harmonic walking. This report aims to establish whether the micro-
and macro-features of walking identified in the laboratory and free-living
environments are associated with measures of physical function, mobility,
fatigability, and fitness.
  Methods. Fifty-one older adults (median age 77.5) enrolled in the
Developmental Epidemiologic Cohort Study in Pittsburgh, Pennsylvania were
included in the analyses. The study included an in-the-lab component as well as
7 days of monitoring in-the-wild. Participants were equipped with hip-worn
Actigraph GT3X+ activity monitors, which collect high-density raw accelerometry
data. We applied a walking identification algorithm to the data and defined
features of walking, such as participant-specific walking acceleration and
cadence. The association between these walking features and physical function,
mobility, fatigability, and fitness was quantified using linear regression
analysis.
  Results. Micro-scale features of walking (acceleration and cadence) estimated
from in-the-lab and in-the-wild data were associated with measures of physical
function, mobility, fatigability, and fitness. In-the-lab median walking
acceleration was strongly inversely associated with physical function,
mobility, fatigability and fitness. Additionally, in-the-wild daily walking
time was inversely associated with usual- and fast-paced 400m walking time.
  Conclusions. The proposed accelerometry-derived walking features are
significantly associated with measures of physical function, mobility,
fatigability, and fitness, which provides evidence of convergent validity.

1. Electronic telemetry is frequently used to document animal movement
through time. Methods that can identify underlying behaviors driving specific
movement patterns can help us understand how and why animals use available
space, thereby aiding conservation and management efforts. For aquatic animal
tracking data with significant measurement error, a Bayesian state-space model
called the first-Difference Correlated Random Walk with Switching (DCRWS) has
often been used for this purpose. However, for aquatic animals, highly accurate
tracking data of animal movement are now becoming more common.
  2. We developed a new Hidden Markov Model (HMM) for identifying behavioral
states from animal tracks with negligible error, which we called the Hidden
Markov Movement Model (HMMM). We implemented as the basis for the HMMM the
process equation of the DCRWS, but we used the method of maximum likelihood and
the R package TMB for rapid model fitting.
  3. We compared the HMMM to a modified version of the DCRWS for highly
accurate tracks, the DCRWSnome, and to a common HMM for animal tracks fitted
with the R package moveHMM. We show that the HMMM is both accurate and suitable
for multiple species by fitting it to real tracks from a grey seal, lake trout,
and blue shark, as well as to simulated data.
  4. The HMMM is a fast and reliable tool for making meaningful inference from
animal movement data that is ideally suited for ecologists who want to use the
popular DCRWS implementation for highly accurate tracking data. It additionally
provides a groundwork for development of more complex modelling of animal
movement with TMB. To facilitate its uptake, we make it available through the R
package swim.

Quantification of system-wide perturbations from time series -omic data (i.e.
a large number of variables with multiple measures in time) provides the basis
for many downstream hypothesis generating tools. Here we propose a method,
Massively Parallel Analysis of Time Series (MPATS) that can be applied to
quantify transcriptome-wide perturbations. The proposed method characterizes
each individual time series through its $\ell_1$ distance to every other time
series. Application of MPATS to compare biological conditions produces a ranked
list of time series based on their magnitude of differences in their $\ell_1$
representation, which then can be further interpreted through enrichment
analysis. The performance of MPATS was validated through its application to a
study of IFN$\alpha$ dendritic cell responses to viral and bacterial infection.
In conjunction with Gene Set Enrichment Analysis (GSEA), MPATS produced
consistently identified signature gene sets of anti-bacterial and anti-viral
response. Traditional methods such as EDGE and GSEA Time Series (GSEA-TS)
failed to identify the relevant signature gene sets. Furthermore, the results
of MPATS highlighted the crucial functional difference between STAT1/STAT2
during anti-viral and anti-bacterial response. In our simulation study, MPATS
exhibited acceptable performance with small group size (n = 3), when the
appropriate effect size is considered. This method can be easily adopted for
other -omic data types.

The Gene Ontology (GO) is a major bioinformatics ontology that provides
structured controlled vocabularies to classify gene and proteins function and
role. The GO and its annotations to gene products are now an integral part of
functional analysis. Recently, the evaluation of similarity among gene products
starting from their annotations (also referred to as semantic similarities) has
become an increasing area in bioinformatics. While many research on updates to
the structure of GO and on the annotation corpora have been made, the impact of
GO evolution on semantic similarities is quite unobserved. Here we extensively
analyze how GO changes that should be carefully considered by all users of
semantic similarities. GO changes in particular have a big impact on
information content (IC) of GO terms. Since many semantic similarities rely on
calculation of IC it is obvious that the study of these changes should be
deeply investigated. Here we consider GO versions from 2005 to 2014 and we
calculate IC of all GO Terms considering five different formulation. Then we
compare these results. Analysis confirm that there exists a statistically
significant difference among different calculation on the same version of the
ontology (and this is quite obvious) and there exists a statistically
difference among the results obtained with different GO version on the same IC
formula. Results evidence there exist a remarkable bias due to the GO evolution
that has not been considered so far. Possible future works should keep into
account this consideration.

In the stochastic formulation of chemical kinetics, the stationary moments of
the population count of species can be described via a set of linear equations.
However, except for some specific cases such as systems with linear reaction
propensities, the moment equations are underdetermined as a lower order moment
might depend upon a higher order moment. Here, we propose a method to find
lower, and upper bounds on stationary moments of molecular counts in a chemical
reaction system. The method exploits the fact that statistical moments of any
positive-valued random variable must satisfy some constraints. Such constraints
can be expressed as nonlinear inequalities on moments in terms of their lower
order moments, and solving them in conjugation with the stationary moment
equations results in bounds on the moments. Using two examples of biochemical
systems, we illustrate that not only one obtains upper and lower bounds on a
given stationary moment, but these bounds also improve as one uses more moment
equations and utilizes the inequalities for the corresponding higher order
moments. Our results provide avenues for development of moment approximations
that provide explicit bounds on moment dynamics for systems whose dynamics are
otherwise intractable.

Assessing the performance and the characteristics (e.g. yield, quality,
disease resistance, abiotic stress tolerance) of new varieties is a key
component of crop performance improvement. However, the variety testing process
is presently exclusively based on experimental field approaches which
inherently reduces the number and the diversity of experienced combinations of
varieties x environmental conditions in regard of the multiplicity of growing
conditions within the cultivation area. Our aim is to make a greater and faster
use of the information issuing from these trials using crop modeling and
simulation to amplify the environmental and agronomic conditions in which the
new varieties are tested.
  In this study, we present a model-based approach to assist variety testing
and implement this approach on sunflower crop, using the SUNFLO simulation
model and a subset of 80 trials from a large multi-environment trial (MET)
conducted each year by agricultural extension services to compare newly
released sunflower hybrids. After estimating parameter values (using plant
phenotyping) to account for new genetic material, we independently evaluated
the model prediction capacity on the MET (model accuracy was 54.4 %) and its
capacity to rank commercial hybrids for performance level (Kendall's $\tau$ =
0.41, P < 0.01). We then designed a numerical experiment by combining the
previously tested genetic and new cropping conditions (2100 virtual trials) to
determine the best varieties and related management in representative French
production regions.
  We suggest that this approach could find operational outcomes to recommend
varieties according to environment types. Such spatial management of genetic
resources could potentially improve crop performance by reducing the
genotype-phenotype mismatch in farming environments.

The evolutionary success of ants and other social insects is considered to be
intrinsically linked to division of labor and emergent collective intelligence.
The role of the brains of individual ants in generating these processes,
however, is poorly understood. One genus of ant of special interest is
Pheidole, which includes more than a thousand species, most of which are
dimorphic, i.e. their colonies contain two subcastes of workers: minors and
majors. Using confocal imaging and manual annotations, it has been demonstrated
that minor and major workers of different ages of three species of Pheidole
have distinct patterns of brain size and subregion scaling. However, these
studies require laborious effort to quantify brain region volumes and are
subject to potential bias. To address these issues, we propose a group-wise 3D
registration approach to build for the first time bias-free brain atlases of
intra- and inter-subcaste individuals and automatize the segmentation of new
individuals.

Identification and alignment of three-dimensional folding of proteins may
yield useful information about relationships too remote to be detected by
conventional methods, such as sequence comparison, and may potentially lead to
prediction of patterns and motifs in mutual structural fragments. With the
exponential increase of structural proteomics data, the methods that scale with
the rate of increase of data lose efficiency. Hence, new methods that reduce
the computational expense of this problem should be developed. We present a
novel framework through which we are able to find and align protein structure
neighbors via hierarchical clustering and entropy-based query search, and
present a web-based protein database search and alignment tool to demonstrate
the applicability of our approach. The resulting method replicates the results
of the current gold standard with a minimal loss in sensitivity in a
significantly shorter amount of time, while ameliorating the existing web
workspace of protein structure comparison with a customized and dynamic
web-based environment. Our tool serves as both a functional industrial means of
protein structure comparison and a valid demonstration of heuristics in
proteomics.

A key challenge in drug delivery systems is the real time monitoring of
delivered drug and subsequent response. Recent advancement in nanotechnology
has enabled the design and preclinical implementation of novel drug delivery
systems (DDS) with theranostic abilities. Herein, fluorescent cerium fluoride
(CeF3) nanoparticles (nps) were synthesized and their surface modified with a
coat of polyethylenimine (PEI). Thereafter, Methotrexate was conjugated upon it
through glutaraldehyde crosslinking for a pH-sensitive release. This was
followed by the addition of a Hyaluronic acid (HA) receptor via
1-Ethyl-3-(3-dimethylaminopropyl)-carbodiimide and N-hydroxysuccinimide
(EDC-NHS) chemistry to achieve a possible active drug targeting system. The
obtained drug delivery nano-agent retains and exhibits unique photo-luminescent
properties attributed to the nps while exhibiting potential theranostic
capabilities.

Fire propagation is a major concern in the world in general and in
Argentinian northwestern Patagonia in particular where every year hundreds of
hectares are affected by both natural and anthropogenic forest fires. We
developed an efficient cellular automata model in Graphic Processing Units
(GPUs) to simulate fire propagation. The graphical advantages of GPUs were
exploded by overlapping wind direction maps, as well as vegetation, slope and
aspect maps, taking into account relevant landscape characteristics for fire
propagation. Stochastic propagation was performed with a probability model that
depends on aspect, slope, wind direction and vegetation type. Implementing a
genetic algorithm search strategy we show, using simulated fires, that we
recover the five parameter values that characterize fire propagation. The
efficiency of the fire simulation procedure allowed us to also estimate the
fire ignition point when it is unknown as well as its associated uncertainty,
making this approach suitable for the analysis of fire spread based on maps of
burned areas without knowing the point of origin of the fires or how they
spread.

This paper surveys various distance measures for networks and graphs that
were introduced in persistent homology. The scope of the paper is limited to
network distances that were actually used in brain networks but the methods can
be easily adapted to any weighted graph in other fields. The network version of
Gromov-Hausdorff, bottleneck, kernel distances are introduced. We also
introduce a recently developed KS-test like distance based on monotonic
topology features such as the zeroth Betti number. Numerous toy examples and
the result of applying many different distances to the brain networks of
different clinical status and populations are given.

Cellular Electron Cryotomography (CryoET) offers the ability to look inside
cells and observe macromolecules frozen in action. A primary challenge for this
technique is identifying and extracting the molecular components within the
crowded cellular environment. We introduce a method using neural networks to
dramatically reduce the time and human effort required for subcellular
annotation and feature extraction. Subsequent subtomogram classification and
averaging yields in-situ structures of molecular components of interest.

Extreme climatic events have been shown to be strong drivers of tree growth,
forest dynamics, and range contraction. Here we study the climatic drivers of
Picea crassifolia Kom., an endemic to northwest China where climate has
significantly warmed. Picea crassifolia was sampled from its lower
distributional margin to its upper distributional margin on the Helan Mountains
to test the hypothesis that 1) growth at the upper limit is limited by cool
temperatures and 2) is limited by drought at its lower limit. We found that
trees at the lower distributional margin have experienced a higher rate of
stem-growth cessation events since 2001 compared to trees at other elevations.
While all populations have a similar climatic sensitivity, stem-growth
cessation events in trees at lower distributional margin appear to be driven by
low precipitation in June as the monsoon begins to deliver moisture to the
region. Evidence indicates that mid-summer (July) vapor pressure deficit (VPD)
exacerbates the frequency of these events. These data and our analysis makes it
evident that an increase in severity and frequency of drought early in the
monsoon season could increase the frequency and severity of stem-growth
cessation in Picea crassifolia trees at lower elevations. Increases in VPD and
warming would likely exacerbate the growth stress of this species on Helan
Mountain. Hypothetically, if the combinations of low moisture and increased VPD
stress becomes more common, the mortality rate of lower distributional margin
trees could increase, especially of those that are already experiencing events
of temporary growth cessation.

Estimates of age-specific natural (M) and fishing (F) mortalities among
economically important stocks are required to determine sustainable yields and,
ultimately, facilitate effective resource management. Here we used hazard
functions to estimate mortality rates for eastern sea garfish, Hyporhamphus
australis, a pelagic species that forms the basis of an Australian commercial
lampara-net fishery. Data describing annual (2004 to 2015) age frequencies (0-1
to 5-6 years), yield, effort (boat-days), and average weights at age were used
to fit various stochastic models to estimate mortality rates by maximum
likelihood. The model best supported by the data implied: (i) the escape of
fish aged 0-1 years increased from approximately 90 to 97% as a result of a
mandated increase in stretched mesh opening from 25 to 28 mm; (ii) full
selectivity among older age groups; (iii) a constant M of 0.52 +- 0.06 per
year; and (iv) a decline in F between 2004 and 2015. Recruitment and biomass
were estimated to vary, but increased during the sampled period. The results
reiterate the utility of hazard functions to estimate and partition mortality
rates, and support traditional input controls designed to reduce both accounted
and unaccounted F.

Motivation: Cellular Electron CryoTomography (CECT) enables 3D visualization
of cellular organization at near-native state and in sub-molecular resolution,
making it a powerful tool for analyzing structures of macromolecular complexes
and their spatial organizations inside single cells. However, high degree of
structural complexity together with practical imaging limitations make the
systematic de novo discovery of structures within cells challenging. It would
likely require averaging and classifying millions of subtomograms potentially
containing hundreds of highly heterogeneous structural classes. Although it is
no longer difficult to acquire CECT data containing such amount of subtomograms
due to advances in data acquisition automation, existing computational
approaches have very limited scalability or discrimination ability, making them
incapable of processing such amount of data.
  Results: To complement existing approaches, in this paper we propose a new
approach for subdividing subtomograms into smaller but relatively homogeneous
subsets. The structures in these subsets can then be separately recovered using
existing computation intensive methods. Our approach is based on supervised
structural feature extraction using deep learning, in combination with
unsupervised clustering and reference-free classification. Our experiments show
that, compared to existing unsupervised rotation invariant feature and
pose-normalization based approaches, our new approach achieves significant
improvements in both discrimination ability and scalability. More importantly,
our new approach is able to discover new structural classes and recover
structures that do not exist in training data.

Human movements are physical processes combining the classical mechanics of
the human body moving in space and the biomechanics of the muscles generating
the forces acting on the body under sophisticated sensory-motor control. The
characterization of the performance of human movements is a problem with
important applications in clinical and sports research. One way to characterize
movement performance is through measures of energy efficiency that relate the
mechanical energy of the body and metabolic energy expended by the muscles.
Such a characterization provides information about the performance of a
movement insofar as subjects select movements with the aim of maximizing the
energy efficiency. We examine the case of the energy efficiency of asynchronous
arm-cranking doing external mechanical work, that is, using the arms to turn an
asynchronous arm-crank that performs external mechanical work. We construct a
metabolic energy model and use it estimate how cranking may be performed with
maximum energy efficiency, and recover the intuitive result that for larger
external forces the crank-handles should be placed as far from the center of
the crank as is comfortable for the subject to turn. We further examine
mechanical advantage in asynchronous arm-cranking by constructing an idealized
system that is driven by a crank and which involves an adjustable mechanical
advantage, and analyze the case in which the avg. frequency is fixed and derive
the mechanical advantages that maximize energy efficiency.

Heparan sulfate (HS) is a linear, polydisperse sulfated polysaccharide
belonging to the glycosaminoglycan family. HS proteoglycans are ubiquitously
found at the cell surface and extracellular matrix in animal species. HS is
involved in the interaction with a wide variety of proteins and the regulation
of many biological activities. In certain pathologic conditions, expression and
shedding of HS proteoglycans is overregulated, or enzymatic degradation of HS
in lysosomes is deficient, both leading to excess circulating free HS chains in
blood plasma. HS has therefore been suggested as a biomarker for various severe
disease states. The structural heterogeneity makes the quantification of
heparan sulfate in complex matrices such as human plasma challenging. HS plasma
levels are usually quantified by either disaccharide analysis or enzyme linked
immunosorbent assay(ELISA). Both methods require time-consuming
multistep-protocols. We describe here the instant detection of heparan sulfate
in spiked plasma samples by the Heparin Red Kit, a commercial mix-and-read
fluorescence microplate assay. The method enables HS quantification in the low
microgram per mL range without sample pretreatment. Heparin Red appears to be
sufficiently sensitive for the detection of highly elevated HS levels as
reported for mucopolysaccharidosis, graft versus host disease after
transplantation, dengue infection or septic shock. This study is a significant
step toward the development of a convenient and fast method for the
quantification of HS in human plasma, with the potential to simplify the
detection and advance the acceptance of HS as a biomarker.

Low grade gliomas (LGGs) are infiltrative and incurable primary brain tumours
with typically slow evolution. These tumours usually occur in young and
otherwise healthy patients, bringing controversies in treatment planning since
aggressive treatment may lead to undesirable side effects. Thus, for management
decisions it would be valuable to obtain early estimates of LGG growth
potential. Here we propose a simple mathematical model of LGG growth and its
response to chemotherapy which allows the growth of LGGs to be described in
real patients. The model predicts, and our clinical data confirms, that the
speed of response to chemotherapy is related to tumour aggressiveness.
Moreover, we provide a formula for the time to radiological progression, which
can be possibly used as a measure of tumour aggressiveness. Finally, we suggest
that the response to a few chemotherapy cycles upon diagnosis might be used to
predict tumour growth and to guide therapeutical actions on the basis of the
findings.

In scientific literature, there are many programs that predict linear B-cell
epitopes from a protein sequence. Each program generates multiple B-cell
epitopes that can be individually studied. This paper defines a function called
<C> that combines results from five different prediction programs concerning
the linear B-cell epitopes (ie., BebiPred, EPMLR, BCPred, ABCPred and Emini
Prediction) for selecting the best B-cell epitopes. We obtained 17 potential
linear B cells consensus epitopes from Glycoprotein E from serotype IV of the
dengue virus for exploring new possibilities in vaccine development. The direct
implication of the results obtained is to open the way to experimentally
validate more epitopes to increase the efficiency of the available treatments
against dengue and to explore the methodology in other diseases.

The increasing capacity of high-throughput genomic technologies for
generating time-course data has stimulated a rich debate on the most
appropriate methods to highlight crucial aspects of data structure. In this
work, we address the problem of sparse co-expression network representation of
several time-course stress responses in {\it Saccharomyces cerevisiae}. We
quantify the information preserved from the original datasets under a
graph-theoretical framework and evaluate how cross-stress features can be
identified. This is performed both from a node and a network community
organization point of view. Cluster analysis, here viewed as a problem of
network partitioning, is achieved under state-of-the-art algorithms relying on
the properties of stochastic processes on the constructed graphs. Relative
performance with respect to a metric-free Bayesian clustering analysis is
evaluated and possible extensions are discussed. We further cluster the
stress-induced co-expression networks generated independently by using their
community organization at multiple scales. This type of protocol allows for an
integration of multiple datasets that may not be immediately comparable, either
due to diverse experimental variations or because they represent different
types of information about the same genes.

To understand the nature of a cell, one needs to understand the structure of
its genome. For this purpose, experimental techniques such as Hi-C detecting
chromosomal contacts are used to probe the three-dimensional genomic structure.
These experiments yield topological information, consistently showing a
hierarchical subdivision of the genome into self-interacting domains across
many organisms. Current methods for detecting these domains using the Hi-C
contact matrix, i.e. a doubly-stochastic matrix, are mostly based on the
assumption that the domains are distinct, thus non-overlapping. For overcoming
this simplification and for being able to unravel a possible nested domain
structure, we developed a probabilistic graphical model that makes no a priori
assumptions on the domain structure. Within this approach, the Hi-C contact
matrix is analyzed using an Ising like probabilistic graphical model whose
coupling constant is proportional to each lattice point (entry in the contact
matrix). The results show clear boundaries between identified domains and the
background. These domain boundaries are dependent on the coupling constant, so
that one matrix yields several clusters of different sizes, which show the
self-interaction of the genome on different scales.

1. Animal movement patterns contribute to our understanding of variation in
breeding success and survival of individuals, and the implications for
population dynamics. 2. Over time, sensor technology for measuring movement
patterns has improved. Although older technologies may be rendered obsolete,
the existing data are still valuable, especially if new and old data can be
compared to test whether a behaviour has changed over time. 3. We used
simulated data to assess the ability to quantify and correctly identify
patterns of seabird flight lengths under observational regimes used in
successive generations of tracking technology. 4. Care must be taken when
comparing data collected at differing time-scales, even when using inference
procedures that incorporate the observational process, as model selection and
parameter estimation may be biased. In practice, comparisons may only be valid
when degrading all data to match the lowest resolution in a set. 5. Changes in
tracking technology that lead to aggregation of measurements at different
temporal scales make comparisons challenging. We therefore urge ecologists to
use synthetic data to assess whether accurate parameter estimation is possible
for models comparing disparate data sets before conducting analyses such as
responses to environmental changes or the assessment of management actions.

Camera-traps is a relatively new but already popular instrument in the
estimation of abundance of non-identifiable animals. Although camera-traps are
convenient in application, there remain both theoretical complications such as
spatial autocorrelation or false negative problem and practical difficulties,
for example, laborious random sampling. In the article we propose an
alternative way to bypass the mentioned problems.
  In the proposed approach, the raw video information collected from the
camera-traps situated at the spots of natural attraction is turned into the
frequency of visits, and the latter is transformed into the desired abundance
estimate. The key for such a transformation is the application of the
correction coefficients, computed for each particular observation environment
using the Bayesian approach and the massive database (DB) of observations under
various conditions.
  The main result of the article is a new method of census based on video-data
from camera-traps at the spots of natural attraction and information from a
special community-driven database.
  The proposed method is based on automated video-capturing at a moderate
number of easy to reach spots, so in the long term many laborious census works
may be conducted easier, cheaper and cause less disturbance for the wild life.
Information post-processing is strictly formalized, which leaves little chance
for subjective alterations. However, the method heavily relies on the volume
and quality of the DB, which in its turn heavily relies on the efforts of the
community. There is realistic hope that the community of zoologists and
environment specialists could create and maintain a DB similar to the proposed
one. Such a rich DB of visits might benefit not only censuses, but also many
behavioral studies.

Motivation: Metabolomics data is typically scaled to a common reference like
a constant volume of body fluid, a constant creatinine level, or a constant
area under the spectrum. Such normalization of the data, however, may affect
the selection of biomarkers and the biological interpretation of results in
unforeseen ways.
  Results: First, we study how the outcome of hypothesis tests for differential
metabolite concentration is affected by the choice of scale. Furthermore, we
observe this interdependence also for different classification approaches.
Second, to overcome this problem and establish a scale-invariant biomarker
discovery algorithm, we extend linear zero-sum regression to the logistic
regression framework and show in two applications to ${}^1$H NMR-based
metabolomics data how this approach overcomes the scaling problem.
  Availability: Logistic zero-sum regression is available as an R package as
well as a high-performance computing implementation that can be downloaded at
https://github.com/rehbergT/zeroSum

Most human protein-coding genes can be transcribed into multiple possible
distinct mRNA isoforms. These alternative splicing patterns encourage molecular
diversity and dysregulation of isoform expression plays an important role in
disease etiology. However, isoforms are difficult to characterize from
short-read RNA-seq data because they share identical subsequences and exist in
tissue- and sample-specific frequencies. Here, we develop BIISQ, a Bayesian
nonparametric model to discover Isoforms and Individual Specific Quantification
from RNA-seq data. BIISQ does not require known isoform reference sequences but
instead estimates isoform composition directly with an isoform catalog shared
across samples. We develop a stochastic variational inference approach for
efficient and robust posterior inference and demonstrate superior precision and
recall for short read RNA-seq simulations and simulated short read data from
PacBio long read sequencing when compared to state-of-the-art isoform
reconstruction methods. BIISQ achieves the most significant gains for longer
(in terms of exons) isoforms and isoforms that are lowly expressed (over 500%
more transcripts correctly inferred at low coverage in simulations). Finally,
we estimate isoforms in the GEUVADIS RNA-seq data, identify genetic variants
that regulate transcript ratios, and demonstrate variant enrichment in
functional elements related to mRNA splicing regulation.

Advances in molecular biology are enabling rapid and efficient analyses for
effective intervention in domains such as biology research, infectious disease
management, food safety, and biodefense. The emergence of microfluidics and
nanotechnologies has enabled both new capabilities and instrument sizes
practical for point-of-care. It has also introduced new functionality, enhanced
sensitivity, and reduced the time and cost involved in conventional molecular
diagnostic techniques. This chapter reviews the application of microfluidics
for molecular diagnostics methods such as nucleic acid amplification,
next-generation sequencing, high resolution melting analysis, cytogenetics,
protein detection and analysis, and cell sorting. We also review microfluidic
sample preparation platforms applied to molecular diagnostics and targeted to
sample-in, answer-out capabilities.

The Drosophila melanogaster white-eyed w1118 line serves as a blank control,
allowing genetic recombination of any gene of interest along with a readily
recognizable marker. w1118 flies display behavioral susceptibility to
environmental stimulation such as light. It is of great importance to
characterize the behavioral performance of w1118 flies because this would
provide a baseline from which the effect of the gene of interest could be
differentiated. Little work has been performed to characterize the walking
behavior in adult w1118 flies. Here we show that pulsed light stimulation
increased the regularity of walking trajectories of w1118 flies in circular
arenas. We statistically modeled the distribution of distances to center and
extracted the walking structures of w1118 flies. Pulsed light stimulation
redistributed the time proportions for individual walking structures.
Specifically, pulsed light stimulation reduced the episodes of crossing over
the central region of the arena. An addition of four genomic copies of
mini-white, a common marker gene for eye color, mimicked the effect of pulsed
light stimulation in reducing crossing in a circular arena. The reducing effect
of mini-white was copy-number-dependent. These findings highlight the rhythmic
light stimulation-evoked modifications of walking behavior in w1118 flies and
an unexpected behavioral consequence of mini-white in transgenic flies carrying
w1118 isogenic background.

Motivation: Site directed mutagenesis is widely used to understand the
structure and function of biomolecules. Computational prediction of protein
mutation impacts offers a fast, economical and potentially accurate alternative
to laboratory mutagenesis. Most existing methods rely on geometric
descriptions, this work introduces a topology based approach to provide an
entirely new representation of protein mutation impacts that could not be
obtained from conventional techniques. Results: Topology based mutation
predictor (T-MP) is introduced to dramatically reduce the geometric complexity
and number of degrees of freedom of proteins, while element specific persistent
homology is proposed to retain essential biological information. The present
approach is found to outperform other existing methods in globular protein
mutation impact predictions. A Pearson correlation coefficient of 0.82 with an
RMSE of 0.92 kcal/mol is obtained on a test set of 350 mutation samples. For
the prediction of membrane protein stability changes upon mutation, the
proposed topological approach has a 84% higher Pearson correlation coefficient
than the current state-of-the-art empirical methods, achieving a Pearson
correlation of 0.57 and an RMSE of 1.09 kcal/mol in a 5-fold cross validation
on a set of 223 membrane protein mutation samples.

Epistasis, or the context-dependence of the effects of mutations, limits our
ability to predict the functional impact of combinations of mutations, and
ultimately our ability to predict evolutionary trajectories. Information about
the context-dependence of mutations can essentially be obtained in two ways:
First, by experimental measurement the functional effects of combinations of
mutations and calculating the epistatic contributions directly, and second, by
statistical analysis of the frequencies and co-occurrences of protein residues
in a multiple sequence alignment of protein homologs. In this manuscript, we
derive the mathematical relationship between epistasis calculated on the basis
of functional measurements, and the covariance calculated from a multiple
sequence alignment. There is no one-to-one mapping between covariance and
epistatic terms: covariance implies epistasis, but epistasis does not
necessarily lead to covariance, indicating that covariance in itself is not the
directly relevant quantity for functional prediction. Having calculated
epistatic contributions from the alignment, we can directly obtain a functional
prediction from the alignment statistics by applying a Walsh-Hadamard
transform, fully analogous to the transformation that reconstructs functional
data from measured epistatic contributions. This embedding into the Hadamard
framework is directly relevant for solidifying our theoretical understanding of
statistical methods that predict function and three-dimensional structure from
natural alignments.

Although deep learning approaches have had tremendous success in image, video
and audio processing, computer vision, and speech recognition, their
applications to three-dimensional (3D) biomolecular structural data sets have
been hindered by the entangled geometric complexity and biological complexity.
We introduce topology, i.e., element specific persistent homology (ESPH), to
untangle geometric complexity and biological complexity. ESPH represents 3D
complex geometry by one-dimensional (1D) topological invariants and retains
crucial biological information via a multichannel image representation. It is
able to reveal hidden structure-function relationships in biomolecules. We
further integrate ESPH and convolutional neural networks to construct a
multichannel topological neural network (TopologyNet) for the predictions of
protein-ligand binding affinities and protein stability changes upon mutation.
To overcome the limitations to deep learning arising from small and noisy
training sets, we present a multitask topological convolutional neural network
(MT-TCNN). We demonstrate that the present TopologyNet architectures outperform
other state-of-the-art methods in the predictions of protein-ligand binding
affinities, globular protein mutation impacts, and membrane protein mutation
impacts.

Particle tracking is a powerful biophysical tool that requires conversion of
large video files into position time series, i.e. traces of the species of
interest for data analysis. Current tracking methods, based on a limited set of
input parameters to identify bright objects, are ill-equipped to handle the
spectrum of spatiotemporal heterogeneity and poor signal-to-noise ratios
typically presented by submicron species in complex biological environments.
Extensive user involvement is frequently necessary to optimize and execute
tracking methods, which is not only inefficient but introduces user bias. To
develop a fully automated tracking method, we developed a convolutional neural
network for particle localization from image data, comprised of over 6,000
parameters, and employed machine learning techniques to train the network on a
diverse portfolio of video conditions. The neural network tracker provides
unprecedented automation and accuracy, with exceptionally low false positive
and false negative rates on both 2D and 3D simulated videos and 2D experimental
videos of difficult-to-track species.

Identifying disease genes from human genome is an important and fundamental
problem in biomedical research. Despite many publications of machine learning
methods applied to discover new disease genes, it still remains a challenge
because of the pleiotropy of genes, the limited number of confirmed disease
genes among whole genome and the genetic heterogeneity of diseases. Recent
approaches have applied the concept of 'guilty by association' to investigate
the association between a disease phenotype and its causative genes, which
means that candidate genes with similar characteristics as known disease genes
are more likely to be associated with diseases. However, due to the imbalance
issues (few genes are experimentally confirmed as disease related genes within
human genome) in disease gene identification, semi-supervised approaches, like
label propagation approaches and positive-unlabeled learning, are used to
identify candidate disease genes via making use of unknown genes for training -
typically in the scenario of a small amount of confirmed disease genes (labeled
data) with a large amount of unknown genome (unlabeled data). The performance
of Disease gene prediction models are limited by potential bias of single
learning models and incompleteness and noise of single biological data sources,
therefore ensemble learning models are applied via combining multiple diverse
biological sources and learning models to obtain better predictive performance.
In this thesis, we propose three computational models for identifying candidate
disease genes.

Objective: Due to the non-linearity of numerous biomedical signals,
non-linear analysis of multi-channel time series, notably multivariate
multiscale entropy (mvMSE), has been extensively used in biomedical signal
processing. However, mvMSE has three drawbacks: 1) mvMSE values are either
undefined or unreliable for short signals; 2) mvMSE is not fast enough for
real-time applications; and 3) the computation of mvMSE for signals with a
large number of channels requires the storage of a huge number of elements.
Methods: To deal with these problems and improve the stability of mvMSE, we
introduce multivariate multiscale dispersion entropy (MDE - mvMDE) as an
extension of our recently developed MDE, to quantify the complexity of
multivariate time series. Results: We assess mvMDE, in comparison with mvMSE
and multivariate multiscale fuzzy entropy (mvMFE), on correlated and
uncorrelated multi-channel noise signals, bivariate autoregressive processes,
and three biomedical datasets. The results show that mvMDE takes into account
dependencies in patterns across both the time and spatial domains. The mvMDE,
mvMSE, and mvMFE methods are consistent in that they lead to similar
conclusions about the underlying physiological conditions. However, the
proposed mvMDE discriminates various physiological states of the biomedical
recordings better than mvMSE and mvMFE. In addition, for both the short and
long time series, the mvMDE-based results are noticeably more stable than the
mvMSE- and mvMFE-based ones. Conclusion: For short multivariate time series,
mvMDE, unlike mvMSE, does not result in undefined values. Furthermore, mvMDE is
noticeably faster than mvMFE and mvMSE and also needs to store a considerably
smaller number of elements. Significance: Due to its ability to detect
different kinds of dynamics of multivariate signals, mvMDE has great potential
to analyse various physiological signals.

The aim of this paper was to develop statistical models to estimate
individual breed composition based on the previously proposed idea of
regressing discrete random variables corresponding to counts of reference
alleles of biallelic molecular markers located across the genome on the allele
frequencies of each marker in the pure (base) breeds. Some of the existing
regression-based methods do not guarantee that estimators of breed composition
will lie in the appropriate parameter space and none of them account for
uncertainty about allele frequencies in the pure breeds, that is, uncertainty
about the design matrix. In order to overcome these limitations, we proposed
two Bayesian generalized linear models. For each individual, both models assume
that the counts of the reference allele at each marker locus follow independent
Binomial distributions, use the logit link, and pose a Dirichlet prior over the
vector of regression coefficients (which corresponds to breed composition).
This prior guarantees that point estimators of breed composition like the
posterior mean pertain to the appropriate space. The difference between these
models is that model termed BIBI does not account for uncertainty about the
design matrix, while model termed BIBI2 accounts for such an uncertainty by
assigning independent Beta priors to the entries of this matrix. We implemented
these models in a multibreed Angus-Brahman population. Posterior means were
used as point estimators of breed composition. In addition, the ordinary least
squares estimator proposed by Kuehn et al. (2011) (OLSK) was also computed.
BIBI and BIBI2 estimated breed composition more accurately than OLSK, and BIBI2
had an 8.3% improvement in accuracy as compared to BIBI.

Vector tomography methods intend to reconstruct and visualize vector fields
in restricted domains by measuring line integrals of projections of these
vector fields. Here, we deal with the reconstruction of irrotational vector
functions from boundary measurements. As the majority of inverse problems,
vector field recovery is an ill posed in the continuous domain and therefore
further assumptions, measurements and constraints should be imposed for the
full vector field estimation. The reconstruction idea in the discrete domain
relies on solving a numerical system of linear equations which derives from the
approximation of the line integrals along lines which trace the bounded domain.
This work presents an extensive description of a vector field recovery, the
fundamental assumptions and the ill conditioning of this inverse problem. More
importantly we show that this inverse problem is regularized via the domain
discretization, i.e. we show that the recovery of an irrotational vector field
within a discrete grid employing a finite set of longitudinal line integrals,
leads to a consistent linear system which has bounded solution errors. We
elaborate on the estimation of the solution's error and we prove that this
relative error is finite and therefore a stable vector field reconstruction is
ensured. Such theoretical aspects are critical for future implementations of
vector tomography in practical applications like the inverse bioelectric field
problem. We validate our theoretical results by performing simulations that
reconstruct smooth irrotational fields based solely on a finite number of
boundary measurements and without the need of any additional or prior
information (e.g. transversal line integrals or source free assumption).

The household secondary attack risk (SAR), often called the secondary attack
rate or secondary infection risk, is the probability of infectious contact from
an infectious household member A to a given household member B, where we define
infectious contact to be a contact sufficient to infect B if he or she is
susceptible. Estimation of the SAR is an important part of understanding and
controlling the transmission of infectious diseases. In practice, it is most
often estimated using binomial models such as logistic regression, which
implicitly attribute all secondary infections in a household to the primary
case. In the simplest case, the number of secondary infections in a household
with m susceptibles and a single primary case is modeled as a binomial(m, p)
random variable where p is the SAR. Although it has long been understood that
transmission within households is not binomial, it is thought that multiple
generations of transmission can be safely neglected when p is small. We use
probability generating functions and simulations to show that this is a
mistake. The proportion of susceptible household members infected can be
substantially larger than the SAR even when p is small. As a result, binomial
estimates of the SAR are biased upward and their confidence intervals have poor
coverage probabilities even if adjusted for clustering. Accurate point and
interval estimates of the SAR can be obtained using longitudinal chain binomial
models or pairwise survival analysis, which account for multiple generations of
transmission within households, the ongoing risk of infection from outside the
household, and incomplete follow-up. We illustrate the practical implications
of these results in an analysis of household surveillance data collected by the
Los Angeles County Department of Public Health during the 2009 influenza A
(H1N1) pandemic.

We investigate usage of dynamic time warping (DTW) algorithm for aligning raw
signal data from MinION sequencer. DTW is mostly using for fast alignment for
selective sequencing to quickly determine whether a read comes from sequence of
interest.
  We show that standard usage of DTW has low discriminative power mainly due to
problem with accurate estimation of scaling parameters. We propose a simple
variation of DTW algorithm, which does not suffer from scaling problems and has
much higher discriminative power.

Developing an accurate and reliable injury predictor is central to the
biomechanical studies of traumatic brain injury. State-of-the-art efforts
continue to rely on empirical, scalar metrics based on kinematics or
model-estimated tissue responses explicitly pre-defined in a specific brain
region of interest. They could suffer from loss of information. A single
training dataset has also been used to evaluate performance but without
cross-validation. In this study, we developed a deep learning approach for
concussion classification using implicit features of the entire voxel-wise
white matter fiber strains. Using reconstructed American National Football
League (NFL) injury cases, leave-one-out cross-validation was employed to
objectively compare injury prediction performances against two baseline machine
learning classifiers (support vector machine (SVM) and random forest (RF)) and
four scalar metrics via univariate logistic regression (Brain Injury Criterion
(BrIC), cumulative strain damage measure of the whole brain (CSDM-WB) and the
corpus callosum (CSDM-CC), and peak fiber strain in the CC). Feature-based deep
learning and machine learning classifiers consistently outperformed all scalar
injury metrics across all performance categories in cross-validation (e.g.,
average accuracy of 0.844 vs. 0.746, and average area under the receiver
operating curve (AUC) of 0.873 vs. 0.769, respectively, based on the testing
dataset). Nevertheless, deep learning achieved the best cross-validation
accuracy, sensitivity, and AUC (e.g., accuracy of 0.862 vs. 0.828 and 0.842 for
SVM and RF, respectively). These findings demonstrate the superior performances
of deep learning in concussion prediction, and suggest its promise for future
applications in biomechanical investigations of traumatic brain injury.

In recent years, deep learning algorithms have outperformed the state-of-the
art methods in several areas thanks to the efficient methods for training and
for preventing overfitting, advancement in computer hardware, the availability
of vast amount data. The high performance of multi-task deep neural networks in
drug discovery has attracted the attention to deep learning algorithms in
bioinformatics area. Here, we proposed a hierarchical multi-task deep neural
network architecture based on Gene Ontology (GO) terms as a solution to protein
function prediction problem and investigated various aspects of the proposed
architecture by performing several experiments. First, we showed that there is
a positive correlation between performance of the system and the size of
training datasets. Second, we investigated whether the level of GO terms on GO
hierarchy related to their performance. We showed that there is no relation
between the depth of GO terms on GO hierarchy and their performance. In
addition, we included all annotations to the training of a set of GO terms to
investigate whether including noisy data to the training datasets change the
performance of the system. The results showed that including less reliable
annotations in training of deep neural networks increased the performance of
the low performed GO terms, significantly. We evaluated the performance of the
system using hierarchical evaluation method. Mathews correlation coefficient
was calculated as 0.75, 0.49 and 0.63 for molecular function, biological
process and cellular component categories, respectively. We showed that deep
learning algorithms have a great potential in protein function prediction area.
We plan to further improve the DEEPred by including other types of annotations
from various biological data sources. We plan to construct DEEPred as an open
access online tool.

Dynamic cerebral autoregulation, that is the transient response of cerebral
blood flow to changes in arterial blood pressure, is currently assessed using a
variety of different time series methods and data collection protocols. In the
continuing absence of a gold standard for the study of cerebral autoregulation
it is unclear to what extent does the assessment depend on the choice of a
computational method and protocol. We use continuous measurements of blood
pressure and cerebral blood flow velocity in the middle cerebral artery from
the cohorts of 18 normotensive subjects performing sit-to-stand manoeuvre. We
estimate cerebral autoregulation using a wide variety of black-box approaches
(ARI, Mx, Sx, Dx, FIR and ARX) and compare them in the context of
reproducibility and variability. For all autoregulation indices, considered
here, the ICC was greater during the standing protocol, however, it was
significantly greater (Fisher's Z-test) for Mx (p < 0.03), Sx (p<0.003)$ and Dx
(p<0.03). In the specific case of the sit-to-stand manoeuvre, measurements
taken immediately after standing up greatly improve the reproducibility of the
autoregulation coefficients. This is generally coupled with an increase of the
within-group spread of the estimates.

Accurate predictions of peptide retention times (RT) in liquid chromatography
have many applications in mass spectrometry-based proteomics. Herein, we
present DeepRT, a deep learning based software for peptide retention time
prediction. DeepRT automatically learns features directly from the peptide
sequences using the deep convolutional Neural Network (CNN) and Recurrent
Neural Network (RNN) model, which eliminates the need to use hand-crafted
features or rules. After the feature learning, principal component analysis
(PCA) was used for dimensionality reduction, then three conventional machine
learning methods were utilized to perform modeling. Two published datasets were
used to evaluate the performance of DeepRT and we demonstrate that DeepRT
greatly outperforms previous state-of-the-art approaches ELUDE and GPTime.

Prediction of poly(lactic co glycolic acid) (PLGA) micro- and nanoparticles'
dissolution rates plays a significant role in pharmaceutical and medical
industries. The prediction of PLGA dissolution rate is crucial for drug
manufacturing. Therefore, a model that predicts the PLGA dissolution rate could
be beneficial. PLGA dissolution is influenced by numerous factors (features),
and counting the known features leads to a dataset with 300 features. This
large number of features and high redundancy within the dataset makes the
prediction task very difficult and inaccurate. In this study, dimensionality
reduction techniques were applied in order to simplify the task and eliminate
irrelevant and redundant features. A heterogeneous pool of several regression
algorithms were independently tested and evaluated. In addition, several
ensemble methods were tested in order to improve the accuracy of prediction.
The empirical results revealed that the proposed evolutionary weighted ensemble
method offered the lowest margin of error and significantly outperformed the
individual algorithms and the other ensemble techniques.

Segmentation, the process of delineating tumor apart from healthy tissue, is
a vital part of both the clinical assessment and the quantitative analysis of
brain cancers. Here, we provide an open-source algorithm (MITKats), built on
the Medical Imaging Interaction Toolkit, to provide user-friendly and expedient
tools for semi-automatic segmentation. To evaluate its performance against
competing algorithms, we applied MITKats to 38 high-grade glioma cases from
publicly available benchmarks. The similarity of the segmentations to
expert-delineated ground truths approached the discrepancies among different
manual raters, the theoretically maximal precision. The average time spent on
each segmentation was 5 minutes, making MITKats between 4 and 11 times faster
than competing semi-automatic algorithms, while retaining similar accuracy.

Borneo contains some of the world's most biodiverse and carbon dense tropical
forest, but this 750,000-km2 island has lost 62% of its old-growth forests
within the last 40 years. Efforts to protect and restore the remaining forests
of Borneo hinge on recognising the ecosystem services they provide, including
their ability to store and sequester carbon. Airborne Laser Scanning (ALS) is a
remote sensing technology that allows forest structural properties to be
captured in great detail across vast geographic areas. In recent years ALS has
been integrated into state-wide assessment of forest carbon in Neotropical and
African regions, but not yet in Asia. For this to happen new regional models
need to be developed for estimating carbon stocks from ALS in tropical Asia, as
the forests of this region are structurally and compositionally distinct from
those found elsewhere in the tropics. By combining ALS imagery with data from
173 permanent forest plots spanning the lowland rain forests of Sabah, on the
island of Borneo, we develop a simple-yet-general model for estimating forest
carbon stocks using ALS-derived canopy height and canopy cover as input
metrics. An advanced feature of this new model is the propagation of
uncertainty in both ALS- and ground-based data, allowing uncertainty in
hectare-scale estimates of carbon stocks to be quantified robustly. We show
that the model effectively captures variation in aboveground carbons stocks
across extreme disturbance gradients spanning tall dipterocarp forests and
heavily logged regions, and clearly outperforms existing ALS-based models
calibrated for the tropics, as well as currently available satellite-derived
products. Our model provides a simple, generalised and effective approach for
mapping forest carbon stocks in Borneo, providing a key tool to support the
protection and restoration of its tropical forests.

The stochastic simulation algorithm commonly known as Gillespie's algorithm
is now used ubiquitously in the modelling of biological processes in which
stochastic effects play an important role. In well-mixed scenarios at the
sub-cellular level it is often reasonable to assume that times between
successive reaction/interaction events are exponentially distributed and can be
appropriately modelled as a Markov process and hence simulated by the Gillespie
algorithm. However, Gillespie's algorithm is routinely applied to model
biological systems for which it was never intended. In particular, processes in
which cell proliferation is important should not be simulated naively using the
Gillespie algorithm since the history-dependent nature of the cell cycle breaks
the Markov process. The variance in experimentally measured cell cycle times is
far less than in an exponential cell cycle time distribution with the same
mean.
  Here we suggest a method of modelling the cell cycle that restores the
memoryless property to the system and is therefore consistent with simulation
via the Gillespie algorithm. By breaking the cell cycle into a number of
independent exponentially distributed stages we can restore the Markov property
at the same time as more accurately approximating the appropriate cell cycle
time distributions. The consequences of our revised mathematical model are
explored analytically. We demonstrate the importance of employing the correct
cell cycle time distribution by considering two models incorporating cellular
proliferation (one spatial and one non-spatial) and demonstrating that changing
the cell cycle time distribution makes quantitative and qualitative differences
to their outcomes. Our adaptation will allow modellers and experimentalists
alike to appropriately represent cellular proliferation, whilst still being
able to take advantage of the Gillespie algorithm.

The Gaussian scale mixture model (GSM) is a simple yet powerful probabilistic
generative model of natural image patches. In line with the well-established
idea that sensory processing is adapted to the statistics of the natural
environment, the GSM has also been considered a model of the early visual
system, as a reasonable "first-order" approximation of the internal model that
the primary visual cortex (V1) implements. According to this view, neural
activities in V1 represent the posterior distribution under the GSM given a
particular visual stimulus. Indeed, (approximate) inference under the GSM has
successfully accounted for various nonlinearities in the mean (trial-average)
responses of V1 neurons, as well as the dependence of (across-trial) response
variability with stimulus contrast found in V1 recordings. However, previous
work almost exclusively relied on numerical simulations to obtain these
results. Thus, for a deeper insight into the realm of possible behaviours the
GSM can (and cannot) exhibit and predict, here we present analytical
derivations for the limiting behaviour of the mean and (co)variance of the GSM
posterior at very low and very high contrast levels. These results should guide
future work exploring neural circuit dynamics appropriate for implementing
inference under the GSM.

Spatio-temporal systems exhibiting multi-scale behaviour are common in
applications ranging from cyber-physical systems to systems biology, yet they
present formidable challenges for computational modelling and analysis. Here we
consider a prototypic scenario where spatially distributed agents decide their
movement based on external inputs and a fast-equilibrating internal
computation. We propose a generally applicable strategy based on statistically
abstracting the internal system using Gaussian Processes, a powerful class of
non-parametric regression techniques from Bayesian Machine Learning. We show on
a running example of bacterial chemotaxis that this approach leads to accurate
and much faster simulations in a variety of scenarios.

The goals of the Triple Aim of health care and the goals of P4 medicine
outline objectives that require a significant health informatics component.
However, the goals do not provide specifications about how all of the new
individual patient data will be combined in meaningful ways and with data from
other sources, like epidemiological data, to promote the health of individuals
and society. We seem to have more data than ever before but few resources and
means to use it efficiently. We need a general, extensible solution that
integrates and homogenizes data of disparate origin, incompatible formats, and
multiple spatial and temporal scales. To address this problem, we introduce the
Scientific Knowledge Extraction from Data (SKED) architecture, as a
technology-agnostic framework to minimize the overhead of data integration,
permit reuse of analytical pipelines, and guarantee reproducible quantitative
results. The SKED architecture consists of a Resource Allocation Service to
locate resources, and the definition of data primitives to simplify and
harmonize data. SKED allows automated knowledge discovery and provides a
platform for the realization of the major goals of modern health care.

Background: The past few years have seen a tremendous increase in the size
and complexity of datasets. Scientific and clinical studies must to incorporate
datasets that cross multiple spatial and temporal scales to describe a
particular phenomenon. The storage and accessibility of these heterogeneous
datasets in a way that is useful to researchers and yet extensible to new data
types is a major challenge.
  Methods: In order to overcome these obstacles, we propose the use of data
primitives as a common currency between analytical methods. The four data
primitives we have identified are time series, text, annotated graph and
triangulated mesh, with associated metadata. Using only data primitives to
store data and as algorithm input, output, and intermediate results, promotes
interoperability, scalability, and reproducibility in scientific studies.
  Results: Data primitives were used in a multi-omic, multi-scale systems
biology study of malaria infection in non-human primates to perform many types
of integrative analysis quickly and efficiently.
  Conclusions: Using data primitives as a common currency for both data storage
and for cross talk between analytical methods enables the analysis of complex
multi-omic, multi-scale datasets in a reproducible modular fashion.

A critical component of preventing the spread of vector borne diseases such
as Chagas disease are door-to-door campaigns by public health officials that
implement insecticide application in order to eradicate the vector infestation
of households. The success of such campaigns depends on adequate household
participation during the active phase as well as on sufficient follow-up during
the surveillance phase when newly infested houses or infested houses that had
not participated in the active phase will receive treatment. Queueing models
which are widely used in operations management give us a mathematical
representation of the operational efforts needed to contain the spread of
infestation. By modeling the queue as consisting of all infested houses in a
given locality, we capture the dynamics of the insect population due to
prevalence of infestation and to the additional growth of infestation by
redispersion, i.e. by the spread of infestation to previously uninfested houses
during the wait time for treatment. In contrast to traditional queueing models,
houses waiting for treatment are not known but must be identified through a
search process by public health workers. Thus, both the arrival rate of houses
to the queue as well as the removal rate from the queue depend on the current
level of infestation. We incorporate these dependencies through a load
dependent queueing model which allows us to estimate the long run average rate
of removing houses from the queue and therefore the cost associated with a
given surveillance program. The model is motivated by and applied to an ongoing
Chagas disease control campaign in Arequipa, Peru.

Many single-cell observables are highly heterogeneous. A part of this
heterogeneity stems from age-related phenomena: the fact that there is a
nonuniform distribution of cells with different ages. This has led to a renewed
interest in analytic methodologies including use of the "von Foerster equation"
for predicting population growth and cell age distributions. Here we discuss
how some of the most popular implementations of this machinery assume a strong
condition on the ergodicity of the cell cycle duration ensemble. We show that
one common definition for the term ergodicity, "a single individual observed
over many generations recapitulates the behavior of the entire ensemble" is
implied by the other, "the probability of observing any state is conserved
across time and over all individuals" in an ensemble with a fixed number of
individuals but that this is not true when the ensemble is growing. We further
explore the impact of generational correlations between cell cycle durations on
the population growth rate. Finally, we explore the "growth rate gain" - the
phenomenon that variations in the cell cycle duration lead to an improved
population-level growth rate - in this context. We highlight that,
fundamentally, this effect is due to asymmetric division.

In many situations, the gene expression signature is a unique marker of the
biological state. We study the modification of the gene expression distribution
function when the biological state of a system experiences a change. This
change may be the result of a selective pressure, as in the Long Term Evolution
Experiment with E. Coli populations, or the progression to Alzheimer disease in
aged brains, or the progression from a normal tissue to the cancer state. The
first two cases seem to belong to a class of transitions, where the initial and
final states are relatively close to each other, and the distribution function
for the differential expressions is short ranged, with a tail of only a few
dozens of strongly varying genes. In the latter case, cancer, the initial and
final states are far apart and separated by a low-fitness barrier. The
distribution function shows a very heavy tail, with thousands of silenced and
over-expressed genes. We characterize the biological states by means of their
principal component representations, and the expression distribution functions
by their maximal and minimal differential expression values and the exponents
of the Pareto laws describing the tails.

Sunflower (Helianthus annuus L.) grain and oil quality are defined by grain
weight and oil percentage, oil fatty acid composition and the amount of
antioxidants. The aim of this work was to establish and validate a simple
model, based on published relationships, which can estimate not only yield and
its components, but also grain and oil quality aspects which are of relevance
for industrial processes or human health. The model we developed provided good
estimations of grain yield (similar to those of a more complex model) and oil
quality from independent experiments. It explained known differences in
potential yield and grain and oil quality between locations, in terms of
differences in incident radiation, mean or minimum temperature. Simulations
showed that recent climatic changes could have caused a decrease in sunflower
yield and changes in oil quality. Our results suggest that at locations at
lower latitudes, sunflower oil with high nutritious value and oxidative
stability could compensate for relatively low yields, while at higher
latitudes, high-linoleic acid oil production should be compatible with high
yield potentials. Our model could facilitate the selection of the best
location, sowing date or density for the production of sunflower oil with
specific quality characteristics.

In this paper, I introduce a Sequence-based Multiscale Model (SeqMM) for the
biomolecular data analysis. With the combination of spectral graph method, I
reveal the essential difference between the global scale models and local scale
ones in structure clustering, i.e., different optimization on Euclidean (or
spatial) distances and sequential (or genomic) distances. More specifically,
clusters from global scale models optimize Euclidean distance relations. Local
scale models, on the other hand, result in clusters that optimize the genomic
distance relations. For a biomolecular data, Euclidean distances and sequential
distances are two independent variables, which can never be optimized
simultaneously in data clustering. However, sequence scale in my SeqMM can work
as a tuning parameter that balances these two variables and deliver different
clusterings based on my purposes. Further, my SeqMM is used to explore the
hierarchical structures of chromosomes. I find that in global scale, the
Fiedler vector from my SeqMM bears a great similarity with the principal vector
from principal component analysis, and can be used to study genomic
compartments. In TAD analysis, I find that TADs evaluated from different scales
are not consistent and vary a lot. Particularly when the sequence scale is
small, the calculated TAD boundaries are dramatically different. Even for
regions with high contact frequencies, TAD regions show no obvious consistence.
However, when the scale value increases further, although TADs are still quite
different, TAD boundaries in these high contact frequency regions become more
and more consistent. Finally, I find that for a fixed local scale, my method
can deliver very robust TAD boundaries in different cluster numbers.

Cellular processes are governed by macromolecular complexes inside the cell.
Study of the native structures of macromolecular complexes has been extremely
difficult due to lack of data. With recent breakthroughs in Cellular electron
cryo tomography (CECT) 3D imaging technology, it is now possible for
researchers to gain accesses to fully study and understand the macromolecular
structures single cells. However, systematic recovery of macromolecular
structures from CECT is very difficult due to high degree of structural
complexity and practical imaging limitations. Specifically, we proposed a deep
learning based image classification approach for large-scale systematic
macromolecular structure separation from CECT data. However, our previous work
was only a very initial step towards exploration of the full potential of deep
learning based macromolecule separation. In this paper, we focus on improving
classification performance by proposing three newly designed individual CNN
models: an extended version of (Deep Small Receptive Field) DSRF3D, donated as
DSRF3D-v2, a 3D residual block based neural network, named as RB3D and a
convolutional 3D(C3D) based model, CB3D. We compare them with our previously
developed model (DSRF3D) on 12 datasets with different SNRs and tilt angle
ranges. The experiments show that our new models achieved significantly higher
classification accuracies. The accuracies are not only higher than 0.9 on
normal datasets, but also demonstrate potentials to operate on datasets with
high levels of noises and missing wedge effects presented.

Bioinformatics research depends on high-quality databases to provide accurate
results. In silico experiments, correctly performed, may prospect novel
discoveries and elucidates pathways for biological experiments through data
analysis in large scale. However, most biological databases have presented
mistakes, such as data incorrectly classified or incomplete information. Also,
sometimes, data mining algorithms cannot treat these errors, leading to serious
problems for the in silico analysis. Manual curation of data extracted from
literature is a possible solution for this problem. Systematic Literature
Review (SLR), or Systematic Review, is a method to identify, evaluate and
summarize the state-of-the-art of a specific theme. Moreover, SLR allows the
collection from databases restrictively, which allows an analysis with lower
bias than traditional reviews. The SRL approaches have been widely used for
decision-making in medical and environmental studies. However, other research
areas, such as bioinformatics, do not have a specific step-by-step to guide
researchers undertaking the procedures of an SLR. In this study, we propose a
guideline, called BiSRL, to perform SLR in bioinformatics. Our procedures cover
the most traditional guides to produce SLRs adapted to bioinformatics. To
evaluate our method, we propose a case study to detect and summarize SLRs
developed for bioinformatics data. We used two databases: PubMed and
ScienceDirect. A total of 207 papers were screened in four steps: title,
abstract, diagonal and full-text reading. Four evaluators performed the SLR
independently to reduce bias risk. A total of 8 papers was included in the SLR
case study. The case study demonstrates how to implement the methods of BiSLR
to procedure SLR for bioinformatics. BiSLR may guide bioinformaticians to
perform systematic reviews reproducible to collect accurate data for higher
quality analysis.

As very large studies of complex neuroimaging phenotypes become more common,
human quality assessment of MRI-derived data remains one of the last major
bottlenecks. Few attempts have so far been made to address this issue with
machine learning. In this work, we optimize predictive models of quality for
meshes representing deep brain structure shapes. We use standard vertex-wise
and global shape features computed homologously across 19 cohorts and over 7500
human-rated subjects, training kernelized Support Vector Machine and Gradient
Boosted Decision Trees classifiers to detect meshes of failing quality. Our
models generalize across datasets and diseases, reducing human workload by
30-70\%, or equivalently hundreds of human rater hours for datasets of
comparable size, with recall rates approaching inter-rater reliability.

Kinetic rate constants fundamentally characterize the dynamics of the
chemical interaction of macromolecules, and thus their study sets a major
direction in experimental biochemistry. The estimation of such constants is
often challenging, partly due to the noisiness of data, and partly due to the
theoretical framework. Novel and qualitatively reasonable methods are presented
for the estimation of the rate constants of complex formation and dissociation
in Kinetic Capillary Electrophoresis (KCE). This also serves the broader effort
to resolve the inverse problem of KCE, where these estimates pose as initial
starting points in the non-linear optimization space, along with the asymmetric
Gaussian parameters describing the injected plug concentration profiles, which
is also hereby estimated. This rate constant estimation method is also compared
to an earlier one.

Determining kinetic rate constants is a highly relevant problem in
biochemistry, so various methods have been designed to extract them from
experimental data. Such methods have two main components: the experimental
apparatus and the subsequent analysis, the latter often dependent on
mathematical theory. Thus the theoretical approach taken influences the
effectiveness of constant determination. A computational inverse problem
approach is hereby presented, which does not merely give a single rough
approximation of the sought constants, but is inherently capable of determining
them from exact signals to arbitrary accuracy. This approach is thus not merely
novel, but opens a whole new category of solution approaches in the field,
enabled primarily by an efficient direct solver.

The discrete chemical master equation (dCME) provides a fundamental framework
for studying stochasticity in mesoscopic networks. Because of the multi-scale
nature of many networks where reaction rates have large disparity, directly
solving dCMEs is intractable due to the exploding size of the state space. It
is important to truncate the state space effectively with quantified errors, so
accurate solutions can be computed. It is also important to know if all major
probabilistic peaks have been computed. Here we introduce the Accurate CME
(ACME) algorithm for obtaining direct solutions to dCMEs. With multi-finite
buffers for reducing the state space by O(n!), exact steady-state and
time-evolving network probability landscapes can be computed. We further
describe a theoretical framework of aggregating microstates into a smaller
number of macrostates by decomposing a network into independent aggregated
birth and death processes, and give an a priori method for rapidly determining
steady-state truncation errors. The maximal sizes of the finite buffers for a
given error tolerance can also be pre-computed without costly trial solutions
of dCMEs. We show exactly computed probability landscapes of three multi-scale
networks, namely, a 6-node toggle switch, 11-node phage-lambda epigenetic
circuit, and 16-node MAPK cascade network, the latter two with no known
solutions. We also show how probabilities of rare events can be computed from
first-passage times, another class of unsolved problems challenging for
simulation-based techniques due to large separations in time scales. Overall,
the ACME method enables accurate and efficient solutions of the dCME for a
large class of networks.

The discrete chemical master equation (dCME) provides a general framework for
studying stochasticity in mesoscopic reaction networks. Since its direct
solution rapidly becomes intractable due to the increasing size of the state
space, truncation of the state space is necessary for solving most dCMEs. It is
therefore important to assess the consequences of state space truncations so
errors can be quantified and minimized. Here we describe a novel method for
state space truncation. By partitioning a reaction network into multiple
molecular equivalence groups (MEG), we truncate the state space by limiting the
total molecular copy numbers in each MEG. We further describe a theoretical
framework for analysis of the truncation error in the steady state probability
landscape using reflecting boundaries. By aggregating the state space based on
the usage of a MEG and constructing an aggregated Markov process, we show that
the truncation error of a MEG can be asymptotically bounded by the probability
of states on the reflecting boundary of the MEG. Furthermore, truncating states
of an arbitrary MEG will not undermine the estimated error of truncating any
other MEGs. We then provide an error estimate for networks with multiple MEGs.
To rapidly determine the appropriate size of an arbitrary MEG, we introduce an
a priori method to estimate the upper bound of its truncation error, which can
be rapidly computed from reaction rates, without costly trial solutions of the
dCME. We show results of applying our methods to four stochastic networks. We
demonstrate how truncation errors and steady state probability landscapes can
be computed using different sizes of the MEG(s) and how the results validate
out theories. Overall, the novel state space truncation and error analysis
methods developed here can be used to ensure accurate direct solutions to the
dCME for a large class of stochastic networks.

The metabolism of an organism is regulated at the cellular level, yet is
strongly influenced by its environment. The precise metabolomic study of living
organisms is currently hampered by measurement sensitivity: most metabolomic
measurement techniques involve some compromise, in that averaging is performed
over a volume significantly larger than a single cell, or require invasion of
the organism, or arrest the state of the organism. NMR is an inherently
non-invasive chemometric and imaging method, and hence in principle suitable
for metabolomic measurements. The digital twin of metabolomics is computational
systems biology, so that NMR microscopy is potentially a viable approach with
which to join the theoretical and experimental exploration of the metabolomic
and behavioural response of organisms. This prospect paper considers the
challenge of performing in vivo NMR-based metabolomics on the small organism C.
elegans, points the way towards possible solutions created using MEMS
techniques, and highlights currently insurmountable challenges.

In this work, we consider the problem of estimating summary statistics to
characterise biochemical reaction networks of interest. Such networks are often
described using the framework of the Chemical Master Equation (CME). For
physically-realistic models, the CME is widely considered to be analytically
intractable. A variety of Monte Carlo algorithms have therefore been developed
to explore the dynamics of such networks empirically. Amongst them is the
multi-level method, which uses estimates from multiple ensembles of sample
paths of different accuracies to estimate a summary statistic of interest. {In
this work, we develop the multi-level method in two directions: (1) to increase
the robustness, reliability and performance of the multi-level method, we
implement an improved variance reduction method for generating the sample paths
of each ensemble; and (2) to improve computational performance, we demonstrate
the successful use of a different mechanism for choosing which ensembles should
be included in the multi-level algorithm.

Analyzing the relation between a set of biological sequences can help to
identify and understand the evolutionary history of these sequences and the
functional relations among them. Multiple Sequence Alignment (MSA) is the main
obstacle to proper design and develop homology and evolutionary modeling
applications since these kinds of applications require an effective MSA
technique with high accuracy. This work proposes a novel Position-based
Multiple Sequence Alignment (PoMSA) technique -- which depends on generating a
position matrix for a given set of biological sequences. This position matrix
can be used to reconstruct the given set of sequences in more aligned format.
On the contrary of existing techniques, PoMSA uses position matrix instead of
distance matrix to correctly adding gaps in sequences which improve the
efficiency of the alignment operation. We have evaluated the proposed technique
with different datasets benchmarks such as BAliBASE, OXBench, and SMART. The
experiments show that PoMSA technique satisfies higher alignment score compared
to existing state-of-art algorithms: Clustal-Omega, MAFTT, and MUSCLE.

Imaging data has become widely available to study biological systems at
various scales, for example the motile behaviour of bacteria or the transport
of mRNA, and it has the potential to transform our understanding of key
transport mechanisms. Often these imaging studies require us to compare
biological species or mutants, and to do this we need to quantitatively
characterise their behaviour. Mathematical models offer a quantitative
description of a system that enables us to perform this comparison, but to
relate these mechanistic mathematical models to imaging data, we need to
estimate the parameters of the models. In this work, we study the impact of
collecting data at different temporal resolutions on parameter inference for
biological transport models by performing exact inference for simple velocity
jump process models in a Bayesian framework. This issue is prominent in a host
of studies because the majority of imaging technologies place constraints on
the frequency with which images can be collected, and the discrete nature of
observations can introduce errors into parameter estimates. In this work, we
avoid such errors by formulating the velocity jump process model within a
hidden states framework. This allows us to obtain estimates of the
reorientation rate and noise amplitude for noisy observations of a simple
velocity jump process. We demonstrate the sensitivity of these estimates to
temporal variations in the sampling resolution and extent of measurement noise.
We use our methodology to provide experimental guidelines for researchers
aiming to characterise motile behaviour that can be described by a velocity
jump process. In particular, we consider how experimental constraints resulting
in a trade-off between temporal sampling resolution and observation noise may
affect parameter estimates.

Metabolomics is a key approach in modern functional genomics and systems
biology. Due to the complexity of metabolomics data, the variety of
experimental designs, and the variety of existing bioinformatics tools,
providing experimenters with a simple and efficient resource to conduct
comprehensive and rigorous analysis of their data is of utmost importance. In
2014, we launched the Workflow4Metabolomics (W4M,
http://workflow4metabolomics.org) online infrastructure for metabolomics built
on the Galaxy environment, which offers user-friendly features to build and run
data analysis workflows including preprocessing, statistical analysis, and
annotation steps. Here we present the new W4M 3.0 release, which contains twice
as many tools as the first version, and provides two features which are, to our
knowledge, unique among online resources. First, data from the four major
metabolomics technologies (i.e., LC-MS, FIA-MS, GC-MS, and NMR) can be analyzed
on a single platform. By using three studies in human physiology, alga
evolution, and animal toxicology, we demonstrate how the 40 available tools can
be easily combined to address biological issues. Second, the full analysis
(including the workflow, the parameter values, the input data and output
results) can be referenced with a permanent digital object identifier (DOI).
Publication of data analyses is of major importance for robust and reproducible
science. Furthermore, the publicly shared workflows are of high-value for
e-learning and training. The Workflow4Metabolomics 3.0 e-infrastructure thus
not only offers a unique online environment for analysis of data from the main
metabolomics technologies, but it is also the first reference repository for
metabolomics workflows.

Motivation: Flow Injection Analysis coupled to High-Resolution Mass
Spectrometry (FIA-HRMS) is a promising approach for high-throughput
metabolomics. FIA-HRMS data, however, cannot be preprocessed with current
software tools which rely on liquid chromatography separation, or handle low
resolution data only. Results: We thus developed the proFIA package, which
implements a suite of innovative algorithms to preprocess FIA-HRMS raw files,
and generates the table of peak intensities. The workflow consists of 3 steps:
i) noise estimation, peak detection and quantification, ii) peak grouping
across samples, and iii) missing value imputation. In addition, we have
implemented a new indicator to quantify the potential alteration of the feature
peak shape due to matrix effect. The preprocessing is fast (less than 15 s per
file), and the value of the main parameters (ppm and dmz) can be easily
inferred from the mass resolution of the instrument. Application to two
metabolomics datasets (including spiked serum samples) showed high precision
(96%) and recall (98%) compared with manual integration. These results
demonstrate that proFIA achieves very efficient and robust detection and
quantification of FIA-HRMS data, and opens new opportunities for
high-throughput phenotyping. Availability: The proFIA software (as well as the
plasFIA data set) is available as an R package on the Bioconductor repository
(http://bioconductor.org/packages/proFIA), and as a Galaxy module on the Main
Toolshed (https://toolshed.g2.bx.psu.edu/) and on the Workflow4Metabolomics
online infrastructure (http://workflow4metabolomics.org). Contacts:
alexis.delabriere@cea.fr and etienne.thevenot@cea.fr.

Reaction-diffusion systems are used to represent many biological and physical
phenomena. They model the random motion of particles (diffusion) and
interactions between them (reactions). Such systems can be modelled at multiple
scales with varying degrees of accuracy and computational efficiency. When
representing genuinely multiscale phenomena, fine-scale models can be
prohibitively expensive, whereas coarser models, although cheaper, often lack
sufficient detail to accurately represent the phenomenon at hand. Spatial
hybrid methods couple two or more of these representations in order to improve
efficiency without compromising accuracy.
  In this paper, we present a novel spatial hybrid method, which we call the
auxiliary region method (ARM), which couples PDE and Brownian-based
representations of reaction-diffusion systems. Numerical PDE solutions on one
side of an interface are coupled to Brownian-based dynamics on the other side
using compartment-based "auxiliary regions". We demonstrate that the hybrid
method is able to simulate reaction-diffusion dynamics for a number of
different test problems with high accuracy. Further, we undertake error
analysis on the ARM which demonstrates that it is robust to changes in the free
parameters in the model, where previous coupling algorithms are not. In
particular, we envisage that the method will be applicable for a wide range of
spatial multi-scales problems including, filopodial dynamics, intracellular
signalling, embryogenesis and travelling wave phenomena.

Common wheat (Triticum aestivum L.) is one of the most important cereal
crops. Wheat powdery mildew caused by Blumeria graminis f. sp. tritici (Bgt) is
a continuing threat to wheat production. The Pm21 gene, originating from
Dasypyrum villosum, confers high resistance to all known Bgt races and has been
widely applied in wheat breeding in China. In this research, we identify Pm21
as a typical coiled-coil, nucleotide-binding site, leucine-rich repeat gene by
an integrated strategy of resistance gene analog (RGA)-based cloning via
comparative genomics, physical and genetic mapping, BSMV-induced gene silencing
(BSMV-VIGS), large-scale mutagenesis and genetic transformation.

Standard techniques for studying biological systems largely focus on their
dynamical, or, more recently, their informational properties, usually taking
either a reductionist or holistic perspective. Yet, studying only individual
system elements or the dynamics of the system as a whole disregards the
organisational structure of the system - whether there are subsets of elements
with joint causes or effects, and whether the system is strongly integrated or
composed of several loosely interacting components. Integrated information
theory (IIT), offers a theoretical framework to (1) investigate the
compositional cause-effect structure of a system, and to (2) identify causal
borders of highly integrated elements comprising local maxima of intrinsic
cause-effect power. Here we apply this comprehensive causal analysis to a
Boolean network model of the fission yeast (Schizosaccharomyces pombe)
cell-cycle. We demonstrate that this biological model features a non-trivial
causal architecture, whose discovery may provide insights about the real cell
cycle that could not be gained from holistic or reductionist approaches. We
also show how some specific properties of this underlying causal architecture
relate to the biological notion of autonomy. Ultimately, we suggest that
analysing the causal organisation of a system, including key features like
intrinsic control and stable causal borders, should prove relevant for
distinguishing life from non-life, and thus could also illuminate the origin of
life problem.

A mutation in a protein-coding gene in DNA can alter the protein structure
coded by the same gene. Structurally altered proteins usually lose their
functions and sometimes gain an undesirable function instead. These types of
mutations and their effects can result in genetic diseases or antibiotic
resistant bacteria, among other health issues. Important curing methods have
been developed for detecting mutations against AIDS as well as genetic
diseases. Another example is the influenza virus. The reasons why a vaccination
developed to fight against influenza does not work the following year are (a)
the mutation of its DNA and (b) the outbreak of the virus after it has been
mutated especially if it is a virus that escaped the vaccinations target. Due
to such reasons, it is highly important to know in advance the location of a
potential mutation in a protein as well as the problems it might cause the
medical sciences. In this study we have used artificial neural networks, which
are one of the latest artificial intelligence technologies, to determine the
effects of cancer mutations. The model we developed has given more successful
results compared to other methods. We foresee that our model will bring a new
dimension to medical research and the medical industry.

This article seeks to address the prevailing issue of how to measure specific
process components of psychobiological stress responses. Particularly the
change of cortisol secretion due to stress exposure has been discussed as an
endophenotype of many psychosomatic health outcomes. To assess its process
components, a large variety of non-compartmental parameters (i.e., composite
measures of substance concentrations at different points in time) like the area
under the concentration-time curve (AUC) are utilized. However, a systematic
evaluation and validation of these parameters based on a physiologically
plausible model of cortisol secretion has not been performed so far. Thus, a
population pharmacokinetic (mixed-effects SDE) model was developed and fitted
to densely sampled salivary cortisol data of 10 males from Montreal, Canada,
and sparsely sampled data of 200 mixed-sex participants from Dresden, Germany,
who completed the Trier Social Stress Test (TSST). Besides the two major
process components representing (1) stress-related cortisol secretion
(reactivity) and (2) cortisol elimination (recovery), the model incorporates
two additional, often disregarded components: (3) the secretory delay after
stress onset, and (4) deviations from the projected steady-state concentration.
The fitted model (R2 = 99%) was thereafter used to investigate the correlation
structure of the four individually varying, and readily interpretable model
parameters and eleven popular non-compartmental parameters. Based on these
analyses, we recommend to use the minimum-maximum cortisol difference and the
minimum concentration as proxy measures of reactivity and recovery,
respectively. Finally, statistical power analyses of the reactivity-related sex
effect illustrate the consequences of using impure non-compartmental measures
of the different process components that underlie the cortisol stress response.

Flow-Imaging Microscopy (FIM) is commonly used in both academia and industry
to characterize subvisible particles (those $\le 25 \mu m$ in size) in protein
therapeutics. Pharmaceutical companies are required to record vast volumes of
FIM data on protein therapeutic products, but are only mandated under US FDA
regulations (i.e., USP $\big \langle 788 \big \rangle$) to control the number
of particles exceeding $10$ and $25 \mu m$ in delivered products. Hence, a vast
amount of digital images are available to analyze. Current state-of-the-art
methods rely on a relatively low-dimensional list of "morphological features"
to characterize particles, but these methods ignore an enormous amount of
information encoded in the existing large digital image repositories. Deep
Convolutional Neural Networks (CNNs or "ConvNets") have demonstrated the
ability to extract predictive information from raw macroscopic image data
without requiring the selection or specification of "morphological features" in
a variety of tasks. However, the heterogeneity, polydispersity of protein
therapeutics, and optical phenomena associated with subvisible FIM particle
measurements introduce new challenges regarding the application of CNNs to FIM
image analysis. In this article, we demonstrate a supervised learning technique
leveraging CNNs to extract information from raw images in order to predict the
process conditions or stress states (freeze-thaw, mechanical shaking, etc.)
that produced a variety of different protein images. We demonstrate that our
new classifier (in combination with a sample "image pooling" strategy) can
obtain nearly perfect predictions using as few as 20 FIM images from a given
protein formulation in a variety of scenarios of relevance to protein
therapeutics quality control and process monitoring.

Backgr: Digital pathology images are increasingly used both for diagnosis and
research, because slide scanners are nowadays broadly available and because the
quantitative study of these images yields new insights in systems biology.
However, such virtual slides build up a technical challenge since the images
occupy often several gigabytes and cannot be fully opened in a computer's
memory. Moreover, there is no standard format. Therefore, most common open
source tools such as ImageJ fail at treating them, and the others require
expensive hardware while still being prohibitively slow.
  Res: We have developed several cross-platform open source software tools to
overcome these limitations. The NDPITools provide a way to transform microscopy
images initially in the loosely supported NDPI format into one or several
standard TIFF files, and to create mosaics (division of huge images into small
ones, with or without overlap) in various TIFF and JPEG formats. They can be
driven through ImageJ plugins. The LargeTIFFTools achieve similar functionality
for huge TIFF images which do not fit into RAM. We test the performance of
these tools on several digital slides and compare them, when applicable, to
standard software. A statistical study of the cells in a tissue sample from an
oligodendroglioma was performed on an average laptop computer to demonstrate
the efficiency of the tools.
  Concl: Our open source software enables dealing with huge images with
standard software on average computers. Our tools are cross-platform,
independent of proprietary libraries, and very modular, allowing them to be
used in other open source projects. They have excellent performance in terms of
execution speed and RAM requirements. They open promising perspectives both to
the clinician who wants to study a single slide and to the research team or
data centre who do image analysis of many slides on a computer cluster.

3-carboxy-4-methyl-5-propyl-2-furanpropanoic acid (CMPF) is a major
endogenous ligand found in the human serum albumin (HSA) of renal failure
patients. It gets accumulated in the HSA and its concentration in sera of
patients may reflect the chronicity of renal failure [1-4]. It is considered
uremic toxin due to its damaging effect on the renal cells. The high
concentrations of CMPF inhibit the binding of other ligands to HSA. Removal of
CMPF is difficult through conventional hemodialysis due to its strong binding
affinity. We hypothesized that the competitive inhibition may be helpful in
removal of CMPF binding to HSA. A compound with higher HSA binding affinity
than CMPF could be useful to prevent CMPF from binding so that CMPF could be
excreted by the body through the urine. We studied an active compound
dihydrothymoquinone/ dithymoquinone (DTQ) found in black cumin seed (Nigella
sativa), which has higher binding affinity for HSA. Molecular docking
simulations were performed to find the binding affinity of CMPF and DTQ with
HSA. DTQ was found to have higher binding affinity possessing more interactions
with the binding residues than the CMPF. We studied the binding pocket
flexibility of CMPF and DTQ to analyze the binding abilities of both the
compounds. We have also predicted the ADME properties for DTQ which shows
higher lipophilicity, higher gastrointestinal (GI) absorption, and blood-brain
barrier (BBB) permeability. We discovered that DTQ has potential to act as an
inhibitor of CMPF and can be considered as a candidate for the formation of the
therapeutic drug against CMPF.

Light microscopy as well as image acquisition and processing suffer from
physical and technical prejudices which preclude a correct interpretation of
biological observations which can be reflected in, e.g., medical and
pharmacological praxis. Using the examples of a diffracting microbead and
fluorescently labelled tissue, this article clarifies some ignored aspects of
image build-up in the light microscope and introduce algorithms for maximal
extraction of information from the 3D microscopic experiments. We provided a
correct set-up of the microscope and we sought a voxel (3D pixel) called an
electromagnetic centroid which localizes the information about the object. In
diffraction imaging and light emission, this voxel shows a minimal intensity
change in two consecutive optical cuts. This approach further enabled us to
identify z-stack of a DAPI-stained tissue section where at least one object of
a relevant fluorescent marker was in focus. The spatial corrections (overlaps)
of the DAPI-labelled region with in-focus autofluorescent regions then enabled
us to co-localize these three regions in the optimal way when considering
physical laws and information theory. We demonstrate that superresolution down
to the Nobelish level can be obtained from commonplace widefield bright-field
and fluorescence microscopy and bring new perspectives on co-localization in
fluorescent microscopy.

We develop a method to reconstruct, from measured displacements of an
underlying elastic substrate, the spatially dependent forces that cells or
tissues impart on it. Given newly available high-resolution images of substrate
displacements, it is desirable to be able to reconstruct small scale, compactly
supported focal adhesions which are often localized and exist only within the
footprint of a cell. In addition to the standard quadratic data mismatch terms
that define least-squares fitting, we motivate a regularization term in the
objective function that penalizes vectorial invariants of the reconstructed
surface stress while preserving boundaries. We solve this inverse problem by
providing a numerical method for setting up a discretized inverse problem that
is solvable by standard convex optimization techniques. By minimizing the
objective function subject to a number of important physically motivated
constraints, we are able to efficiently reconstruct stress fields with
localized structure from simulated and experimental substrate displacements.
Our method incorporates the exact solution for the stress tensor accurate to
first-order finite-differences and motivates the use of distance-based cut-offs
for data inclusion and problem sparsification.

Effective methods of fluid transport vary across scale. A commonly used
dimensionless number for quantifying the effective scale of fluid transport is
the Reynolds number, Re, which gives the ratio of inertial to viscous forces.
What may work well for one Re regime may not produce significant flows for
another. These differences in scale have implications for many organisms,
ranging from the mechanics of how organisms move through their fluid
environment to how hearts pump at various stages in development. Some
organisms, such as soft pulsing corals, actively contract their tentacles to
generate mixing currents that enhance photosynthesis. Their unique morphology
and intermediate scale where both viscous and inertial forces are significant
make them a unique model organism for understanding fluid mixing. In this
paper, 3D fluid-structure interaction simulations of a pulsing soft coral are
used to quantify fluid transport and fluid mixing across a wide range of Re.
The results show that net transport is negligible for $Re<10$, and continuous
upward flow is produced for $Re\geq 10$.

BackgroundLowering the gut exposure to antibiotics during treatments can
prevent microbiota disruption. We evaluated the effect of an activated
charcoal-based adsorbent, DAV131A, on fecal free moxifloxacin concentration and
mortality in a hamster model of moxifloxacin-induced C. difficile
infection.Methods215 hamsters receiving moxifloxacin subcutaneously (D1-D5)
were orally infected at D3 with C. difficile spores. They received various
doses (0-1800mg/kg/day) and schedules (BID, TID) of DAV131A (D1-D8).
Moxifloxacin concentration and C. difficile counts were determined at D3, and
mortality at D12. We compared mortality, moxifloxacin concentration and C.
difficile counts according to DAV131A regimens, and modelled the link between
DAV131A regimen, moxifloxacin concentration and mortality. ResultsAll hamsters
that received no DAV131A died, but none of those that received 1800mg/kg/day. A
significant dose-dependent relationship between DAV131A dose and (i) mortality
rates, (ii) moxifloxacin concentration and (iii) C. difficile counts was
evidenced. Mathematical modeling suggested that (i) lowering moxifloxacin
concentration at D3, which was 58$\mu$g/g (95%CI=50-66) without DAV131A, to
17$\mu$g/g (14-21) would reduce mortality by 90% and (ii) this would be
achieved with a daily DAV131A dose of 703mg/kg (596-809).ConclusionsIn this
model of C. difficile infection, DAV131A reduced mortality in a dose-dependent
manner by decreasing fecal free moxifloxacin concentration.

Within animals, oxygen exchange occurs within networks containing potentially
billions of microvessels that are distributed throughout the animal's body.
Innovative imaging methods now allow for mapping of the architecture and blood
flows within real microvascular networks. However, these data streams have so
far yielded little new understanding of the physical principles that underlie
the organization of microvascular networks, which could allow healthy networks
to be quantitatively compared with networks that have been damaged, e.g. due to
diabetes. A natural mathematical starting point for understanding network
organization is to construct networks that are optimized accordingly to
specified functions. Here we present a method for deriving transport networks
that optimize general functions involving the fluxes and conductances within
the network. In our method Kirchoff's laws are imposed via Lagrange
multipliers, creating a large, but sparse system of auxiliary equations. By
treating network conductances as adiabatic variables, we derive a gradient
descent method in which conductances are iteratively adjusted, and auxiliary
variables are solved for by two inversions of O(N^2) sized sparse matrices. In
particular our algorithm allows us to validate the hypothesis that
microvascular networks are organized to uniformly partition the flow of red
blood cells through vessels. The theoretical framework can also be used to
consider more general sets of objective functions and constraints within
transport networks, including incorporating the non-Newtonian rheology of blood
(i.e. the Fahraeus-Lindqvist effect). More generally by forming linear
combinations of objective functions, we can explore tradeoffs between different
optimization functions, giving more insight into the diversity of biological
transport networks seen in nature.

Manuka honey (MH) is used as an antibacterial agent in bioactive wound
dressings via direct impregnation onto a suitable substrate. MH provides unique
antibacterial activity when compared with conventional honeys, owing partly to
one of its constituents, methylglyoxal (MGO). Aiming to investigate an
antibiotic-free antimicrobial strategy, we studied the antibacterial activity
of both MH and MGO (at equivalent MGO concentrations) when applied as a
physical coating to a nonwoven fabric wound dressing. When physically coated on
to a cellulosic hydroentangled nonwoven fabric, it was found that
concentrations of 0.0054 mg cm-2 of MGO in the form of MH and MGO was
sufficient to achieve 100 CFU% bacteria reduction against gram-positive
Staphylococcus aureus and gram-negative Klebsiella pneumoniae, based on BS EN
ISO 20743:2007. A 3- to 20- fold increase in MGO concentration (0.0170 - 0.1 mg
cm-2) was required to facilitate a good antibacterial effect (based on BS EN
ISO 20645:2004) in terms of zone of inhibition and lack of growth under the
sample. The minimum inhibitory concentration (MIC) and minimum bactericidal
concentration (MBC) was also assessed for MGO in liquid form against three
prevalent wound and healthcare-associated pathogens, i.e. Staphylococcus
aureus, gram-negative Pseudomonas aeruginosa and gram-positive Enterococcus
faecalis. Other than the case of MGO-containing fabrics, solutions with much
higher MGO concentrations (128 mg L-1 - 1024 mg L-1) were required to provide
either a bacteriostatic or bactericidal effect. The results presented in this
study therefore demonstrate the relevance of MGO-based coating as an
environment-friendly strategy for the design of functional dressings with
antibiotic-free antimicrobial chemistries.

Anti-staphylococcal penicillins (ASPs) are recommended as first-line agents
in methicillin-susceptible Staphylococcus aureus (MSSA) bacteraemia. Concerns
about their safety profile have contributed to the increased use of cefazolin.
The comparative clinical effectiveness and safety profile of cefazolin versus
ASPs for such infections remain unclear. Furthermore, uncertainty persists
concerning the use of cefazolin due to controversies over its efficacy in deep
MSSA infections and its possible negative ecological impact.

Alzheimer disease (AD) is the leading cause of dementia, accounts for 60 to
80 percent cases. Two main factors called beta amyloid plaques and tangles are
prime suspects in damaging and killing nerve cells. However, oxidative stress,
the process which produces free radicals in cells, is believed to promote its
progression to the extent that it may responsible for the cognitive and
functional decline observed in AD. As of today there are few FDA approved drugs
in the market for treatment, but their cholinergic adverse effect, potentially
distressing toxicity and limited targets in AD pathology limits their use.
Therefore, it is crucial to find an effective compounds to combat AD. We choose
45 plant derived natural compounds that have antioxidant properties to slow
down disease progression by quenching free redicals or promoting endogenous
antioxidant capacity. However, we performed molecular docking studies to
investigate the binding interactions between natural compounds and 13 various
anti Alzheimer drug targets. Three known Cholinesterase inhibitors (Donepezil,
Galantamine and Rivastigmine) were taken as reference drugs over natural
compounds for comparison and drug likeness studies. Few of these compounds
showed good inhibitory activity besides anti oxidant activity. Most of these
compounds followed pharmacokinetics properties that make them potentially
promising drug candidates for the treatment of Alzheimer disease.

In genomics, pattern matching against a sequence of nucleotides plays a
pivotal role for DNA sequence alignment and comparing genomes. This helps
tackling some diseases, such as cancer in humans. The complexity of searching
biological sequences in big databases has transformed sequence alignment
problem into a challenging field of research in bioinformatics. A large number
of research has been carried to solve this problem based on electronic
computers. The required extensive amount of computations for handling this huge
database in electronic computers leads to vast amounts of energy consumption
for electrical processing and cooling. On the other hand, optical processing
due to its parallel nature is much faster than electrical counterpart at a
fraction of energy consumption level and cost. In this paper, an algorithm
based on optical parallel processing is proposed that not only locate
similarity between sequences but also determines the exact location of edits.
The proposed algorithm is based on partitioning the read sequence into some
parts, namely, windows, then, computing their correlation with reference
sequence in parallel. Multiple metamaterial based optical correlators are used
in parallel to optically implement the architecture. Design limitations and
challenges of the architecture are also discussed in details. The simulation
results, comparing with the well-known BLAST algorithm, demonstrate superior
speed, accuracy, and much lower power consumption.

Whole-cell computational models aim to predict cellular phenotypes from
genotype by representing the entire genome, the structure and concentration of
each molecular species, each molecular interaction, and the extracellular
environment. Whole-cell models have great potential to transform bioscience,
bioengineering, and medicine. However, numerous challenges remain to achieve
whole-cell models. Nevertheless, researchers are beginning to leverage recent
progress in measurement technology, bioinformatics, data sharing, rule-based
modeling, and multi-algorithmic simulation to build the first whole-cell
models. We anticipate that ongoing efforts to develop scalable whole-cell
modeling tools will enable dramatically more comprehensive and more accurate
models, including models of human cells.

Far-from-equilibrium thermodynamics underpins the emergence of life, but how
has been a long-outstanding puzzle. Best candidate theories based on the
maximum entropy production principle could not be unequivocally proven, in part
due to complicated physics, unintuitive stochastic thermodynamics, and the
existence of alternative theories such as the minimum entropy production
principle. Here, we use a simple, analytically solvable, one-dimensional
bistable chemical system to demonstrate the validity of the maximum entropy
production principle. To generalize to multistable stochastic system, we use
the stochastic least-action principle to derive the entropy production and its
role in the stability of nonequilibrium steady states. This shows that in a
multistable system, all else being equal, the steady state with the highest
entropy production is favored, with a number of implications for the evolution
of biological, physical, and geological systems.

During organogenesis tissue grows and deforms. The growth processes are
controlled by diffusible proteins, so-called morphogens. Many different
patterning mechanisms have been proposed. The stereotypic branching program
during lung development can be recapitulated by a receptor-ligand based Turing
model. Our group has previously used the Arbitrary Lagrangian-Eulerian (ALE)
framework for solving the receptor-ligand Turing model on growing lung domains.
However, complex mesh deformations which occur during lung growth severely
limit the number of branch generations that can be simulated. A new Phase-Field
implementation avoids mesh deformations by considering the surface of the
modelling domains as interfaces between phases, and by coupling the
reaction-diffusion framework to these surfaces. In this paper, we present a
rigorous comparison between the Phase-Field approach and the ALE-based
simulation.

Recent experimental and theoretical work on neural populations belonging to
two separate early sensory systems, olfaction and vision, has challenged the
notion that the two operate under different computational paradigms by
providing evidence for the respective neural population codes having three
central, common features: they are highly redundant; they are organized such
that information is carried in the identity, and not the relative timing, of
the active neurons; they are capable of error correction. We present the first
model that captures these three properties in a general manner, making it
possible to investigate whether similar structure is present in other
population codes. Our model also makes specific predictions about additional,
as yet unseen, structure in such codes. If these predictions are found in real
data, this would provide new evidence that such population codes are operating
under more general computational principles.

COnstraint-Based Reconstruction and Analysis (COBRA) provides a molecular
mechanistic framework for integrative analysis of experimental data and
quantitative prediction of physicochemically and biochemically feasible
phenotypic states. The COBRA Toolbox is a comprehensive software suite of
interoperable COBRA methods. It has found widespread applications in biology,
biomedicine, and biotechnology because its functions can be flexibly combined
to implement tailored COBRA protocols for any biochemical network. Version 3.0
includes new methods for quality controlled reconstruction, modelling,
topological analysis, strain and experimental design, network visualisation as
well as network integration of chemoinformatic, metabolomic, transcriptomic,
proteomic, and thermochemical data. New multi-lingual code integration also
enables an expansion in COBRA application scope via high-precision,
high-performance, and nonlinear numerical optimisation solvers for multi-scale,
multi-cellular and reaction kinetic modelling, respectively. This protocol can
be adapted for the generation and analysis of a constraint-based model in a
wide variety of molecular systems biology scenarios. This protocol is an update
to the COBRA Toolbox 1.0 and 2.0. The COBRA Toolbox 3.0 provides an
unparalleled depth of constraint-based reconstruction and analysis methods.

In most models of collective motion in animal groups each individual updates
its heading based on the current positions and headings of its neighbors.
Several authors have investigated the effects of including anticipation into
models of this type, and have found that anticipation inhibits polarized
collective motion in alignment based models and promotes milling and swarming
in the one attraction-repulsion model studied. However, it was recently
reported that polarized collective motion does emerge in an alignment based
asynchronous lattice model with mutual anticipation. To our knowledge this is
the only reported case where polarized collective motion has been observed in a
model with anticipation. Here we show that including anticipation induces
polarized collective motion in a synchronous, off lattice, attraction based
model. This establishes that neither asynchrony, mutual anticipation nor motion
restricted to a lattice environment are strict requirements for anticipation to
promote polarized collective motion. In addition, unlike alignment based models
the attraction based model used here does not produce any type of polarized
collective motion in the absence of anticipation. Here anticipation is a direct
polarization inducing mechanism. We believe that utilizing anticipation instead
of frequently used alternatives such as explicit alignment terms, asynchronous
updates and asymmetric interactions to generate polarized collective motion may
be advantageous in some cases.

Surface Enhanced Laser Desorption/Ionization-Time Of Flight Mass Spectrometry
(SELDI-TOF MS) is a variant of the MALDI. It is uses in many cases especially
for the analysis of protein profiling and for preliminary screening tasks of
complex sample aimed for the searching of biomarker. Unfortunately, these
analysis are time consuming and strictly limited about the protein
identification. Seldi analysis of mass spectra (SELYMATRA) is a Web Application
(WA) developed with the aim of reduce these lacks automating the identification
processes and introducing the possibility to predict the proteins present in
complex mixtures from cells and tissues analysed by Mass Spectrometry.
SELYMATRA has the following characteristics. The architectural pattern used to
develop the WA is the Model-View-Controller (MVC), extremely used in the
development of software system. The WA expects an user to upload data in a
Microsoft Excel spreadsheet file format, usually generated by means of the
proprietary Mass Spectrometry softwares. Several parameters can be set such as
experiment conditions, range of isoelectric point, range of pH, relative errors
and so on. The WA compare the mass value among two mass spectra (sample vs
control) to extract differences, and according to the parameters set, it
queries a local database for the prediction of the most likely proteins related
to the masses differently expressed. The WA was validated in a cellular model
overexpressing a tagged NURR1 receptor. SELYMATRA is available at
http://140.164.61.23:8080/SELYMATRA.

We revisit the size distribution of finite components in infinite
Configuration Model networks. We provide an elementary combinatorial proof
about the sizes of birth-death trees which is more intuitive than previous
proofs. We use this to rederive the component size distribution for
Configuration Model networks. Our derivation provides a more intuitive
interpretation of the formula as contrasted with the previous derivation based
on contour integrations. We demonstrate that the formula performs well, even on
networks with heavy tails which violate assumptions of the derivation. We
explain why the result should remain robust for these networks.

The master equation plays an important role in many scientific fields
including physics, chemistry, systems biology, physical finance, and
sociodynamics. We consider the master equation with periodic transition rates.
This may represent an external periodic excitation like the 24h solar day in
biological systems or periodic traffic lights in a model of vehicular traffic.
Using tools from systems and control theory, we prove that under mild technical
conditions every solution of the master equation converges to a periodic
solution with the same period as the rates. In other words, the master equation
entrains (or phase locks) to periodic excitations. We describe two applications
of our theoretical results to important models from statistical mechanics and
epidemiology.

One of the great challenges in biology is to observe, at sufficient detail,
the real-time workings of the cell. Many methods exist to do cell measurements
invasively. For example, mass spectrometry has tremendous mass sensitivity but
destroys the cell. Molecular tagging can reveal exquisite detail using STED
microscopy, but is currently neither relevant for a large number of different
molecules, nor is it applicable to very small molecules. For marker free
non-invasive measurements, only magnetic resonance has sufficient molecular
specificity, but the technique suffers from low sensitivity and resolution. In
this presentation we will consider the roadmap for achieving in vivo
metabolomic measurements with more sensitivity and resolution. The roadmap will
point towards the technological advances that are necessary for magnetic
resonance microscopy to answer questions relevant to cell biology.

In Schiebinger et al. (2017), the authors use optimal transport of measures
on empirical distributions arising from biological experiments to relate the
single cell RNA sequencing profiles for induced pluripotent stem cells
differentiating. But such algorithms could be arbitrarily applied to any
datasets from any collection of experiments. We consider here a natural
question that arises: in a manner consistent with conventionally accepted
assumptions about biology, in which cases can the results of two experiments be
mapped to each other in this manner? The answer to this question is of
fundamental practical importance in developing algorithms that use this method
for analysing and integrating complex datasets collected as part of the Human
Cell Atlas. Here, we develop a formulation of biology in terms of sheaves of
$C^*(X)$-modules for a smooth manifold $X$ equipped with certain structures,
that enables this question to be formally answered, leading to formal
statements about experimental inference and phenotypic identifiability. These
structures capture a perspective on biology that is consistent with a standard,
widely accepted biological perspective and is mathematically intuitive. Our
methods provide a framework in which to design complex experiments and the
algorithms to analyse them in a way that their conclusions can be believed.

Mathematical models are essential tools to study how the cardiovascular
system maintains homeostasis. The utility of such models is limited by the
accuracy of their predictions, which can be determined by uncertainty
quantification (UQ). A challenge associated with the use of UQ is that many
published methods assume that the underlying model is identifiable (e.g. that a
one-to-one mapping exists from the parameter space to the model output). In
this study we present a novel methodology that is used here to calibrate a
lumped-parameter model to left ventricular pressure and volume time series data
sets. Key steps include using (1) literature and available data to determine
nominal parameter values; (2) sensitivity analysis and subset selection to
determine a set of identifiable parameters; (3) optimization to find a point
estimate for identifiable parameters; and (4) frequentist and Bayesian UQ
calculations to assess the predictive capability of the model. Our results show
that it is possible to determine 5 identifiable model parameters that can be
estimated to our experimental data from three rats, and that computed UQ
intervals capture the measurement and model error.

Fluorescence spectroscopy is an image correlation technique to analyze and
characterize the molecular dynamics from a sequence of fluorescence images.
Many image correlation techniques have been developed for different
applications [1]. But in practice the use of these techniques is often limited
to a manually selected region of analysis where it is assumed that the observed
molecules have homogeneous and constant behavior over time. Due to the spatial
and temporal complexity of biological objects, this assumption is frequently at
fault. It is then necessary to propose a robust method for discriminating the
different behaviors over time from experience, as well as identification of the
type of dynamics (diffusion or flow). This paper presents an original system of
automatic discrimination and identification of spatially and temporally uniform
regions for the analysis of molecular different dynamics over time by
calculating STICS (Spatio-Temporal Image Correlation Spectroscopy) at different
time lags. An evaluation of system performance is presented on simulated images
and images acquired by fluorescence microscopy on actin cytoskeleton.

Treadmill walking is a convenient tool for studying the human gait; however,
a common gait parameter, stride length, can be difficult to calculate directly
because relevant reference points continually move backwards. Although there is
no direct calculation of stride length itself, we can use positional
heel-marker data to directly determine a similar parameter, step length, and we
can sum two step lengths to result in one stride length. This proposed method
of calculation is simple but seems to be unexplored in other literature, so
this paper displays the details of the calculation. Our experimental results
differed from the expected values by 2.2% and had a very low standard
deviation, suggesting that this method is viable for practical use. The ability
to calculate stride length for treadmill walking using heel-marker data may
allow for quick and accurate gait calculations that further contribute to the
versatility of heel data as a tool for gait analysis.

Advances in OMICS technologies emerged both massive expression data sets and
huge networks modelling the molecular interplay of genes, RNAs, proteins and
metabolites. Network enrichment methods combine these two data types to extract
subnetwork responses from case/control setups. However, no methods exist to
integrate time series data with networks, thus preventing the identification of
time-dependent systems biology responses. We close this gap with Time Course
Network Enrichment (TiCoNE). It combines a new kind of human-augmented
clustering with a novel approach to network enrichment. It finds temporal
expression prototypes that are mapped to a network and investigated for
enriched prototype pairs interacting more often than expected by chance. Such
patterns of temporal subnetwork co-enrichment can be compared between different
conditions. With TiCoNE, we identified the first distinguishing temporal
systems biology profiles in time series gene expression data of human lung
cells after infection with Influenza and Rhino virus. TiCoNE is available
online (https://ticone.compbio.sdu.dk) and as Cytoscape app in the Cytoscape
App Store (http://apps.cytoscape.org/).

Discrete-state, continuous-time Markov models are becoming commonplace in the
modelling of biochemical processes. The mathematical formulations that such
models lead to are opaque, and, due to their complexity, are often considered
analytically intractable. As such, a variety of Monte Carlo simulation
algorithms have been developed to explore model dynamics empirically. Whilst
well-known methods, such as the Gillespie Algorithm, can be implemented to
investigate a given model, the computational demands of traditional simulation
techniques remain a significant barrier to modern research.
  In order to further develop and explore biologically relevant stochastic
models, new and efficient computational methods are required. In this thesis,
high-performance simulation algorithms are developed to estimate summary
statistics that characterise a chosen reaction network. The algorithms make use
of variance reduction techniques, which exploit statistical properties of the
model dynamics, to improve performance.
  The multi-level method is an example of a variance reduction technique. The
method estimates summary statistics of well-mixed, spatially homogeneous models
by using estimates from multiple ensembles of sample paths of different
accuracies. In this thesis, the multi-level method is developed in three
directions: firstly, a nuanced implementation framework is described; secondly,
a reformulated method is applied to stiff reaction systems; and, finally,
different approaches to variance reduction are implemented and compared.
  The variance reduction methods that underpin the multi-level method are then
re-purposed to understand how the dynamics of a spatially-extended Markov model
are affected by changes in its input parameters. By exploiting the inherent
dynamics of spatially-extended models, an efficient finite difference scheme is
used to estimate parametric sensitivities robustly.

Motivation: Mass spectrometry-based proteomics is among the most commonly
used methods for scrutinizing proteomic profiles in different organs for
biological or medical researches. All the proteomic analyses including
peptide/protein identification and quantification, differential expression
analysis, biomarker discovery and so on are all based on the matching of mass
spectra with peptide sequences, which is significantly influenced by the
quality of the spectra, such as the peak numbers, noisy peaks, signal-to-noise
ratios, etc. Hence, it is crucial to assess the quality of the spectra in order
for filtering and/or post-processing after identification. The handcrafted
features representing spectra quality, however, need human expertise to design
and are difficult to optimize, and thus the existing assessing algorithms are
still lacking in accuracy. Thus, there is a critical need for the robust and
adaptive algorithm for mass spectra quality assessment. Results: We have
developed a novel mass spectrum assessment software DeepQuality, based on the
state-of-the-art compressed sensing and deep learning algorithms. We evaluated
the algorithm on two publicly available tandem MS data sets, resulting in the
AUC of 0.96 and 0.92, respectively, a significant improvement compared with the
AUC of 0.85 and 0.91 of the existing method SpectrumQuality v2.0. Availability:
Software available at https://github.com/horsepurve/DeepQuality

A fully automatic prediction for peptide retention time (RT) in liquid
chromatography (LC), termed as DeepRT, was developed using deep learning
approach, an ensemble of Residual Network (ResNet) and Long Short-Term Memory
(LSTM). In contrast to the traditional predictor based on the hand-crafted
features for peptides, DeepRT learns features from raw amino acid sequences and
makes relatively accurate prediction of peptide RTs with 0.987 R2 for
unmodified peptides. Furthermore, by virtue of transfer learning, DeepRT
enables utilization of the peptides datasets generated from different LC
conditions and of different modification status, resulting in the RT prediction
of 0.992 R2 for unmodified peptides and 0.978 R2 for post-translationally
modified peptides. Even though chromatographic behaviors of peptides are quite
complicated, the study here demonstrated that peptide RT prediction could be
largely improved by deep transfer learning. The DeepRT software is freely
available at https://github.com/horsepurve/DeepRT, under Apache2 open source
License.

Breast cancer is the most common type of cancer among women worldwide. The
standard histopathology of breast tissue, the primary means of disease
diagnosis, involves manual microscopic examination of stained tissue by a
pathologist. Because this method relies on qualitative information, it can
result in inter-observer variation. Furthermore, for difficult cases the
pathologist often needs additional markers of malignancy to help in making a
diagnosis. We present a quantitative method for label-free tissue screening
using Spatial Light Interference Microscopy (SLIM). By extracting tissue
markers of malignancy based on the nanostructure revealed by the optical
path-length, our method provides an objective and potentially automatable
method for rapidly flagging suspicious tissue. We demonstrated our method by
imaging a tissue microarray comprising 68 different subjects - 34 with
malignant and 34 with benign tissues. Three-fold cross validation results
showed a sensitivity of 94% and specificity of 85% for detecting cancer. The
quantitative biomarkers we extract provide a repeatable and objective basis for
determining malignancy. Thus, these disease signatures can be automatically
classified through machine learning packages, since our images do not vary from
scan to scan or instrument to instrument, i.e., they represent intrinsic
physical attributes of the sample, independent of staining quality.

Cryosurgery has been consistently used as an effective treatment to eradicate
irregular tumor tissues. During this process, many difficulties occur such as
intense cooling may also damage the neighboring normal tissues due to the
release of large amount of cold from the cooling probe. In order to protect the
normal tissues in the vicinity of target tumor tissues, coolant was released in
a regulated manner accompanied with the nanoparticle to regulate the size and
orientation of ice balls formed together with improved probe capacity. The
phase change occurs in the target tumor tissues during cryosurgery treatment.
The effective heat capacity method is used for simulation of phase change in
bio-heat transfer equation to take into account the latent heat of phase
transition. The bio-heat transfer equation is solved by using element free
Galerkin method (EFGM) to simulate the phase change problem of biological
tissues subject to nano cryosurgery. In this study, Murshed model with
cylindrical nanoparticles is used for the high thermal conductivity of
nanofluids as compared to Leong Model with the spherical nanoparticle. The
important effects of the interfacial layer at the mushy region (i.e. liquid to
the solid interface), size and concentration of nanoparticles are shown on the
freezing process. This type of problem has applications in biomedical treatment
such as drug delivery. Application of cryosurgery in bio-fluids used for drug
delivery in cancer therapy can be made more efficient in the presence of
nanoparticles (such as Iron oxide ($Fe_{3}O_{4}$), alumina ($Al_{2}O_{3}$) and
gold ($Au$)).

Behavior of natural and artificial agents consists of behavioral episodes or
acts. This study introduces a quantitative measure of behavioral acts -- their
apparent complexity. The measure is based on the concept of the Kolmogorov
complexity. It is an apparent measure because it is determined solely by the
readings of the signals that directly encode percepts and actions during
behavior. The article describes an algorithm of generating behavioral acts of
predetermined apparent complexity. Such acts can be used to evaluate and
develop learning abilities of artificial agents.

The possibility of carbohydrate separation in BEH HILIC (Ethylene Bridged
Hybride, Hydrophilic Interaction Liquid Chromatography) column was studied by
ultra-performance liquid chromatography (UPLC) with evaporative light
scattering detector (ELSD) and mobile phase containing amine compounds as
modifiers. The chromatography conditions and ELSD parameters were optimized to
separate five typical carbohydrates and applied to analysis of four infant milk
powders. The linear ranges of carbohydrate determination were 20-300mg/L for
fructose and glucose, 20-250mg/L for sucrose and lactose, and 35-180mg/L for
fructo-oligosaccharide. The LODs were 16.4mg/L for fructose and glucose,
17.3mg/L for sucrose, 20.0mg/L for lactose, and 46.7mg/L for
fructo-oligosaccharide. Relative standard deviations (RSDs) ranged between
3.45-4.23%, 1.46-4.17%, 4.14-5.60%, 1.39-4.09%, and 2.49-3.61% for fructose,
glucose, sucrose, lactose, and fructo-oilgosaccharide, respectively and
recoveries ranged between 95.0 and 105.4%

Ordinary differential equation models have become a standard tool for the
mechanistic description of biochemical processes. If parameters are inferred
from experimental data, such mechanistic models can provide accurate
predictions about the behavior of latent variables or the process under new
experimental conditions. Complementarily, inference of model structure can be
used to identify the most plausible model structure from a set of candidates,
and thus gain novel biological insight. Several toolboxes can infer model
parameters and structure for small- to medium-scale mechanistic models out of
the box. However, models for highly multiplexed datasets can require hundreds
to thousands of state variables and parameters. For the analysis of such
large-scale models, most algorithms require intractably high computation times.
This chapter provides an overview of state-of-the-art methods for parameter and
model inference, with an emphasis on scalability.

Given the recent controversies in some neuroimaging statistical methods, we
compare the most frequently used functional Magnetic Resonance Imaging (fMRI)
analysis packages: AFNI, FSL and SPM, with regard to temporal autocorrelation
modeling. This process, sometimes known as pre-whitening, is conducted in
virtually all task fMRI studies. We employ eleven datasets containing 980 scans
corresponding to different fMRI protocols and subject populations. Though
autocorrelation modeling in AFNI is not perfect, its performance is much higher
than the performance of autocorrelation modeling in FSL and SPM. The residual
autocorrelated noise in FSL and SPM leads to heavily confounded first level
results, particularly for low-frequency experimental designs. Our results show
superior performance of SPM's alternative pre-whitening: FAST, over SPM's
default. The reliability of task fMRI studies would increase with more accurate
autocorrelation modeling. Furthermore, reliability could increase if the
packages provided diagnostic plots. This way the investigator would be aware of
pre-whitening problems.

The adaptive immune system recognizes antigens via an immense array of
antigen-binding antibodies and T-cell receptors, the immune repertoire. The
interrogation of immune repertoires is of high relevance for understanding the
adaptive immune response in disease and infection (e.g., autoimmunity, cancer,
HIV). Adaptive immune receptor repertoire sequencing (AIRR-seq) has driven the
quantitative and molecular-level profiling of immune repertoires thereby
revealing the high-dimensional complexity of the immune receptor sequence
landscape. Several methods for the computational and statistical analysis of
large-scale AIRR-seq data have been developed to resolve immune repertoire
complexity in order to understand the dynamics of adaptive immunity. Here, we
review the current research on (i) diversity, (ii) clustering and network,
(iii) phylogenetic and (iv) machine learning methods applied to dissect,
quantify and compare the architecture, evolution, and specificity of immune
repertoires. We summarize outstanding questions in computational immunology and
propose future directions for systems immunology towards coupling AIRR-seq with
the computational discovery of immunotherapeutics, vaccines, and
immunodiagnostics.

The identification of reproducible biological patterns from high-dimensional
data is a bottleneck for understanding the biology of complex illnesses such as
schizophrenia. To address this, we developed a biologically informed,
multi-stage machine learning (BioMM) framework. BioMM incorporates biological
pathway information to stratify and aggregate high-dimensional biological data.
We demonstrate the utility of this method using genome-wide DNA methylation
data and show that it substantially outperforms conventional machine learning
approaches. Therefore, the BioMM framework may be a fruitful machine learning
strategy in high-dimensional data and be the basis for future, integrative
analysis approaches.

Integrative biological simulations have a varied and controversial history in
the biological sciences. From computational models of organelles, cells, and
simple organisms, to physiological models of tissues, organ systems, and
ecosystems, a diverse array of biological systems have been the target of
large-scale computational modeling efforts. Nonetheless, these research agendas
have yet to prove decisively their value among the broader community of
theoretical and experimental biologists. In this commentary, we examine a range
of philosophical and practical issues relevant to understanding the potential
of integrative simulations. We discuss the role of theory and modeling in
different areas of physics and suggest that certain sub-disciplines of physics
provide useful cultural analogies for imagining the future role of simulations
in biological research. We examine philosophical issues related to modeling
which consistently arise in discussions about integrative simulations and
suggest a pragmatic viewpoint that balances a belief in philosophy with the
recognition of the relative infancy of our state of philosophical
understanding. Finally, we discuss community workflow and publication practices
to allow research to be readily discoverable and amenable to incorporation into
simulations. We argue that there are aligned incentives in widespread adoption
of practices which will both advance the needs of integrative simulation
efforts as well as other contemporary trends in the biological sciences,
ranging from open science and data sharing to improving reproducibility.

Chronic Kidney Disease (CKD) is an increasingly prevalent condition affecting
13% of the US population. The disease is often a silent condition, making its
diagnosis challenging. Identifying CKD stages from standard office visit
records can help in early detection of the disease and lead to timely
intervention. The dataset we use is highly imbalanced. We propose a
hierarchical meta-classification method, aiming to stratify CKD by severity
levels, employing simple quantitative non-text features gathered from office
visit records, while addressing data imbalance. Our method effectively
stratifies CKD severity levels obtaining high average sensitivity, precision
and F-measure (~93%). We also conduct experiments in which the dimensionality
of the data is significantly reduced to include only the most salient features.
Our results show that the good performance of our system is retained even when
using the reduced feature sets, as well as under much reduced training sets,
indicating that our method is stable and generalizable.

Many biological and physical systems exhibit behaviour at multiple spatial,
temporal or population scales. Multiscale processes provide challenges when
they are to be simulated using numerical techniques. While coarser methods such
as partial differential equations are typically fast to simulate, they lack the
individual-level detail that may be required in regions of low concentration or
small spatial scale. However, to simulate at such an individual-level
throughout a domain and in regions where concentrations are high can be
computationally expensive. Spatially-coupled hybrid methods provide a bridge,
allowing for multiple representations of the same species in one spatial domain
by partitioning space into distinct modelling subdomains. Over the past twenty
years, such hybrid methods have risen to prominence, leading to what is now a
very active research area across multiple disciplines including chemistry,
physics and mathematics.
  There are three main motivations for undertaking this review. Firstly, we
have collated a large number of spatially-extended hybrid methods and presented
them in a single coherent document, while comparing and contrasting them, so
that anyone with a need for a multi-scale hybrid method will be able to find
the most appropriate one for their need. Secondly, we have provided canonical
examples with algorithms and accompanying code, serving to demonstrate how
these types of methods work in practice. Finally, we have presented papers that
employ these methods on real biological and physical problems, demonstrating
their utility. We also consider some open research questions in the area of
hybrid method development and the future directions for the field.

The understanding of toxicity is of paramount importance to human health and
environmental protection. Quantitative toxicity analysis has become a new
standard in the field. This work introduces element specific persistent
homology (ESPH), an algebraic topology approach, for quantitative toxicity
prediction. ESPH retains crucial chemical information during the topological
abstraction of geometric complexity and provides a representation of small
molecules that cannot be obtained by any other method. To investigate the
representability and predictive power of ESPH for small molecules, ancillary
descriptors have also been developed based on physical models. Topological and
physical descriptors are paired with advanced machine learning algorithms, such
as deep neural network (DNN), random forest (RF) and gradient boosting decision
tree (GBDT), to facilitate their applications to quantitative toxicity
predictions. A topology based multi-task strategy is proposed to take the
advantage of the availability of large data sets while dealing with small data
sets. Four benchmark toxicity data sets that involve quantitative measurements
are used to validate the proposed approaches. Extensive numerical studies
indicate that the proposed topological learning methods are able to outperform
the state-of-the-art methods in the literature for quantitative toxicity
analysis. Our online server for computing element-specific topological
descriptors (ESTDs) is available at http://weilab.math.msu.edu/TopTox/

This paper reports some experimental results validating in a broader context
a variant of PCR, called XPCR, previously introduced and tested on relatively
short synthetic DNA sequences. Basic XPCR technique confirmed to work as
expected, to concatenate two genes of different lengths, while a library of all
permutations of three different genes (extracted from the bacterial strain
Bulkolderia fungorum DBT1) has been realized in one step by multiple XPCR.
  Limits and potentialities of the protocols have been discussed, and tested in
several experimental conditions, by aside showing that overlap concatenation of
multiple copies of one only gene is not realizable by these procedures, due to
strand displacement phenomena. In this case, in fact, one copy of the gene is
obtained as a unique amplification product.

Seagrass meadows, one of the worlds most important and productive coastal
habitats, are threatened by a range of anthropogenic actions. Burial of
seagrass plants due to coastal activities is one important anthropogenic
pressure leading to decline of local populations. In our study, we assessed the
response of eelgrass Zostera marina to sediment burial from physiological,
morphological, and population parameters. In a full factorial field experiment,
burial level (5-20 cm) and burial duration (4-16 weeks) were manipulated.
Negative effects were visible even at the lowest burial level (5 cm) and
shortest duration (4 weeks), with increasing effects over time and burial
level. Buried seagrasses showed higher shoot mortality, delayed growth and
flowering and lower carbohydrate storage. The observed effects will likely have
an impact on next years survival of buried plants. Our results have
implications for the management of this important coastal plant.

In a recent paper we presented a simple two compartment model which describes
the influence of inhaled concentrations on exhaled breath concentrations for
volatile organic compounds (VOCs) with small Henry constants. In this paper we
extend this investigation concerning the influence of inhaled concentrations on
exhaled breath concentrations for VOCs with higher Henry constants.
  To this end we extend our model with an additional compartment which takes
into account the influence of the upper airways on exhaled breath VOC
concentrations.

Sulfated polysaccharides constitute a large and complex group of
macromolecules which possess a wide range of important biological properties.
Many of them hold promise as new therapeutics, but determination of their blood
levels during pharmacokinetic studies can be challenging. Heparin Red, a
commercial mix-and-read fluorescence assay, has recently emerged as a tool in
clinical drug development and pharmacokinetic analysis for the quantification
of sulfated polysaccharides in human plasma. The present study describes the
application of Heparin Red to the detection of heparin, a highly sulfated
polysaccharide, and fucoidan, a less sulfated polysaccharide, in spiked mouse
and rat plasmas. While the standard assay protocol for human plasma matrix gave
less satisfactory results, a modified protocol was developed that provides
within a detection range 0 to 10 micrograms per mL better limits of
quantification, 1.1 to 2.3 micrograms per mL for heparin, and 1.7 to 3.4
micrograms per mL for fucoidan. The required plasma sample volume of only 20
microliters is advantegous in particular when blood samples need to be
collected from mice. Our results suggest that Heparin Red is a promising tool
for the preclinical evaluation of sulfated polysaccharides with varying
sulfation degrees in mouse and rat models.

In this work, we examine effects of large permanent charges on ionic flow
through ion channels based on a quasi-one dimensional Poisson-Nernst-Planck
model. It turns out large positive permanent charges inhibit the flux of cation
as expected, but strikingly, as the transmembrane electrochemical potential for
anion increases in a particular way, the flux of anion decreases. The latter
phenomenon was observed experimentally but the cause seemed to be unclear. The
mechanisms for these phenomena are examined with the help of the profiles of
the ionic concentrations, electric fields and electrochemical potentials. The
underlying reasons for the near zero flux of cation and for the decreasing flux
of anion are shown to be different over different regions of the permanent
charge. Our model is oversimplified. More structural detail and more
correlations between ions can and should be included. But the basic finding
seems striking and important and deserving of further investigation.

BioDynaMo is a biological processes simulator developed by an international
community of researchers and software engineers working closely with
neuroscientists. The authors have been working on gene expression, i.e. the
process by which the heritable information in a gene - the sequence of DNA base
pairs - is made into a functional gene product, such as protein or RNA.
Typically, gene regulatory models employ either statistical or analytical
approaches, being the former already well understood and broadly used. In this
paper, we utilize analytical approaches representing the regulatory networks by
means of differential equations, such as Euler and Runge-Kutta methods. The two
solutions are implemented and have been submitted for inclusion in the
BioDynaMo project and are compared for accuracy and performance.

Due to the heterogeneity of the phenotype defined by Diagnostic and
Statistical Manual of Mental Disorders (DSM) IV, it is not an optimal option to
identify the genetic variation that underlies the risk for alcohol dependence
(AD) and identifying subtypes of AD becomes an important topic. Traditional
unsupervised cluster analysis and latent class analysis are the most commonly
used methods to obtain the subtypes, but without the guidance of the genetic
information, all these methods may lead to subtypes of little utility in
genetic analysis. Recently, some multi-view co-clustering methods are proposed
to ameliorate this drawback. However, these new methods did not take the
missing values inside the data into consideration. To get around this
limitation, we extended one of the multi-view methods to dealing with the
missing values and clustering simultaneously. We applied this method to 2230
European-American sample and found that the well-known generic variant
rs1229984 (in the ADH1B candidate gene) for the subtype is more significant
than that corresponding to case-control association test. Finally, we verify it
on the 1707 replication sample and find it significant, too.

Electronic health records (EHR) contain a large variety of information on the
clinical history of patients such as vital signs, demographics, diagnostic
codes and imaging data. The enormous potential for discovery in this rich
dataset is hampered by its complexity and heterogeneity.
  We present the first study to assess unsupervised homogenization pipelines
designed for EHR clustering. To identify the optimal pipeline, we tested
accuracy on simulated data with varying amounts of redundancy, heterogeneity,
and missingness. We identified two optimal pipelines: 1) Multiple Imputation by
Chained Equations (MICE) combined with Local Linear Embedding; and 2) MICE,
Z-scoring, and Deep Autoencoders.

Aqueous solubility and partition coefficient are important physical
properties of small molecules. Accurate theoretical prediction of aqueous
solubility and partition coefficient plays an important role in drug design and
discovery. The prediction accuracy depends crucially on molecular descriptors
which are typically derived from theoretical understanding of the chemistry and
physics of small molecules. The present work introduces an algebraic topology
based method, called element specific persistent homology (ESPH), as a new
representation of small molecules that is entirely different from conventional
chemical and/or physical representations. ESPH describes molecular properties
in terms of multiscale and multicomponent topological invariants. Such
topological representation is systematical, comprehensive, and scalable with
respect to molecular size and composition variations. However, it cannot be
literally translated into a physical interpretation. Fortunately, it is readily
suitable for machine learning methods, rendering topological learning
algorithms. Due to the inherent correlation between solubility and partition
coefficient, a uniform ESPH representation is developed for both properties,
which facilitates multi-task deep neural networks for their simultaneous
predictions. This strategy leads to more accurate prediction of relatively
small data sets. A total of six data sets is considered in the present work to
validate the proposed topological and multi-task deep learning approaches. It
is demonstrate that the proposed approaches achieve some of the most accurate
predictions of aqueous solubility and partition coefficient. Our software is
available online at {\url{http://weilab.math.msu.edu/TopP-S/}}

BACKGROUND: Most studies of CAD revascularization have been based on and
reported according to angiographic criteria which don't consider the relation
between the resulting effective flow distal to the stenosis and the demand of a
hypertrophied myocardial tissue.
  MODEL: Mathematical model of the myocardial perfusion in comorbid CAD and
ventricular hypertrophy using Poiseuille's law. The analysis yields that the
curve, which represents the relation between the perfusion and the severity of
CAD depending on angiographic and/or angiophysiologic criteria, is shifted to
the right by the effect of myocardial tissue hypertrophy. The right shift of
said curve, which is directly proportional to the degree of ventricular
hypertrophy, indicates that the perfusion of the corresponding myocardial
tissue is compromised at angiographically and/or angiophysiologically
subsignificant stenosis of the supplying epicardial vessel.
  RESULTS: Patients with comorbid CAD and left ventricular hypertrophy are more
sensitive to CAD-related hemodynamic changes. They are more prone to develop
ischemic complications, than their peers with isolated CAD regarding the same
degree of coronary stenosis.
  CONCLUSION: Patients with comorbid CAD and ventricular hypertrophy suffer
from myocardial hypoperfusion at angiographically and/or angiophysiologically
subcritical epicardial stenosis. Accordingly; the comorbidity of both diseases
should be considered upon designing of the treatment regime.

Within the diverse interdisciplinary life sciences domains, semantic,
workflow, and methodological ambiguities can prevent the appreciation of
explanations of phenomena, handicap the use of computational models, and hamper
communication among scientists, engineers, and the public. Members of the life
sciences community commonly, and too often loosely, draw on "mechanistic model"
and similar phrases when referring to the processes of discovering and
establishing causal explanations of biological phenomena. Ambiguities in
modeling and simulation terminology and methods diminish clarity, credibility,
and the perceived significance of research findings. To encourage improved
semantic and methodological clarity, we describe the spectrum of
Mechanism-oriented Models being used to develop explanations of biological
phenomena. We cluster them into three broad groups. We then expand the three
groups into a total of seven workflow-related model types having clearly
distinguishable features. We name each type and illustrate with diverse
examples drawn from the literature. These model types are intended to
contribute to the foundation of an ontology of mechanism-based simulation
research in the life sciences. We show that it is within the model-development
workflows that the different model types manifest and exert their scientific
usefulness by enhancing and extending different forms and degrees of
explanation. The process starts with knowledge about the phenomenon and
continues with explanatory and mathematical descriptions. Those descriptions
are transformed into software and used to perform experimental explorations by
running and examining simulation output. The credibility of inferences is thus
linked to having easy access to the scientific and technical provenance from
each workflow stage.

Genetic sequence data of pathogens are increasingly used to investigate
transmission dynamics in both endemic diseases and disease outbreaks; such
research can aid in development of appropriate interventions and in design of
studies to evaluate them. Several methods have been proposed to infer
transmission chains from sequence data; however, existing methods do not
generally reliably reconstruct transmission trees because genetic sequence data
or inferred phylogenetic trees from such data are insufficient for accurate
inference regarding transmission chains. In this paper, we demonstrate the lack
of a one-to-one relationship between phylogenies and transmission trees, and
also show that information regarding infection times together with genetic
sequences permit accurate reconstruction of transmission trees. We propose a
Bayesian inference method for this purpose and demonstrate that precision of
inference regarding these transmission trees depends on precision of the
estimated times of infection. We also illustrate the use of these methods to
study features of epidemic dynamics, such as the relationship between
characteristics of nodes and average number of outbound edges or inbound
edges-- signifying possible transmission events from and to nodes. We study the
performance of the proposed method in simulation experiments and demonstrate
its superiority in comparison to an alternative method. We apply them to a
transmission cluster in San Diego and investigate the impact of biological,
behavioral, and demographic factors.

The gene expression profile of a tissue averages the expression profiles of
all cells in this tissue. Digital tissue deconvolution (DTD) addresses the
following inverse problem: Given the expression profile $y$ of a tissue, what
is the cellular composition $c$ of that tissue? If $X$ is a matrix whose
columns are reference profiles of individual cell types, the composition $c$
can be computed by minimizing $\mathcal L(y-Xc)$ for a given loss function
$\mathcal L$. Current methods use predefined all-purpose loss functions. They
successfully quantify the dominating cells of a tissue, while often falling
short in detecting small cell populations.
  Here we learn the loss function $\mathcal L$ along with the composition $c$.
This allows us to adapt to application-specific requirements such as focusing
on small cell populations or distinguishing phenotypically similar cell
populations. Our method quantifies large cell fractions as accurately as
existing methods and significantly improves the detection of small cell
populations and the distinction of similar cell types.

Oscillatory processes are central for the understanding of the neural bases
of cognition and behaviour. To analyse these processes, time-frequency (TF)
decomposition methods are applied and non-parametric cluster-based statistical
procedure are used for comparing two or more conditions. While this combination
is a powerful method, it has two drawbacks. One the unreliable estimation of
signals outside the cone-of-influence and the second relates to the length of
the time frequency window used for the analysis. Both impose constrains on the
non-parametric statistical procedure for inferring an effect in the TF domain.
Here we extend the method to reliably infer oscillatory differences within the
full TF map and to test single conditions. We show that it can be applied in
small time windows irrespective of the cone-of-influence and we further develop
its application to single-condition case for testing the hypothesis of the
presence or not of time-varying signals. We present tests of this new method on
real EEG and behavioural data and show that its sensitivity to single-condition
tests is at least as good as classic Fourier analysis. Statistical inference in
the full TF map is available and efficient in detecting differences between
conditions as well as the presence of time-varying signal in single condition.

Models of biological systems often have many unknown parameters that must be
determined in order for model behavior to match experimental observations.
Commonly-used methods for parameter estimation that return point estimates of
the best-fit parameters are insufficient when models are high dimensional and
under-constrained. As a result, Bayesian methods, which treat model parameters
as random variables and attempt to estimate their probability distributions
given data, have become popular in systems biology. Bayesian parameter
estimation often relies on Markov Chain Monte Carlo (MCMC) methods to sample
model parameter distributions, but the slow convergence of MCMC sampling can be
a major bottleneck. One approach to improving performance is parallel tempering
(PT), a physics-based method that uses swapping between multiple Markov chains
run in parallel at different temperatures to accelerate sampling. The
temperature of a Markov chain determines the probability of accepting an
unfavorable move, so swapping with higher temperatures chains enables the
sampling chain to escape from local minima. In this work we compared the MCMC
performance of PT and the commonly-used Metropolis-Hastings (MH) algorithm on
six biological models of varying complexity. We found that for simpler models
PT accelerated convergence and sampling, and that for more complex models, PT
often converged in cases MH became trapped in non-optimal local minima. We also
developed a freely-available MATLAB package for Bayesian parameter estimation
called PTempEst (http://github.com/RuleWorld/ptempest), which is closely
integrated with the popular BioNetGen software for rule-based modeling of
biological systems.

Public data archives are the backbone of modern biological and biomedical
research. While archives for biological molecules and structures are
well-established, resources for imaging data do not yet cover the full range of
spatial and temporal scales or application domains used by the scientific
community. In the last few years, the technical barriers to building such
resources have been solved and the first examples of scientific outputs from
public image data resources, often through linkage to existing molecular
resources, have been published. Using the successes of existing biomolecular
resources as a guide, we present the rationale and principles for the
construction of image data archives and databases that will be the foundation
of the next revolution in biological and biomedical informatics and discovery.

In the past decade, digital technologies have started to profoundly influence
healthcare systems. Digital self-tracking has facilitated more precise
epidemiological studies, and in the field of nutritional epidemiology, mobile
apps have the potential to alleviate a significant part of the journaling
burden by, for example, allowing users to record their food intake via a simple
scan of packaged products barcodes. Such studies thus rely on databases of
commercialized products, their barcodes, ingredients, and nutritional values,
which are not yet openly available with sufficient geographical and product
coverage. In this paper, we present FoodRepo (https://www.foodrepo.org), an
open food repository of barcoded food items, whose database is programmatically
accessible through an application programming interface (API). Furthermore, an
open source license gives the appropriate rights to anyone to share and reuse
FoodRepo data, including for commercial purposes. With currently more than
21,000 items available on the Swiss market, our database represents a solid
starting point for large-scale studies in the field of digital nutrition, with
the aim to lead to a better understanding of the intricate connections between
diets and health in general, and metabolic disorders in particular.

Quasi-Monte Carlo methods have proven to be effective extensions of
traditional Monte Carlo methods in, amongst others, problems of quadrature and
the sample path simulation of stochastic differential equations. By replacing
the random number input stream in a simulation procedure by a low-discrepancy
number input stream, variance reductions of several orders have been observed
in financial applications.
  Analysis of stochastic effects in well-mixed chemical reaction networks often
relies on sample path simulation using Monte Carlo methods, even though these
methods suffer from typical slow $\mathcal{O}(N^{-1/2})$ convergence rates as a
function of the number of sample paths $N$. This paper investigates the
combination of (randomised) quasi-Monte Carlo methods with an efficient sample
path simulation procedure, namely $\tau$-leaping. We show that this combination
is often more effective than traditional Monte Carlo simulation in terms of the
decay of statistical errors. The observed convergence rate behaviour is,
however, non-trivial due to the discrete nature of the models of chemical
reactions. We explain how this affects the performance of quasi-Monte Carlo
methods by looking at a test problem in standard quadrature.

The nature of neural codes is central to neuroscience. Do neurons encode
information through relatively slow changes in the emission rates of individual
spikes (rate code), or by the precise timing of every spike (temporal codes)?
Here we compare the loss of information due to correlations for these two
possible neural codes. The essence of Shannon's definition of information is to
combine information with uncertainty: the higher the uncertainty of a given
event, the more information is conveyed by that event. Correlations can reduce
uncertainty or the amount of information, but by how much? In this paper we
address this question by a direct comparison of the information per symbol
conveyed by the words coming from a binary Markov source (temporal codes) with
the information per symbol coming from the corresponding Bernoulli source
(uncorrelated, rate code source). In a previous paper we found that a crucial
role in the relation between Information Transmission Rates (ITR) and Firing
Rates is played by a parameter s, which is the sum of transitions probabilities
from the no-spike-state to the spike-state and vice versa. It turned out that
also in this case a crucial role is played by the same parameter s. We found
bounds of the quotient of ITRs for these sources, i.e. this quotient's minimal
and maximal values. Next, making use of the entropy grouping axiom, we
determined the loss of information in a Markov source in relation to its
corresponding Bernoulli source for a given length of word. Our results show
that in practical situations in the case of correlated signals the loss of
information is relatively small, thus temporal codes, which are more
energetically efficient, can replace the rate code effectively. These phenomena
were confirmed by experiments.

The choice of reference for electroencephalogram (EEG) is a long-lasting
unsolved issue resulting in inconsistent usages and endless debates. Currently,
both average reference (AR) and reference electrode standardization technique
(REST) are two primary, irreconcilable contenders. We propose a theoretical
framework to resolve this issue by formulating both a) estimation of potentials
at infinity, and, b) determination of the reference, as a unified Bayesian
linear inverse problem. We find that AR and REST are very particular cases of
this unified framework: AR results from biophysically non-informative prior;
while REST utilizes the prior of EEG generative model. We develop the
regularized versions of AR and REST, named rAR, and rREST, respectively. Both
depend on a regularization parameter that is the noise to signal ratio.
Traditional and new estimators are evaluated with this framework, by both
simulations and analysis of real EEGs. Generated artificial EEGs, show that
relative error in estimating the EEG potentials at infinity is lowest for
rREST. It also reveals that realistic volume conductor models improve the
performances of REST and rREST. For practical applications, it is shown that
average lead field gives the results comparable to the individual lead field.
Finally, it is shown that the selection of the regularization parameter with
Generalized Cross-Validation (GCV) is close to the 'oracle' choice based on the
ground truth. When evaluated with the real 89 resting state EEGs, rREST
consistently yields the lowest GCV. This study provides a novel perspective on
the EEG reference problem by means of a unified inverse solution framework. It
may allow additional principled theoretical formulations and numerical
evaluation of performance.

Clustering of gene expression time series gives insight into which genes may
be coregulated, allowing us to discern the activity of pathways in a given
microarray experiment. Of particular interest is how a given group of genes
varies with different model conditions or genetic background. Amyotrophic
lateral sclerosis (ALS), an irreversible diverse neurodegenerative disorder
showed consistent phenotypic differences and the disease progression is
heterogeneous with significant variability. This paper demonstrated about
finding some significant gene expression profiles and its associated or
co-regulated cluster of gene expressions from four groups of data with
different genetic background or models conditions. Gene enrichment score
analysis and pathway analysis of judicially selected clusters lead toward
identifying features underlying the differential speed of disease progression.
Gene ontology overrepresentation analysis showed clusters from the proposed
method are less likely to be clustered just by chance. In this paper, we
develop a new clustering method that allows each cluster to be parameterised
according to whether the behaviour of the genes across conditions is correlated
or anti-correlated. Our proposed method unveil the potency of latent
information shared between multiple model conditions and their replicates
during modelling gene expression data.

The local dynamic stability method (maximum Lyapunov exponent) can assess
gait stability. Two variants of the method exist: the short-term divergence
exponent (DE), and the long-term DE. Only the short-term DE can predict fall
risk. The significance of long-term DE has been unclear so far. Some studies
have suggested that the complex, fractal-like structure of fluctuations among
consecutive strides correlates with long-term DE. The aim, therefore, was to
assess whether the long-term DE is a gait complexity index. The study
reanalyzed a dataset of trunk accelerations from 100 healthy adults walking at
preferred speed on a treadmill for 10 minutes. By interpolation, the stride
intervals were modified within the acceleration signals for the purpose of
conserving the original shape of the signal, while imposing a known
stride-to-stride fluctuation structure. 4 types of hybrid signals with
different noise structures were built: constant, anti-correlated, random, and
correlated (fractal). Short- and long-term DEs were then computed. The results
show that long-term DEs, but not short-term DEs, are sensitive to the noise
structure of stride intervals. It was that observed that random hybrid signals
exhibited significantly lower long-term DEs than hybrid correlated signals did
(0.100 vs 0.144, i.e. a 44% difference). Long-term DEs from constant hybrid
signals were close to zero (0.006). Short-term DEs of anti-correlated, random,
and correlated hybrid signals were closely grouped (2.49, 2.50, and 2.51). The
short- and long-term DEs, although they are both computed from divergence
curves, should not be interpreted in a similar way. The long-term DE is very
likely an index of gait complexity, which may be associated with gait
automaticity or cautiousness. To better differentiate between short- and
long-term DEs, the use of the term attractor complexity index (ACI) is proposed
for the latter.

Shape variability represents an important direct response of organisms to
selective environments. Here, we use a combination of geometric morphometrics
and generalised additive mixed models (GAMMs) to identify spatial patterns of
natural shell shape variation in the North Atlantic and Arctic blue mussels,
Mytilus edulis and M. trossulus, with environmental gradients of temperature,
salinity and food availability across 3980 km of coastlines. New statistical
methods and multiple study systems at various geographical scales allowed the
uncoupling of the developmental and genetic contributions to shell shape and
made it possible to identify general relationships between blue mussel shape
variation and environment that are independent of age and species influences.
We find salinity had the strongest effect on the latitudinal patterns of
Mytilus shape, producing shells that were more elongated, narrower and with
more parallel dorsoventral margins at lower salinities. Temperature and food
supply, however, were the main drivers of mussel shape heterogeneity. Our
findings revealed similar shell shape responses in Mytilus to less favourable
environmental conditions across the different geographical scales analysed. Our
results show how shell shape plasticity represents a powerful indicator to
understand the alterations of blue mussel communities in rapidly changing
environments.

Fluid-structure interaction in the developing heart is an active area of
research in developmental biology. However, investigation of heart dynamics is
mostly limited to computational fluid dynamics simulations using heart wall
structure information only, or single plane blood flow information - so there
is a need for 3D + time resolved data to fully understand cardiac function. We
present an imaging platform combining selective plane illumination microscopy
(SPIM) with micro particle image velocimetry ({\textmu}PIV) to enable
3D-resolved flow mapping in a microscopic environment, free from many of the
sources of error and bias present in traditional epifluorescence-based
{\textmu}PIV systems. By using our new system in conjunction with optical heart
beat synchronisation, we demonstrte the ability obtain non-invasive 3D + time
resolved blood flow measurements in the heart of a living zebrafish embryo.

A model of morphogenesis is proposed based on seven explicit postulates. The
mathematical import and biological significance of the postulates are explored
and discussed.

We propose a computational method to quantitatively evaluate the systematic
uncertainties that arise from undetectable sources in biological measurements
using live-cell imaging techniques. We then demonstrate this method in
measuring biological cooperativity of molecular binding networks: in
particular, ligand molecules binding to cell surface receptor proteins. Our
results show how the non-statistical uncertainties lead to invalid
identification of the measured cooperativity. Through this computational
scheme, the biological interpretation can be more objectively evaluated and
understood under a specific experimental configuration of interest.

Discerning how a mutation affects the stability of a protein is central to
the study of a wide range of diseases. Machine learning and statistical
analysis techniques can inform how to allocate limited resources to the
considerable time and cost associated with wet lab mutagenesis experiments. In
this work we explore the effectiveness of using a neural network classifier to
predict the change in the stability of a protein due to a mutation. Assessing
the accuracy of our approach is dependent on the use of experimental data about
the effects of mutations performed in vitro. Because the experimental data is
prone to discrepancies when similar experiments have been performed by multiple
laboratories, the use of the data near the juncture of stabilizing and
destabilizing mutations is questionable. We address this later problem via a
systematic approach in which we explore the use of a three-way classification
scheme with stabilizing, destabilizing, and inconclusive labels. For a
systematic search of potential classification cutoff values our classifier
achieved 68 percent accuracy on ternary classification for cutoff values of
-0.6 and 0.7 with a low rate of classifying stabilizing as destabilizing and
vice versa.

RuleBuilder is a tool for drawing graphs that can be represented by the
BioNetGen language (BNGL), which is used to formulate mathematical, rule-based
models of biochemical systems. BNGL provides an intuitive plain-text, or
string, representation of such systems, which is based on a graphical
formalism. Reactions are defined in terms of graph-rewriting rules that specify
the necessary intrinsic properties of the reactants, a transformation, and a
rate law. Rules may also contain contextual constraints that restrict
application of the rule. In some cases, the specification of contextual
constraints can be verbose, making a rule difficult to read. RuleBuilder is
designed to ease the task of reading and writing individual reaction rules, as
well as individual BNGL patterns similar to those found in rules. The software
assists in the reading of existing models by converting BNGL strings of
interest into a graph-based representation composed of nodes and edges.
RuleBuilder also enables the user to construct de novo a visual representation
of BNGL strings using drawing tools available in its interface. As objects are
added to the drawing canvas, the corresponding BNGL string is generated on the
fly, and objects are similarly drawn on the fly as BNGL strings are entered
into the application. RuleBuilder thus facilitates construction and
interpretation of rule-based models.

Absent experimental evidence, a robust methodology to predict the likelihood
of N-glycosylation in human proteins is essential for guiding experimental
work. Based on the distribution of amino acids in the neighborhood of the NxS/T
sequon (N-site); the structural attributes of the N-site that include
Accessible Surface Area, secondary structural elements, main-chain phi-psi,
turn types; the relative location of the N-site in the primary sequence; and
the nature of the glycan bound, the ridge regression estimated linear
probability model is used to predict this likelihood. This model yields a
Kolmogorov-Smirnov (Gini coefficient) statistic value of about 74% (89%), which
is reasonable.

Identification of patients at high risk for readmission could help reduce
morbidity and mortality as well as healthcare costs. Most of the existing
studies on readmission prediction did not compare the contribution of data
categories. In this study we analyzed relative contribution of 90,101 variables
across 398,884 admission records corresponding to 163,468 patients, including
patient demographics, historical hospitalization information, discharge
disposition, diagnoses, procedures, medications and laboratory test results. We
established an interpretable readmission prediction model based on Logistic
Regression in scikit-learn, and added the available variables to the model one
by one in order to analyze the influences of individual data categories on
readmission prediction accuracy. Diagnosis related groups (c-statistic
increment of 0.0933) and discharge disposition (c-statistic increment of
0.0269) were the strongest contributors to model accuracy. Additionally, we
also identified the top ten contributing variables in every data category.

Recent large cancer studies have measured somatic alterations in an
unprecedented number of tumours. These large datasets allow the identification
of cancer-related sets of genetic alterations by identifying relevant
combinatorial patterns. Among such patterns, mutual exclusivity has been
employed by several recent methods that have shown its effectivenes in
characterizing gene sets associated to cancer. Mutual exclusivity arises
because of the complementarity, at the functional level, of alterations in
genes which are part of a group (e.g., a pathway) performing a given function.
The availability of quantitative target profiles, from genetic perturbations or
from clinical phenotypes, provides additional information that can be leveraged
to improve the identification of cancer related gene sets by discovering groups
with complementary functional associations with such targets.
  In this work we study the problem of finding groups of mutually exclusive
alterations associated with a quantitative (functional) target. We propose a
combinatorial formulation for the problem, and prove that the associated
computation problem is computationally hard. We design two algorithms to solve
the problem and implement them in our tool UNCOVER. We provide analytic
evidence of the effectiveness of UNCOVER in finding high-quality solutions and
show experimentally that UNCOVER finds sets of alterations significantly
associated with functional targets in a variety of scenarios. In addition, our
algorithms are much faster than the state-of-the-art, allowing the analysis of
large datasets of thousands of target profiles from cancer cell lines. We show
that on one such dataset from project Achilles our methods identify several
significant gene sets with complementary functional associations with targets.

Single cell segmentation is critical and challenging in live cell imaging
data analysis. Traditional image processing methods and tools require
time-consuming and labor-intensive efforts of manually fine-tuning parameters.
Slight variations of image setting may lead to poor segmentation results.
Recent development of deep convolutional neural networks(CNN) provides a
potentially efficient, general and robust method for segmentation. Most
existing CNN-based methods treat segmentation as a pixel-wise classification
problem. However, three unique problems of cell images adversely affect
segmentation accuracy: lack of established training dataset, few pixels on cell
boundaries, and ubiquitous blurry features. The problem becomes especially
severe with densely packed cells, where a pixel-wise classification method
tends to identify two neighboring cells with blurry shared boundary as one
cell, leading to poor cell count accuracy and affecting subsequent analysis.
Here we developed a different learning strategy that combines strengths of CNN
and watershed algorithm. The method first trains a CNN to learn Euclidean
distance transform of binary masks corresponding to the input images. Then
another CNN is trained to detect individual cells in the Euclidean distance
transform. In the third step, the watershed algorithm takes the outputs from
the previous steps as inputs and performs the segmentation. We tested the
combined method and various forms of the pixel-wise classification algorithm on
segmenting fluorescence and transmitted light images. The new method achieves
similar pixel accuracy but significant higher cell count accuracy than
pixel-wise classification methods do, and the advantage is most obvious when
applying on noisy images of densely packed cells.

The design of multi-stable RNA molecules has important applications in
biology, medicine, and biotechnology. Synthetic design approaches profit
strongly from effective in-silico methods, which can tremendously impact their
cost and feasibility. We revisit a central ingredient of most in-silico design
methods: the sampling of sequences for the design of multi-target structures,
possibly including pseudoknots. For this task, we present the efficient, tree
decomposition-based algorithm. Our fixed parameter tractable approach is
underpinned by establishing the P-hardness of uniform sampling. Modeling the
problem as a constraint network, our program supports generic
Boltzmann-weighted sampling for arbitrary additive RNA energy models; this
enables the generation of RNA sequences meeting specific goals like expected
free energies or \GCb-content. Finally, we empirically study general properties
of the approach and generate biologically relevant multi-target
Boltzmann-weighted designs for a common design benchmark. Generating seed
sequences with our program, we demonstrate significant improvements over the
previously best multi-target sampling strategy (uniform sampling).Our software
is freely available at: https://github.com/yannponty/RNARedPrint .

We review the general problem of finding a global rotation that transforms a
given set of points and/or coordinate frames (the "test" data) into the best
possible alignment with a corresponding set (the "reference" data). For 3D
point data, this "orthogonal Procrustes problem" is often phrased in terms of
minimizing a root-mean-square deviation or RMSD corresponding to a Euclidean
distance measure relating the two sets of matched coordinates. We focus on
quaternion eigensystem methods that have been exploited to solve this problem
for at least five decades in several different bodies of scientific literature
where they were discovered independently. While numerical methods for the
eigenvalue solutions dominate much of this literature, it has long been
realized that the quaternion-based RMSD optimization problem can also be solved
using exact algebraic expressions based on the form of the quartic equation
solution published by Cardano in 1545; we focus on these exact solutions to
expose the structure of the entire eigensystem for the traditional 3D spatial
alignment problem. We then explore the structure of the less-studied
orientation data context, investigating how quaternion methods can be extended
to solve the corresponding 3D quaternion orientation frame alignment (QFA)
problem, noting the interesting equivalence of this problem to the
rotation-averaging problem, which also has been the subject of independent
literature threads. We conclude with a brief discussion of the combined 3D
translation-orientation data alignment problem. Appendices are devoted to a
tutorial on quaternion frames, a related quaternion technique for extracting
quaternions from rotation matrices, and a review of quaternion
rotation-averaging methods relevant to the orientation-frame alignment problem.
Supplementary Material covers extensions of quaternion methods to the 4D
problem.

The home range of a specific animal describes the geographic area where this
individual spends most of the time while carrying out its usual activities
(eating, resting, reproduction, ...). Although a well-established definition of
this concept is lacking, there is a variety of home range estimators. The first
objective of this work is to review and categorize the statistical
methodologies proposed in the literature to approximate the home range of an
animal, based on a sample of observed locations. The second aim is to address
the open question of choosing the "best" home range from a collection of them
based on the same sample. We introduce a numerical index, based on a
penalization criterion, to rank the estimated home ranges. The key idea is to
balance the excess area covered by the estimator (with respect to the original
sample) and a shape descriptor measuring the over-adjustment of the home range
to the data. To our knowledge, apart from computing the home range area, our
ranking procedure is the first one which is both applicable to real data and to
any type of home range estimator. Further, the optimization of the selection
index provides in fact a way to select the smoothing parameter for the kernel
home range estimator. For clarity of exposition, we have applied all the
estimation procedures and our selection proposal to a set of real locations of
a Mongolian wolf using R as the statistical software. As a byproduct, this
review contains a thorough revision of the implementation of home range
estimators in the R language.

Optimizing amino-acid mutations has been a most challenging task in modern
bio- industrial enzyme designing. It is well known that many successful designs
often hinge on extensive correlations among mutations at different sites within
the enzyme, however, the underpinning mechanism for these correlations is far
from clear. Here, we present a topology-based model to quantitively
characterize correlation effects between mutations. The method is based on the
molecular dynamic simulations and the amino-acid network clique analysis that
simply examines if two single mutation sites belong to some 3-clique. We
analyzed 13 dual mutations of T4 phage lysozyme and found that the clique-based
model successfully distinguishes highly correlated or non-additive double-site
mutations from those with less correlation or additive mutations. We also
applied the model to the protein Eglin c whose topology is significantly
distinct from that of T4 phage lysozyme, and found that the model can, to some
extension, still identify non-additive mutations from additive ones. Our
calculations showed that mutation correlation effects may heavily depend on
topology relationship among mutation sites, which can be quantitatively
characterized using amino-acid network k-cliques. We also showed that
double-site mutation correlations can be significantly altered by exerting a
third mutation, indicating that more detailed physico-chemistry interactions
might be considered with the network model for better understanding of the
elusive mutation-correlation principle.

Understanding intertrochanteric fracture distribution is an important topic
in orthopaedics due to its high morbidity and mortality. The intertrochanteric
fracture can contain high-dimensional information including complicated 3D
fracture lines, which often make it difficult to visualize or to obtain
valuable statistics for clinical diagnosis and prognosis applications. This
paper proposed a map projection technique to map the high-dimensional
information into a 2D parametric space. This method can preserve the 3D
proximal femur surface and structure while visualizing the entire fracture line
with a single plot/view. Using this method and a standardization technique, a
total of 100 patients with different ages and genders are studied based on the
original radiographs acquired by CT scan. The comparison shows that the
proposed map projection representation is more efficient and rich in
information visualization than the conventional heat map technique. Using the
proposed method, a fracture probability can be obtained at any location in the
2D parametric space, from which the most probable fracture region can be
accurately identified. The study shows that age and gender have significant
influences on intertrochanteric fracture frequency and fracture line
distribution.

We suggest an explanation of typical incubation times statistical features
based on the universal behavior of exit times for diffusion models. We give a
mathematically rigorous proof of the characteristic right skewness of the
incubation time distribution for very general one-dimensional diffusion models.
Imposing natural simple conditions on the drift coefficient, we also study
these diffusion models under the assumption of noise smallness and show that
the limiting exit time distributions in the limit of vanishing noise are
Gaussian and Gumbel. Thus they match the existing data as well as the other
existing models do. The character of our models, however, allows us to argue
that the features of the exit time distributions that we describe are universal
and manifest themselves in various other situations where the times involved
can be described as detection or halting times, for example, response times
studied in psychology.

Recently, interest in crassulacean acid metabolism (CAM) photosynthesis has
risen and new, physiologically based CAM models have emerged. These models show
promise, yet unlike the more widely used physiological models of C3 and C4
photosynthesis, their complexity has thus far inhibited their adoption in the
general community. Indeed, most efforts to assess the potential of CAM still
rely on empirically based environmental productivity indices, which makes
uniform comparisons between CAM and non-CAM species difficult. In order to
represent C3, C4, and CAM photosynthesis in a consistent, physiologically based
manner, we introduce the Photo3 model. This work builds on a common
photosynthetic and hydraulic core and adds additional components to depict the
circadian rhythm of CAM photosynthesis and the carbon-concentrating mechanism
of C4 photosynthesis. This allows consistent comparisons of the three
photosynthetic types for the first time. It also allows the representation of
intermediate C3-CAM behavior through the adjustment of a single model
parameter. Model simulations of *Opuntia ficus-indica* (CAM), *Sorghum bicolor*
(C4), and *Triticum aestivum* (C3) capture the diurnal behavior of each species
as well as the cumulative effects of long-term water limitation. The results
show potential for use in understanding CAM productivity, ecology, and climate
feedbacks and in evaluating the tradeoffs between C3, C4, and CAM
photosynthesis.

Monitoring the distribution of microfossils in stratigraphic successions is
an essential tool for biostratigraphic, evolutionary and
paleoecologic/paleoceanographic studies. To estimate the relative abundance (%)
of a given species, it is necessary to estimate in advance the minimum number
of specimens to be used in the count (n). This requires an a priori assumption
about a specified level of confidence, and about the species population
proportion (p). It is common use to apply the binomial distribution to
determine n to detect the presence of more than one species in the same sample,
although the multinomial distribution should necessarily be used instead.
  The mathematical theory of sample size computation using the multinomial
distribution is adapted to the computation of n for any number of species to be
detected together (K) at any level of confidence. Easy-to-use extensive tables
show n, for a combination of K and p. These tables indicate a large difference
for n between that indicated by the binomial and those by the multinomial
distribution when many species are to be detected simultaneously. Counting only
300 specimens (with 95 % confidence level) or 500 (99 %) is not enough to
detect more than one taxon.
  The reconstructed history of the micro-biosphere may therefore, in many
instances, need to be largely revised. This revision should affect our
understanding of the ecological and evolutionary relationships between the past
changes in the biosphere and the other major reservoirs (hydrosphere, geosphere
and atmosphere). In biostratigraphy and biochronology, using a much larger
sample size, when more than one marker species is to be detected in the
neighborhood of the same biozone boundary, may help clarifying the nature of
the apparent inconsistencies given by the observed reversals in the ordinal
(rank) biostratigraphic data shown as intersections of the correlation lines

The development of precision livestock farming which adjusts the food needs
of each animal requires detailed knowledge of its behavior and particularly
physical activity. Individual differences between animals can be observed for
group-housed sows. Accelerometer technology offers opportunities for automatic
monitoring of animal behavior. The aim of the first step was to develop a
methodology to attach the accelerometer on the sow's leg, and an algorithm to
automatically detect standing and lying posture. Accelerometers (Hobo Pendant
G) were put in a metal case and fastened with two cable ties on the leg of 6
group-housed sows. The data loggers recorded the acceleration on one axis every
20 s. Data were then validated by 9 hours of direct observations. The automatic
recording device showed data of high sensitivity (98.8%) and specificity
(99.8%). Then, accelerometers were placed on 12 to 13 group-housed sows for 2
to 4 consecutive days in 6 commercial farms equipped with electronic sow
feeding. On average each day, sows spent 259 minutes ($\pm$ 114) standing and
changed posture 29 ($\pm$ 12) times. The sow's standing time was repeatable day
to day. Differences between sows and herds were significant. Based on
behavioral data, 5 categories of sows were identified. This study suggests that
the consideration of individual behavior of each animal would improve herd
management.

New statistical methods were employed to improve the ability to distinguish
benign from malignant breast tissue ex vivo in a recent study. The ultimately
aim was to improve the intraoperative assessment of positive tumour margins in
breast-conserving surgery (BCS), potentially reducing patient re-operation
rates. A multivariate Bayesian classifier was applied to the waveform samples
produced by a Terahertz Pulsed Imaging (TPI) handheld probe system in order to
discriminate tumour from benign breast tissue, obtaining a sensitivity of 96%
and specificity of 95%.
  We compare these results to traditional and to state-of-the-art methods for
determining resection margins. Given the general nature of the classifier, it
is expected that this method can be applied to other tumour types where
resection margins are also critical.

1. Advances in tracking technology have led to an exponential increase in
animal location data, greatly enhancing our ability to address interesting
questions in movement ecology, but also presenting new challenges related to
data manage- ment and analysis. 2. Step-Selection Functions (SSFs) are commonly
used to link environmental covariates to animal location data collected at fine
temporal resolution. SSFs are estimated by comparing observed steps connecting
successive animal locations to random steps, using a likelihood equivalent of a
Cox proportional hazards model. By using common statistical distributions to
model step length and turn angle distributions, and including habitat- and
movement-related covariates (functions of distances between points, angular
deviations), it is possible to make inference regarding habitat selection and
movement processes, or to control one process while investigating the other.
The fitted model can also be used to estimate utilization distributions and
mechanistic home ranges. 3. Here, we present the R-package amt (animal movement
tools) that allows users to fit SSFs to data and to simulate space use of
animals from fitted models. The amt package also provides tools for managing
telemetry data. 4. Using fisher (Pekania pennanti ) data as a case study, we
illustrate a four-step approach to the analysis of animal movement data,
consisting of data management, exploratory data analysis, fitting of models,
and simulating from fitted models.

PanGeneHome is a web server dedicated to the analysis of available microbial
pangenomes. For any prokaryotic taxon with at least three sequenced genomes,
PanGeneHome provides (i) conservation level of genes, (ii) pangenome and
core-genome curves, estimated pangenome size and other metrics, (iii)
dendrograms based on gene content and average amino acid identity (AAI) for
these genomes, and (iv) functional categories and metabolic pathways
represented in the core, accessory and unique gene pools of the selected taxon.
In addition, the results for these different analyses can be compared for any
set of taxa. With the availability of 615 taxa, covering 182 species and 49
orders, PanGeneHome provides an easy way to get a glimpse on the pangenome of a
microbial group of interest. The server and its documentation are available at
http://pangenehome.lmge.uca.fr.

In a recent study entitled "Cell nuclei have lower refractive index and mass
density than cytoplasm", we provided strong evidence indicating that the
nuclear refractive index (RI) is lower than the RI of the cytoplasm for several
cell lines. In a complementary study in 2017, entitled "Is the nuclear
refractive index lower than cytoplasm? Validation of phase measurements and
implications for light scattering technologies", Steelman et al. observed a
lower nuclear RI also for other cell lines and ruled out methodological error
sources such as phase wrapping and scattering effects. Recently, Yurkin
composed a comment on these 2 publications, entitled "How a phase image of a
cell with nucleus refractive index smaller than that of the cytoplasm should
look like?", putting into question the methods used for measuring the cellular
and nuclear RI in the aforementioned publications by suggesting that a lower
nuclear RI would produce a characteristic dip in the measured phase profile in
situ. We point out the difficulty of identifying this dip in the presence of
other cell organelles, noise, or blurring due to the imaging point spread
function. Furthermore, we mitigate Yurkin's concerns regarding the ability of
the simple-transmission approximation to compare cellular and nuclear RI by
analyzing a set of phase images with a novel, scattering-based approach. We
conclude that the absence of a characteristic dip in the measured phase
profiles does not contradict the usage of the simple-transmission approximation
for the determination of the average cellular or nuclear RI. Our response can
be regarded as an addition to the response by Steelman, Eldridge and Wax. We
kindly ask the reader to attend to their thorough ascertainment prior to
reading our response.

The human electroencephalogram (EEG) of sleep undergoes profound changes with
age. These changes can be conceptualized as "brain age", which can be compared
to an age norm to reflect the deviation from normal aging process. Here, we
develop an interpretable machine learning model to predict brain age based on
two large sleep EEG datasets: the Massachusetts General Hospital sleep lab
dataset (MGH, N = 2,621) covering age 18 to 80; and the Sleep Hearth Health
Study (SHHS, N = 3,520) covering age 40 to 80. The model obtains a mean
absolute deviation of 8.1 years between brain age and chronological age in the
healthy participants in the MGH dataset. As validation, we analyze a subset of
SHHS containing longitudinal EEGs 5 years apart, which shows a 5.5 years
difference in brain age. Participants with neurological and psychiatric
diseases, as well as diabetes and hypertension medications show an older brain
age compared to chronological age. The findings raise the prospect of using
sleep EEG as a biomarker for healthy brain aging.

At the physiological level, aging is neither rigid nor unchangeable. Instead,
the molecular and mechanisms driving aging are sufficiently plastic that a
variety of diverse interventions--dietary, pharmaceutical, and genetic--have
been developed to radically manipulate aging. These interventions, shown to
increase the health and lifespan of laboratory animals, are now being explored
for therapeutic applications in humans.
  This clinical potential makes it especially important to understand how,
quantitatively, aging is altered by lifespan-extending interventions. Do
interventions delay the onset of aging? Slow it down? Ameliorate only its
symptoms? Perhaps some interventions will alter only a subset of aging
mechanisms, leading to complex and unintuitive systemic outcomes. Statistical
and analytic models provide a crucial framework in which to interpret the
physiological responses to interventions in aging.
  This review covers a range of quantitative models of lifespan data and places
them in the context of recent experimental work. The careful application of
these models can help experimentalists move beyond merely identifying
statistically significant differences in lifespan, to instead use lifespan data
as a versatile means for probing the underlying physiological dynamics of
aging.

Defining subtypes of complex diseases such as cancer and stratifying patient
groups with the same disease but different subtypes for targeted treatments is
important for personalized and precision medicine. Approaches that incorporate
multi-omic data are more advantageous to those using only one data type for
patient clustering and disease subtype discovery. However, it is challenging to
integrate multi-omic data as they are heterogeneous and noisy. In this paper,
we present Affinity Network Fusion (ANF) to integrate multi-omic data for
patient clustering. ANF first constructs patient affinity networks for each
omic data type, and then calculates a fused network for spectral clustering. We
applied ANF to a processed harmonized cancer dataset downloaded from GDC data
portal consisting of 2193 patients, and generated promising results on
clustering patients into correct disease types. Moreover, we developed a
semi-supervised model combining ANF and neural network for few-shot learning.
In several cases, the model can achieve greater than 90% acccuracy on test set
with training less than 1% of the data. This demonstrates the power of ANF in
learning a good representation of patients, and shows the great potential of
semi-supervised learning in cancer patient clustering.

The likelihood of O-GlcNAc glycosylation in human proteins is predicted using
the ridge regression estimated linear probability model (LPM). To achieve this,
sequences from three similar post-translational modifications (PTMs) of
proteins occurring at, or very near, the S or T site are analyzed:
N-glycosylation, O-mucin type (O-GalNAc) glycosylation, and phosphorylation.
Results found include: 1) The consensus composite sequon for O-glycosylation
does NOT have W on either side of the glycosylation site. 2) The same holds for
the consensus sequon for phosphorylation. 3) For LPM estimation, N-glycosylated
sequences are found to be good approximations to non-O-glycosylatable
sequences. 4) The selective positioning of an amino acid along the sequence,
differentiates the PTMs of proteins. 5) Some N-glycosylated sequences are also
phosphorylated at the S or T site. 6) ASA values for N-glycosylated sequences
are stochastically larger than those for O-GlcNAc glycosylated sequences. 7)
Structural attributes (beta turn II, II', helix, beta bridges, beta hairpin,
and the phi angle) are significant LPM predictors of O-GlcNAc glycosylation.
The LPM with sequence and structural data as explanatory variables yields a
Kolmogorov-Smirnov (KS) statistic value of 99%. 8) With only sequence data, the
KS statistic erodes to 80%, underscoring the germaneness of structural
information, which is sparse on O-glycosylated sequences. With 50% as the
cutoff probability for predicting O-GlcNAc glycosylation, this LPM mispredicts
21% of out-of-sample O-GlcNAc glycosylated sequences as not being glycosylated.
The 95% confidence interval around this mispredictions rate is 16% to 26%

While neural networks can be trained to map from one specific dataset to
another, they usually do not learn a generalized transformation that can
extrapolate accurately outside the space of training. For instance, a
generative adversarial network (GAN) exclusively trained to transform images of
black-haired men to blond-haired men might not have the same effect on images
of black-haired women. This is because neural networks are good at generation
within the manifold of the data that they are trained on. However, generating
new samples outside of the manifold or extrapolating "out-of-sample" is a much
harder problem that has been less well studied. To address this, we introduce a
technique called neuron editing that learns how neurons encode an edit for a
particular transformation in a latent space. We use an autoencoder to decompose
the variation within the dataset into activations of different neurons and
generate transformed data by defining an editing transformation on those
neurons. By performing the transformation in a latent trained space, we encode
fairly complex and non-linear transformations to the data with much simpler
distribution shifts to the neuron's activations. We motivate our technique on
an image domain and then move to our two main biological applications: removal
of batch artifacts representing unwanted noise and modeling the effect of drug
treatments to predict synergy between drugs.

The movement of ionic solutions is an essential part of biology and
technology. Fluidics, from nano- to micro- to microfluidics, is a burgeoning
area of technology which is all about the movement of ionic solutions, on
various scales. Many cells, tissues, and organs of animals and plants depend on
osmosis, as the movement of fluids is called in biology. Indeed, the movement
of fluids through channel proteins (that have a hole down their middle) is
fluidics on an atomic scale. Ionic fluids are complex fluids, with energy
stored in many ways. Ionic fluids flow driven by gradients of concentration,
chemical and electrical potential, and hydrostatic pressure. Each flow is
classically described by its own field theory, independent of the others, but
of course, in reality every gradient drives every kind of flow to a varying
extent. Combining field equations is tricky and so the theory of complex fluids
derives the equations, rather than assumes their interactions. When field
equations are derived, rather than assumed, their variables are consistent.
That is to say all variables satisfy all equations under all conditions with
one set of parameters. Here we treat a classical osmotic cell in this spirit,
using a sharp interface method to derive boundary conditions consistent with
all flows and fields. We allow volume to change with concentration, since
changes of volume are a property of ionic solutions known to all who make them
in the laboratory. We consider flexible and inflexible membranes. We show how
to combine the energetics of the membrane with the energetics of the
surrounding complex fluids. The results seem general but need application to
specific situations of technological, biological and experimental importance
before the consequences of consistency can be understood.

In this paper, we provide an extensive overview of machine learning
techniques applied to structural magnetic resonance imaging (MRI) data to
obtain clinical classifiers. We specifically address practical problems
commonly encountered in the literature, with the aim of helping researchers
improve the application of these techniques in future works. Additionally, we
survey how these algorithms are applied to a wide range of diseases and
disorders (e.g. Alzheimer's disease (AD), Parkinson's disease (PD), autism,
multiple sclerosis, traumatic brain injury, etc.) in order to provide a
comprehensive view of the state of the art in different fields.

Increasingly sophisticated experiments, coupled with large-scale
computational models, have the potential to systematically test biological
hypotheses to drive our understanding of multicellular systems. In this short
review, we explore key challenges that must be overcome to achieve robust,
repeatable data-driven multicellular systems biology. If these challenges can
be solved, we can grow beyond the current state of isolated tools and datasets
to a community-driven ecosystem of interoperable data, software utilities, and
computational modeling platforms. Progress is within our grasp, but it will
take community (and financial) commitment.

Animal telemetry data are often analysed with discrete time movement models
assuming rotation in the movement. These models are defined with equidistant
distant time steps. However, telemetry data from marine animals are observed
irregularly. To account for irregular data, a time-irregularised
first-difference correlated random walk model with drift is introduced. The
model generalizes the commonly used first-difference correlated random walk
with regular time steps by allowing irregular time steps, including a drift
term, and by allowing different autocorrelation in the two coordinates. The
model is applied to data from a ringed seal collected through the Argos
satellite system, and is compared to related movement models through
simulations. Accounting for irregular data in the movement model results in
accurate parameter estimates and reconstruction of movement paths. Measured by
distance, the introduced model can provide more accurate movement paths than
the regular time counterpart. Extracting accurate movement paths from uncertain
telemetry data is important for evaluating space use patterns for marine
animals, which in turn is crucial for management. Further, handling irregular
data directly in the movement model allows efficient simultaneous analysis of
several animals.

There is an ever growing need to ensure the quality of food and assess
specific quality parameters in all the links of the food chain, ranging from
processing, distribution and retail to preparing food. Various imaging and
sensing technologies, including X-ray imaging, ultrasound, and near infrared
reflectance spectroscopy have been applied to the problem. Cost and other
constraints restrict the application of some of these technologies. In this
study we test a novel Multiplexing Electric Field Sensor (MEFS), an approach
that allows for a completely non-invasive and non-destructive testing approach.
Our experiments demonstrate the reliable detection of certain foreign objects
and provide evidence that this sensor technology has the capability of
measuring fat content in minced meat. Given the fact that this technology can
already be deployed at very low cost, low maintenance and in various different
form factors, we conclude that this type of MEFS is an extremely promising
technology for addressing specific food quality issues.

Motivation: The scratch assay is a standard experimental protocol used to
characterize cell migration. It can be used to identify genes that regulate
migration and evaluate the efficacy of potential drugs that inhibit cancer
invasion. In these experiments, a scratch is made on a cell monolayer and
recolonisation of the scratched region is imaged to quantify cell migration
rates. A drawback of this methodology is the lack of its reproducibility
resulting in irregular cell-free areas with crooked leading edges. Existing
quantification methods deal poorly with such resulting irregularities present
in the data. Results: We introduce a new quantification method that can analyse
low quality experimental data. By considering in-silico and in-vitro data, we
show that the method provides a more accurate statistical classification of the
migration rates than two established quantification methods. The application of
this method will enable the quantification of migration rates of scratch assay
data previously unsuitable for analysis. Availability and Implementation: The
source code and the implementation of the algorithm as a GUI along with an
example dataset and user instructions, are available in
https://bitbucket.org/anavictoria-ponce/local_migration_quantification_scratch_assays/src/master/.
The datasets are available in
https://ganymed.math.uni-heidelberg.de/~victoria/publications.shtml.

Protein activity is a significant characteristic for recombinant proteins
which can be used as biocatalysts. High activity of proteins reduces the cost
of biocatalysts. A model that can predict protein activity from amino acid
sequence is highly desired, as it aids experimental improvement of proteins.
However, only limited data for protein activity are currently available, which
prevents the development of such models. Since protein activity and solubility
are correlated for some proteins, the publicly available solubility dataset may
be adopted to develop models that can predict protein solubility from sequence.
The models could serve as a tool to indirectly predict protein activity from
sequence. In literature, predicting protein solubility from sequence has been
intensively explored, but the predicted solubility represented in binary values
from all the developed models was not suitable for guiding experimental designs
to improve protein solubility. Here we propose new machine learning models for
improving protein solubility in vivo. We first implemented a novel approach
that predicted protein solubility in continuous numerical values instead of
binary ones. After combing it with various machine learning algorithms, we
achieved a prediction accuracy of 76.28 percent when Support Vector Machine
algorithm was used. Continuous values of solubility are more meaningful in
protein engineering, as they enable researchers to choose proteins with higher
predicted solubility for experimental validation, while binary values fail to
distinguish proteins with the same value. There are only two possible values so
many proteins have the same one.

Accurate gene regulatory networks can be used to explain the emergence of
different phenotypes, disease mechanisms, and other biological functions. Many
methods have been proposed to infer networks from gene expression data but have
been hampered by problems such as low sample size, inaccurate constraints, and
incomplete characterizations of regulatory dynamics. Since expression
regulation is dynamic, time-course data can be used to infer causality, but
these datasets tend to be short or sparsely sampled. In addition, temporal
methods typically assume that the expression of a gene at a time point depends
on the expression of other genes at only the immediately preceding time point,
while other methods include additional time points without any constraints to
account for their temporal distance. These limitations can contribute to
inaccurate networks with many missing and anomalous links.
  We adapted the time-lagged Ordered Lasso, a regularized regression method
with temporal monotonicity constraints, for \textit{de novo} reconstruction. We
also developed a semi-supervised method that embeds prior network information
into the Ordered Lasso to discover novel regulatory dependencies in existing
pathways. We evaluated these approaches on simulated data for a repressilator,
time-course data from past DREAM challenges, and a HeLa cell cycle dataset to
show that they can produce accurate networks subject to the dynamics and
assumptions of the time-lagged Ordered Lasso regression.

Although co/multi-morbidities are associated with significant increase in
mortality, the lack of appropriate quantitative exploratory techniques often
impede their analysis. In the current study, we study the clustering of
multimorbid patients in the Texas patient population. To this end we employ
agglomerative hierarchical clustering to find clusters within the patient
population. The analysis revealed the presence of nine distinct, clinically
relevant clusters of co/multi-morbidities within the study population of
interest. This technique provides a quantitative exploratory analysis of the
co/multi-morbidities present in a specific population.

The objective of this paper is to present the design, construction and
operation of 3 full scale semi-closed, horizontal tubular photobioreactors
(PBR) used to remove nutrients of a mixture of agricultural run-off (90%) and
treated domestic wastewater (10%). The microalgal biomass produced in the PBRs
was harvested in a static lamella settling tank. Each PBR treated in average
2.3 m3/d. PBRs were submitted to strong seasonal changes regarding solar
radiation and temperature, which had a direct impact in the activity of
microalgae and the efficiency of the system. Higher mixed liquor pH values were
registered in summer (daily average> 10). Most of the influent and effluent
nitrogen content was inorganic (average of 9.0 mg N/L and 3.17 mg N/L,
respectively), and in the form of nitrate (62% and 50%, respectively). Average
nitrogen removal efficiency was 65%, with values of around 90% in summer, 80%
in autumn, 50 % in winter and 60% in spring. Most of the influent and effluent
phosphorus content was in the form of ortophosphate. Influent average was 0.62
mg P/L, but with great variations and in a considerable number of samples not
detected. Removal efficiency (when influent values were detected) was very high
during all the study, usually greater than 95%, and there were not clear
seasonal trends for efficiency as observed for TIN. Volumetric biomass
production greatly changed between seasons with much lower values in winter (7
g VSS/m3d) than in summer (43 g VSS/m3d). Biomass separation efficiency of the
settler was very good in either terms of turbidity and total suspended solids,
being most of the time lower than 5 UNT and 15 mg/L, respectively. Overall this
study demonstrated the reliable and good effectiveness of microalgae based
technologies such as the PBR to remove nutrients at a full scale size.

Due to the recent advances in high-throughput sequencing technologies, it
becomes possible to directly analyze microbial communities in the human body
and in the environment. Knowledge of how microbes interact with each other and
form functional communities can provide a solid foundation to understand
microbiome related diseases; this can serve as a key step towards precision
medicine. In order to understand how microbes form communities, we propose a
two step approach: First, we infer the microbial co-occurrence network by
integrating a graph inference algorithm with phylogenetic information obtained
directly from metagenomic data. Next, we utilize a network-based community
detection algorithm to cluster microbes into functional groups where microbes
in each group are highly correlated. We also curate a "gold standard" network
based on the microbe-metabolic relationships which are extracted directly from
the metagenomic data. Utilizing community detection on the resulting microbial
metabolic pathway bipartite graph, the community membership for each microbe
can be viewed as the true label when evaluating against other existing methods.
Overall, our proposed framework Phylogenetic Graphical Lasso (PGLasso)
outperforms existing methods with gains larger than 100% in terms of Adjusted
Rand Index (ARI) which is commonly used to quantify the goodness of
clusterings.

Purpose: subject motion and static field (B$_0$) drift are known to reduce
the quality of single voxel MR spectroscopy data due to incoherent averaging.
Retrospective correction has previously been shown to improve data quality by
adjusting the phase and frequency offset of each average to match a reference
spectrum. In this work, a new method (RATS) is developed to be tolerant to
large frequency shifts (greater than 7Hz) and baseline instability resulting
from inconsistent water suppression. Methods: in contrast to previous
approaches, the variable-projection method and baseline fitting is incorporated
into the correction procedure to improve robustness to fluctuating baseline
signals and optimization instability. RATS is compared to an alternative
method, based on time-domain spectral registration (TDSR), using simulated data
to model frequency, phase and baseline instability. In addition, a J-difference
edited glutathione in-vivo dataset is processed using both approaches and
compared. Results: RATS offers improved accuracy and stability for large
frequency shifts and unstable baselines. Reduced subtraction artifacts are
demonstrated for glutathione edited MRS when using RATS, compared with
uncorrected or TDSR corrected spectra. Conclusion: the RATS algorithm has been
shown to provide accurate retrospective correction of SVS MRS data in the
presence of large frequency shifts and baseline instability. The method is
rapid, generic and therefore readily incorporated into MRS processing pipelines
to improve lineshape, SNR and aid quality assessment.

Data science has emerged from the proliferation of digital data, coupled with
advances in algorithms, software and hardware (e.g., GPU computing).
Innovations in structural biology have been driven by similar factors, spurring
us to ask: can these two fields impact one another in deep and hitherto
unforeseen ways? We posit that the answer is yes. New biological knowledge lies
in the relationships between sequence, structure, function and disease, all of
which play out on the stage of evolution, and data science enables us to
elucidate these relationships at scale. Here, we consider the above question
from the five key pillars of data science: acquisition, engineering, analytics,
visualization and policy, with an emphasis on machine learning as the premier
analytics approach.

Gut microbes play a key role in colorectal carcinogenesis, yet reaching a
consensus on microbial signatures remains a challenge. This is in part due to a
reliance on mean value estimates. We present an extreme value analysis for
overcoming these limitations. By characterizing a power law fit to the relative
abundances of microbes, we capture the same microbial signatures as more
complex meta-analyses. Importantly, we show that our method is robust to the
variations inherent in microbial community profiling and point to future
directions for developing sensitive, reliable analytical methods.

The dynamics of complex systems generally include high-dimensional,
non-stationary and non-linear behavior, all of which pose fundamental
challenges to quantitative understanding. To address these difficulties we
detail a new approach based on local linear models within windows determined
adaptively from the data. While the dynamics within each window are simple,
consisting of exponential decay, growth and oscillations, the collection of
local parameters across all windows provides a principled characterization of
the full time series. To explore the resulting model space, we develop a novel
likelihood-based hierarchical clustering and we examine the eigenvalues of the
linear dynamics. We demonstrate our analysis with the Lorenz system undergoing
stable spiral dynamics and in the standard chaotic regime. Applied to the
posture dynamics of the nematode $C. elegans$ our approach identifies
fine-grained behavioral states and model dynamics which fluctuate close to an
instability boundary, and we detail a bifurcation in a transition from forward
to backward crawling. Finally, we analyze whole-brain imaging in $C. elegans$
and show that the stability of global brain states changes with oxygen
concentration.

Parameter estimation is a major challenge in computational modeling of
biological processes. This is especially the case in image-based modeling where
the inherently quantitative output of the model is measured against image data,
which is typically noisy and non-quantitative. In addition, these models can
have a high computational cost, limiting the number of feasible simulations,
and therefore rendering most traditional parameter estimation methods
unsuitable. In this paper, we present a pipeline that uses Gaussian process
learning to estimate biological parameters from noisy, non-quantitative image
data when the model has a high computational cost. This approach is first
successfully tested on a parametric function with the goal of retrieving the
original parameters. We then apply it to estimating parameters in a biological
setting by fitting artificial in-situ hybridization (ISH) data of the
developing murine limb bud. We expect that this method will be of use in a
variety of modeling scenarios where quantitative data is missing and the use of
standard parameter estimation approaches in biological modeling is prohibited
by the computational cost of the model.

Alzheimer s disease (AD) pathophysiology is still imperfectly understood and
current paradigms have not led to curative outcome. Omics technologies offer
great promises for improving our understanding and generating new hypotheses.
However, integration and interpretation of such data pose major challenges,
calling for adequate knowledge models. AlzPathway is a disease map that gives a
detailed and broad account of AD pathophysiology. However, AlzPathway lacks
formalism, which can lead to ambiguity and misinterpretation. Ontologies are an
adequate framework to overcome this limitation, through their axiomatic
definitions and logical reasoning properties. We introduce the AD Map Ontology
(ADMO) an ontological upper model based on systems biology terms. We then
propose to convert AlzPathway into an ontology and to integrate it into ADMO.
We demonstrate that it allows one to deal with issues related to redundancy,
naming, consistency, process classification and pathway relationships. Further,
it opens opportunities to expand the model using elements from other resources,
such as generic pathways from Reactome or clinical features contained in the
ADO (AD Ontology). A version of the ontology will be made freely available to
the community on Bioportal at the time of the confer-ence.

Models for human running performances of various complexities and underlying
principles have been proposed, often combining data from world record
performances and bio-energetic facts of human physiology. Here we present a
novel, minimal and universal model for human running performance that employs a
relative metabolic power scale. The main component is a self-consistency
relation for the time dependent maximal power output. The analytic approach
presented here is the first to derive the observed logarithmic scaling between
world (and other) record running speeds and times from basic principles of
metabolic power supply. Various female and male record performances (world,
national) and also personal best performances of individual runners for
distances from 800m to the marathon are excellently described by this model,
with mean errors of (often much) less than 1%. The model defines endurance in a
way that demonstrates symmetry between long and short racing events that are
separated by a characteristic time scale comparable to the time over which a
runner can sustain maximal oxygen uptake. As an application of our model, we
derive personalized characteristic race speeds for different durations and
distances.

Cloud computing offers on-demand, scalable computing and storage, and has
become an essential resource for the analyses of big biomedical data. The usual
approach to cloud computing requires users to reserve and provision virtual
servers. An emerging alternative is to have the provider allocate machine
resources dynamically. This type of serverless computing has tremendous
potential for biomedical research in terms of ease-of-use, instantaneous
scalability and cost effectiveness. In our proof of concept example, we
demonstrate how serverless computing provides low cost access to hundreds of
CPUs, on demand, with little or no setup. In particular, we illustrate that the
all-against-all pairwise comparison among all unique human proteins can be
accomplished in approximately 2 minutes, at a cost of less than $1, using
Amazon Web Services Lambda. This is a 250x speedup compared to running the same
task on a typical laptop computer.

Searching for local sequence patterns is one of the basic tasks in
bioinformatics. Sequence patterns might have structural, functional or some
other relevance, and numerous methods have been developed to detect and analyze
them. These methods often depend on the wealth of information already
collected. The explosion in the number of newly available sequences calls for
novel methods to explore local sequence similarity. We have developed a high
sensitivity web-based iterative local similarity scanner, that finds sequence
patterns similar to a submitted query.

Ubiquitous in eukaryotic organisms, the flagellum is a well-studied organelle
that is well-known to be responsible for motility in a variety of organisms.
Commonly necessitated in their study is the capability to image and
subsequently track the movement of one or more flagella using videomicroscopy,
requiring digital isolation and location of the flagellum within a sequence of
frames. Such a process in general currently requires some researcher input,
providing some manual estimate or reliance on an experiment-specific heuristic
to correctly identify and track the motion of a flagellum. Here we present a
fully-automated method of flagellum identification from videomicroscopy based
on the fact that the flagella are of approximately constant width when viewed
by microscopy. We demonstrate the effectiveness of the algorithm by application
to captured videomicroscopy of Leishmania mexicana, a parasitic monoflagellate
of the family Trypanosomatidae. ImageJ Macros for flagellar identification are
provided, and high accuracy and remarkable throughput are achieved via this
unsupervised method, obtaining results comparable in quality to previous
studies of closely-related species but achieved without the need for precursory
measurements or the development of a specialised heuristic, enabling in general
the automated generation of digitised kinematic descriptions of flagellar
beating from videomicroscopy.

Continuous-time models have been developed to capture features of animal
movement across temporal scales. In particular, one popular model is the
continuous-time correlated random walk, in which the velocity of an animal is
formulated as an Ornstein-Uhlenbeck process, to capture the autocorrelation in
the speed and direction of its movement. In telemetry analyses, discrete-time
state-switching models (such as hidden Markov models) have been increasingly
popular to identify behavioural phases from animal tracking data. We propose a
multistate formulation of the continuous-time correlated random walk, with an
underlying Markov process used as a proxy for the animal's behavioural state
process. We present a Markov chain Monte Carlo algorithm to carry out Bayesian
inference for this multistate continuous-time model. Posterior samples of the
hidden state sequence, of the state transition rates, and of the
state-dependent movement parameters can be obtained. We investigate the
performance of the method in a simulation study, and we illustrate its use in a
case study of grey seal (Halichoerus grypus) tracking data. The method we
present makes use of the state-space model formulation of the continuous-time
correlated random walk, and can accommodate irregular sampling frequency and
measurement error. It will facilitate the use of continuous-time models to
estimate movement characteristics and infer behavioural states from animal
telemetry data.

Unveiling pathological brain changes associated with Alzheimer's disease (AD)
is a challenging task especially that people do not show symptoms of dementia
until it is late. Over the past years, neuroimaging techniques paved the way
for computer-based diagnosis and prognosis to facilitate the automation of
medical decision support and help clinicians identify cognitively intact
subjects that are at high-risk of developing AD. As a progressive
neurodegenerative disorder, researchers investigated how AD affects the brain
using different approaches: 1) image-based methods where mainly neuroimaging
modalities are used to provide early AD biomarkers, and 2) network-based
methods which focus on functional and structural brain connectivities to give
insights into how AD alters brain wiring. In this study, we reviewed
neuroimaging-based technical methods developed for AD and mild-cognitive
impairment (MCI) classification and prediction tasks, selected by screening all
MICCAI proceedings published between 2010 and 2016. We included papers that fit
into image-based or network-based categories. The majority of papers focused on
classifying MCI vs. AD brain states, which has enabled the discovery of
discriminative or altered brain regions and connections. However, very few
works aimed to predict MCI progression based on early neuroimaging-based
observations. Despite the high importance of reliably identifying which early
MCI patient will convert to AD, remain stable or reverse to normal over
months/years, predictive models are still lagging behind.

The volume tensor provides robust estimate of object shape and orientation in
space. The tensor is estimated from 3D data set by the Fakir probe, an
interactive method using intersections of the objects boundary with a virtual
lines. The method thus can be applied to objects that cannot be segmented
automatically. Marking the intersections instead of segmenting the whole object
reduces the workload required for obtaining sufficiently precise results. We
present theoretical results on the variance of estimate of integrals by
systematic sampling that enable calculation of the shape estimate precision. To
demonstrate the ability of Fakir technique, we measure the changes in shape and
orientation of pheasant brain compartments during development.

The majority of the proteins encoded in the genomes of eukaryotes contain
more than one domain. Reasons for high prevalence of multi-domain proteins in
various organisms have been attributed to higher stability and functional and
folding advantages over single-domain proteins. Despite these advantages, many
proteins are composed of only one domain while their homologous domains are
part of multi-domain proteins. In the study presented here, differences in the
properties of protein domains in single-domain and multi-domain systems and
their influence on functions are discussed. We studied 20 pairs of identical
protein domains, which were crystallized in two forms (a) tethered to other
proteins domains and (b) tethered to fewer protein domains than (a) or not
tethered to any protein domain. Results suggest that tethering of domains in
multi-domain proteins influences the structural, dynamic and energetic
properties of the constituent protein domains. 50% of the protein domain pairs
show significant structural deviations while 90% of the protein domain pairs
show differences in dynamics and 12% of the residues show differences in the
energetics. To gain further insights on the influence of tethering on the
function of the domains, 4 pairs of homologous protein domains, where one of
them is a full-length single-domain protein and the other protein domain is a
part of a multi-domain protein, were studied. Analyses showed that identical
and structurally equivalent functional residues show differential dynamics in
homologous protein domains, though comparable dynamics between in-silico
generated chimera protein and multi-domain proteins were observed. From these
observations, the differences observed in the functions of homologous proteins
could be attributed to the presence of tethered domain. Overall, we conclude
that tethered domains in multi-domain proteins not only provide stability or
folding advantages but also influence pathways resulting in differences in
function or regulatory properties.

In movement ecology, the few works that have taken collective behaviour into
account are data-driven and rely on simplistic theoretical assumptions, relying
in metrics that may or may not be measuring what is intended. In the present
paper, we focus on pairwise joint-movement behaviour, where individuals move
together during at least a segment of their path. We investigate the adequacy
of twelve metrics introduced in previous works for assessing joint movement by
analysing their theoretical properties and confronting them with contrasting
case scenarios. Three criteria are taken into account for review of those
metrics: 1) practical use, 2) dependence on parameters and underlying
assumptions, and 3) computational cost. When analysing the similarities between
the metrics as defined, we show how some of them can be expressed using general
mathematical forms. In addition, we evaluate the ability of each metric to
assess specific aspects of joint-movement behaviour: proximity (closeness in
space-time) and coordination (synchrony) in direction and speed. We found that
some metrics are better suited to assess proximity and others are more
sensitive to coordination. To help readers choose metrics, we elaborate a
graphical representation of the metrics in the coordination and proximity space
based on our results, and give a few examples of proximity and coordination
focus in different movement studies.

Alignment of structural RNAs is an important problem with a wide range of
applications. Since function is often determined by molecular structure, RNA
alignment programs should take into account both sequence and base-pairing
information for structural homology identi^Lcation. A number of successful
alignment programs are heuristic versions of Sanko^K's optimal algorithm. Most
of them require O(n4) run time. This paper describes C++ software,
RNAmountAlign, for RNA sequence/structure alignment that runs in O(n3) time and
O(n2) space; moreover, our software returns a p-value (transformable to expect
value E) based on Karlin-Altschul statistics for local alignment, as well as
parameter ^Ltting for local and global alignment. Using incremental mountain
height, a representation of structural information computable in cubic time,
RNAmountAlign implements quadratic time pairwise local, global and
global/semiglobal (query search) alignment using a weighted combination of
sequence and structural similarity. RNAmountAlign is capable of performing
progressive multiple alignment as well. Benchmarking of RNAmountAlign against
LocARNA, LARA, FOLDALIGN, DYNALIGN and STRAL shows that RNAmountAlign has
reasonably good accuracy and much faster run time supporting all alignment
types.

Background: Disulfide bonds are crucial to protein structural formation.
Developing an effective method topredict disulfide bonding formation is
important for protein structural modeling and functional study. Mostcurrent
methods still have shortcomings, including low accuracy and strict requirements
for the selection ofdiscriminative features. Results: In this study, we
introduced a nested structure of Bidirectional Long-short Term
Memory(BLSTM)neural network called Global-Local-BLSTM (GL-BLSTM) for disulfide
bonding state prediction. Based on thepatterns of disulfide bond formation, a
BLSTM network called Local-BLSTM is used to extract context-basedfeatures
around every Cys residue. Another BLSTM network called Global-BLSTM is
introduced aboveLocal-BLSTM layer to integrate context-based features of all
Cys residues in the same protein chain, therebyinvolving inter-residue
relationships in the training process. According to our experimental
results,GL-BLSTMnetwork performs much better than other methods, GL-BLSTM
reached 90.26% accuracy at residue-level and 83.66% at protein-level. This
model has reached the state of the art in this field. Conclusion: GL-BLSTMs
special structure and mechanisms are beneficial to disulfide bonding state
prediction.By applying bidirectional LSTM, it can extract context based
features by processing protein sequences, therebyobtain more discriminative
information than traditional machine learning methods. Whats more,
GL-BLSTMsspecial two-layer structure enables it to extract both local and
global features, in which global features areplaying important roles in
improving prediction accuracy, especially at protein-level. Keywords: Disulfide
bonds; Prediction; Deep learning; Bidirectional long short term memory

P-values and null-hypothesis significance testing are popular data-analytical
tools in functional neuroimaging. Sparked by the analysis of resting-state fMRI
data, there has been a resurgence of interest in the validity of some of the
p-values evaluated with the widely used software SPM in recent years. The
default parametric p-values reported in SPM are based on the application of
results from random field theory to statistical parametric maps, a framework
commonly referred to as RFT. While RFT was established two decades ago and has
since been applied in a plethora of fMRI studies, there does not exist a
unified documentation of the mathematical and computational underpinnings of
RFT as implemented in current versions of SPM. Here, we provide such a
documentation with the aim of contributing to contemporary efforts towards
higher levels of computational transparency in functional neuroimaging.

Mathematical methods of information theory constitute essential tools to
describe how stimuli are encoded in activities of signaling effectors.
Exploring the information-theoretic perspective, however, remains conceptually,
experimentally and computationally challenging. Specifically, existing
computational tools enable efficient analysis of relatively simple systems,
usually with one input and output only. Moreover, their robust and readily
applicable implementations are missing. Here, we propose a novel algorithm to
analyze signaling data within the framework of information theory. Our approach
enables robust as well as statistically and computationally efficient analysis
of signaling systems with high-dimensional outputs and a large number of input
values. Analysis of the NF-kB single - cell signaling responses to TNF-a
uniquely reveals that the NF-kB signaling dynamics improves discrimination of
high concentrations of TNF-a with a modest impact on discrimination of low
concentrations. Our readily applicable R-package, SLEMI - statistical learning
based estimation of mutual information, allows the approach to be used by
computational biologists with only elementary knowledge of information theory.

This tutorial introduces participants to the design and implementation of an
agent-based model using NetLogo through one of two different projects:
modelling T cell movement within a lymph node or modelling the progress of a
viral infection in an in vitro cell culture monolayer. Each project is broken
into a series of incremental steps of increasing complexity. Each step is
described in detail and the code to type in is initially provided. However,
each project has room to grow in complexity and biological realism so
participants are encouraged to expand their project beyond the scope of the
tutorial or to develop a project of their own.

Gut microbial composition has been linked to multiple health outcomes. Yet,
temporal analysis of this composition had been limited to deterministic models.
In this paper, we introduce a probabilistic model for the dynamics of
intestinal microbiomes that takes into account interaction among bacteria as
well as external effects such as antibiotics. The model successfully deals with
pragmatic issues such as random measurement error and varying time intervals
between measurements through latent space modeling. We demonstrate utility of
the model by using latent state features to predict the clinical events of
intestinal domination and bacteremia, improving accuracy over existing methods.
We further leverage this framework to validate known links between antibiotics
and clinical outcomes, while discovering new ones.

Statistical and mathematical modeling are crucial to describe, interpret,
compare and predict the behavior of complex biological systems including the
organization of hematopoietic stem and progenitor cells in the bone marrow
environment. The current prominence of high-resolution and live-cell imaging
data provides an unprecedented opportunity to study the spatiotemporal dynamics
of these cells within their stem cell niche and learn more about aberrant, but
also unperturbed, normal hematopoiesis. However, this requires careful
quantitative statistical analysis of the spatial and temporal behavior of cells
and the interaction with their microenvironment. Moreover, such quantification
is a prerequisite for the construction of hypothesis-driven mathematical models
that can provide mechanistic explanations by generating spatiotemporal dynamics
that can be directly compared to experimental observations. Here, we provide a
brief overview of statistical methods in analyzing spatial distribution of
cells, cell motility, cell shapes and cellular genealogies. We also describe
cell- based modeling formalisms that allow researchers to simulate emergent
behavior in a multicellular system based on a set of hypothesized mechanisms.
Together, these methods provide a quantitative workflow for the analytic and
synthetic study of the spatiotemporal behavior of hematopoietic stem and
progenitor cells.

A rapid, cost-effective and easy method that allows on-site determination of
the concentration of live and dead bacterial cells using a fibre-based
spectroscopic device (the optrode system) is proposed and demonstrated.
Identification of live and dead bacteria was achieved by using the commercially
available dyes SYTO 9 and propidium iodide, and fluorescence spectra were
measured by the optrode. Three spectral processing methods were evaluated for
their effectiveness in predicting the original bacterial concentration in the
samples: principal components regression (PCR), partial least squares
regression (PLSR) and support vector regression (SVR). Without any sample
pre-concentration, PCR achieved the most reliable results. It was able to
quantify live bacteria from $10^{8}$ down to $10^{6.2}$ bacteria/mL and showed
the potential to detect as low as $10^{5.7}$ bacteria/mL. Meanwhile,
enumeration of dead bacteria using PCR was achieved between $10^{8}$ and
$10^{7}$ bacteria/mL. The general procedures described in this article can be
applied or modified for the enumeration of bacteria within populations stained
with fluorescent dyes. The optrode is a promising device for the enumeration of
live and dead bacterial populations particularly where rapid, on-site
measurement and analysis is required.

Missing values are largely inevitable in gene expression microarray studies.
Data sets often have significant omissions due to individuals dropping out of
experiments, errors in data collection, image corruptions, and so on. Missing
data could potentially undermine the validity of research results - leading to
inaccurate predictive models and misleading conclusions. Imputation - a
relatively flexible, general purpose approach towards dealing with missing data
- is now available in massive numbers, making it possible to handle missing
data. While these estimation methods are becoming increasingly more effective
in resolving the discrepancies between true and estimated values, its effect on
clustering outcomes is largely disregarded.
  This study seeks to reveal the vast differences in agglomerative hierarchal
clustering outcomes estimation methods can construct in comparison to the
precision exhibited (presented through the cophenetic correlation coefficient)
in comparison to their high efficiency and effectivity in value preservation of
true and imputed values (presented through the root-mean-squared error). We
argue against the traditional approach towards the development of imputation
methods and instead, advocate towards methods that reproduce a data set's
original, natural cluster.
  By using a number of advanced imputation methods, we reveal extensive
differences between original and reconstructed clusters that could
significantly transform the interpretations of the data as a whole.

Many biological, psychological and economic experiments have been designed
where an organism or individual must choose between two options that have the
same expected reward but differ in the variance of reward received. In this
way, designed empirical approaches have been developed for evaluating risk
preferences. Here, however, we show that if the experimental subject is
inferring the reward distribution (to optimize some process), they will never
agree in finite time that the expected rewards are equal. In turn, we argue
that this makes discussions of risk preferences, and indeed the motivations of
behaviour, not so simple or straightforward to interpret. We use this
particular experiment to highlight the serious need to consider the frame of
reference of the experimental subject in studies of behaviour.

We introduce Ordinal Synchronization ($OS$) as a new measure to quantify
synchronization between dynamical systems. $OS$ is calculated from the
extraction of the ordinal patterns related to two time series, their
transformation into $D$-dimensional ordinal vectors and the adequate
quantification of their alignment. $OS$ provides a fast and robust-to noise
tool to assess synchronization without any implicit assumption about the
distribution of data sets nor their dynamical properties, capturing in-phase
and anti-phase synchronization. Furthermore, varying the length of the ordinal
vectors required to compute $OS$ it is possible to detect synchronization at
different time scales. We test the performance of $OS$ with data sets coming
from unidirectionally coupled electronic Lorenz oscillators and brain imaging
datasets obtained from magnetoencephalographic recordings, comparing the
performance of $OS$ with other classical metrics that quantify synchronization
between dynamical systems.

If Electronic Health Records contain a large amount of information about the
patients condition and response to treatment, which can potentially
revolutionize the clinical practice, such information is seldom considered due
to the complexity of its extraction and analysis. We here report on a first
integration of an NLP framework for the analysis of clinical records of lung
cancer patients making use of a telephone assistance service of a major Spanish
hospital. We specifically show how some relevant data, about patient
demographics and health condition, can be extracted; and how some relevant
analyses can be performed, aimed at improving the usefulness of the service. We
thus demonstrate that the use of EHR texts, and their integration inside a data
analysis framework, is technically feasible and worth of further study.

RKappa is a framework for the development, simulation and analysis of
rule-base models within the mature statistically empowered R environment. It is
designed for model editing, parameter identification, simulation, sensitivity
analysis and visualisation. The framework is optimised for high-performance
computing platforms and facilitates analysis of large-scale systems biology
models where knowledge of exact mechanisms is limited and parameter values are
uncertain. The RKappa software is an open source (GLP3 license) package for R,
which is freely available online ( https://github.com/lptolik/R4Kappa ).

BioNetFit is a software tool designed for solving parameter identification
problems that arise in the development of rule-based models. It solves these
problems through curve fitting (i.e., nonlinear regression). BioNetFit is
compatible with deterministic and stochastic simulators that accept BioNetGen
language (BNGL)-formatted files as inputs, such those available within the
BioNetGen framework. BioNetFit can be used on a laptop or standalone multicore
workstation as well as on many Linux clusters, such as those that use the Slurm
Workload Manager to schedule jobs. BioNetFit implements a metaheuristic
population-based global optimization procedure, an evolutionary algorithm (EA),
to minimize a user-defined objective function, such as a residual sum of
squares (RSS) function. BioNetFit also implements a bootstrapping procedure for
determining confidence intervals for parameter estimates. Here, we provide
step-by-step instructions for using BioNetFit to estimate the values of
parameters of a BNGL-encoded model and to define bootstrap confidence
intervals. The process entails the use of several plain-text files, which are
processed by BioNetFit and BioNetGen. In general, these files include 1) one or
more EXP files, which each contains (experimental) data to be used in parameter
identification/bootstrapping; 2) a BNGL file containing a model section, which
defines a (rule-based) model, and an actions section, which defines simulation
protocols that generate GDAT and/or SCAN files with model predictions
corresponding to the data in the EXP file(s); and 3) a CONF file that
configures the fitting/bootstrapping job and that defines algorithmic parameter
settings.

This chapter provides a brief introduction to the theory and practice of
spatial stochastic simulations. It begins with an overview of different methods
available for biochemical simulations highlighting their strengths and
limitations. Spatial stochastic modeling approaches are indicated when
diffusion is relatively slow and spatial inhomogeneities involve relatively
small numbers of particles. The popular software package MCell allows
particle-based stochastic simulations of biochemical systems in complex three
dimensional (3D) geometries, which are important for many cell biology
applications. Here, we provide an overview of the simulation algorithms used by
MCell and the underlying theory. We then give a tutorial on building and
simulating MCell models using the CellBlender graphical user interface, that is
built as a plug-in to Blender, a widely-used and freely available software
platform for 3D modeling. The tutorial starts with simple models that
demonstrate basic MCell functionality and then advances to a number of more
complex examples that demonstrate a range of features and provide examples of
important biophysical effects that require spatially-resolved stochastic
dynamics to capture.

Focused Ion Beam Scanning Electron Microscope (FIB-SEM) imaging is a
technique that image materials section-by-section at nano-resolution, e.g.,5
nanometer width voxels. FIB-SEM is well suited for imaging ultrastructures in
cells. Unfortunately, typical setups will introduce a slight sub-pixel
translation from section to section typically referred to as drift. Over
multiple sections, drift compound to skew distance measures and geometric
structures significantly from the pre-imaged stage. Popular correction
approaches often involve standard image registration methods available in
packages such as ImageJ or similar software. These methods transform the images
to maximize the similarity between consecutive two-dimensional sections under
some measure. We show how these standard approaches will both significantly
underestimate the drift, as well as producing biased corrections as they tend
to align the images such that the normal of planar biological structures are
perpendicular to the sectioning direction causing poor or incorrect correction
of the images. In this paper, we present a highly accurate correction method
for estimating drift in isotropic electron microscope images with visible
vesicles.

The drive for reproducibility in the computational sciences has provoked
discussion and effort across a broad range of perspectives: technological,
legislative/policy, education, and publishing. Discussion on these topics is
not new, but the need to adopt standards for reproducibility of claims made
based on computational results is now clear to researchers, publishers and
policymakers alike. Many technologies exist to support and promote reproduction
of computational results: containerisation tools like Docker, literate
programming approaches such as Sweave, knitr, iPython or cloud environments
like Amazon Web Services. But these technologies are tied to specific
programming languages (e.g. Sweave/knitr to R; iPython to Python) or to
platforms (e.g. Docker for 64-bit Linux environments only). To date, no single
approach is able to span the broad range of technologies and platforms
represented in computational biology and biotechnology.
  To enable reproducibility across computational biology, we demonstrate an
approach and provide a set of tools that is suitable for all computational work
and is not tied to a particular programming language or platform. We present
published examples from a series of papers in different areas of computational
biology, spanning the major languages and technologies in the field
(Python/R/MATLAB/Fortran/C/Java). Our approach produces a transparent and
flexible process for replication and recomputation of results. Ultimately, its
most valuable aspect is the decoupling of methods in computational biology from
their implementation. Separating the 'how' (method) of a publication from the
'where' (implementation) promotes genuinely open science and benefits the
scientific community as a whole.

In this work we study the characteristics of the heart rate variability (HRV)
as a function of age and gender. The analyzed data include previous results
reported in the literature. The data obtained in this work expand the range of
age studied until now revealing new behaviors not reported before. We analyze
some measurements in the time domain,in the frequency domain and nonlinear
measurements. We report scaling behaviors and abrupt changes in some
measurements. There is also a progressive decrease in the dimensionality of the
dynamic system governing the HRV, with the increase in age that is interpreted
in terms ofautonomic regulation of cardiac activity.

This paper addresses the problem of reconstructing the motion trajectories of
the individuals in a large collection of flying objects using two temporally
synchronized and geometrically calibrated cameras. The 3D trajectory
reconstruction problem involves two challenging tasks - stereo matching and
temporal tracking. Existing methods separate the two and process them one at a
time sequentially, and suffer from frequent irresolvable ambiguities in stereo
matching and in tracking. We unify the two tasks, and propose an optimization
approach to solving stereo matching and temporal tracking simultaneously. It
treats 3D trajectory acquisition problem as selecting appropriate stereo
correspondence out of all possible ones for each object via minimizing a cost
function. Experiment results show that the proposed method offers significant
performance advantage over existing approaches. The proposed method has
successfully been applied to reconstruct 3D motion trajectories of hundreds of
simultaneously flying fruit flies (Drosophila Melanogaster), which could
facilitate the study the insect's collective behavior.

Optogenetics is a revolutionary new field of biotechnology, achieving optical
control over biological functions in living cells by genetically inserting
light sensitive proteins into cellular signaling pathways. Applications of
optogenetic switches are expanding rapidly, but the technique is hampered by
spectral cross-talk: the broad absorption spectra of compatible biochemical
chromophores limits the number of switches that can be independently controlled
and restricts the dynamic range of each switch. In the present work we develop
and implement a non-linear optical photoswitching capability, Stimulated
Depletion Quenching (SDQ), is used to overcome spectral cross-talk by
exploiting the molecules' unique dynamic response to ultrashort laser pulses.
SDQ is employed to enhance the control of Cph8, a photo-reversible phytochrome
based optogenetic switch designed to control gene expression in E. Coli
bacteria. The Cph8 switch can not be fully converted to it's biologically
inactive state ($P_{FR}$) by linear photos-witching, as spectral cross-talk
causes a reverse photoswitching reaction to revert to it back to the active
state ($P_{R}$). SDQ selectively halts this reverse reaction while allowing the
forward reaction to proceed. The results of this proof of concept experiment
lay the foundation for future experiments that will use optimal pulse shaping
to further enhance control of Cph8 and enable simultaneous, multiplexed control
of multiple optogenetic switches.

The interaction of the actin cytoskeleton with cell-substrate adhesions is
necessary for cell migration. While the trajectories of motile cells have a
stochastic character, investigations of cell motility mechanisms rarely
elaborate on the origins of the observed randomness. Here, guided by a few
fundamental attributes of cell motility, we construct a minimal stochastic cell
migration model from ground-up. The resulting model couples a deterministic
actomyosin contractility mechanism with stochastic cell-substrate adhesion
kinetics, and yields a well-defined piecewise deterministic process. Numerical
simulations reproduce several experimentally observed results, including
anomalous diffusion, tactic migration, and contact guidance. This work provides
a basis for the development of cell-cell collision and population migration
models.

We consider the problem of estimating an unbiased and reference-free ab-inito
model for non-symmetric molecules from images generated by single-particle
cryo-electron microscopy. The proposed algorithm finds the globally optimal
assignment of orientations that simultaneously respects all common lines
between all images. The contribution of each common line to the estimated
orientations is weighted according to a statistical model for common lines'
detection errors. The key property of the proposed algorithm is that it finds
the global optimum for the orientations given the common lines. In particular,
any local optima in the common lines energy landscape do not affect the
proposed algorithm. As a result, it is applicable to thousands of images at
once, very robust to noise, completely reference free, and not biased towards
any initial model. A byproduct of the algorithm is a set of measures that allow
to asses the reliability of the obtained ab-initio model. We demonstrate the
algorithm using class averages from two experimental data sets, resulting in
ab-initio models with resolutions of 20A or better, even from class averages
consisting of as few as three raw images per class.

Nitrous oxide (N2O) is a potent greenhouse gas emitted during biological
wastewater treatment. A pseudo-mechanistic model describing three biological
pathways for nitric oxide (NO) and N2O production was calibrated for mixed
culture biomass from an activated sludge process using laboratory-scale
experiments. The model (NDHA) comprehensively describes N2O producing pathways
by both autotrophic ammonium oxidizing bacteria and heterotrophic bacteria.
Extant respirometric assays and anaerobic batch experiments were designed to
calibrate endogenous and exogenous processes (heterotrophic denitrification and
autotrophic ammonium/nitrite oxidation) together with the associated net N2O
production. Ten parameters describing heterotrophic processes and seven for
autotrophic processes were accurately estimated (variance/mean < 25%). The
model predicted NO and N2O dynamics at varying dissolved oxygen, ammonium and
nitrite levels and was validated against an independent set of experiments with
the same biomass. Aerobic ammonium oxidation experiments at two oxygen levels
used for model evaluation (2 and 0.5 mg/L) indicated that both the nitrifier
denitrification (42, 64%) and heterotrophic denitrification (7, 17%) pathways
increased and dominated N2O production at high nitrite and low oxygen
concentrations; while the nitrifier nitrification pathway showed the largest
contribution at high dissolved oxygen levels (51, 19%). The uncertainty of the
biological parameter estimates was propagated to N2O model outputs via Monte
Carlo simulations as 95% confidence intervals. The accuracy of the estimated
parameters resulted in a low uncertainty of the N2O emission factors (4.6 +-
0.6% and 1.2 +- 0.1%).

Seed priming is one of the well-established and low cost method to improve
seed germination properties, productivity, and stress tolerance in different
crops. It is a pre-germination treatment that partially hydrates the seed and
allows controlled imbibition. This stimulates and induces initial germination
process, but prevents radicle emergence. Consequently, treated seeds are
fortified with enhanced germination characteristics, improved physiological
parameters, uniformity in growth, and improved capability to cope up with
different biotic and abiotic stresses. Existing techniques for evaluating the
effectiveness of seed priming suffer from several drawbacks, including very
high operating time, indirect and destructive analysis, bulky experimental
arrangement, high cost, and require extensive analytical expertise. To
circumvent these drawbacks, we propose a biospeckle based technique to analyse
the effects of different priming treatments on germination characteristics of
seeds. The study employs non-primed (T0) and priming treatments (T1-T75),
including hydropriming and chemical priming (using three chemical agents namely
sodium chloride, potassium nitrate, and urea) for different time durations and
solution concentrations. The results conclusively establish biospeckle analysis
as an efficient active tool for seed priming analysis. Furthermore, the
proposed setup is extremely simple, low-cost, involves non-mechanical scanning
and is highly stable.

This work considers the method of uniformisation for continuous-time Markov
chains in the context of chemical reaction networks. Previous work in the
literature has shown that uniformisation can be beneficial in the context of
time-inhomogeneous models, such as chemical reaction networks incorporating
extrinsic noise. This paper lays focus on the understanding of uniformisation
from the viewpoint of sample paths of chemical reaction networks. In
particular, an efficient pathwise stochastic simulation algorithm for
time-homogeneous models is presented which is complexity-wise equal to
Gillespie's direct method. This new approach therefore enlarges the class of
problems for which the uniformisation approach forms a computationally
attractive choice. Furthermore, as a new application of the uniformisation
method, we provide a novel variance reduction method for (raw) moment
estimators of chemical reaction networks based upon the combination of
stratification and uniformisation.

Video analysis is currently the main non-intrusive method for the study of
collective behavior. However, 3D-to-2D projection leads to overlapping of
observed objects. The situation is further complicated by the absence of stall
shapes for the majority of living objects. Fortunately, living objects often
possess a certain symmetry which was used as a basis for morphological
fingerprinting. This technique allowed us to record forms of symmetrical
objects in a pose-invariant way. When combined with image skeletonization, this
gives a robust, nonlinear, optimization-free, and fast method for detection of
overlapping objects, even without any rigid pattern. This novel method was
verified on fish (European bass, Dicentrarchus labrax, and tiger barbs, Puntius
tetrazona) swimming in a reasonably small tank, which forced them to exhibit a
large variety of shapes. Compared with manual detection, the correct number of
objects was determined for up to almost $90 \%$ of overlaps, and the mean
Dice-Sorensen coefficient was around $0.83$. This implies that this method is
feasible in real-life applications such as toxicity testing.

Reliable determination of sensory thresholds is the holy grail of signal
detection theory. However, there exists no gold standard for the estimation of
thresholds based on neurophysiological parameters, although a reliable
estimation method is crucial for both scientific investigations and clinical
diagnosis. Whenever it is impossible to communicate with the subjects, as in
studies with animals or neonatales, thresholds have to be derived from neural
recordings. In such cases when the threshold is estimated based on neuronal
measures, the standard approach is still the subjective setting of the
threshold to the value where at least a "clear" neuronal signal is detectable.
These measures are highly subjective, strongly depend on the noise, and
fluctuate due to the low signal-to-noise ratio near the threshold. Here we show
a novel method to reliably estimate physiological thresholds based on
neurophysiological parameters. Using surrogate data, we demonstrate that
fitting the responses to different stimulus intensities with a hard sigmoid
function, in combination with subsampling, provides a robust threshold value as
well as an accurate uncertainty estimate. This method has no systematic
dependence on the noise and does not even require samples in the full dynamic
range of the sensory system. It is universally applicable to all types of
sensory systems, ranging from somatosensory stimulus processing in the cortex
to auditory processing in the brain stem.

Diffusion kurtosis imaging (DKI), is an imaging modality that yields novel
disease biomarkers and in combination with nervous tissue modeling, provides
access to microstructural parameters. Recently, DKI and subsequent estimation
of microstructural model parameters has been used for assessment of tissue
changes in neurodegenerative diseases and their animal models. In this study,
mouse spinal cords from the experimental autoimmune encephalomyelitis (EAE)
model of multiple sclerosis (MS) were investigated for the first time using DKI
in combination with biophysical modeling to study the relationship between
microstructural metrics and degree of animal dysfunction. Thirteen spinal cords
were extracted from animals of variable disability and scanned in a high-field
MRI scanner along with five control specimen. Diffusion weighted data were
acquired together with high resolution T2* images. Diffusion data were fit to
estimate diffusion and kurtosis tensors and white matter modeling parameters,
which were all used for subsequent statistical analysis using a linear mixed
effects model. T2* images were used to delineate focal
demyelination/inflammation. Our results unveil a strong relationship between
disability and measured microstructural parameters in normal appearing white
matter and gray matter. The changes we found in biophysical modeling parameters
and in particular in extra-axonal axial diffusivity were clearly different from
previous studies employing other animal models of MS. In conclusion, our data
suggest that DKI and microstructural modeling can provide a unique contrast
capable of detecting EAE-specific changes correlating with clinical disability.
These findings could close the gap between MRI findings and clinical
presentation in patients and deepen our understanding of EAE and the MS
mechanisms.

Persistent homology has been applied to brain network analysis for finding
the shape of brain networks across multiple thresholds. In the persistent
homology, the shape of networks is often quantified by the sequence of
$k$-dimensional holes and Betti numbers.The Betti numbers are more widely used
than holes themselves in topological brain network analysis. However, the holes
show the local connectivity of networks, and they can be very informative
features in analysis. In this study, we propose a new method of measuring
network differences based on the dissimilarity measure of harmonic holes (HHs).
The HHs, which represent the substructure of brain networks, are extracted by
the Hodge Laplacian of brain networks. We also find the most contributed HHs to
the network difference based on the HH dissimilarity. We applied our proposed
method to clustering the networks of 4 groups, normal control (NC), stable and
progressive mild cognitive impairment (sMCI and pMCI), and Alzheimer's disease
(AD). The results showed that the clustering performance of the proposed method
was better than that of network distances based on only the global change of
topology.

Optogenetics is a rapidly growing field of biotechnology, potentially
allowing a deeper understanding and control of complex biological networks. The
major challenge is the multiplexed control of several optogenetic components in
the presence of significant spectral cross talk. We propose and demonstrate
through simulations a new control approach of Stimulated Depletion Quenching.
This approach is applied to the phytochrome Cph8 bidirectional optogenetic
switch, and the results show significant improvement of its dynamic range.

Protein solubility plays a critical role in improving production yield of
recombinant proteins in biocatalyst and pharmaceutical field. To some extent,
protein solubility can represent the function and activity of biocatalysts
which are mainly composed of recombinant proteins. Highly soluble proteins are
more effective in biocatalytic processes and can reduce the cost of
biocatalysts. Screening proteins by experiments in vivo is time-consuming and
expensive. In literature, large amounts of machine learning models have been
investigated, whereas parameters of those models are underdetermined with
insufficient data of protein solubility. A data augmentation algorithm that can
enlarge the dataset of protein solubility and improve the performance of
prediction model is highly desired, which can alleviate the common issue of
insufficient data in biotechnology applications for developing machine learning
models. We first implemented a novel approach that a data augmentation
algorithm, conditional WGAN was used to improve prediction performance of DNN
for protein solubility from protein sequence by generating artificial data.
After adding mimic data produced from conditional WGAN, the prediction
performance represented by $R^{2}$ was improved compared with the $R^{2}$
without data augmentation. After tuning the hyperparameters of two algorithms
and organizing the dataset, we achieved a $R^{2}$ value of $45.04\%$, which
enhanced $R^{2}$ about $10\%$ compared with the previous study using the same
dataset. Data augmentation opens the door to applications of machine learning
models on biological data, as machine learning models always fail to be well
trained by small datasets.

Over the last decades, honeybees have been a fascinating model to study
insect navigation. While there is some controversy about the complexity of
underlying neural correlates, the research of honeybee navigation makes
progress through both the analysis of flight behavior and the synthesis of
agent models. Since visual cues are believed to play a crucial role for the
behavioral output of a navigating bee we have developed a realistic
3-dimensional virtual world, in which simulated agents can be tested, or in
which the visual input of experimentally traced animals can be reconstructed.
In this paper we present implementation details on how we reconstructed a large
3-dimensional world from aerial imagery of one of our field sites, how the
distribution of ommatidia and their view geometry was modeled, and how the
system samples from the scene to obtain realistic bee views. This system is
made available as an open-source project to the community on
\url{http://github.com/bioroboticslab/bee_view}.

Approximate Bayesian Computation is widely used to infer the parameters of
discrete-state continuous-time Markov networks. In this work, we focus on
models that are governed by the Chemical Master Equation (the CME). Whilst
originally designed to model biochemical reactions, CME-based models are now
frequently used to describe a wide range of biological phenomena
mathematically. We describe and implement an efficient multi-level ABC method
for investigating model parameters. In short, we generate sample paths of
CME-based models with varying time resolutions. We start by generating
low-resolution sample paths, which require only limited computational resources
to construct. Those sample paths that compare well with experimental data are
selected, and the temporal resolutions of the chosen sample paths are
recursively increased. Those sample paths unlikely to aid in parameter
inference are discarded at an early stage, leading to an optimal use of
computational resources. The efficacy of the multi-level ABC is demonstrated
through two case studies.

Quantification of sulfated polysaccharides in urine samples is relevant to
pharmacokinetic studies in drug development projects and to the non-invasive
diagnosis and therapy monitoring of mucopolysaccharidoses. The Heparin Red Kit
is a particularly simple and user friendly fluorescence assay for the
quantification of sulfated polysaccharides and has recently emerged as a novel
tool for the monitoring of their blood levels during pharmacokinetic studies in
clinical trials. The standard protocol for the blood plasma matrix is, however,
not suited for urine samples due to matrix interference. The present study
identifies inorganic sulfate as the interfering component in urine. The sulfate
level of urine is typically 1-2 orders of magnitude higher compared with the
blood plasma level. Addition of either hydrochloric acid or magnesium chloride
counteracts sulfate interference but still enables sensitive detection of
sulfated polysaccharides such as heparin, heparan sulfate and dermatan sulfate
at low microgramm per milliliter levels. This study extends the application
range of the Heparin Red Kit by a simple modification of the assay protocol to
the direct quantification of various sulfated polysaccharides in human urine.

Identification of nearly all proteins in a system using data-dependent
acquisition (DDA) mass spectrometry has become routine for simple organisms,
such as bacteria and yeast. Still, quantification of the identified proteins
may be a complex process and require multiple different software packages. This
protocol describes identification and label-free quantification of proteins
from bottom-up proteomics experiments. This method can be used to quantify all
the detectable proteins in any DDA dataset collected with high-resolution
precursor scans. This protocol may be used to quantify proteome remodeling in
response to a drug treatment or a gene knockout. Notably, the method uses the
latest and fastest freely-available software, and the entire protocol can be
completed in a few hours with data from organisms with relatively small
genomes, such as yeast or bacteria.

Movement-based indices such as moves per minute (MPM) and proportion time
moving (PTM) are common methodologies to quantify foraging behaviour. Hundreds
of studies have reported these indices without specifying the temporal
resolution of their original data, despite the likelihood that the minimal stop
and move durations can affect MPM and PTM estimates. Our goal was to
empirically determine the sensitivity of these foraging indices to changes in
the temporal resolution of the observation. We used a high speed camera to
record movement sequences of 20 Acanthodactylus boskianus lizards. We gradually
decreased the data resolution by ignoring short stops and either ignoring,
elongating or leaving short moves unchanged. We then used the manipulated data
to calculate the foraging indices at different temporal resolutions. We found
that movement-based indices are very sensitive to the observation resolution,
so that realistic variation in the minimal duration of stops and moves could
lead to 68 percent and 48 percent difference in MPM and PTM estimates,
respectively. When using the highest resolution, our estimate of MPM was an
order of magnitude higher than all prior reported values for lizards. Also, the
distribution of stop durations was well described by a single heavy tailed
distribution above 0.35 seconds. This suggests that for A. boskianus there is
no reason to ignore short stops above this threshold. Our results raise major
concerns regarding the use of already published movement based indices, and
enable us to recommend how new foraging data should be collected.

Several scalp EEG functional connectivity studies, mostly clinical, seem to
overlook the reference electrode impact. The subsequent interpretation of brain
connectivity is thus often biased by the choice a non-neutral reference. This
study aims at systematically investigating these effects. As EEG reference, we
examined: the vertex electrode (Cz), the digitally linked mastoids (DLM), the
average reference (AVE), and the Reference Electrode Standardization Technique
(REST). As a connectivity metric, we used the imaginary part of coherency. We
tested simulated and real data (eyes open resting state), by evaluating the
influence of electrode density, effect of head model accuracy in the REST
transformation, and impact on the characterization of the topology of
functional networks from graph analysis. Simulations demonstrated that REST
significantly reduced the distortion of connectivity patterns when compared to
AVE, Cz and DLM references. Moreover, the availability of high-density EEG
systems and an accurate knowledge of the head model are crucial elements to
improve REST performance. For real data, a systematic change of the spatial
pattern of functional connectivity depending on the chosen reference was also
observed. The distortion of connectivity patterns was larger for the Cz
reference, and progressively decreases when using the DLM, the AVE, the REST.
Strikingly, we also showed that network attributes derived from graph analysis,
i.e., node degree and local efficiency, are significantly influenced by the EEG
reference choice. Overall, this study highlights that significant differences
arise in scalp EEG functional connectivity and graph network properties, in
dependence of the chosen reference. We hope our study will convey the message
that caution should be taken when interpreting and comparing results obtained
from different laboratories when using different reference schemes.

Synthetic DNA can in principle be used for the archival storage of arbitrary
data. Because errors are introduced during DNA synthesis, storage, and
sequencing, an error-correcting code (ECC) is necessary for error-free recovery
of the data. Previous work has utilized ECCs that can correct substitution
errors, but not insertion or deletion errors (indels), instead relying on
sequencing depth and multiple alignment to detect and correct indels -- in
effect an inefficient multiple-repetition code. This paper describes an ECC,
termed "HEDGES", that corrects simultaneously for substitutions, insertions,
and deletions in a single read. Varying code rates allow for correction of up
to ~10% nucleotide errors and achieve 50% or better of the estimated Shannon
limit.

Recent clinical trials have shown that the adaptive drug therapy can be more
efficient than a standard MTD-based policy in treatment of cancer patients. The
adaptive therapy paradigm is not based on a preset schedule; instead, the doses
are administered based on the current state of tumor. But the adaptive
treatment policies examined so far have been largely ad hoc. In this paper we
propose a method for systematically optimizing the rules of adaptive policies
based on an Evolutionary Game Theory model of cancer dynamics. Given a set of
treatment objectives, we use the framework of dynamic programming to find the
optimal treatment strategies. In particular, we optimize the total drug usage
and time to recovery by solving a Hamilton-Jacobi-Bellman equation based on a
mathematical model of tumor evolution. We compare adaptive/optimal treatment
strategy with MTD-based treatment policy. We show that optimal treatment
strategies can dramatically decrease the total amount of drugs prescribed as
well as increase the fraction of initial tumour states from which the recovery
is possible. We also examine the optimization trade-offs between the total
administered drugs and recovery time. The adaptive therapy combined with
optimal control theory is a promising concept in the cancer treatment and
should be integrated into clinical trial design.

Radiotherapy plays a vital role in cancer treatment, for which accurate
prognosis is important for guiding sequential treatment and improving the
curative effect for patients. An issue of great significance in radiotherapy is
to assess tumor radiosensitivity for devising the optimal treatment strategy.
Previous studies focused on gene expression in cells closely associated with
radiosensitivity, but factors such as the response of a cancer patient to
irradiation and the patient survival time are largely ignored. For clinical
cancer treatment, a specific pre-treatment indicator taking into account cancer
cell type and patient radiosensitivity is of great value but it has been
missing. Here, we propose an effective indicator for radiosensitivity:
radiosensitive gene group centrality (RSGGC), which characterizes the
importance of the group of genes that are radiosensitive in the whole gene
correlation network. We demonstrate, using both clinical patient data and
experimental cancer cell lines, which RSGGC can provide a quantitative estimate
of the effect of radiotherapy, with factors such as the patient survival time
and the survived fraction of cancer cell lines under radiotherapy fully taken
into account. Our main finding is that, for patients with a higher RSGGC score
before radiotherapy, cancer treatment tends to be more effective. The RSGGC can
have significant applications in clinical prognosis, serving as a key measure
to classifying radiosensitive and radioresistant patients.

Epileptic seizures detection and forecasting is nowadays widely recognized as
a problem of great significance and social resonance, and still remains an
open, grand challenge. Furthermore, the development of mobile warning systems
and wearable, non invasive, advisory devices are increasingly and strongly
requested, from the patient community and their families and also from
institutional stakeholders. According to the many recent studies, exploiting
machine learning capabilities upon intracranial EEG (iEEG), in this work we
investigate a combination of novel game theory dynamical model on networks for
brain electrical activity and nonlinear time series analysis based on
recurrences quantification. These two methods are then melted together within a
supervised learning scheme and finally, prediction performances are assessed
using EEG scalp datasets, specifically recorded for this study. Our study
achieved mean sensitivity of 70.9% and a mean time in warning of 20.3%, thus
showing an increase of the improvement over chance metric from 42%, reported in
the most recent study, to 50.5%. Moreover, the real time implementation of the
proposed approach is currently under development on a prototype of a wearable
device.

Epicormic branches arise from dormant buds patterned during the growth of
previous years. Dormant epicormic buds remain on the surface of trees, pushed
outward from the pith during secondary growth, but maintaining vascular
connections. Epicormic buds can be reactivated, either through natural
processes or intentionally, to rejuvenate orchards and control tree
architecture. Because epicormic structures are embedded within secondary
growth, tomographic approaches are a useful method to study them and understand
their development.
  We apply techniques from image processing to determine the locations of
epicormic vascular traces embedded within secondary growth of sweet cherry
(Prunus avium L.), revealing the juvenile phyllotactic pattern in the trunk of
an adult tree. Techniques include breadth-first search to find the pith of the
tree, edge detection to approximate the radius, and a conversion to polar
coordinates to threshold and segment phyllotactic features. Intensity values
from Magnetic Resonance Imaging (MRI) of the trunk are projected onto the
surface of a perfect cylinder to find the locations of traces in the "boundary
image". Mathematical phyllotaxy provides a means to capture the patterns in the
boundary image by modeling phyllotactic parameters. Our cherry tree specimen
has the conspicuous parastichy pair $(2,3)$, phyllotactic fraction 2/5, and
divergence angle of approximately 143 degrees.
  The methods described not only provide a framework to study phyllotaxy, but
for image processing of volumetric image data in plants. Our results have
practical implications for orchard rejuvenation and directed approaches to
influence tree architecture. The study of epicormic structures, which are
hidden within secondary growth, using tomographic methods also opens the
possibility of studying the genetic and environmental basis of such structures.

Potts statistical models have become a popular and promising way to analyze
mutational covariation in protein Multiple Sequence Alignments (MSAs) in order
to understand protein structure, function and fitness. But the statistical
limitations of these models, which can have millions of parameters and are fit
to MSAs of only thousands or hundreds of effective sequences using a procedure
known as inverse Ising inference, are incompletely understood. In this work we
predict how model quality degrades as a function of the number of sequences
$N$, sequence length $L$, amino-acid alphabet size $q$, and the degree of
conservation of the MSA, in different applications of the Potts models: In
"fitness" predictions of individual protein sequences, in predictions of the
effects of single-point mutations, in "double mutant cycle" predictions of
epistasis, and in 3-d contact prediction in protein structure. We show how as
MSA depth $N$ decreases an "overfitting" effect occurs such that sequences in
the training MSA have overestimated fitness, and we predict the magnitude of
this effect and discuss how regularization can help correct for it, use a
regularization procedure motivated by statistical analysis of the effects of
finite sampling. We find that as $N$ decreases the quality of point-mutation
effect predictions degrade least, fitness and epistasis predictions degrade
more rapidly, and contact predictions are most affected. However, overfitting
becomes negligible for MSA depths of more than a few thousand effective
sequences, as often used in practice, and regularization becomes less
necessary. We discuss the implications of these results for users of Potts
covariation analysis.

Observability is a modelling property that describes the possibility of
inferring the internal state of a system from observations of its output. A
related property, structural identifiability, refers to the theoretical
possibility of determining the parameter values from the output. In fact,
structural identifiability becomes a particular case of observability if the
parameters are considered as constant state variables. It is possible to
simultaneously analyse the observability and structural identifiability of a
model using the conceptual tools of differential geometry. Many complex
biological processes can be described by systems of nonlinear ordinary
differential equations, and can therefore be analysed with this approach. The
purpose of this review article is threefold: (I) to serve as a tutorial on
observability and structural identifiability of nonlinear systems, using the
differential geometry approach for their analysis; (II) to review recent
advances in the field; and (III) to identify open problems and suggest new
avenues for research in this area.

Colon cancer is the second leading cause of cancer-related death in the
United States of America. Its prognosis has significantly improved with the
advancement of targeted therapies based on underlying molecular changes. The
KRAS mutation is one of the most frequent molecular alterations seen in colon
cancer and its presence can affect treatment selection. We attempted to use
Apple machine learning algorithms to diagnose colon cancer and predict the KRAS
mutation status from histopathological images. We captured 250 colon cancer
images and 250 benign colon tissue images. Half of colon cancer images were
captured from KRAS mutation-positive tumors and another half from KRAS
mutation-negative tumors. Next, we created Image Classifier Model using Apple
CreateML machine learning module. The trained and validated model was able to
successfully differentiate between colon cancer and benign colon tissue images
with 98 % recall and 98 % precision. However, our model failed to reliably
identify KRAS mutations, with the highest realized accuracy of 66 %. Although
not yet perfected, in the near future Apple CreateML modules can be used in
diagnostic smartphone-based applications and potentially alleviate shortages of
medical professionals in understaffed parts of the world.

Models of coupled oscillators are useful in describing a wide variety of
phenomena in physics, biology and economics. These models typically rest on the
premise that the oscillators are weakly coupled, meaning that amplitudes can be
assumed to be constant and dynamics can therefore be described purely in terms
of phase differences. Whilst mathematically convenient, the restrictive nature
of the weak coupling assumption can limit the explanatory power of these
phase-coupled oscillator models. We therefore propose an extension to the
weakly-coupled oscillator model that incorporates both amplitude and phase as
dependent variables. We use the bilinear neuronal state equations of dynamic
causal modelling as a foundation in deriving coupled differential equations
that describe the activity of both weakly and strongly coupled oscillators. We
show that weakly-coupled oscillator models are inadequate in describing the
processes underlying the temporally variable signals observed in a variety of
systems. We demonstrate that phase-coupled models perform well on simulations
of weakly coupled systems but fail when connectivity is no longer weak. On the
other hand, using Bayesian model selection, we show that our phase-amplitude
coupling model can describe non-weakly coupled systems more effectively despite
the added complexity associated with using amplitude as an extra dependent
variable. We demonstrate the advantage of our phase-amplitude model in the
context of model-generated data, as well as of a simulation of inter-connected
pendula, neural local field potential recordings in rodents under anaesthesia
and international economic gross domestic product data.

Traumatic brain injury (TBI) is a complex injury that is hard to predict and
diagnose, with many studies focused on associating head kinematics to brain
injury risk. Recently, there has been a push towards using computationally
expensive finite element (FE) models of the brain to create tissue deformation
metrics of brain injury. Here, we developed a 3 degree-of-freedom
lumped-parameter brain model, built based on the measured natural frequencies
of a FE brain model simulated with live human impact data, to be used to
rapidly estimate peak brain strains experienced during head rotational
accelerations. On our dataset, the simplified model correlates with peak
principal FE strain by an R2 of 0.80. Further, coronal and axial model
displacement correlated with fiber-oriented peak strain in the corpus callosum
with an R2 of 0.77. Using the maximum displacement predicted by our brain
model, we propose an injury criteria and compare it against a number of
existing rotational and translational kinematic injury metrics on a dataset of
head kinematics from 27 clinically diagnosed injuries and 887 non-injuries. We
found that our proposed metric performed comparably to peak angular
acceleration, linear acceleration, and angular velocity in classifying injury
and non-injury events. Metrics which separated time traces into their
directional components had improved deviance to those which combined components
into a single time trace magnitude. Our brain model can be used in future work
as a computationally efficient alternative to FE models for classifying
injuries over a wide range of loading conditions.

Microchip electrokinetic methods are capable of increasing the sensitivity of
molecular assays by enriching and purifying target analytes. However, their use
is currently limited to assays that can be performed under a high external
electric field, as spatial separation and focusing is lost when the electric
field is removed. We present a novel method that uses two-phase encapsulation
to overcome this limitation. The method uses passive filling and pinning of an
oil phase in hydrophobic channels to encapsulate electrokinetically separated
and focused analytes with a brief pressure pulse. The resulting encapsulated
sample droplet maintains its concentration over long periods of time without
requiring an electric field and can be manipulated for further analysis, either
on- or off- chip. We demonstrate the method by encapsulating DNA
oligonucleotides in a 240 pL aqueous segment after isotachophoresis (ITP)
focusing, and show that the concentration remains at 60% of the initial value
for tens of minutes, a 22-fold increase over free diffusion after 20 minutes.
Furthermore, we demonstrate manipulation of a single droplet by selectively
encapsulating amplicon after ITP purification from a polymerase chain reaction
(PCR) mix, and performing parallel off-chip detection reactions using the
droplet. We provide geometrical design guidelines for devices implementing the
encapsulation method, and show how the method can be scaled to multiple analyte
zones.

Across diverse biological systems -- ranging from neural networks to
intracellular signaling and genetic regulatory networks -- the information
about changes in the environment is frequently encoded in the full temporal
dynamics of the network nodes. A pressing data-analysis challenge has thus been
to efficiently estimate the amount of information that these dynamics convey
from experimental data. Here we develop and evaluate decoding-based estimation
methods to lower bound the mutual information about a finite set of inputs,
encoded in single-cell high-dimensional time series data. For biological
reaction networks governed by the chemical Master equation, we derive
model-based information approximations and analytical upper bounds, against
which we benchmark our proposed model-free decoding estimators. In contrast to
the frequently-used k-nearest-neighbor estimator, decoding-based estimators
robustly extract a large fraction of the available information from
high-dimensional trajectories with a realistic number of data samples. We apply
these estimators to previously published data on Erk and Ca signaling in
mammalian cells and to yeast stress-response, and find that substantial amount
of information about environmental state can be encoded by non-trivial response
statistics even in stationary signals. We argue that these single-cell,
decoding-based information estimates, rather than the commonly-used tests for
significant differences between selected population response statistics,
provide a proper and unbiased measure for the performance of biological
signaling networks.

Determining the best model or models for a particular data set, a process
known as Bayesian model comparison, is a critical part of probabilistic
inference. Typically, this process assumes a fixed model-space (that is, a
fixed set of candidate models). However, it is also possible to perform
Bayesian inference over model-spaces themselves, thus determining which spaces
provide the best explanation for observed data. Model-space inference (MSI)
allows the effective exclusion of poorly performing models (a process analogous
to Automatic Relevance Detection), and thus mitigates against the well-known
phenomenon of model dilution, resulting in posterior probability estimates that
are, on average, more accurate than those produced when using a fixed
model-space. We focus on model comparison in the context of multiple
independent data sets (as produced, for example, by multi-subject behavioural
or neuroimaging studies), and cast our proposal as a development of
random-effects Bayesian Model Selection, the current state-of-the-art in the
field. We demonstrate the increased accuracy of MSI using simulated behavioural
and neuroimaging data, as well as by assessing predictive performance in
previously-acquired empirical data. Additionally, we explore other applications
of MSI, including formal testing for a diversity of models within a population,
and comparison of model-spaces between populations. Our approach thus provides
an important new tool for model comparison.

Microtubules are inherently dynamic sub-cellular filamentuous polymers that
are spatially organized within the cell by motor proteins which cross-link and
move microtubules. In-vitro microtubule motility assays, in which motors
attached to a surface move microtubules along it, have been used traditionally
to study motor function. However, the way in which microtubule-microtubule
interactions affect microtubule movement remains largely unexplored. To address
this question, time-lapse image series of in-vitro microtubule motility assays
were obtained using total internal reflection fluorescence (TIRF) microscopy.
Categorized as a general problem of multiple object tracking (MOT), particular
challenges arising in this project include low feature diversity, dynamic
instability, sudden changes in microtubules motility patterns, as well as their
instantaneous appearance/disappearance. This work describes a new application
of piecewise-stationary multiple motion model Kalman smoother (PMMS) for
modeling individual microtubules motility trends. To both evaluate the
capability of this procedure and optimize its hyper-parameters, a large dataset
simulating the series of time-lapse images was used first. Next, we applied it
to the sequence of frames from the real data. Results of our analyses provide a
quantitative description of microtubule velocity which, in turn, enumerates the
occurrence of microtubule-microtubule interactions per frame.

In this review we make the statement that hybrid models in oncology are
required as a mean for enhanced data integration. In the context of systems
oncology, experimental and clinical data need to be at the heart of the models
developments from conception to validation to ensure a relevant use of the
models in the clinical context. The main applications pursued are to improve
diagnosis and to optimize therapies.We first present the Successes achieved
thanks to hybrid modelling approaches to advance knowledge, treatments or drug
discovery. Then we present the Challenges than need to be addressed to allow
for a better integration of the model parts and of the data into the models.
And Finally, the Hopes with a focus towards making personalised medicine a
reality. Mathematics Subject Classification. 35Q92, 68U20, 68T05, 92-08, 92B05.

Point 1: Shape characterizers are metrics that quantify aspects of the
overall geometry of a 3D digital surface. When computed for biological objects,
the values of a shape characterizer are largely independent of homology
interpretations and often contain a strong ecological and functional signal.
Thus shape characterizers are useful for understanding evolutionary processes.
Dirichlet Normal Energy (DNE) is a widely used shape characterizer in
morphological studies.
  Point 2: Recent studies found that DNE is sensitive to various procedures for
preparing 3D mesh from raw scan data, raising concerns regarding comparability
and objectivity when utilizing DNE in morphological research. We provide a
robustly implemented algorithm for computing the Dirichlet energy of the normal
(ariaDNE) on 3D meshes.
  Point 3: We show through simulation that the effects of preparation-related
mesh surface attributes such as triangle count, mesh representation, noise,
smoothing and boundary triangles are much more limited on ariaDNE than DNE.
Furthermore, ariaDNE retains the potential of DNE for biological studies,
illustrated by its effectiveness in differentiating species by dietary
preferences.
  Point 4: Use of ariaDNE can dramatically enhance assessment of ecological
aspects of morphological variation by its stability under different 3D model
acquisition methods and preparation procedure. Towards this goal, we provide
scripts for computing ariaDNE and ariaDNE values for specimens used in
previously published DNE analyses.

Fetal heart rate variability (fHRV) is an important indicator of health and
disease, yet its physiological origins, neural contributions in particular, are
not well understood. We aimed to develop novel experimental and data analytical
approaches to identify fHRV measures reflecting the vagus nerve contributions
to fHRV. In near-term ovine fetuses, a comprehensive set of 46 fHRV measures
was computed from fetal pre-cordial electrocardiogram recorded during surgery
and 72 hours later without (n=24) and with intra-surgical bilateral cervical
vagotomy (n=15). The fetal heart rate did not change due to vagotomy. We
identify fHRV measures specific to the vagal modulation of fHRV: Multiscale
time irreversibility asymmetry index (AsymI), Detrended fluctuation analysis
(DFA) alpha1, Kullback-Leibler permutation entropy (KLPE) and Scale dependent
Lyapunov exponent slope (SDLE alpha). We provide a systematic delineation of
vagal contributions to fHRV across signal-analytical domains which should be
relevant for the emerging field of bioelectronic medicine and the deciphering
of the vagus code. Our findings also have clinical significance for in utero
monitoring of fetal health during surgery.

Calcium (Ca2+) signalling is one of the most important mechanisms of
information propagation in the body. In embryogenesis the interplay between
Ca2+ signalling and mechanical forces is critical to the healthy development of
an embryo but poorly understood. Several types of embryonic cells exhibit
calcium-induced contractions and many experiments indicate that Ca2+ signals
and contractions are coupled via a two-way mechanochemical coupling. We present
a new analysis of experimental data that supports the existence of this
coupling during Apical Constriction in Neural Tube Closure. We then propose a
mechanochemical model, building on early models that couple Ca2+ dynamics to
cell mechanics and replace the bistable Ca2+ release with modern,
experimentally validated Ca2+ dynamics. We assume that the cell is a linear
viscoelastic material and model the Ca2+-induced contraction stress with a Hill
function saturating at high Ca2+ levels. We also express, for the first time,
the "stretch-activation" Ca2+ flux in the early mechanochemical models as a
bottom-up contribution from stretch-sensitive Ca2+ channels on the cell
membrane. We reduce the model to three ordinary differential equations and
analyse its bifurcation structure semi-analytically as the $IP_3$
concentration, and the "strength" of stretch activation, $\lambda$ vary. The
Ca2+ system ($\lambda=0$, no mechanics) exhibits relaxation oscillations for a
certain range of $IP_3$ values. As $\lambda$ is increased the range of $IP_3$
values decreases, the oscillation amplitude decreases and the frequency
increases. Oscillations vanish for a sufficiently high value of $\lambda$.
These results agree with experiments in embryonic cells that also link the loss
of Ca2+ oscillations to embryo abnormalities. The work addresses a very
important and understudied question on the coupling of chemical and mechanical
signalling in embryogenesis.

The fluorescence spectra of bacterial samples stained with SYTO 9 and
propidium iodide (PI) were used to monitor bacterial viability. Stained
mixtures of live and dead Escherichia coli with proportions of live:dead cells
varying from 0 to 100% were measured using the optrode, a cost effective and
convenient fibre-based spectroscopic device. We demonstrated several approaches
to obtaining the proportions of live:dead E. coli in a mixture of both live and
dead, from analyses of the fluorescence spectra collected by the optrode. To
find a suitable technique for predicting the percentage of live bacteria in a
sample, four analysis methods were assessed and compared: SYTO 9:PI
fluorescence intensity ratio, an adjusted fluorescence intensity ratio,
single-spectrum support vector regression (SVR) and multi-spectra SVR. Of the
four analysis methods, multi-spectra SVR obtained the most reliable results and
was able to predict the percentage of live bacteria in 10^8 bacteria/mL samples
between c. 7% and 100% live, and in 10^7 bacteria/mL samples between c. 7% and
73% live. By demonstrating the use of multi-spectra SVR and the optrode to
monitor E. coli viability, we raise points of consideration for spectroscopic
analysis of SYTO 9 and PI and aim to lay the foundation for future work that
use similar methods for different bacterial species.

BACKGROUND Indicators of relative inequality of lifespans are important
because they capture the dimensionless shape of aging. They are markers of
inequality at the population level and express the uncertainty at the time of
death at the individual level. In particular, Keyfitz' entropy $\bar{H}$
represents the elasticity of life expectancy to a change in mortality and it
has been used as an indicator of lifespan variation. However, it is unknown how
this measure changes over time and whether a threshold age exists, as it does
for other lifespan variation indicators.
  RESULTS The time derivative of $\bar{H}$ can be decomposed into changes in
life disparity $e^\dagger$ and life expectancy at birth $e_o$. Likewise,
changes over time in $\bar{H}$ are a weighted average of age-specific rates of
mortality improvements. These weights reflect the sensitivity of $\bar{H}$ and
show how mortality improvements can increase (or decrease) the relative
inequality of lifespans. Further, we prove that $\bar{H}$, as well as
$e^\dagger$, in the case that mortality is reduced in every age, has a
threshold age below which saving lives reduces entropy, whereas improvements
above that age increase entropy.
  CONTRIBUTION We give a formal expression for changes over time of $\bar{H}$
and provide a formal proof of the threshold age that separates reductions and
increases in lifespan inequality from age-specific mortality improvements.

Diseases involve complex processes and modifications to the cellular
machinery. The gene expression profile of the affected cells contains
characteristic patterns linked to a disease. Hence, biological knowledge
pertaining to a disease can be derived from a patient cell's profile, improving
our diagnosis ability, as well as our grasp of disease risks. This knowledge
can be used for drug re-purposing, or by physicians to evaluate a patient's
condition and co-morbidity risk. Here, we look at differential gene expression
obtained from microarray technology for patients diagnosed with various
diseases. Based on this data and cellular multi-scale organization, we aim to
uncover disease--disease links, as well as disease-gene and disease--pathways
associations. We propose neural networks with structures inspired by the
multi-scale organization of a cell. We show that these models are able to
correctly predict the diagnosis for the majority of the patients. Through the
analysis of the trained models, we predict and validate disease-disease,
disease-pathway, and disease-gene associations with comparisons to known
interactions and literature search, proposing putative explanations for the
novel predictions that come from our study.

Models of misfolded proteins (MP) aim at discovering the bio-mechanical
propagation properties of neurological diseases (ND) by identifying plausible
associated dynamical systems. Solving these systems along the full disease
trajectory is usually challenging, due to the lack of a well defined time axis
for the pathology. This issue is addressed by disease progression models (DPM)
where long-term progression trajectories are estimated via time
reparametrization of individual observations. However, due to their loose
assumptions on the dynamics, DPM do not provide insights on the bio-mechanical
properties of MP propagation. Here we propose a unified model of
spatio-temporal protein dynamics based on the joint estimation of long-term MP
dynamics and time reparameterization of individuals observations. The model is
expressed within a Gaussian Process (GP) regression setting, where constraints
on the MP dynamics are imposed through non--linear dynamical systems. We use
stochastic variational inference on both GP and dynamical system parameters for
scalable inference and uncertainty quantification of the trajectories.
Experiments on simulated data show that our model accurately recovers
prescribed rates along graph dynamics and precisely reconstructs the underlying
progression. When applied to brain imaging data our model allows the
bio-mechanical interpretation of amyloid deposition in Alzheimer's disease,
leading to plausible simulations of MP propagation, and achieving accurate
predictions of individual MP deposition in unseen data.

This postdoctoral thesis starts by reviewing the historic development of
airplane structures and high lift devices from an engineering point of view.
However, the main purpose of this document is the development of a novel
concept for shape changing, gapless high lift devices that is inspired by the
nastic movement of plants. A particular focus is put on the efficient
simulation and optimization of compliant pressure actuated cellular structures.

Recent advances in electron microscopy have enabled the imaging of single
cells in 3D at nanometer length scale resolutions. An uncharted frontier for in
silico biology is the ability to simulate cellular processes using these
observed geometries. Enabling such simulations requires watertight meshing of
electron micrograph images into 3D volume meshes, which can then form the basis
of computer simulations of such processes using numerical techniques such as
the Finite Element Method. In this paper, we describe the use of our recently
rewritten mesh processing software, GAMer 2, to bridge the gap between poorly
conditioned meshes generated from segmented micrographs and boundary marked
tetrahedral meshes which are compatible with simulation. We demonstrate the
application of a workflow using GAMer 2 to a series of electron micrographs of
neuronal dendrite morphology explored at three different length scales and show
that the resulting meshes are suitable for finite element simulations. This
work is an important step towards making physical simulations of biological
processes in realistic geometries routine. Innovations in algorithms to
reconstruct and simulate cellular length scale phenomena based on emerging
structural data will enable realistic physical models and advance discovery at
the interface of geometry and cellular processes. We posit that a new frontier
at the intersection of computational technologies and single cell biology is
now open.

Background. Most surgical procedures involve structures deeper than the skin.
However, the difference in surgical noxious stimulation between skin incision
and laparoscopic trocar insertion is unknown. By analyzing instantaneous heart
rate (IHR) calculated from the electrocardiogram, in particular the transient
bradycardia in response to surgical stimuli, this study investigates surgical
noxious stimuli arising from skin incision and laparoscopic trocar insertion.
Methods. Thirty-five patients undergoing laparoscopic cholecystectomy were
enrolled in this prospective observational study. Sequential surgical steps
including umbilical skin incision (11 mm), umbilical trocar insertion (11 mm),
xiphoid skin incision (5 mm), xiphoid trocar insertion (5 mm), subcostal skin
incision (3 mm), and subcostal trocar insertion (3 mm) were investigated. IHR
was derived from electrocardiography and calculated by the modern time-varying
power spectrum. Similar to the classical heart rate variability analysis, the
time-varying low frequency power (tvLF), time-varying high frequency power
(tvHF), and tvLF-to-tvHF ratio (tvLHR) were calculated. Prediction probability
(PK) analysis and global pointwise F-test were used to compare the performance
between indices and the heart rate readings from the patient monitor. Results.
Analysis of IHR showed that surgical stimulus elicits a transient bradycardia,
followed by the increase of heart rate. Transient bradycardia is more
significant in trocar insertion than skin incision. The IHR change quantifies
differential responses to different surgical intensity. Serial PK analysis
demonstrates de-sensitization in skin incision, but not in laparoscopic trocar
insertion. Conclusions. Quantitative indices present the transient bradycardia
introduced by noxious stimulation. The results indicate different effects
between skin incision and trocar insertion.

Neuroanatomical segmentation in magnetic resonance imaging (MRI) of the brain
is a prerequisite for volume, thickness and shape measurements. This work
introduces a new highly accurate and versatile method based on 3D convolutional
neural networks for the automatic segmentation of neuroanatomy in T1-weighted
MRI. In combination with a deep 3D fully convolutional architecture, efficient
linear registration-derived spatial priors are used to incorporate additional
spatial context into the network. An aggressive data augmentation scheme using
random elastic deformations is also used to regularize the networks, allowing
for excellent performance even in cases where only limited labelled training
data are available. Applied to hippocampus segmentation in an elderly
population (mean Dice coefficient = 92.1%) and sub-cortical segmentation in a
healthy adult population (mean Dice coefficient = 89.5%), we demonstrate new
state-of-the-art accuracies and a high robustness to outliers with the same
architecture. Further validation on a multi-structure segmentation task in a
scan-rescan dataset demonstrates accuracy (mean Dice coefficient = 86.6%)
similar to the scan-rescan reliability of expert manual segmentations (mean
Dice coefficient = 86.9%), and improved reliability compared to both expert
manual segmentations and automated segmentations using FIRST. Furthermore, our
method maintains a highly competitive runtime performance (e.g. requiring only
10 seconds for left/right hippocampal segmentation in 1x1x1 MNI stereotaxic
space), orders of magnitude faster than conventional multi-atlas segmentation
methods.

Darwin is a genomics co-processor that achieved a 15000x acceleration on long
read assembly through innovative hardware and algorithm co-design. Darwins
algorithms and hardware implementation were specifically designed for DNA
analysis pipelines. This paper analyzes the feasibility of applying Darwins
algorithms to the problem of protein sequence alignment. In addition to a
behavioral analysis of Darwin when aligning proteins, we propose an algorithmic
improvement to Darwins alignment algorithm, GACT, in the form of a multi-pass
variant that increases its accuracy on protein sequence alignment. Concretely,
our proposed multi-pass variant of GACT achieves on average 14\% better
alignment scores.

Key processes in biological and chemical systems are described by networks of
chemical reactions. From molecular biology to biotechnology applications,
computational models of reaction networks are used extensively to elucidate
their non-linear dynamics. Model dynamics are crucially dependent on parameter
values which are often estimated from observations. Over past decade, the
interest in parameter and state estimation in models of (bio-)chemical reaction
networks (BRNs) grew considerably. Statistical inference problems are also
encountered in many other tasks including model calibration, discrimination,
identifiability and checking as well as optimum experiment design, sensitivity
analysis, bifurcation analysis and other. The aim of this review paper is to
explore developments of past decade to understand what BRN models are commonly
used in literature, and for what inference tasks and inference methods. Initial
collection of about 700 publications excluding books in computational biology
and chemistry were screened to select over 260 research papers and 20 graduate
theses concerning estimation problems in BRNs. The paper selection was
performed as text mining using scripts to automate search for relevant keywords
and terms. The outcome are tables revealing the level of interest in different
inference tasks and methods for given models in literature as well as recent
trends. In addition, a brief survey of general estimation strategies is
provided to facilitate understanding of estimation methods which are used for
BRNs. Our findings indicate that many combinations of models, tasks and methods
are still relatively sparse representing new research opportunities to explore
those that have not been considered - perhaps for a good reason. The paper
concludes by discussing future research directions including research problems
which cannot be directly deduced from presented tables.

Heart rate variability studies depend on the robust calculation of the
tachogram, the heart rate times series, usually by the detection of R peaks in
the electrocardiogram (ECG). ECGs however are subject to a number of sources of
noise which are difficult to filter and therefore reduce the tachogram
accuracy. We describe a pipeline for fast calculation of tachograms from noisy
ECGs of several hours' length. The pipeline consists of three stages. A neural
network (NN) trained to detect R peaks and distinguish these from noise; a
measure to robustly detect false positives (FPs) and negatives (FNs) produced
by the NN; a simple "alarm" algorithm for automatically removing FPs and
interpolating FNs. In addition, we introduce the approach of encoding ECGs,
tachograms and other cardiac time series in the form of raster images, which
greatly speeds and eases their visual inspection and analysis.

Lead is a naturally-occurring element. It has been known to man for a long
time, and it is one of the longest established poisons. The current consensus
is that no level of lead exposure should be deemed "safe." New evidence
regarding the blood levels at which morbidities occur has prompted the CDC to
reduce the screening guideline of 10 $\mu$g/dl to 2 $\mu$g/dl. Measurable
cognitive decline (reduced IQ, academic deficits) have been found to occur at
levels below 10mg/dl.
  Knowledge of lead pharmacology allows us to better understand its absorption
and metabolization, mechanisms that produce its medical consequences. Based
upon an original and very simplified compartmental model of Rabinowitz (1973)
with only three major compartments (blood, bone and soft tissue), extensive
biophysical models sprouted over the following two decades. However, none of
these models have been specifically designed to use new knowledge of lead
molecular dynamics to understand its deleterious effects on the brain. We build
and analyze a compartmental model of lead pharmacokinetics, focused
specifically on addressing neurotoxicity. We use traditional phase space
methods, parameter sensitivity analysis and bifurcation theory to study the
transitions in the system's behavior in response to various physiological
parameters.
  We conclude that modeling the complex interaction of lead and calcium along
their dynamic trajectory may successfully explain counter-intuitive effects on
systemic function and neural behavior which could not be addressed by existing
linear models. Our results encourage further efforts towards using nonlinear
phenomenology in conjunction with empirically driven system parameters, to
obtain a biophysical model able to provide clinical assessments and
predictions.

A perturbed gut microbiome has recently been linked with multiple disease
processes, yet researchers currently lack tools that can provide in vivo,
quantitative, and real-time insight into these processes and associated
host-microbe interactions. We propose an in vivo wireless implant for
monitoring gastrointestinal tract redox states using oxidation-reduction
potentials (ORP). The implant is powered and conveniently interrogated via
ultrasonic waves. We engineer the sensor electronics, electrodes, and
encapsulation materials for robustness in vivo, and integrate them into an
implant that endures autoclave sterilization and measures ORP for 12 days
implanted in the cecum of a live rat. The presented implant platform paves the
way for long-term experimental testing of biological hypotheses, offering new
opportunities for understanding gut redox pathophysiology mechanisms, and
facilitating translation to disease diagnosis and treatment applications.

Timing features such as the silence gaps between vocal units -- inter-call
intervals (ICIs) -- often correlate with biological information such as context
or genetic information. Such correlates between the ICIs and biological
information have been reported for a diversity of animals. Yet, few
quantitative approaches for investigating timing exist to date. Here, we
propose a novel approach for quantitatively comparing timing in animal
vocalisations in terms of the typical ICIs. As features, we use the
distribution of silence gaps parametrised with a kernel density estimate (KDE)
and compare the distributions with the symmetric Kullback-Leibler divergence
(sKL-divergence). We use this technique to compare timing in vocalisations of
two frog species, a group of zebra finches and calls from parrots of the same
species. As a main finding, we demonstrate that in our dataset, closely related
species have more similar distributions than species genetically more distant,
with sKL-divergences across-species larger than within-species distances.
Compared with more standard methods such as Fourier analysis, the proposed
method is more robust to different durations present in the data samples,
flexibly applicable to different species and easy to interpret. Investigating
timing in animal vocalisations may thus contribute to taxonomy, support
conservation efforts by helping monitoring animals in the wild and may shed
light onto the origins of timing structures in animal vocal communication.

Single-particle trajectories measured in microscopy experiments contain
important information about dynamic processes undergoing in a range of
materials including living cells and tissues. However, extracting that
information is not a trivial task due to the stochastic nature of particles'
movement and the sampling noise. In this paper, we adopt a deep-learning method
known as a convolutional neural network (CNN) to classify modes of diffusion
from given trajectories. We compare this fully automated approach working with
raw data to classical machine learning techniques that require data
preprocessing and extraction of human-engineered features from the trajectories
to feed classifiers like random forest or gradient boosting. All methods are
tested using simulated trajectories for which the underlying physical model is
known. From the results it follows that CNN is usually slightly better than the
feature-based methods, but at the costs of much longer processing times.
Moreover, there are still some borderline cases, in which the classical methods
perform better than CNN.

Recent experimental evidence suggests that interactions in flocks of birds do
not involve a characteristic length scale. Bird flocks have also been revealed
to have an inhomogeneous density distribution, with the density of birds near
the border greater than near the centre. We introduce a strictly metric-free
model for collective behaviour that incorporates a distributed motional bias,
providing control of the density distribution. A simple version of this model
is then able to provide a good fit to published data for the density variation
across flocks of Starlings. We find that it is necessary for individuals on the
edge of the flock to have an inward motional bias but that birds in the
interior of the flock instead must have an outward bias. We discuss the ability
of individuals to determine their depth within a flock and show how this might
be achieved by relatively simple analysis of their visual environment.

The Categorical Compositional Distributional (DisCoCat) Model is a powerful
mathematical model for composing the meaning of sentences in natural languages.
Since we can think of biological sequences as the "language of life", it is
attempting to apply the DisCoCat model on the language of life to see if we can
obtain new insights and a better understanding of the latter. In this work, we
took an initial step towards that direction. In particular, we choose to focus
on proteins as the linguistic features of protein are the most prominent as
compared with other macromolecules such as DNA or RNA. Concretely, we treat
each protein as a sentence and its constituent domains as words. The meaning of
a word or the sentence is just its biological function, and the arrangement of
domains in a protein corresponds to the syntax. Putting all those into the
DisCoCat framework, we can "compute" the function of a protein based on the
functions of its domains with the grammar rules that combine them together.
Since the functions of both the protein and its domains are represented in
vector spaces, we provide a novel way to formalize the functional
representation of proteins.

Better understanding of feeding behaviour will be vital in reducing obesity
and metabolic syndrome, but we lack a standard model that captures the
complexity of feeding behaviour. We construct an accurate stochastic model of
rodent feeding at the bout level in order to perform quantitative behavioural
analysis. Analysing the different effects on feeding behaviour of PYY 3-36,
lithium chloride, GLP-1 and leptin shows the precise behavioural changes caused
by each anorectic agent, and demonstrates that these changes do not mimic
satiety. In the ad libitum fed state during the light period, meal initiation
is governed by complete stomach emptying, whereas in all other conditions there
is a graduated response. We show how robust homeostatic control of feeding
thwarts attempts to reduce food intake, and how this might be overcome. In
silico experiments suggest that introducing a minimum intermeal interval or
modulating gastric emptying can be as effective as anorectic drug
administration.

Objective: We hypothesized that prenatal stress (PS) exerts lasting impact on
fetal heart rate (fHR). We sought to validate the presence of such PS signature
in fHR by measuring coupling between maternal HR (mHR) and fHR. Study design:
Prospective observational cohort study in stressed group (SG) mothers with
controls matched for gestational age during screening at third trimester using
Cohen Perceived Stress Scale (PSS) questionnaire with PSS-10 equal or above 19
classified as SG. Women with PSS-10 less than 19 served as control group (CG).
Setting: Klinikum rechts der Isar of the Technical University of Munich.
Population: Singleton 3rd trimester pregnant women. Methods: Transabdominal
fetal electrocardiograms (fECG) were recorded. We deployed a signal processing
algorithm termed bivariate phase-rectified signal averaging (BPRSA) to quantify
coupling between mHR and fHR resulting in a fetal stress index (FSI). Maternal
hair cortisol was measured at birth. Differences were assumed to be significant
for p value less than 0.05. Main Outcome Measures: Differences for FSI between
both groups. Results: We screened 1500 women enrolling 538 of which 16.5 %
showed a PSS-10 score equal or above 19 at 34+0 weeks. Fifty five women
eventually comprised the SG and n=55 served as CG. Median PSS was 22.0 (IQR
21.0-24.0) in the SG and 9.0 (6.0-12.0) in the CG, respectively. Maternal hair
cortisol was higher in SG than CG at 86.6 (48.0-169.2) versus 53.0 (34.4-105.9)
pg/mg. At 36+5 weeks, FSI was significantly higher in fetuses of stressed
mothers when compared to controls [0.43 (0.18-0.85) versus 0.00 (-0.49-0.18)].
Conclusion: Our findings show a persistent effect of PS affecting fetuses in
the last trimester.

This tutorial provides a worked example of using Dynamic Causal Modelling
(DCM) and Parametric Empirical Bayes (PEB) to characterise inter-subject
variability in neural circuitry (effective connectivity). This involves
specifying a hierarchical model with two or more levels. At the first level,
state space models (DCMs) are used to infer the effective connectivity that
best explains a subject's neuroimaging timeseries (e.g. fMRI, MEG, EEG).
Subject-specific connectivity parameters are then taken to the group level,
where they are modelled using a General Linear Model (GLM) that partitions
between-subject variability into designed effects and additive random effects.
The ensuing (Bayesian) hierarchical model conveys both the estimated connection
strengths and their uncertainty (i.e., posterior covariance) from the subject
to the group level; enabling hypotheses to be tested about the commonalities
and differences across subjects. This approach can also finesse parameter
estimation at the subject level, by using the group-level parameters as
empirical priors. We walk through this approach in detail, using data from a
published fMRI experiment that characterised individual differences in
hemispheric lateralization in a semantic processing task. The preliminary
subject specific DCM analysis is covered in detail in a companion paper. This
tutorial is accompanied by the example dataset and step-by-step instructions to
reproduce the analyses.

Here we present a novel approach to protein design and phenotypic inference
using a generative model for protein sequences. BioSeqVAE, a variational
autoencoder variant, can hallucinate syntactically valid protein sequences that
are likely to fold and function. BioSeqVAE is trained on the entire known
protein sequence space and learns to generate valid examples of protein
sequences in an unsupervised manner. The model is validated by showing that its
latent feature space is useful and that it accurately reconstructs sequences.
Its usefulness is demonstrated with a selection of relevant downstream design
tasks. This work is intended to serve as a computational first step towards a
general purpose structure free protein design tool.

A. Bornh\"oft, R. Hanke-Rauschenbach, and K. Sundmacher, [Nonlinear Dyn., 73
(2013), pp. 535-549] introduced a qualitative simplification to the ADM1 model
for anaerobic digestion. We obtain global results for this model by first
analyzing the limiting system, a model of single species growth in the
chemostat in which the response function is non-monotone and the species decay
rate is included. Using a Lyapunov function argument and the theory of
asymptotically autonomous systems, we prove that even in the parameter regime
where there is bistability, no periodic orbits exist and every solution
converges to one of the equilibrium points. We then describe two algorithms for
stochastically perturbing the parameters of the model. Simulations done with
these two algorithms are compared with simulations done using the Gillespie and
tau-leaping algorithms. They illustrate the severe impact environmental factors
may have on anaerobic digestion in the transient phase.

Colour patterning contributes to important plant traits that influence
ecological interactions, horticultural breeding, and agricultural performance.
High-throughput phenotyping of colour is valuable for understanding plant
biology and selecting for traits related to colour during plant breeding. Here
we present ColourQuant, an automated high-throughput pipeline that allows users
to extract colour phenotypes from images. This pipeline includes methods for
colour phenotyping using mean pixel values, Gaussian density estimator of Lab
colour, and the analysis of shape-independent colour patterning by circular
deformation.

A variety of microparticles have been proposed for the sustained and
localized delivery of drugs whit the objective of increasing therapeutic
indexes by circumventing filtering organs and biological barriers. Yet, the
geometrical, mechanical and therapeutic properties of such microparticles
cannot be simultaneously and independently tailored during the fabrication
process in order to optimize their performance. In this work, a top-down
approach is employed to realize micron-sized polymeric particles, called
microPlates (uPLs), for the sustained release of therapeutic agents. uPLs are
square hydrogel particles, with an edge length of 20 um and a height of 5 um,
made out of poly (lactic co glycolic acid) (PLGA). During the synthesis
process, the uPL Young's modulus can be varied from 0.6 to 5 MPa by changing
PLGA amounts from 1 to 7.5 mg, without affecting the uPL geometry. Within the
porous uPL matrix, different classes of therapeutic payloads can be
incorporated including molecular agents, such as the anti-inflammatory
dexamethasone (DEX), and nanoparticles, containing themselves imaging and
therapeutic molecules. As a proof of principle, uPLs are loaded with free DEX
and 200 nm spherical polymeric nanoparticles, carrying DEX molecules
(DEX-SPNs). Electron and fluorescent confocal microscopy analyses document the
uniform distribution and stability of molecular and nano agents within the uPL
matrix. This multiscale, hierarchical microparticle releases DEX for at least
10 days. The inclusion of DEX-SPNs serves to minimize the initial burst release
and modulate the diffusion of DEX molecules out of the uPL matrix. The
pharmacological and therapeutic properties together with the fine tuning of
geometry and mechanical stiffness make uPLs a unique polymeric depot for the
potential treatment of cancer, cardiovascular and chronic, inflammatory
diseases.

The relation between ecological conditions and geomorphological factors is
considered the basis for species distribution in Romania. In this context, the
location of each species within parts of the mountain slopes is difficult on a
medium to brad scale level. The paper presents methodology to combine
vegetation data, obtained from IKONOS satellite images, and Digital Elevation
Model obtained from digitized topographic maps. The study area is a northern
slope of the Stanisoarei Mountains with a gradient of species from beech mixed
and coniferous stands.

Branching in vascular networks and in overall organismic form is one of the
most common and ancient features of multicellular plants, fungi, and animals.
By combining machine-learning techniques with new theory that relates vascular
form to metabolic function, we enable novel classification of diverse branching
networks--mouse lung, human head and torso, angiosperm and gymnosperm plants.
We find that ratios of limb radii--which dictate essential biologic functions
related to resource transport and supply--are best at distinguishing branching
networks. We also show how variation in vascular and branching geometry
persists despite observing a convergent relationship across organisms for how
metabolic rate depends on body mass.

Stability landscapes are useful for understanding the properties of dynamical
systems. These landscapes can be calculated from the system's dynamical
equations using the physical concept of scalar potential. Unfortunately, for
most biological systems with two or more state variables such potentials do not
exist. Here we use an analogy with art to provide an accessible explanation of
why this happens. Additionally, we introduce a numerical method for decomposing
differential equations into two terms: the gradient term that has an associated
potential, and the non-gradient term that lacks it. In regions of the state
space where the magnitude of the non-gradient term is small compared to the
gradient part, we use the gradient term to approximate the potential as
quasi-potential. The non-gradient to gradient ratio can be used to estimate the
local error introduced by our approximation. Both the algorithm and a
ready-to-use implementation in the form of an R package are provided.

Summary: The AptaBlocks Web Interface is focused on providing graphical,
intuitive, and platform independent access to AptaBlocks, an experimentally
validated algorithmic approach for the in-silico design of oligonucleotide
sticky bridges. The availability of AptaBlocks online to the nucleic acid
research community at large makes this software a highly effective tool for
accelerating the design and development of novel oligonucleotide based drugs
and other biotechnologies.
  Availability: The AptaBlocks Web Interface is freely available at
www.ncbi.nlm.nih.gov/CBBresearch/Przytycka/index.cgi\#aptablocks

Over the last years, the SWATH data-independent acquisition protocol
(Sequential Window acquisition of All THeoretical mass spectra) has become a
cornerstone for the worldwide proteomics community. In this approach, a
high-resolution quadrupole-ToF mass spectrometer acquires thousands of MS/MS
data by selecting not just a single precursor at a time, but by allowing a
broad m/z range to be fragmented. This acquisition window is then sequentially
moved from the lowest to the highest mass selection range. This technique
enables the acquisition of thousands of high-resolution MS/MS spectra per
minute in a standard LC-MS run. In the subsequent data analysis phase, the
corresponding dataset is searched in a triple quadrupole-like mode, thus not
considering the whole MS/MS scan spectrum, but by searching for several
precursor to fragment transitions that identify and quantify the corresponding
peptide. This search is made possible with the use of an ion library,
previously acquired in a classical data dependent, full-spectrum mode. The
SWATH protocol, combining the protein identification power of high-resolution
MS/MS spectra with the robustness and accuracy in analyte quantification of
triple-quad targeted workflows, has become very popular in proteomics research.
The major drawback lies in the ion library itself, which is normally demanding
and time-consuming to build. Conversely, through the realignment of
chromatographic retention times, an ion library of a given proteome can
relatively easily be tailored upon any proteomics experiment done on the same
proteome. We are thus hereby sharing with the worldwide proteomics community
our newly acquired ion library of mouse adult hippocampal neural stem cells.
Given the growing effort in neuroscience research involving proteomics
experiments, we believe that this data might be of great help for the
neuroscience community.

Deep neural networks have led to state-of-the-art results in many medical
imaging tasks including Alzheimer's disease (AD) detection based on structural
magnetic resonance imaging (MRI) data. However, the network decisions are often
perceived as being highly non-transparent, making it difficult to apply these
algorithms in clinical routine. In this study, we propose using layer-wise
relevance propagation (LRP) to visualize convolutional neural network decisions
for AD based on MRI data. Similarly to other visualization methods, LRP
produces a heatmap in the input space indicating the importance/relevance of
each voxel contributing to the final classification outcome. In contrast to
susceptibility maps produced by guided backpropagation ("Which change in voxels
would change the outcome most?"), the LRP method is able to directly highlight
positive contributions to the network classification in the input space. In
particular, we show that (1) the LRP method is very specific for individuals
("Why does this person have AD?") with high inter-patient variability, (2)
there is very little relevance for AD in healthy controls and (3) areas that
exhibit a lot of relevance correlate well with what is known from literature.
To quantify the latter, we compute size-corrected metrics of the summed
relevance per brain area, e.g., relevance density or relevance gain. Although
these metrics produce very individual "fingerprints" of relevance patterns for
AD patients, a lot of importance is put on areas in the temporal lobe including
the hippocampus. After discussing several limitations such as sensitivity
toward the underlying model and computation parameters, we conclude that LRP
might have a high potential to assist clinicians in explaining neural network
decisions for diagnosing AD (and potentially other diseases) based on
structural MRI data.

This technical note presents a framework for investigating the underlying
mechanisms of neurovascular coupling in the human brain using multi-modal
magnetoencephalography (MEG) and functional magnetic resonance (fMRI)
neuroimaging data. This amounts to estimating the evidence for several
biologically informed models of neurovascular coupling using variational
Bayesian methods and selecting the most plausible explanation using Bayesian
model comparison. First, fMRI data is used to localise active neuronal sources.
The coordinates of neuronal sources are then used as priors in the
specification of a DCM for MEG, in order to estimate the underlying generators
of the electrophysiological responses. The ensuing estimates of neuronal
parameters are used to generate neuronal drive functions, which model the pre
or post synaptic responses to each experimental condition in the fMRI paradigm.
These functions form the input to a model of neurovascular coupling, the
parameters of which are estimated from the fMRI data. This establishes a
Bayesian fusion technique that characterises the BOLD response - asking, for
example, whether instantaneous or delayed pre or post synaptic signals mediate
haemodynamic responses. Bayesian model comparison is used to identify the most
plausible hypotheses about the causes of the multimodal data. We illustrate
this procedure by comparing a set of models of a single-subject auditory fMRI
and MEG dataset. Our exemplar analysis suggests that the origin of the BOLD
signal is mediated instantaneously by intrinsic neuronal dynamics and that
neurovascular coupling mechanisms are region-specific. The code and example
dataset associated with this technical note are available through the
statistical parametric mapping (SPM) software package.

A seagull ({\it Larus crassirostris}) has a high ability to realize its safe,
accurate and smooth landing. We examined how a seagull controls its angle of
attack when landing on a specific target. First, we recorded the landing
behavior of an actual seagull by multiple video cameras and quantified the
flight trajectory and the angle of attack as time series data. Second, we
introduced a mathematical model that describes how a seagull controls its speed
by changing its angle of attack. Based on the numerical simulation combining
the mathematical model and empirical data, we succeeded in qualitatively
explaining the landing behavior of an actual seagull, which demonstrates that
the control the angle of attack is important for landing behavior.

Risk assessment services fulfil the task of generating a risk report from
personal information and are developed for purposes like disease prognosis,
resource utilization prioritization, and informing clinical interventions. A
major component of a risk assessment service is a risk prediction model. For a
model to be easily integrated into risk assessment services, efforts are needed
to design a detailed development roadmap for the intended service at the time
of model development. However, methodology for such design is less described.
We thus reviewed existing literature and formulated a six-stage risk assessment
service development paradigm, from requirements analysis, service development,
model validation, pilot study, to iterative service deployment and assessment
and refinement. The study aims at providing a prototypic development roadmap
with checkpoints for the design of risk assessment services.

Tissue plasminogen activator (tPA) is the sole approved therapeutic molecule
for the treatment of acute ischemic stroke. Yet, only a small percentage of
patients could benefit from this life-saving treatment because of medical
contraindications and severe side effects, including brain hemorrhage,
associated with delayed administration. Here, a nano therapeutic agent is
realized by directly associating the clinical formulation of tPA to the porous
structure of soft discoidal polymeric nanoconstructs (tPA-DPNs). The porous
matrix of DPNs protects tPA from rapid degradation, allowing tPA-DPNs to
preserve over 70 % of the tPA original activity after 3 h of exposure to serum
proteins. Under dynamic conditions, tPA-DPNs dissolve clots more efficiently
than free tPA, as demonstrated in a microfluidic chip where clots are formed
mimicking in vivo conditions. At 60 min post treatment initiation, the clot
area reduces by half (57 + 8 %) with tPA-DPNs, whereas a similar result (56 +
21 %) is obtained only after 90 min for free tPA. In murine mesentery venules,
the intravenous administration of 2.5 mg/kg of tPA-DPNs resolves almost 90 % of
the blood clots, whereas a similar dose of free tPA successfully recanalize
only about 40 % of the treated vessels. At about 1/10 of the clinical dose (1.0
mg/kg), tPA-DPNs still effectively dissolve 70 % of the clots, whereas free tPA
works efficiently only on 16 % of the vessels. In vivo, discoidal tPA-DPNs
outperform the lytic activity of 200 nm spherical tPA-coated nanoconstructs in
terms of both percentage of successful recanalization events and clot area
reduction. The conjugation of tPA with preserved lytic activity, the
deformability and blood circulating time of DPNs together with the faster blood
clot dissolution would make tPA-DPNs a promising nanotool for enhancing both
potency and safety of thrombolytic therapies.

In systems biology modeling, important steps include model parameterization,
uncertainty quantification, and evaluation of agreement with experimental
observations. To help modelers perform these steps, we developed the software
PyBioNetFit. PyBioNetFit is designed for parameterization, and also supports
uncertainty quantification, checking models against known system properties,
and solving design problems. PyBioNetFit introduces the Biological Property
Specification Language (BPSL) for the formal declaration of system properties.
BPSL allows qualitative data to be used alone or in combination with
quantitative data for parameterization model checking, and design. PyBioNetFit
performs parameterization with parallelized metaheuristic optimization
algorithms (differential evolution, particle swarm optimization, scatter
search) that work directly with existing model definition standards: BioNetGen
Language (BNGL) and Systems Biology Markup Language (SBML). We demonstrate
PyBioNetFit's capabilities by solving 31 example problems, including the
challenging problem of parameterizing a model of cell cycle control in yeast.
We benchmark PyBioNetFit's parallelization efficiency on computer clusters,
using up to 288 cores. Finally, we demonstrate the model checking and design
applications of PyBioNetFit and BPSL by analyzing a model of therapeutic
interventions in autophagy signaling.

Artificial Intelligence is set to revolutionize multiple fields in the coming
years. One subset of AI, machine learning, shows immense potential for
application in a diverse set of medical specialties, including diagnostic
pathology. In this study, we investigate the utility of the Apple Create ML and
Google Cloud Auto ML, two machine learning platforms, in a variety of
pathological scenarios involving lung and colon pathology. First, we evaluate
the ability of the platforms to differentiate normal lung tissue from cancerous
lung tissue. Also, the ability to accurately distinguish two subtypes of lung
cancer (adenocarcinoma and squamous cell carcinoma) is examined and compared.
Similarly, the ability of the two programs to differentiate colon
adenocarcinoma from normal colon is assessed as is done with lung tissue. Also,
cases of colon adenocarcinoma are evaluated for the presence or absence of a
specific gene mutation known as KRAS. Finally, our last experiment examines the
ability of the Apple and Google platforms to differentiate between
adenocarcinomas of lung origin versus colon origin. In our trained models for
lung and colon cancer diagnosis, both Apple and Google machine learning
algorithms performed very well individually and with no statistically
significant differences found between the two platforms. However, some critical
factors set them apart. Apple Create ML can be used on local computers but is
limited to an Apple ecosystem. Google Auto ML is not platform specific but runs
only in Google Cloud with associated computational fees. In the end, both are
excellent machine learning tools that have great potential in the field of
diagnostic pathology, and which one to choose would depend on personal
preference, programming experience, and available storage space.

Technological advances in underwater video recording are opening novel
opportunities for monitoring wild fish. However, extracting data from videos is
often challenging. Nevertheless, it has been recently demonstrated that
accurate and precise estimates of density for animals (whose normal activities
are restricted to a bounded area or home range) can be obtained from counts
averaged across a relatively low number of video frames. The method, however,
requires that individual detectability (PID, the probability of detecting a
given animal provided that it is actually within the area surveyed by a camera)
has to be known. Here we propose a Bayesian implementation for estimating PID
after combining counts from cameras with counts from any reference method. The
proposed framework was demonstrated using Serranus scriba as a case-study, a
widely distributed and resident coastal fish. Density and PID were calculated
after combining fish counts from unbaited remote underwater video (RUV) and
underwater visual censuses (UVC) as reference method. The relevance of the
proposed framework is that after estimating PID, fish density can be estimated
accurately and precisely at the UVC scale (or at the scale of the preferred
reference method) using RUV only. This key statement has been extensively
demonstrated using computer simulations yielded by real empirical data.
Finally, we provide a simulation tool-kit for comparing the expected precision
attainable for different sampling effort and for species with different levels
of PID. Overall, the proposed method may contribute to substantially enlarge
the spatio-temporal scope of density monitoring programs for many resident
fish.

We propose a general framework for a collaborative machine learning system to
assist bioscience researchers with the task of labeling specific cell
identities from microscopic still or video imaging. The distinguishing features
of this approach versus prior approaches include: (1) use of a statistical
model of cell features that is iteratively improved, (2) generation of
probabilistic guesses at cell ID rather than single best-guesses for each cell,
(3) tracking of joint probabilities of features within and across cells, and
(4) ability to exploit multi-modal features, such as cell position, morphology,
reporter intensities, and activity. We provide an example implementation of
such a system applicable to labeling fluorescently tagged \textit{C. elegans}
neurons. As a proof of concept, we use a generative spring-mass model to
simulate sequences of cell imaging datasets with variable cell positions and
fluorescence intensities. Training on synthetic data, we find that atlases that
track inter-cell positional correlations give higher labeling accuracies than
those that treat cell positions independently. Tracking an additional feature
type, fluorescence intensity, boosts accuracy relative to a position-only
atlas, suggesting that multiple cell features could be leveraged to improve
automated label predictions.

Estimation of mutual information between (multidimensional) real-valued
variables is used in analysis of complex systems, biological systems, and
recently also quantum systems. This estimation is a hard problem, and
universally good estimators provably do not exist. Kraskov et al. (PRE, 2004)
introduced a successful mutual information estimation approach based on the
statistics of distances between neighboring data points, which empirically
works for a wide class of underlying probability distributions. Here we improve
this estimator by (i) expanding its range of applicability, and by providing
(ii) a self-consistent way of verifying the absence of bias, (iii) a method for
estimation of its variance, and (iv) a criterion for choosing the values of the
free parameter of the estimator. We demonstrate the performance of our
estimator on synthetic data sets, as well as on neurophysiological and systems
biology data sets.

At rest, human brain functional networks display striking modular
architecture in which coherent clusters of brain regions are activated. The
modular account of brain function is pervasive, reliable, and reproducible.
Yet, a complementary perspective posits a core-periphery or rich-club account
of brain function, where hubs are densely interconnected with one another,
allowing for integrative processing. Unifying these two perspectives has
remained difficult due to the fact that the methodological tools to identify
modules are entirely distinct from the methodological tools to identify
core-periphery structure. Here we leverage a recently-developed model-based
approach -- the weighted stochastic block model -- that simultaneously uncovers
modular and core-periphery structure, and we apply it to fMRI data acquired at
rest in 872 youth of the Philadelphia Neurodevelopmental Cohort. We demonstrate
that functional brain networks display rich meso-scale organization beyond that
sought by modularity maximization techniques. Moreover, we show that this
meso-scale organization changes appreciably over the course of
neurodevelopment, and that individual differences in this organization predict
individual differences in cognition more accurately than module organization
alone. Broadly, our study provides a unified assessment of modular and
core-periphery structure in functional brain networks, providing novel insights
into their development and implications for behavior.

Domain growth is a key process in many areas of biology, including embryonic
development, the growth of tissue, and limb regeneration. As a result,
mechanisms for incorporating it into traditional models for cell movement,
interaction, and proliferation are of great importance. A previously well-used
method in order to incorporate domain growth into on-lattice reaction-diffusion
models causes a build up of particles on the boundaries of the domain, which is
particularly evident when diffusion is low in comparison to the rate of domain
growth. Here, we present a new method which addresses this unphysical build up
of particles at the boundaries, and demonstrate that it is accurate even for
scenarios in which the previous method fails. Further, we discuss for which
parameter regimes it is feasible to continue using the original method due to
diffusion dominating the domain growth mechanism.

Deep phenotyping is an emerging conceptual paradigm and experimental approach
that seeks to measure many aspects of phenotypes and link them to understand
the underlying biology. Successful deep phenotyping has mostly been applied in
cultured cells, less so in multicellular organisms. Recently, however, it has
been recognized that such an approach could lead to better understanding of how
genetics, the environment, and stochasticity affect development, physiology,
and behavior of an organism. Over the last 50 years, the nematode
Caenorhabditis elegans has become an invaluable model system for understanding
the role of the genes underlying a phenotypic trait. Recent technological
innovation has taken advantage of the worm physical attributes to increase the
throughput and informational content of experiments. Coupling these technical
advancements with computational or analytical tools has enabled a boom in deep
phenotyping studies of C. elegans. In this review, we highlight how these new
technologies and tools are digging into the biological origins of complex
multidimensional phenotypes seen in the worm.

In this paper, we propose a new framework to analyze the electrical activity
of the uterus recorded by electrohysterography (EHG), from abdominal electrodes
(a grid of 4x4 electrodes) during pregnancy and labor. We evaluate the
potential use of the synchronization between EHG signals in characterizing
electrical activity of the uterus during pregnancy and labor. The complete
processing pipeline consists of i) estimating the correlation between the
different EHG signals, ii) quantifying the connectivity matrices using graph
theory-based analysis and iii) testing the clinical impact of network measures
in pregnancy monitoring and labor detection. We first compared several
connectivity methods to compute the adjacency matrix represented as a graph of
a set of nodes (electrodes) connected by edges (connectivity values). We then
evaluated the performance of different graph measures in the classification of
pregnancy and labor contractions (number of women=35). A comparison with the
already existing parameters used in the state of the art of labor detection and
preterm labor prediction was also performed. Results show higher performance of
connectivity methods when combined with network measures. Denser graphs were
observed during labor than during pregnancy. The network-based metrics showed
the highest classification rate when compared to already existing features.
This network-based approach can be used not only to characterize the
propagation of the uterine contractions, but also may have high clinical impact
in labor detection and likely in the prediction of premature labor.

This paper presents a simple physical model for self-similar (gnomonic, or
first-order) seashell growth which is expressed in coordinate-free terms. The
shell is expressed as the solution of a differential equation which expresses
the growth dynamics, and may be used to investigate shell growth from both the
local viewpoint of the organism building it and moving with the shell opening
(aperture), as well as that of a researcher making global measurements upon a
complete motionless shell. Coordinate systems needed to express the global and
local descriptions of the shell are chosen. The parameters of growth, or their
information equivalent, remain constant in the local system, and are used by
the organism to build the shell, and are likely mirrored in the DNA of the
organism building it. The transformations between local and global
representations are provided. The global model of Cortie, which is very similar
to the present model, is expressed in terms of the present model, and the
global parameters provided by Cortie for various species of mollusk may be used
to calculate the equivalent local parameters.Mathematica code is provided to
implement these transformations, as well as to plot the shells using both
global and local parameters.

Cellular signaling is essential in information processing and decision
making. Therefore, a variety of experimental approaches have been developed to
study signaling on bulk and single-cell level. Single-cell measurements of
signaling molecules demonstrated a substantial cell-to-cell variability,
raising questions about its causes and mechanisms and about how cell
populations cope with or exploit cellular heterogeneity. To gain insights from
single-cell signaling data, analysis and modeling approaches have been
introduced. This review discusses these modeling approaches, with a focus on
recent advances in the development and calibration of mechanistic models.
Additionally, it outlines current and future challenges.

We have advanced a point-process based framework for the regulation of heart
beats by the autonomous nervous system and analyzed the model with and without
feedback. The model without feedback was found amenable to several analytical
results that help develop an intuition about the way the heart interacts with
the nervous system. However, in reality, feedback, baroreflex and chemoreflex
controls are important to model healthy and unhealthy scenarios for the heart.
Based on the Hurst exponent as an index of health of the heart we show how the
state of the nervous system may tune it in health and disease. Monte Carlo
simulation is used to generate RR interval series of the Electrocardiogram
(ECG) for different sympathetic and parasympathetic nerve excitations.

Identifying groups of similar objects using clustering approaches is one of
the most frequently employed first steps in exploratory biomedical data
analysis. Many clustering methods have been developed that pursue different
strategies to identify the optimal clustering for a data set.
  We previously published TiCoNE, an interactive clustering approach coupled
with de-novo network enrichment of identified clusters. However, in this first
version time-series and network analysis remained two separate steps in that
only time-series data was clustered, and identified clusters mapped to and
enriched within a network in a second separate step.
  In this work, we present TiCoNE 2: An extension that can now seamlessly
incorporate multiple data types within its composite clustering model.
Systematic evaluation on 50 random data sets, as well as on 2,400 data sets
containing enriched cluster structure and varying levels of noise, shows that
our approach is able to successfully recover cluster patterns embedded in
random data and that it is more robust towards noise than non-composite models
using only one data type, when applied to two data types simultaneously.
  Herein, each data set was clustered using five different similarity functions
into k=10/30 clusters, resulting to ~5,000 clusterings in total. We evaluated
the quality of each derived clustering with the Jaccard index and an internal
validity score. We used TiCoNE to calculate empirical p-values for all
generated clusters with different permutation functions, resulting in ~80,000
cluster p-values. We show, that derived p-values can be used to reliably
distinguish between foreground and background clusters.
  TiCoNE 2 allows researchers to seamlessly analyze time-series data together
with biological interaction networks in an intuitive way and thereby provides
more robust results than single data type cluster analyses.

Deep phenotyping study has become an emerging field to understand the gene
function and the structure of biological networks. For the living animal C.
elegans, recent advances in genome-editing tools, microfluidic devices and
phenotypic analyses allow for a deeper understanding of the
genotype-to-phenotype pathway. In this article, I reviewed the evolution of
deep phenotyping study in cell development, neuron activity, and the behaviors
of intact animals.

Diabetes in pregnancy (DIP) is an increasing public health priority in the
Australian Capital Territory, particularly due to its impact on risk for
developing Type 2 diabetes. While earlier diagnostic screening results in
greater capacity for early detection and treatment, such benefits must be
balanced with the greater demands this imposes on public health services. To
address such planning challenges, a multi-scale hybrid simulation model of DIP
was built to explore the interaction of risk factors and capture the dynamics
underlying the development of DIP. The impact of interventions on health
outcomes at the physiological, health service and population level is measured.
Of particular central significance in the model is a compartmental model
representing the underlying physiological regulation of glycemic status based
on beta-cell dynamics and insulin resistance. The model also simulated the
dynamics of continuous BMI evolution, glycemic status change during pregnancy
and diabetes classification driven by the individual-level physiological model.
We further modeled public health service pathways providing diagnosis and care
for DIP to explore the optimization of resource use during service delivery.
The model was extensively calibrated against empirical data.

Background: Data preparation, such as missing values imputation and
transformation, is the first step in any data analysis and requires crucial
attention. Particularly, analysis of metabolites demands more preparation since
those small compounds have recently been measurable in large scales with mass
spectrometry techniques. We introduce novel statistical techniques for
metabolite missing values imputation by utilizing replication samples. Results:
To understand the nature of the missing values using replication samples, we
obtained the empirical distribution of missing values and observed that the
rate of missing values is approximately distributed as uniform across the
metabolite range. Therefore, the missing values cannot be imputed with the
lowest values. Using the identified distribution, we illustrated a simulation
study to find an optimal imputation approach for metabolites. Conclusions: We
demonstrated that the missing values in metabolomic data sets might not be
necessarily low value. After identification of the nature of missing values, we
validated K nearest neighborhood as an optimal approach for imputation.

Thalassaemia, triggered by defects in the globin genes, is one of the most
common monogenic diseases. The beta-thalassaemia carrier state is clinically
asymptomatic, thus, making it onerous to diagnose. The current gold standard
technique is implausible to be used for onsite carrier detection as the method
necessitates expensive instruments, skilled manpower and time. In this study,
we have tried to classify the carriers from the healthy samples based on their
blood droplet drying patterns using image analysis based tools and subsequently
develop an in-house program for automated classification of the same. This
automatic, rapid, less laborious and cost-effective technique will
significantly increase the total number of carriers that are screened for
thalassaemia per year in the country, thus, reducing the burden in the state
run advanced health facilities.

Although the open-field test has been widely used, its reliability and
compatibility are frequently questioned. Although many indicating parameters
were introduced for this test, they did not take data distributions into
consideration. This oversight may have caused the problems mentioned above.
Here, an exploratory approach for the analysis of video records of tests of
elderly mice was taken that described the distributions using the least number
of parameters. First, the locomotor activity of the animals was separated into
two clusters: dash and search. The accelerations found in each of the clusters
were distributed normally. The speed and the duration of the clusters exhibited
an exponential distribution. Although the exponential model includes a single
parameter, an additional parameter that indicated instability of the behaviour
was required in many cases for fitting to the data. As this instability
parameter exhibited an inverse correlation with speed, the function of the
brain that maintained stability would be required for a better performance.
According to the distributions, the travel distance, which has been regarded as
an important indicator, was not a robust estimator of the animals' condition.

As biomedical sciences discover new layers of complexity in the mechanisms of
life and disease, mathematical models trying to catch up with these
developments become mathematically intractable. As a result, in the grand
scheme of things, mathematical models have so far played an auxiliary role in
biomedical sciences. We propose a new methodology allowing mathematical
modeling to give, in certain cases, definitive answers to systemic biomedical
questions that elude empirical resolution. Our methodology is based on two
ideas: (1) employing mathematical models that are firmly rooted in established
biomedical knowledge yet so general that they can account for any, or at least
many, biological mechanisms, both known and unknown; (2) finding model
parameters whose likelihood-maximizing values are independent of observations
(existence of such parameters implies that the model must not meet regularity
conditions required for the consistency of maximum likelihood estimator). These
universal parameter values may reveal general patterns (that we call natural
laws) in biomedical processes. We illustrate this approach with the discovery
of a clinically important natural law governing cancer metastasis.
Specifically, we found that under minimal, and fairly realistic, mathematical
and biomedical assumptions the likelihood-maximizing scenario of metastatic
cancer progression in an individual patient is invariably the same: Complete
suppression of metastatic growth before primary tumor resection followed by an
abrupt growth acceleration after surgery. This scenario is widely observed in
clinical practice and supported by a wealth of experimental studies on animals
and clinical case reports published over the last 110 years. The above most
likely scenario does not preclude other possibilities e.g. metastases may start
aggressive growth before primary tumor resection or remain dormant after
surgery.

We present an attention-based Transformer model for automatic retrosynthesis
route planning. Our approach starts from reactants prediction of single-step
organic reactions for given products, followed by Monte Carlo tree search-based
automatic retrosynthetic pathway prediction. Trained on two datasets from the
United States patent literature, our models achieved a top-1 prediction
accuracy of over 54.6% and 63.0% with more than 95% and 99.6% validity rate of
SMILES, respectively, which is the best up to now to our knowledge. We also
demonstrate the application potential of our model by successfully performing
multi-step retrosynthetic route planning for four case products, i.e.,
antiseizure drug Rufinamide, a novel allosteric activator, an inhibitor of
human acute-myeloid-leukemia cells and a complex intermediate of drug
candidate. Further, by using heuristics Monte Carlo tree search, we achieved
automatic retrosynthetic pathway searching and successfully reproduced
published synthesis pathways. In summary, our model has achieved the
state-of-the-art performance on single-step retrosynthetic prediction and
provides a novel strategy for automatic retrosynthetic pathway planning.

Revealing the functional sites of biological sequences, such as evolutionary
conserved, structurally interacting or co-evolving protein sites, is a
fundamental, and yet challenging task. Different frameworks and models were
developed to approach this challenge, including Position-Specific Scoring
Matrices, Markov Random Fields, Multivariate Gaussian models and most recently
Autoencoders. Each of these methods has certain advantages, and while they have
generated a set of insights for better biological predictions, these have been
restricted to the corresponding methods and were difficult to translate to the
complementary domains. Here we propose a unified framework for the
above-mentioned models, that allows for interpretable transformations between
the different methods and naturally incorporates the advantages and insight
gained individually in the different communities. We show how, by using the
unified framework, we are able to achieve state-of-the-art performance for
protein structure prediction, while enhancing interpretability of the
prediction process.

In this chapter, we present a strategy and the technics to approach a
scientific field from a set of articles gathered from the bibliographic
database, Web of Science. The strategy is based on methods developed to analyze
social networks. We illustrate the use of such strategy in studying the
calmodulin field. Such method allows to structure a huge number of articles
when writing a review, to detect the key opinion leaders in a given field and
to locate his own research topic in the landscape of the themes deciphered by
our own community. We show that the free software VosViewer may be used without
knowledge in computing science and with a short learning period. iii.

Cell image classification methods are currently being used in numerous
applications in cell biology and medicine. Applications include understanding
the effects of genes and drugs in screening experiments, understanding the role
and subcellular localization of different proteins, as well as diagnosis and
prognosis of cancer from images acquired using cytological and histological
techniques. We review three different approaches for cell image classification:
numerical feature extraction, end to end classification with neural networks,
and transport-based morphometry. In addition, we provide comparisons on four
different cell imaging datasets to highlight the relative strength of each
method.

Connectivity across landscapes influences a wide range of
conservation-relevant ecological processes, including species movements, gene
flow, and the spread of wildfire, pests, and diseases. Recent improvements in
remote sensing data suggest great potential to advance connectivity models, but
computational constraints hinder these advances. To address this challenge, we
upgraded the widely-used Circuitscape connectivity package to the high
performance Julia programming language. Circuitscape.jl allows users to solve
problems faster via improved parallel processing and solvers, and supports
applications to larger problems (e.g., datasets with hundreds of millions of
cells). We document speed improvements of up to 1800\%. We also demonstrate
scaling of problem sizes up to 437 million grid cells. These improvements allow
modelers to work with higher resolution data, larger landscapes and perform
sensitivity analysis effortlessly. These improvements accelerate the pace of
innovation, helping modelers address pressing challenges like species range
shifts under climate change. Our collaboration between ecologists and computer
scientists has led to the use of connectivity models to inform conservation
decisions. Further, these next generation connectivity models will produce
results faster, facilitating stronger engagement with decision-makers.

Resource Balance Analysis (RBA) is a computational method based on resource
allocation, which performs accurate quantitative predictions of whole-cell
states (i.e. growth rate, meta-bolic fluxes, abundances of molecular machines
including enzymes) across growth conditions. We present an integrated workflow
of RBA together with the Python package RBApy. RBApy builds bacterial RBA
models from annotated genome-scale metabolic models by add-ing descriptions of
cellular processes relevant for growth and maintenance. The package in-cludes
functions for model simulation and calibration and for interfacing to Escher
maps and Proteomaps for visualization. We demonstrate that RBApy faithfully
reproduces results ob-tained by a hand-curated and experimentally validated RBA
model for Bacillus subtilis. We also present a calibrated RBA model of
Escherichia coli generated from scratch, which ob-tained excellent fits to
measured flux values and enzyme abundances. RBApy makes whole-cell modeling
accessible for a wide range of bacterial wild-type and engineered strains, as
il-lustrated with a CO2-fixing Escherichia coli strain.

Metabolomics is becoming a mature part of analytical chemistry as evidenced
by the growing number of publications and attendees of international
conferences dedicated to this topic. Yet, a systematic treatment of the
fundamental structure and properties of metabolomics data is lagging behind. We
want to fill this gap by introducing two fundamental theories concerning
metabolomics data: data theory and measurement theory. Our approach is to ask
simple questions, the answers of which require applying these theories to
metabolomics. We show that we can distinguish at least four different levels of
metabolomics data with different properties and warn against confusing data
with numbers.

Cronobacter sakazakii is an opportunistic pathogen associated with outbreaks
of neonatal necrotizing enterocolitis, septicemia, and meningitis.
Reconstituted powdered infant formulae (PIF) is the most common vehicle of
infection. Plate count methods do not provide direct information on the
physiological status of cells. Flow cytometry (FC) has been used to gain
insights into the physiological states of C. sakazakii after heat treatments,
and to compare FC results with plate counts. The percentage of compromised
cells increased as the percentage of live cells increased after the 100 C
treatment. However, the number of compromised cells after 60 or 65 C treatments
decreased as the percentage of live cells increased, showing that both mild
temperatures would not be completely effective eliminating all bacteria but
compromising their membranes, and showing that mild heat treatments are not
enough to guarantee the safety of PIF. FC was capable to detect C. sakazakii
compromised cells that cannot be detected with classical plate count methods,
thus it could be used to decreasing the risk of pathogenic viable but
non-culturable cells to be in the ingested food. Linear regression analysis
showed good correlations between plate count results vs FC results.

Mathematical models can provide quantitative insight into immunoreceptor
signaling, but require parameterization and uncertainty quantification before
making reliable predictions. We review currently available methods and software
tools to address these problems. We consider gradient-based and gradient-free
methods for point estimation of parameter values, and methods of profile
likelihood, bootstrapping, and Bayesian inference for uncertainty
quantification. We consider recent and potential future applications of these
methods to systems-level modeling of immune-related phenomena.

Microbial metabolism of fugitive hydrocarbons produces greenhouse gas (GHG)
emissions from oil sands tailings ponds (OSTP) and end pit lakes (EPL) that
retain semisolid wastes from surface mining of oil sands ores. Predicting GHG
production, particularly methane (CH4), would help oil sands operators mitigate
tailings emissions and would assist regulators evaluating the trajectory of
reclamation scenarios. Using empirical datasets from laboratory incubation of
OSTP sediments with pertinent hydrocarbons, we developed a stoichiometric model
for CH4 generation by indigenous microbes. This model improved on previous
first-approximation models by considering long-term biodegradation kinetics for
18 relevant hydrocarbons from three different oil sands operations, lag times,
nutrient limitations, and microbial growth and death rates. Laboratory
measurements were used to estimate model parameter values and to validate the
new model. Goodness of fit analysis showed that the stoichiometric model
predicted CH4 production well; normalized mean square error analysis revealed
that it surpassed previous models. Comparison of model predictions with field
measurements of CH4 emissions further validated the new model. Importantly, the
model also identified parameters that are currently lacking but are needed to
enable future robust modeling of CH4 production from OSTP and EPL in situ.

Fontan operation as the current standard of care for the palliation of single
ventricle defects results in significant late complications. Using a mechanical
circulatory device for the right circulation to serve the function of the
missing subpulmonary ventricle could potentially stabilize the failing Fontan
circulation. This study aims to elucidate the hydraulic operating regions that
should be targeted for designing cavopulmonary blood pumps. By integrating
numerical analysis and available clinical information, the interaction of the
cavopulmonary support via the IVC and full assist configurations with a wide
range of simulated adult failing scenarios was investigated; with IVC and full
assist corresponding to the inferior venous return or the entire venous return,
respectively, being routed through the device. We identified the desired
hydraulic operating regions for a cavopulmonary assist device by clustering all
head pressures and corresponding pump flows that result in hemodynamic
improvement for each simulated failing Fontan physiology. Results show that IVC
support can produce beneficial hemodynamics in only a small fraction of failing
Fontan scenarios. Cavopulmonary assist device could increase cardiac index by
35% and decrease the inferior vena cava pressure by 45% depending on the
patient's pre-support hemodynamic state and surgical configuration of the
cavopulmonary assist device (IVC or full support). The desired flow-pressure
operating regions we identified can serve as the performance criteria for
designing cavopulmonary assist devices as well as evaluating off-label use of
commercially available left-side blood pumps for failing Fontan cavopulmonary
support.

Rapid and accurate phenotypic screening of rice germplasms is crucial in
screening for sources of rice sheath blight resistance. However, visual and/or
caliper-based estimations of coalescing, necrotic, ShB disease lesions are
time-consuming, labor-intensive and exposed to human rater subjectivity. Here,
we propose the use of RGB images and image processing techniques to quantify
ShB disease progression in terms of lesion height and diseased area. To be
specific, we developed a pixel color- and coordinate-based K-Means Clustering
(PCC-KMC) algorithm utilizing Mahalanobis metric aimed at accurate segmentation
of symptomatic and non-symptomatic regions within rice stem images. The
performance of PCC-KMC was evaluated using Lin's concordance correlation
coefficient by comparing its results to visual measurements of ShB lesion
height and to lesion/diseased area measured using ImageJ. Low bias and high
precision were observed for absolute lesion height (bias=0.93, precision=0.94)
and absolute symptomatic area (bias=0.98, precision=0.97) studies. Moreover, we
introduced a convolutional neural network (CNN) for the automatic annotation on
clusters, termed PCC-KMC-CNN. Our CNN was trained based on 85%:15% of
composition for training and testing dataset from total 168 ShB-infected stem
sample images, recording 92% accuracy and 0.21 loss. PCC-KMC-CNN also showed
high accuracy and precision for the absolute lesion height (bias=0.86,
precision=0.90) and absolute diseased area (bias=0.99, precision=0.97) studies.
These results demonstrate that the present methodology has great potential and
promise to substitute the traditional visual-based ShB disease severity
assessment.

Summary: Integration of multi-omics data on chemical exposure of cells or
organisms promises a more complete representation of the responding pathways
than single omics data. Data of different omics layers, like transcriptome or
proteome is deposited in different repositories. Additionally, precisely
specifying a chemical of interest that was used in the exposure experiments
suffers from different nomenclatures and non-uniquely mapping of chemical
identifiers. The manual search for corresponding omics data sets of different
layers for exposure with a chemical of interest is thus a tedious task. We have
developed MOD-Finder (Multi-Omics Data set Finder) to efficiently search for
chemical-related omics data sets in several publicly available databases in an
automated manner. A plain and simple presentation of the returned omics data
sets is augmented with effect information that are assumed to be triggered by
the chemical of interest.
  Availability and Implementation: MOD-Finder is implemented in R using the
Shiny package. The web service is available at https://webapp.ufz.de/mod_finder
and the source code under the GNU GPL v3 license at
https://github.com/yigbt/MOD-Finder.
  Supplementary information: Supplementary data are available at
https://www.ufz.de/index.php?en=44919

Quantitative analysis of cell nuclei in microscopic images is an essential
yet challenging source of biological and pathological information. The major
challenge is accurate detection and segmentation of densely packed nuclei in
images acquired under a variety of conditions. Mask R-CNN-based methods have
achieved state-of-the-art nucleus segmentation. However, the current pipeline
requires fully annotated training images, which are time consuming to create
and sometimes noisy. Importantly, nuclei often appear similar within the same
image. This similarity could be utilized to segment nuclei with only partially
labeled training examples. We propose a simple yet effective region-proposal
module for the current Mask R-CNN pipeline to perform few-exemplar learning. To
capture the similarities between unlabeled regions and labeled nuclei, we apply
decomposed self-attention to learned features. On the self-attention map, we
observe strong activation at the centers and edges of all nuclei, including
unlabeled nuclei. On this basis, our region-proposal module propagates partial
annotations to the whole image and proposes effective bounding boxes for the
bounding box-regression and binary mask-generation modules. Our method
effectively learns from unlabeled regions thereby improving detection
performance. We test our method with various nuclear images. When trained with
only 1/4 of the nuclei annotated, our approach retains a detection accuracy
comparable to that from training with fully annotated data. Moreover, our
method can serve as a bootstrapping step to create full annotations of
datasets, iteratively generating and correcting annotations until a
predetermined coverage and accuracy are reached. The source code is available
at https://github.com/feng-lab/nuclei.

Upcoming immunotherapies for cancer treatment rely on the ability of the
immune system to detect and eliminate tumors in the body. A highly simplified
version of this process can be studied in a Petri dish: starting with a random
distribution of immune and tumor cells, it can be observed in detail how
individual immune cells migrate towards nearby tumor cells, establish contact,
and attack. Nevertheless, it remains unclear whether the immune cells find
their targets by chance, or if they approach them 'on purpose', using remote
sensing mechanisms such as chemotaxis. In this work, we present methods to
infer the strength and range of long-range cell-cell interactions from
time-lapse recorded cell trajectories, using a maximum likelihood method to fit
the model parameters. First, we model the interactions as a distance-dependent
'force' that attracts immune cells towards their nearest tumor cell. While this
approach correctly recovers the interaction parameters of simulated cells with
constant migration properties, it detects spurious interactions in the case of
independent cells that spontaneously change their migration behavior over time.
We therefore use an alternative approach that models the interactions by
distance-dependent probabilities for positive and negative turning angles of
the migrating immune cell. We demonstrate that the latter approach finds the
correct interaction parameters even with temporally switching cell migration.

The definition of an innovative therapeutic protocol requires the fine tuning
of all the involved operations in order to maximize the efficiency. In some
cases, the price of the experiments, or their duration, represents a great
obstacle and the full potential of the protocol risks to be reduced or even
hidden by a non-optimal application.
  The implementation of a numerical model of the protocol may represent the
solution, allowing a systematic exploration of all the different alternatives,
shedding the light on the most promising combination and also identifying the
key elements/parameters.
  In this paper, the injection of a plasmid, preceded by a hyaluronidase
injection, is simulated through a mathematical model. Some key elements of the
administration protocol are identified by means of a mathematical optimization
procedure, maximizing the efficacy of the therapy. As a side effect of the
extensive investigation, robust solutions able to reduce the effects of human
errors in the administration are also obtained.

Scaffold proteins organize cellular processes by bringing signaling molecules
into interaction, sometimes by forming large signalosomes. Several of these
scaffolds are known to polymerize. Their assemblies should therefore not be
understood as stoichiometric aggregates, but as combinatorial ensembles. We
analyze the combinatorial interaction of ligands loaded on polymeric scaffolds,
in both a continuum and discrete setting, and compare it with multivalent
scaffolds with fixed number of binding sites. The quantity of interest is the
abundance of ligand interaction possibilities---the catalytic potential
$Q$---in a configurational mixture. Upon increasing scaffold abundance,
scaffolding systems are known to first increase opportunities for ligand
interaction and then to shut them down as ligands become isolated on distinct
scaffolds. The polymerizing system stands out in that the dependency of $Q$ on
protomer concentration switches from being dominated by a first order to a
second order term within a range determined by the polymerization affinity.
This behavior boosts $Q$ beyond that of any multivalent scaffold system. In
addition, the subsequent drop-off is considerably mitigated in that $Q$
decreases with half the power in protomer concentration than for any
multivalent scaffold. We explain this behavior in terms of how the
concentration profile of the polymer length distribution adjusts to changes in
protomer concentration and affinity. The discrete case turns out to be similar,
but the behavior can be exaggerated at small protomer numbers because of a
maximal polymer size, analogous to finite-size effects in bond percolation on a
lattice.

Falls affect a growing number of the population each year. Clinical methods
to identify those at greatest risk for falls usually evaluate individuals while
they perform specific motions such as balancing or Sit-to-Stand (STS).
Unfortunately these techniques have been shown to have poor predictive power
and are unable to identify the magnitude, direction, and timing of
perturbations that can cause an individual to lose stability during motion. To
address this limitation, the recently proposed Stability Basin (SB) aims to
characterize the set of perturbations that will cause an individual to fall
under a specific motor control strategy. The SB is defined as the set of
configurations that do not lead to failure for an individual under their chosen
control strategy. This paper presents a novel method to compute the SB and the
first experimental validation of the SB with an 11-person perturbative STS
experiment involving forwards or backwards pulls from a motor-driven cable. The
individually-constructed SBs are used to identify when a trial fails, i.e.,
when an individual must switch control strategies (indicated by a step or sit)
to recover from a perturbation. The constructed SBs correctly predict the
outcome of trials where failure was observed with over 90% accuracy, and
correctly predict the outcome of successful trials with over 95% accuracy. The
SB was compared to three other methods and was found to estimate the stable
region with over 45% more accuracy in all cases. This study demonstrates that
SBs offer a novel model-based approach for quantifying stability during motion,
which could be used in physical therapy for individuals at risk of falling.

Ensuring that crops use water and nutrients efficiently is an important
strategy for increasing the profitability of farming and reducing the
environmental load from agriculture. Subsurface irrigation can be an
alternative to surface irrigation as a means of losing less irrigation water,
but the application timing and amount are often difficult to determine.
Well-defined soil and crop models are useful for assisting decision support,
but most of the models developed to date have been for surface irrigation. The
present study examines whether the Decision Support System for Agrotechnology
Transfer (DSSAT, version 4.5) cropping system model is applicable for the
production of processing tomatoes with subsurface irrigation, and it revises
the soil module to simulate irrigation schemes with subsurface irrigation. Five
farmed fields in California, USA, are used to test the performance of the
model. The original DSSAT model fails to produce fruit yield by overestimating
the water deficiency. The soil water module is then revised by introducing the
movement of soil moisture due to a vertical soil moisture gradient. Moreover,
an external parameter optimization system is constructed to minimize the error
between the simulation and observations. The revised module reduces the errors
in the soil moisture profile at each field compared to those by the original
DSSAT model. The average soil moisture error decreases from 0.065m^3/m^3 to
0.029m^3/m^3. The yields estimated by the modified model are in a reasonable
range from 80 to 150 ton/ha, which is commonly observed under well-managed
conditions. The present results show that although further testing is required
for yield prediction, the present modification to the original DSSAT model
improves the precision of the soil moisture profile under subsurface irrigation
and can be used for decision support for efficient producting of processing
tomatoes.

Zemblys et al. \cite{gazeNet} reported on a method for the classification of
eye-movements ("gazeNet"). I have found 3 errors and two problems with that
paper that are explained herein. \underline{\textit{\textbf{Error 1}}} The
gazeNet classification method was built assuming that a hand-scored dataset
from Lund University was all collected at 500 Hz, but in fact, six of the 34
recording files were actually collected at 200Hz. Of the six datasets that were
used as the training set for the gazeNet algorithm, 2 were actually collected
at 200Hz. \underline{\textit{\textbf{Problem 1}}} has to do with the fact that
even among the 500Hz data, the inter-timestamp intervals varied widely.
\underline{\textit{\textbf{Problem 2}}} is that there are many unusual
discontinuities in the saccade trajectories from the Lund University dataset
that make it a very poor choice for the construction of an automatic
classification method. \underline{\textit{\textbf{Error 2}}} The gazeNet
algorithm was trained on the Lund dataset, and then compared to other methods,
not trained on this dataset, in terms of performance on this dataset. This is
an inherently unfair comparison, and yet no where in the gazeNet paper is this
unfairness mentioned. \underline{\textit{\textbf{Error 3}}} arises out of the
novel event-related agreement analysis employed by the gazeNet authors.
Although the authors intended to classify unmatched events as either false
positives or false negatives, many are actually being classified as true
negatives. True negatives are not errors, and any unmatched event misclassified
as a true negative is actually driving kappa higher, whereas unmatched events
should be driving kappa lower.

Preferred walking speed is a widely-used performance measure for people with
mobility issues, but is usually measured in straight line walking for fixed
distances or durations. However, daily walking involves walking for bouts of
different distances and walking with turning. Here, we studied walking for
short distances and walking in circles in unilateral lower-limb amputees
wearing an above or below-knee passive prosthesis, specifically, a Jaipur foot
prosthesis. Analogous to earlier results in non-amputees, we found that their
preferred walking speeds are lower for short distances and lower for circles of
smaller radii. Using inverse optimization, we estimated the cost of changing
speeds and turning such that the observed preferred walking speeds in our
experiments minimizes the total energy cost. The inferred costs of changing
speeds and turning were much larger than for non-amputees. These findings could
inform prosthesis design and rehabilitation therapy to better assist changing
speeds and turning tasks in amputee walking. Further, measuring the preferred
speed for a range of distances and radii is a more robust subject-specific
measure of walking performance.

The land crab Cardisoma guanhumi Latreille, 1828 is harvested in several
countries in Latin America, and a critically endangered species. This is the
first study to conduct bootstrapped tagging analysis (BTA) together with
bootstrapped length-frequency analyses (BLFA). Crabs were sampled monthly in a
mangrove patch at Itamaraca Island (Brazil), over 12 months, and marked with
PIT tags. Both methods (BTA and BLFA) indicate very slow growth and Linf far
above Lmax. BTA estimates were K = 0.12 y-1 (95% CI: 0.024 to 0.26 y-1), Linf =
118 mm (95% CI: 81 to 363 mm), Phi' = 1.23 log10(cm y-1) (95% CI: 0.86 to 1.36
log10(cm y-1)). Seasonality in growth was significant (p = 0.006, 95% CI for C:
0.15 to 0.93, median: C = 0.56). Pairs of K and Linf always followed narrow
Phi' isopleths. Total mortality was Z = 2.18 y-1 (95% CI = 1.7 to 4.5 y-1).
Slow growth and a very high Z/K ratio highlight the need for protective
measures. BTA results were 2.2 to 3 times more precise than BLFA. Traditional
length-based methods produced grossly biased results, indicating the urgent
need for new, robust approaches and a critical reevaluation of long-standing
methods and paradigms.

Tardigrades are microscopic animals widely known for their survival
capabilities under extreme conditions. They are the focus of current research
in the fields of taxonomy, biogeography, genomics, proteomics, development,
space biology, evolution, and ecology. Tardigrades, such as Hypsibius
exemplaris, are being advocated as a next-generation model organism for genomic
and developmental studies. The raw culture of H. exemplaris usually contains
tardigrades themselves, their eggs, and algal food and feces. Experimentation
with tardigrades often requires the demanding and laborious separation of
tardigrades from raw samples to prepare pure and contamination-free tardigrade
samples. In this paper, we propose a two-step acousto-microfluidic separation
method to isolate tardigrades from raw samples. In the first step, a passive
microfluidic filter composed of an array of traps is used to remove large algal
clusters in the raw sample. In the second step, a surface acoustic wave-based
active microfluidic separation device is used to continuously deflect
tardigrades from their original streamlines inside the microchannel and thus
selectively isolate them from algae and eggs. The experimental results
demonstrated the efficient tardigrade separation with a recovery rate of 96%
and an algae impurity of 4% on average in a continuous, contactless, automated,
rapid, biocompatible manner.

Cardiovascular waveforms contain information for clinical diagnosis. By
"learning" and organizing the subtle change of waveform morphology from large
amounts of raw waveform data, unsupervised manifold learning helps delineate a
high-dimensional structure and display it as a novel three-dimensional (3D)
image. We investigate the electrocardiography (ECG) waveform for ischemic heart
disease and arterial blood pressure (ABP) waveform in dynamic vasoactive
episodes. We model each beat or pulse to be a point lying on a manifold, like a
surface, and use the diffusion map (DMap) to establish the relationship among
those pulses. For ECG datasets, first we analyzed the non-ST-elevation ECG
waveform distribution from unstable angina to healthy control, and we
investigated intraoperative ST-elevation ECG waveforms to show the dynamic ECG
waveform changes. For ABP datasets, we analyzed waveforms collected under
endotracheal intubation and administration of vasodilator. To quantify the
dynamic separation, we applied the support vector machine (SVM) analysis and
the trajectory analysis. For the non-ST-elevation ECG, a hierarchical tree
structure comprising consecutive ECG waveforms spanning from unstable angina to
healthy control is presented in the 3D image (accuracy=97.6%, macro-F1=96.1%).
The DMap helps quantify and visualize the evolving direction of intraoperative
ST-elevation myocardial episode in a 1-hour period (accuracy=97.58%,
macro-F1=96.06%). The ABP waveform analysis of Nicardipine administration shows
inter-individual difference (accuracy=95.01%, macro-F1=96.9%) and their common
directions from intra-individual moving trajectories. The dynamic change of the
ABP waveform during endotracheal intubation shows a loop-like trajectory
structure, which can be further divided using the knowledge obtained from
Nicardipine. The 3D images provide clues of underneath physiological
mechanisms.

Use of commercial growth chambers for study of biological processes involved
in biomass growth and production pose certain limitations on the nature of
studies that can be performed in them. Optimization of biomass rearing and
production process requires quantitative study of environment influences on the
organism and eventually the products and byproducts consumed and produced. This
work presents a low cost modular system designed to facilitate quantitative
study of growth processes and resource exchanges in organisms such as plants,
fungi and insect larvae. The proposed system constitutes of modular units each
performing a specific function. A novel compact thermoelectric cooler based
unit is designed for conditioning the air. Sensor cluster for measuring gas
concentrations, air properties (temperature, humidity, pressure), and growing
medium properties is implemented and tested. An actuator cluster for resource
exchange and a wiring and control scheme for light spectrum adjustment is
proposed. A three tier hierarchical software framework consisting of an
open-source cloud platform for data aggregation and user interaction, embedded
firmware for microcontroller, and an application development framework for test
automation and experiment regime design is developed and presented. A series of
experiments and tests were performed using the designed hardware and software
to evaluate its capabilities and limitations. This controlled environment was
used to study the photosynthesis and its dependency on temperature and light
intensity in Ocimum basilicum. In a second experiment, evolution of metabolic
activity of Hermetia illucens larvae over its larval phase was studied and the
metabolic products and byproducts were quantitatively measured.

Electroactive-Polymers (EAPs) are one of the best soft materials with great
applications in active microfluidics. Ionic ones (i-EAPs) have more promising
features for being appropriate candidates to use in active microfluidic
devices. Here, as a case study, we have designed and fabricated a microfluidic
micromixer using an i-EAP named Ionic Polymer-Metal Composite (IPMC). In
microfluidics, active devices have more functionality but due to their required
facilities are less effective for Point of Care Tests (POCTs). In the direction
of solving this paradox, we should use some active components that they need
minimum facilities. IPMC can be one of these components, hence by integrating
the IPMC actuator into a microfluidic channel, a micromixer chip was designed
and put to the simulation and experimental tests. The result showed that the
proposed micromixer is able to mix the micro fluids properly and IPMC actuator
has adequate potential to be an active component for POCT-based microfluidic
chips.

Improvements in experimental and computational technologies have led to
significant increases in data available for analysis. Topological data analysis
(TDA) is an emerging area of mathematical research that can identify structures
in these data sets. Here we develop a TDA method to detect physical structures
in a cell that persist over time. In most cells, protein filaments (actin)
interact with motor proteins (myosins) and organize into polymer networks and
higher-order structures. An example of these structures are ring channels that
maintain constant diameters over time and play key roles in processes such as
cell division, development, and wound healing. The interactions of actin with
myosin can be challenging to investigate experimentally in living systems,
given limitations in filament visualization \textit{in vivo}. We therefore use
complex agent-based models that simulate mechanical and chemical interactions
of polymer proteins in cells. To understand how filaments organize into
structures, we propose a TDA method that assesses effective ring generation in
data consisting of simulated actin filament positions through time. We analyze
the topological structure of point clouds sampled along these actin filaments
and propose an algorithm for connecting significant topological features in
time. We introduce visualization tools that allow the detection of dynamic ring
structure formation. This method provides a rigorous way to investigate how
specific interactions and parameters may impact the timing of filamentous
network organization.

Functional magnetic resonance imaging provides rich spatio-temporal data of
human brain activity during task and rest. Many recent efforts have focussed on
characterising dynamics of brain activity. One notable instance is
co-activation pattern (CAP) analysis, a frame-wise analytical approach that
disentangles the different functional brain networks interacting with a
user-defined seed region. While promising applications in various clinical
settings have been demonstrated, there is not yet any centralised, publicly
accessible resource to facilitate the deployment of the technique.
  Here, we release a working version of TbCAPs, a new toolbox for CAP analysis,
which includes all steps of the analytical pipeline, introduces new
methodological developments that build on already existing concepts, and
enables a facilitated inspection of CAPs and resulting metrics of brain
dynamics. The toolbox is available on a public academic repository
(https://c4science.ch/source/CAP_Toolbox.git).
  In addition, to illustrate the feasibility and usefulness of our pipeline, we
describe an application to the study of human cognition. CAPs are constructed
from resting-state fMRI using as seed the right dorsolateral prefrontal cortex,
and, in a separate sample, we successfully predict a behavioural measure of
continuous attentional performance from the metrics of CAP dynamics (R=0.59).

The discovery that repetitive mild traumatic brain injury (mTBI) can result
in chronic traumatic encephalopathy (CTE) in high risk contact sports has led
to increased scrutiny of head protective gear. In this work, we asked if it was
physically possible to prevent mTBI in American football with helmets alone.
Here, we show that modern helmets of several types are unlikely to prevent mTBI
from high speed collisions as might be seen in the NFL, but that introducing
liquid as an energy absorbing medium can dramatically reduce the forces of
impact across a spectrum of impact severities. We hypothesized that a helmet
which transmits a nearly constant force during football impacts is sufficient
to reduce biomechanical loading in the brain below the threshold of mTBI. To
test this hypothesis, we first show that the optimal impact force transmitted
to the head, in terms of brain strain, is in fact a constant force profile.
Then, to generate a constant force with a helmet, we implement a computational
model of a fluid-based shock absorber that adapts passively to any given impact
speed. Computer simulation of head impacts with liquid shock absorption
indicate that, at the highest impact speed, the average brain tissue strain is
reduced by 27.6% $\pm$ 9.3 compared to existing helmet padding that is
available on the market. These simulations are based on the NFL's helmet test
protocol and predict that adding liquid shock absorbers could reduce the number
of concussions by at least 75%. Taken together, these results suggest that the
majority of mTBI in football could be prevented with more efficient helmet
technology.

In this paper, we build a new, simple, and interpretable mathematical model
to estimate and forecast physiology related to the human glucose-insulin
system, constrained by available data. By constructing a simple yet flexible
model class with interpretable parameters, this general model can be
specialized to work in different settings, such as type 2 diabetes mellitus
(T2DM) and intensive care unit (ICU); different choices of appropriate model
functions describing uptake of nutrition and removal of glucose differentiate
between the models. In both cases, the available data is sparse and collected
in clinical settings, major factors that have constrained our model choice to
the simple form adopted.
  The model has the form of a linear stochastic differential equation (SDE) to
describe the evolution of the BG level. The model includes a term quantifying
glucose removal from the bloodstream through the regulation system of the human
body and two other terms representing the effect of nutrition and externally
delivered insulin. The stochastic fluctuations encapsulate model error
necessitated by the simple model form and enable flexible incorporation of
data. The model parameters must be learned in a patient-specific fashion,
leading to personalized models. We present experimental results on
patient-specific parameter estimation and future BG level forecasting in T2DM
and ICU settings. The resulting model leads to the prediction of the BG level
as an expected value accompanied by a band around this value which accounts for
uncertainties in the prediction. Such predictions, then, have the potential for
use as part of control systems that are robust to model imperfections and noisy
data. Finally, the model's predictive capability is compared with two different
models built explicitly for T2DM and ICU contexts.

In patients with depression, the use of 5-HT reuptake inhibitors can improve
the condition. Topological fingerprints, ECFP4, and molecular descriptors were
used. Some SERT and small molecules combined prediction models were established
by using 5 machine learning methods. We selected the higher accuracy models(RF,
SVM, LR) in five-fold cross-validation of training set to establish an
integrated model (VOL_CLF). The training set is from Chembl database and
oversampled by SMOTE algorithm to eliminate data imbalance. The unbalanced data
from same sources (Chembl) was used as Test set 1; the unbalanced data with
different sources(Drugbank) was used as Test set 2 . The prediction accuracy of
SERT inhibitors in Test set 1 was 90.7%~93.3%(VOL_CLF method was the highest);
the inhibitory recall rate was 84.6%-90.1%(RF method was the highest); the
non-inhibitor prediction accuracy rate was 76.1%~80.2%(RF method is the
highest); the non-inhibitor predictive recall rate is 81.2%~87.5% (SVM and
VOL_CLF methods were the highest) The RF model in Test Set 2 performed better
than the other models. The SERT inhibitor predicted accuracy rate, recall rate,
non-inhibitor predicted accuracy rate, recall rate were 42.9%, 85.7%, 95.7%,
73.3%.This study demonstrates that machine learning methods effectively predict
inhibitors of serotonin transporters and accelerate drug screening.

French recommendations for the screening of hepatitis B virus (HBV) infection
were updated in 2019 with the association of three markers: HBs Ag, anti-HBs Ab
and anti-HBc Ab. These three markers allow identification of infected patients,
vaccinated patients and patients who have been in contact with HBV. A positive
HBs Ag is usually associated with HBV infection but this interpretation must
take into account the clinical context. In particular, the absence of anti-HBc
Ab, normal ALAT levels and the absence of jaundice can be associated with
recent HBV vaccination or false-positive HBs Ag. Recent HBV vaccination can
usually be confirmed by patient questioning, while confirmatory tests are
useful to detect false positive HBs Ag. If necessary, a second sample can be
requested to confirm the interpretation.

Tuberculosis (TB) is one of the deadliest diseases worldwide, with 1,5
million fatalities every year along with potential devastating effects on
society, families and individuals. To address this alarming burden, vaccines
can play a fundamental role, even though to date no fully effective TB vaccine
really exists. Current treatments involve several combinations of antibiotics
administered to TB patients for up to two years, leading often to financial
issues and reduced therapy adherence. Along with this, the development and
spread of drug-resistant TB strains is another big complicating matter. Faced
with these challenges, there is an urgent need to explore new vaccination
strategies in order to boost immunity against tuberculosis and shorten the
duration of treatment. Computational modeling represents an extraordinary way
to simulate and predict the outcome of vaccination strategies, speeding up the
arduous process of vaccine pipeline development and relative time to market.
Here, we present EU - funded STriTuVaD project computational platform able to
predict the artificial immunity induced by RUTI and ID93/GLA-SE, two specific
tuberculosis vaccines. Such an in silico trial will be validated through a
phase 2b clinical trial. Moreover, STriTuVaD computational framework is able to
inform of the reasons for failure should the vaccinations strategies against M.
tuberculosis under testing found not efficient, which will suggest possible
improvements.

The family Asteraceae include large number of Centaurea species which have
been applied in folk medicine. One of the family Asteraceae members is the
Centaurea damascena which authentically been tested for its antibacterial and
antioxidant activity as well as its toxicity. The aims of the study were to
determine the antimicrobial and antioxidant activities and toxicity of
methanolic plant extracts of Centaurea damascena. The methanolic extracts were
screened for their antibacterial activity against nine bacteria (Staphylococcus
aureus ATCC 43300, Bacillus subtilis ATCC 6633, Micrococcus luteus ATCC 10240,
and Staphylococcus epidermidis ATCC 12228, Escherichia coli ATCC 11293,
Pseudomonas aerugino and Klebsiella pneumoniae, Enterobacter aerogenes ATCC
13048 and Salmonella typhi ATCC19430). The antibacterial activity was assessed
by using the disc diffusion methods and the minimum inhibition concentrations
(MIC) using microdilution method. The extracts from Centaurea damascena
possessed antibacterial activity against several of the tested microorganisms.
The MIC of methanol extract of C. damascena ranged from 60 to 1100 microgram
per mL. Free radical scavenging capacity of the C. damascena methanol extract
was calculated by DPPH and FRAP test. DPPH radicals were scavenged with an IC50
value of 17.08 microgram per mL. Antioxidant capacities obtained by the FRAP
was 51.9 and expressed in mg Trolox per gram dry weight. The total phenolic
compounds of the methanol extracts of aerial parts, as estimated by
Folin_Ciocalteu reagent method, was about 460 mg GAE per gram. The phenolic
contents in the extracts highly correlate with their antioxidant activity,
confirming that the antioxidant activity of this plant extracts is considerably
phenolic contents-dependent.

It is worth mentioning that the high output of different physiological
responses under the expression of vgb, may have a considerable effect on the
enzyme productivity, dairy industry, heavy metal uptake, biodegradation of
different organic pollutants and other applications. The expression of
bacterial haemoglobin is useful in lessening the load of perceived toxic
conditions such as high oxygen levels. This in turn probably has the same
impact on some peripheral toxic materials. This, hemoglobin biotechnology can
be extended to enhance production of pollutants degrading enzymes or production
of some valuable manufacturing materials on the case by case bases. It is
likely that the mechanism of bacterial hemoglobin (VHb) effects is
intermediated via an oxygen trapping action. This may drive the enrichment of
ATP production, which is mostly required for higher productivity of needed
substances for that activity.

Antibactrial activity of Asteriscus graveolens methanolic extract and its
synergy effect with fungal mediated silver nanoparticles (AgNPs) against some
enteric bacterial human pathogen was conducted. Silver nanoparticles were
synthesized by the fungal strain namely Tritirachium oryzae W5H as reported
early. In this study, MICs of AgNPs against E. aerogenes, Salmonella sp., E.
coli and C. albicans in order were 2.13, 19.15, 0.08 and 6.38 micrograms per
mL, respectively, while the MICs of A. graveolens ethanolic extract against the
same bacteria were 4, 366, 3300 and 40 micrograms per mL, respectively. The MIC
values at concentration less than 19.15 and 40 micrograms per mL indicating the
potent bacteriostatic effect of AgNPs and A. graveolens ethanolic
extract.Increasing in IFA was reported when Nitrofurantion and Trimethoprim
were combined with Etoh extract with maximum increase in IFA by 6 and 12 folds
for, respectively. Also, 10 folds increasing in IFA was reported when
trimethoprim was combined with AgNPs: Etoh extract.But, there were no
synergistic effect between the antifungal agents (Caspofungin and Micafungin)
combined with AgNPs and or A. graveolens ethanolic extract against C.
albicans.The potent synergistic effect of A. graveolens ethanolic extract
and/or NPs with the conventional antibiotics is novel in inhibiting antibiotics
resistant bacteria. In this study, remarkable increasing in the antibacterial
activity, when the most resistant antibiotics combined with A. graveolense
thanolic extract and/or NPs was reported.

The family Asteraceae include large number of Centaurea species which have
been applied in folk medicine. One of the family Asteraceae members is the
Centaurea damascena which authentically been tested for its antibacterial
activity. The aim of the study was to discuss antibacterial activities of
essential oil composition and methanolic extract of the same plant aerial part
leaves. Thirty-seven components were characterized with 86 of oxygenated
terpenes. The composition in percentage was dominated by 11.45 Fokienol, 8.8
thymol, 8.21 Alpha Terpineol, 7.24 Chrysanthemumic acid, 7.13 Terpinen4-ol and
6.59 Borneol with a high degree of polymorphism in the occurrence of these
compounds as compared with the different species of centaurea.. Free radical
scavenging capacity of the C. damascna methanol extract was calculated by DPPH
and FRAP test. DPPH radicals were scavenged with an IC50 value of 17.08
microgram per ml. Antioxidant capacities obtained by the FRAP was 51.9 and
expressed in mg Trolox gram per Liter dry weight. The total phenolic compounds
of the methanol extracts of aerial parts, as estimated by Folin Ciocalteu
reagent method, was about 460 milligram GAE per gram. The phenolic contents in
the extracts highly correlate with their antioxidant activity, confirming that
the antioxidant activity of this plant extracts is considerably phenolic
contents dependent.

Acute lymphoblastic leukemia is the most common malignancy in childhood.
Successful treatment requires initial high-intensity chemotherapy, followed by
low-intensity oral maintenance therapy with oral 6-mercaptopurine (6MP) and
methotrexate (MTX) until 2-3 years after disease onset. However, intra- and
interindividual variability in the pharmacokinetics (PK) and pharmacodynamics
(PD) of 6MP and MTX make it challenging to balance the desired antileukemic
effects with undesired excessive myelosuppression during maintenance therapy. A
model to simulate the dynamics of different cell types, especially neutrophils,
would be a valuable contribution to improving treatment protocols (6MP and MTX
dosing regimens) and a further step to understanding the heterogeneity in
treatment efficacy and toxicity. We applied and modified a recently developed
semi-mechanistic PK/PD model to neutrophils and analyzed their behavior using a
nonlinear mixed-effects modeling approach and clinical data obtained from 116
patients. The PK model of 6MP influenced the accuracy of absolute neutrophil
count (ANC) predictions, whereas the PD effect of MTX did not. Predictions
based on ANC were more accurate than those based on white blood cell counts.
Using the new cross-validated mathematical model, simulations of different
treatment protocols showed a linear dose-effect relationship and reduced ANC
variability for constant dosages. Advanced modeling allows the identification
of optimized control criteria and the weighting of specific influencing factors
for protocol design and individually adapted therapy to exploit the optimal
effect of maintenance therapy on survival.

Motivation: Combination therapies have been widely used to treat cancers.
However, it is cost- and time-consuming to experimentally screen synergistic
drug pairs due to the enormous number of possible drug combinations. Thus,
computational methods have become an important way to predict and prioritize
synergistic drug pairs.
  Results: We proposed a Deep Tensor Factorization (DTF) model, which
integrated a tensor factorization method and a deep neural network (DNN), to
predict drug synergy. The former extracts latent features from drug synergy
information while the latter constructs a binary classifier to predict the drug
synergy status. Compared to the tensor-based method, the DTF model performed
better in predicting drug synergy. The area under the precision-recall curve
(PR AUC) was 0.57 for DTF and 0.24 for the tensor method. We also compared the
DTF model with DeepSynergy and logistic regression models and found that the
DTF outperformed the logistic regression model and achieved almost the same
performance as DeepSynergy using several typical metrics for the classification
task. Applying the DTF model to predict missing entries in our drug-cell line
tensor, we identified novel synergistic drug combinations for 10 cell lines
from the 5 cancer types. A literature survey showed that some of these
predicted drug synergies have been identified in vivo or in vitro. Thus, the
DTF model could be valuable in silico tool for prioritizing novel synergistic
drug combinations.

Microarray techniques are widely used in Gene expression analysis. These
techniques are based on discovering submatrices of genes that share similar
expression patterns across a set of experimental conditions with coherence
constraint. Actually, these submatrices are called biclusters and the
extraction process is called biclustering. In this paper we present a novel
binary particle swarm optimization model for the gene expression biclustering
problem. Hence, we apply the binary particle swarm optimization algorithm with
a proposed measure, called Discretized Column-based Measure (DCM) as a novel
cost function for evaluating biclusters where biological relevance, MSR and the
size of the bicluster are considered as evaluation metrics for our results.
Results are compared to the existing algorithms and they show the validity of
our proposed approach.

T-cell receptors (TCR) are key proteins of the adaptive immune system,
generated randomly in each individual, whose diversity underlies our ability to
recognize infections and malignancies. Modeling the distribution of TCR
sequences is of key importance for immunology and medical applications. Here,
we compare two inference methods trained on high-throughput sequencing data: a
knowledge-guided approach, which accounts for the details of sequence
generation, supplemented by a physics-inspired model of selection; and a
knowledge-free Variational Auto-Encoder based on deep artificial neural
networks. We show that the knowledge-guided model outperforms the deep network
approach at predicting TCR probabilities, while being more interpretable, at a
lower computational cost.

Background:Diverse tacrolimus population pharmacokinetic models in adult
liver transplant recipients have been established to describe the PK
characteristics of tacrolimus in the last two decades. However, their
extrapolated predictive performance remains unclear.Therefore,in this study,we
aimed to evaluate their external predictability and identify their potential
influencing factors. Methods:The external predictability of each selected popPK
model was evaluated using an independent dataset of 84 patients with 572 trough
concentrations prospectively collected from Huashan Hospital. Prediction and
simulation based diagnostics and Bayesian forecasting were conducted to
evaluate model predictability. Furthermore, the effect of model structure on
the predictive performance was investigated.Results:Sixteen published popPK
models were assessed. In prediction-based diagnostics,the prediction error
within 30% was below 50% in all the published models. The simulation based
normalised prediction distribution error test and visual predictive check
indicated large discrepancies between the observations and simulations in most
of the models. Bayesian forecasting showed improvement in model predictability
with two to three prior observations. Additionally, the predictive performance
of the nonlinear Michaelis Menten model was superior to that of linear
compartment models,indicating the underlying nonlinear kinetics of tacrolimus
in liver transplant recipients.Conclusions:The published models performed
inadequately in prediction and simulation based diagnostics. Bayesian
forecasting may improve the predictive performance of the models. Furthermore,
nonlinear kinetics of tacrolimus may be mainly caused by the properties of the
drug itself, and incorporating nonlinear kinetics may be considered to improve
model predictability.

Background Little is known about the population pharmacokinetics (PPK) of
tacrolimus (TAC) in pediatric primary nephrotic syndrome (PNS). This study
aimed to compare the predictive performance between nonlinear and linear PK
models and investigate the significant factors of TAC PK characteristics in
pediatric PNS. Methods Data were obtained from 71 pediatric patients with PNS,
along with 525 TAC trough concentrations at steady state. The demographic,
medical, and treatment details were collected. Genetic polymorphisms were
analyzed. The PPK models were developed using nonlinear mixed effects model
software. Two modeling strategies, linear compartmental and nonlinear Michaelis
Menten (MM) models, were evaluated and compared. Results Body weight, age,
daily dose of TAC, co-therapy drugs (including azole antifungal agents and
diltiazem), and CYP3A5*3 genotype were important factors in the final linear
model (onecompartment model), whereas only body weight, codrugs, and CYP3A5*3
genotype were the important factors in the nonlinear MM model. Apparent
clearance and volume of distribution in the final linear model were 7.13 L/h
and 142 L, respectively. The maximal dose rate (Vmax) of the nonlinear MM model
was 1.92 mg/day and the average concentration at steady state at half-Vmax (Km)
was 1.98 ng/mL. The nonlinear model described the data better than the linear
model. Dosing regimens were proposed based on the nonlinear PK model.Conclusion
Our findings demonstrate that the nonlinear MM model showed better predictive
performance than the linear compartmental model, providing reliable support for
optimizing TAC dosing and adjustment in children with PNS.

Drug-drug interactions (DDI) can cause severe adverse drug reactions and pose
a major challenge to medication therapy. Recently, informatics-based approaches
are emerging for DDI studies. In this paper, we aim to identify key
pharmacological components in DDI based on large-scale data from DrugBank, a
comprehensive DDI database. With pharmacological components as features,
logistic regression is used to perform DDI classification with a focus on
searching for most predictive features, a process of identifying key
pharmacological components. Using univariate feature selection with chi-squared
statistic as the ranking criteria, our study reveals that top 10% features can
achieve comparable classification performance compared to that using all
features. The top 10% features are identified to be key pharmacological
components. Furthermore, their importance is quantified by feature coefficients
in the classifier, which measures the DDI potential and provides a novel
perspective to evaluate pharmacological components.

A while ago, the ideas of evolutionary biology inspired computer scientists
to develop a thriving nowadays field of evolutionary computation (EC), in
general, and genetic algorithms (GA), in particular. At the same time, the
directed evolution of biological molecules (in vitro evolution) is reasonably
interpreted as an implementation of GA in biochemical experiments. One of the
theoretical foundations of GA, justifying the effectiveness of evolutionary
search, is the concept of building blocks (BB). In EC, it is reasonable to
match these BBs to domains and motifs of macromolecules in evolutionary and
synthetic biology. Computer scientists have shown and carefully studied the
importance of preserving already found BBs for the effectiveness of
evolutionary search. For this purpose, dozens of algorithms have been
developed, including heuristic crossover algorithms. On the other hand, the
experimental procedures defining and preserving domains remain a poorly
developed area in the techniques of evolution in vitro. In this paper, we
demonstrate how several simple algorithms preserving the BBs can increase the
efficiency of in vitro evolution in numerical experiments by almost an order of
magnitude. As test problems, we propose and use such well-known problems of
synthetic biology as the evolutionary search for strong bacterial promoters
(with several motifs) and search for multi-domain RNA devices, as compared to
the classic GA tests (Royal Road functions). The success of these primary tests
with simple algorithms gives us every reason to expect that the implementation
and application of more advanced and modern EC procedures will give an even
greater increase in efficiency. Such an increase in search efficiency will
significantly reduce the cost of in vitro evolution experiments, which will
fully cover the costs of developing new experimental procedures based on these
algorithms.

Chagas disease American trypanosomiasis is caused by a flagellated parasite:
trypanosoma cruzi, transmitted by an insect of the genus Triatoma and also by
blood transfusions. In Latin America the number of infected people is
approximately 6 million, with a population exposed to the risk of infection of
550000. It is our interest to develop a non-invasive, low-cost methodology,
capable of detecting any alteration early on cardiaca produced by T. cruzi. We
analyzed the 24 hour RR records in patients with ECG abnormalities (CH2),
patients without ECG alterations (CH1) who had positive serological findings
for Chagas disease and healthy (Control) matched by sex and age. We found
significant differences between the Control, CH1 and CH2 groups that show
dysautonomy and enervation of the autonomic nervous system.

Motivation. Cancer heterogeneity is observed at multiple biological levels.
To improve our understanding of these differences and their relevance in
medicine, approaches to link organ- and tissue-level information from
diagnostic images and cellular-level information from genomics are needed.
However, these "radiogenomic" studies often use linear, shallow models, depend
on feature selection, or consider one gene at a time to map images to genes.
Moreover, no study has systematically attempted to understand the molecular
basis of imaging traits based on the interpretation of what the neural network
has learned. These current studies are thus limited in their ability to
understand the transcriptomic drivers of imaging traits, which could provide
additional context for determining clinical traits, such as prognosis.
  Results. We present an approach based on neural networks that takes
high-dimensional gene expressions as input and performs nonlinear mapping to an
imaging trait. To interpret the models, we propose gene masking and gene
saliency to extract learned relationships from radiogenomic neural networks. In
glioblastoma patients, our models outperform comparable classifiers (>0.10 AUC)
and our interpretation methods were validated using a similar model to identify
known relationships between genes and molecular subtypes. We found that imaging
traits had specific transcription patterns, e.g., edema and genes related to
cellular invasion, and 15 radiogenomic associations were predictive of
survival. We demonstrate that neural networks can model transcriptomic
heterogeneity to reflect differences in imaging and can be used to derive
radiogenomic associations with clinical value.

Closed loop anesthesia delivery (CLAD) systems can help anesthesiologists
efficiently achieve and maintain desired anesthetic depth over an extended
period of time. A typical CLAD system would use an anesthetic marker,
calculated from physiological signals, as real-time feedback to adjust
anesthetic dosage towards achieving a desired set-point of the marker. Since
control strategies for CLAD vary across the systems reported in recent
literature, a comparative analysis of common control strategies can be useful.
For a nonlinear plant model based on well-established models of compartmental
pharmacokinetics and sigmoid-Emax pharmacodynamics, we numerically analyze the
set-point tracking performance of three output-feedback linear control
strategies: proportional-integral-derivative (PID) control, linear quadratic
Gaussian (LQG) control, and an LQG with integral action (ILQG). Specifically,
we numerically simulate multiple CLAD sessions for the scenario where the plant
model parameters are unavailable for a patient and the controller is designed
based on a nominal model and controller gains are held constant throughout a
session. Based on the numerical analyses performed here, conditioned on our
choice of model and controllers, we infer that in terms of accuracy and bias
PID control performs better than ILQG which in turn performs better than LQG.
In the case of noisy observations, ILQG can be tuned to provide a smoother
infusion rate while achieving comparable steady-state response with respect to
PID. The numerical analyses framework and findings, reported here, can help
CLAD developers in their choice of control strategies. This paper may also
serve as a tutorial paper for teaching control theory for CLAD.

High-throughput sequencing of B- and T-cell receptors makes it possible to
track immune repertoires across time, in different tissues, and in acute and
chronic diseases or in healthy individuals. However, quantitative comparison
between repertoires is confounded by variability in the read count of each
receptor clonotype due to sampling, library preparation, and expression noise.
Here, we present a general Bayesian approach to disentangle repertoire
variations from these stochastic effects. Using replicate experiments, we first
show how to learn the natural variability of read counts by inferring the
distributions of clone sizes as well as an explicit noise model relating true
frequencies of clones to their read count. We then use that null model as a
baseline to infer a model of clonal expansion from two repertoire time points
taken before and after an immune challenge. Applying our approach to yellow
fever vaccination as a model of acute infection in humans, we identify
candidate clones participating in the response.

In this paper, we ask if it is possible to increase the interpretability in
multivariate analysis by aligning and projecting covariates onto comparative
subspaces. We demonstrate our method as well as the interpretative power of PLS
decomposed models and how robust interpretability can lead to quantitative
insights.
  We discuss the statistical properties of the PLS weights, $p$-values
associated with specific axes, as well as their alignment properties.
  The applicability of this approach within life science is also demonstrated
by applying it to three use cases of publically available datasets.
  Further we present hierarchical pathway enrichment results stemming from
aligned $p$-values, which are compared with results derived from enrichment
analysis, as an external validation of our method.
  We find that the method can uncover known results from genomics for all of
the studied use cases, i.e. microarray data from multiple sclerosis and
diabetes patients as well as RNA sequencing data from breast cancer patients.

In a systematic review, we investigate current applications of ultrasound in
locomotion research. Shortcomings in the range of view of ultrasound systems
affect the direct validation of musculoskeletal simulations as inverse
approaches have to be applied. We present currently used methods to estimate
muscle and tendon length in human plantarflexors.

Stochastic simulation algorithms (SSAs) are widely used to numerically
investigate the properties of stochastic, discrete-state models. The Gillespie
Direct Method is the pre-eminent SSA, and is widely used to generate sample
paths of so-called agent-based or individual-based models. However, the
simplicity of the Gillespie Direct Method often renders it impractical where
large-scale models are to be analysed in detail. In this work, we carefully
modify the Gillespie Direct Method so that it uses a customised binary decision
tree to trace out sample paths of the model of interest. We show that a
decision tree can be constructed to exploit the specific features of the chosen
model. Specifically, the events that underpin the model are placed in
carefully-chosen leaves of the decision tree in order to minimise the work
required to keep the tree up-to-date. The computational efficencies that we
realise can provide the apparatus necessary for the investigation of
large-scale, discrete-state models that would otherwise be intractable. Two
case studies are presented to demonstrate the efficiency of the method.

Hooge et al. asked the question: "Is human classification by experienced
untrained observers a gold standard in fixation detection?" They conclude the
answer is no. If they had entitled their paper: "Is human classification by
experienced untrained observers a gold standard in fixation detection when data
quality is very poor, data are error-filled, data presentation was not optimal,
and the analysis was seriously flawed?", I would have no case to make. In the
present report, I will present evidence to support my view that this latter
title is justified. The low quality data assessment is based on using a
relatively imprecise eye-tracker, the absence of head restraint for any
subjects, and the use of infants as the majority of subjects (60 of 70
subjects). Allowing subjects with more than 50% missing data (as much as 95%)
is also evidence of low quality data. The error-filled assessment is based on
evidence that a number of the "fixations" classified by "experts" have obvious
saccades within them, and that, apparently, a number of fixations were
classified on the basis of no signal at all. The evidence for non-optimal data
presentation stems from the fact that, in a number of cases, perfectly good
data was not presented to the coders. The flaws in the analysis are evidenced
by the fact that entire stretches of missing data were considered classified,
and that the measurement of saccade amplitude was based on many cases in which
there was no saccade at all. Without general evidence to the contrary, it is
correct to assume that some human classifiers under some conditions may meet
the criteria for a gold standard, and classifiers under other conditions may
not. This conditionality is not recognized by Hooge et al. A fair assessment
would conclude that whether or not humans can be considered a gold standard is
still very much an open question.

The aim of this study was to investigate the age dependence of the fitness
and body mass index (BMI) in Korean adults and to find an effective exercise to
restore the degradation of fitness due to aging. The age dependence of the
fitness and BMI were calculated using their lump mean values (LMVs) and a
linear regression method. The fitness sensitivity percentage to age (FSPA) and
fitness sensitivity percentage to BMI (FSPB) were introduced as indicators for
the effective improvement of the fitness. The results showed that the
degradation of fitness due to aging, especially the degradation of
cardiorespiratory endurance and muscular endurance, could be improved
effectively by controlling the 20-m multi-stage shuttle run and sit-up scores
for both males and females. The results also showed that the BMIs could be
effectively controlled with enhancing the 10-m shuttle run and standing long
jump scores for both males and females. It is expected that the LMV, FSPA, and
FSPB could be used to improve fitness effectively and to establish personal
exercise aims.

For high-throughput cell culture and associated analytics, droplet-based
cultivation systems open up the opportunities for parallelization and rapid
data generation. In contrast to microfluidics with continuous flow, sessile
droplet approaches enhance the flexibility for fluid manipulation with usually
less operational effort. Generating biologically favorable conditions and
promoting cell growth in a droplet, however, is particularly challenging due to
mass transfer limitations, which has to be solved by implementing an effective
mixing technique. Here, capillary waves induced by vertical oscillation are
used to mix inside a sessile droplet micro-bioreactor (MBR) system avoiding
additional moving parts inside the fluid. Depending on the excitation
frequency, different patterns are formed on the oscillating liquid surface,
which are described by a model of a vibrated sessile droplet. Analyzing mixing
times and oxygen transport into the liquid, a strong dependency of mass
transfer on the oscillation parameters, especially the excitation frequency, is
demonstrated. Oscillations at distinct capillary wave resonant frequencies lead
to rapid homogenization with mixing times of 2 s and volumetric liquid-phase
mass transfer coefficients of more than 340 h-1. This shows that the mass
transfer in a droplet MBR can be specifically controlled via capillary waves,
what is subsequently demonstrated for cultivations of Escherichia coli BL21
cells. Therefore, the presented MBR in combination with vertical oscillation
mixing for intensified mass transfer is a promising tool for highly parallel
cultivation and data generation.

We present the use of single-cell entropy (scEntropy) to measure the order of
the cellular transcriptome profile from single-cell RNA-seq data, which leads
to a method of unsupervised cell type classification through scEntropy followed
by the Gaussian mixture model (scEGMM). scEntropy is straightforward in
defining an intrinsic transcriptional state of a cell. scEGMM is a coherent
method of cell type classification that includes no parameters and no
clustering; however, it is comparable to existing machine learning-based
methods in benchmarking studies and facilitates biological interpretation.

Detection and monitoring of patients with pulmonary hypertension, defined as
mean blood pressure in the main pulmonary artery above 25 mmHg, requires a
combination of imaging and hemodynamic measurements. This study demonstrates
how to combine imaging data from microcomputed tomography (micro-CT) images
with hemodynamic pressure and flow waveforms from control and hypertensive
mice. Specific attention is devoted to developing a tool that processes CT
images, generating subject-specific arterial networks in which 1D fluid
dynamics modeling is used to predict blood pressure and flow. Each arterial
network is modeled as a directed graph representing vessels along the principal
pathway to ensure perfusion of all lobes. The 1D model couples these networks
with structured tree boundary conditions informed by the image data. Fluid
dynamics equations are solved in this network and compared to measurements of
pressure in the main pulmonary artery. Analysis of micro-CT images reveals that
the branching ratio is the same in the control and hypertensive animals, but
that the vessel length to radius ratio is significantly lower in the
hypertensive animals. Fluid dynamics predictions show that in addition to
changed network geometry, vessel stiffness is higher in the hypertensive animal
models than in the control models.

Animals such as insects have provided a rich source of inspiration for
designing robots. For example, animals navigate to goals via efficient
coordination of individual motor actions, and demonstrate natural solutions to
problems also faced by engineers. Recording individual body part positions
during large scale movement would therefore be useful. Such multi-scale
observations, however, are challenging. With video, for example, there is
typically a trade-off between the volume over which an animal can be recorded
and spatial resolution within the volume. Even with high pixel-count cameras,
motion blur can be a challenge when using available light. Here we present a
new approach for tracking animals, such as insects, with an optical system that
bypasses this tradeoff by actively pointing a telephoto video camera at the
animal. This system is based around high-speed pan-tilt mirrors which steer an
optical path shared by a quadrant photodiode and a high-resolution, high-speed
telephoto video recording system. The mirror is directed to lock on to the
image of a 25-milligram retroreflector worn by the animal. This system allows
high-magnification videography with reduced motion blur over a large tracking
volume. With our prototype, we obtained millisecond order closed-loop latency
and recorded videos of flying insects in a tracking volume extending to an
axial distance of 3 meters and horizontally and vertically by 40 degrees. The
system offers increased capabilities compared to other video recording
solutions and may be useful for the study of animal behavior and the design of
bio-inspired robots.

Drug-induced liver injury (DILI) is the most common cause of acute liver
failure and a frequent reason for withdrawal of candidate drugs during
preclinical and clinical testing. An important type of DILI is cholestatic
liver injury, caused by buildup of bile salts within hepatocytes; it is
frequently associated with inhibition of bile salt transporters, such as the
bile salt export pump (BSEP). Reliable in silico models to predict BSEP
inhibition directly from chemical structures would significantly reduce costs
during drug discovery and could help avoid injury to patients. Unfortunately,
models published to date have been insufficiently accurate to encourage wide
adoption. We report our development of classification and regression models for
BSEP inhibition with substantially improved performance over previously
published models. Our model development leveraged the ATOM Modeling PipeLine
(AMPL) developed by the ATOM Consortium, which enabled us to train and evaluate
thousands of candidate models. In the course of model development, we assessed
a variety of schemes for chemical featurization, dataset partitioning and class
labeling, and identified those producing models that generalized best to novel
chemical entities. Our best performing classification model was a neural
network with ROC AUC = 0.88 on our internal test dataset and 0.89 on an
independent external compound set. Our best regression model, the first ever
reported for predicting BSEP IC50s, yielded a test set $R^2 = 0.56$ and mean
absolute error 0.37, corresponding to a mean 2.3-fold error in predicted IC50s,
comparable to experimental variation. These models will thus be useful as
inputs to mechanistic predictions of DILI and as part of computational
pipelines for drug discovery.

The ARF-AID (Auxin Response Factor-Auxin Inducible Degron) system is a
re-engineered auxin-inducible protein degradation system. Inducible degron
systems are widely used to specifically and rapidly deplete proteins of
interest in cell lines and organisms. An advantage of inducible degradation is
that the biological system under study remains intact and functional until
perturbation. This feature necessitates that the endogenous levels of the
protein are maintained. However, endogenous tagging of genes with AID can
result in chronic, auxin-independent proteasome-mediated degradation. The
additional expression of the ARF-PB1 domain in the re-engineered ARF-AID system
prevents chronic degradation of AID-tagged proteins while preserving rapid
degradation of tagged proteins. Here we describe the protocol for engineering
human cell lines to implement the ARF-AID system for specific and inducible
protein degradation. These methods are adaptable and can be extended from cell
lines to organisms.