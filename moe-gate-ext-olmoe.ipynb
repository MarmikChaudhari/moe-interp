{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if DEVICE.type == \"cuda\":\n",
    "    # Print CUDA details\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated()/1024**2:.2f}MB\")\n",
    "    print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved()/1024**2:.2f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567c0bbaed2a447094aeb231a9d4e8e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_model(model_name):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        # use_flash_attention_2=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model.to(DEVICE)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model(\"allenai/OLMoE-1B-7B-0924\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OlmoeForCausalLM(\n",
       "  (model): OlmoeModel(\n",
       "    (embed_tokens): Embedding(50304, 2048, padding_idx=1)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x OlmoeDecoderLayer(\n",
       "        (self_attn): OlmoeSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (q_norm): OlmoeRMSNorm((2048,), eps=1e-05)\n",
       "          (k_norm): OlmoeRMSNorm((2048,), eps=1e-05)\n",
       "        )\n",
       "        (mlp): OlmoeSparseMoeBlock(\n",
       "          (gate): Linear(in_features=2048, out_features=64, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-63): 64 x OlmoeMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              (down_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): OlmoeRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): OlmoeRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): OlmoeRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): OlmoeRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt(prompt, tokenizer, max_tokens=2048):\n",
    "    \"\"\"\n",
    "    Prepare a prompt for processing, splitting if necessary to fit within model context.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The text prompt to prepare\n",
    "        tokenizer: The model's tokenizer\n",
    "        max_tokens: Maximum number of tokens per chunk (default: 2048)\n",
    "        \n",
    "    Returns:\n",
    "        List of prompts that fit within token limit\n",
    "    \"\"\"\n",
    "    # Check if the input is a list of lines/prompts\n",
    "    if isinstance(prompt, list):\n",
    "        all_prepared_prompts = []\n",
    "        for single_prompt in prompt:\n",
    "            # Process each line/prompt individually\n",
    "            prepared_chunks = prepare_prompt(single_prompt, tokenizer, max_tokens)\n",
    "            all_prepared_prompts.extend(prepared_chunks)\n",
    "        return all_prepared_prompts\n",
    "    \n",
    "    # Process a single prompt\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    \n",
    "    # If prompt is small enough, return as is\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return [prompt]\n",
    "    \n",
    "    # Split into manageable chunks\n",
    "    prepared_prompts = []\n",
    "    \n",
    "    # Decode tokens into chunks\n",
    "    start_idx = 0\n",
    "    while start_idx < len(tokens):\n",
    "        end_idx = min(start_idx + max_tokens, len(tokens))\n",
    "        chunk_tokens = tokens[start_idx:end_idx]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens)\n",
    "        prepared_prompts.append(chunk_text)\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    print(f\"Long prompt detected! Split into {len(prepared_prompts)} chunks.\")\n",
    "    return prepared_prompts\n",
    "\n",
    "def get_moe_metadata(model, input_ids):\n",
    "    \"\"\"Get both router logits and expert indices for all MoE layers in OLMoE.\"\"\"\n",
    "    router_logits_list = []\n",
    "    expert_indices_list = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        # For OLMoE, capture router logits (from gate) and routing decisions\n",
    "        hidden_states = input[0]\n",
    "        \n",
    "        # Get router logits\n",
    "        router_logits = module.gate(hidden_states)\n",
    "        router_logits_list.append(router_logits.detach())\n",
    "        \n",
    "        # For OLMoE, we need to manually compute the top-k experts since\n",
    "        # the returned indices aren't directly accessible\n",
    "        routing_weights = F.softmax(router_logits, dim=-1)\n",
    "        _, selected_experts = torch.topk(routing_weights, module.top_k, dim=-1)\n",
    "        expert_indices_list.append(selected_experts.detach())\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    hooks = []\n",
    "    for layer_idx, layer in enumerate(model.model.layers):\n",
    "        if hasattr(layer.mlp, 'experts'):  # Check if this is an MoE layer\n",
    "            hook = layer.mlp.register_forward_hook(hook_fn)\n",
    "            hooks.append(hook)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model(input_ids)\n",
    "    \n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    moe_metadata = {\n",
    "        'router_logits': torch.stack(router_logits_list) if router_logits_list else None,\n",
    "        'expert_indices': torch.stack(expert_indices_list) if expert_indices_list else None\n",
    "    }\n",
    "    \n",
    "    if moe_metadata['router_logits'] is not None:\n",
    "        print(f\"Router logits shape: {moe_metadata['router_logits'].shape}\")\n",
    "    if moe_metadata['expert_indices'] is not None:\n",
    "        print(f\"Expert indices shape: {moe_metadata['expert_indices'].shape}\")\n",
    "    \n",
    "    return moe_metadata\n",
    "\n",
    "def process_text_file_for_expert_counts(file_path, model, tokenizer, output_path=None, max_tokens=4096):\n",
    "    \"\"\"\n",
    "    Process a text file to analyze MoE routing and count tokens per expert in each layer for OLMoE.\n",
    "    Saves a PyTorch file with expert token counts.\n",
    "    \"\"\"\n",
    "    # Load the text file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Check if this is a GitHub code file\n",
    "    if 'github.txt' in file_path:\n",
    "        import re\n",
    "        # Find all code blocks using the file pattern\n",
    "        file_pattern = re.compile(r'.*\\b\\w+\\.(js|py|c|cpp|java|ts|rb|go|rs|cs|swift|kt|php)$', re.MULTILINE)\n",
    "        \n",
    "        # Find all matches (file headers)\n",
    "        matches = list(file_pattern.finditer(content))\n",
    "        \n",
    "        # Extract code blocks between file headers\n",
    "        raw_prompts = []\n",
    "        for i in range(len(matches)):\n",
    "            start_pos = matches[i].start()\n",
    "            # If this is the last match, go to the end of the file\n",
    "            if i == len(matches) - 1:\n",
    "                end_pos = len(content)\n",
    "            else:\n",
    "                end_pos = matches[i+1].start()\n",
    "            \n",
    "            # Extract the code block including the file header\n",
    "            code_block = content[start_pos:end_pos].strip()\n",
    "            raw_prompts.append(code_block)\n",
    "    else:\n",
    "        # Regular text file processing (one prompt per line)\n",
    "        raw_prompts = [line.strip() for line in content.split('\\n') if line.strip()]\n",
    "    \n",
    "    print(f\"Loaded {len(raw_prompts)} raw prompts from {file_path}\")\n",
    "    \n",
    "    # Prepare prompts (handle large prompts by splitting)\n",
    "    prompts = []\n",
    "    for raw_prompt in raw_prompts:\n",
    "        prepared_chunks = prepare_prompt(raw_prompt, tokenizer, max_tokens)\n",
    "        prompts.extend(prepared_chunks)\n",
    "    \n",
    "    print(f\"Processing {len(prompts)} prepared prompts (after splitting large ones)\")\n",
    "    \n",
    "    # Set default output path if not provided\n",
    "    if output_path is None:\n",
    "        output_path = file_path.replace('.txt', '_expert_data.pt')\n",
    "    \n",
    "    # Initialize counter for expert usage\n",
    "    # Structure: {layer_num: {expert_id: count}}\n",
    "    expert_counts = {}\n",
    "    \n",
    "    # Calculate total tokens for progress bar\n",
    "    total_tokens = 0\n",
    "    for prompt in prompts:\n",
    "        tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        total_tokens += tokens.size(1)\n",
    "    \n",
    "    print(f\"Total tokens to process: {total_tokens}\")\n",
    "    \n",
    "    # Initialize progress bar\n",
    "    pbar = tqdm(total=total_tokens, desc=\"Processing tokens\")\n",
    "    processed_tokens = 0\n",
    "    \n",
    "    # Process each prompt\n",
    "    for prompt in prompts:\n",
    "        # Tokenize the prompt\n",
    "        tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        seq_len = tokens.size(1)\n",
    "        \n",
    "        # Get MoE routing metadata\n",
    "        moe_metadata = get_moe_metadata(model, tokens)\n",
    "        \n",
    "        if moe_metadata['expert_indices'] is None:\n",
    "            print(\"No MoE layers detected or no routing information available\")\n",
    "            processed_tokens += seq_len\n",
    "            pbar.update(seq_len)\n",
    "            continue\n",
    "        \n",
    "        # Extract expert indices\n",
    "        expert_indices = moe_metadata['expert_indices']  # shape: [num_layers, batch, seq_len, top_k]\n",
    "        num_moe_layers = expert_indices.size(0)\n",
    "        \n",
    "        # Initialize counter for this batch if needed\n",
    "        for layer_idx in range(num_moe_layers):\n",
    "            layer_num = layer_idx + 1  # 1-based layer indexing\n",
    "            if layer_num not in expert_counts:\n",
    "                expert_counts[layer_num] = {}\n",
    "        \n",
    "        # Count token routing for each layer\n",
    "        for layer_idx in range(num_moe_layers):\n",
    "            layer_num = layer_idx + 1  # 1-based layer indexing\n",
    "            \n",
    "            # Process each token in sequence\n",
    "            for token_idx in range(seq_len):\n",
    "                # Get experts selected for this token in this layer (convert to integer)\n",
    "                # Adjust indexing based on actual shape\n",
    "                if expert_indices.dim() == 4:  # [layers, batch, seq, top_k]\n",
    "                    selected_experts = expert_indices[layer_idx, 0, token_idx].cpu().long().tolist()\n",
    "                else:  # [layers, seq, top_k]\n",
    "                    selected_experts = expert_indices[layer_idx, token_idx].cpu().long().tolist()\n",
    "                \n",
    "                # Count each expert\n",
    "                for expert_id in selected_experts:\n",
    "                    if expert_id not in expert_counts[layer_num]:\n",
    "                        expert_counts[layer_num][expert_id] = 0\n",
    "                    expert_counts[layer_num][expert_id] += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        processed_tokens += seq_len\n",
    "        pbar.update(seq_len)\n",
    "    \n",
    "    # Close progress bar\n",
    "    pbar.close()\n",
    "    \n",
    "    # Convert counts to a simple tensor format for saving\n",
    "    expert_token_counts = {}\n",
    "    for layer_num in sorted(expert_counts.keys()):\n",
    "        layer_data = expert_counts[layer_num]\n",
    "        # Create a tensor with counts for each expert (64 experts for OLMoE)\n",
    "        counts = torch.zeros(64)\n",
    "        for expert_id, count in layer_data.items():\n",
    "            # Convert to int to ensure it's a valid index\n",
    "            expert_id_int = int(expert_id)\n",
    "            counts[expert_id_int] = count\n",
    "        expert_token_counts[layer_num] = counts\n",
    "    \n",
    "    # Save just the token counts per expert\n",
    "    torch.save(expert_token_counts, output_path)\n",
    "    print(f\"Expert token counts saved to {output_path}\")\n",
    "    \n",
    "    # Create a DataFrame for visualization purposes\n",
    "    rows = []\n",
    "    for layer_num in sorted(expert_counts.keys()):\n",
    "        layer_data = expert_counts[layer_num]\n",
    "        for expert_id in range(64):  # 64 experts for OLMoE\n",
    "            count = layer_data.get(expert_id, 0)\n",
    "            rows.append({\n",
    "                'layer': layer_num,\n",
    "                'expert_id': expert_id,\n",
    "                'token_count': count\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "def analyze_text_file_routing(model, tokenizer, file_path):\n",
    "    \"\"\"\n",
    "    Main function to analyze MoE routing for a text file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to text file with prompts (one per line)\n",
    "        model_name: Name of DeepSeek MoE model to use\n",
    "    \"\"\"\n",
    "    # Process the file for expert counts and save as PyTorch file\n",
    "    df = process_text_file_for_expert_counts(file_path, model, tokenizer)\n",
    "    \n",
    "    print(f\"Analysis completed for {file_path}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 raw prompts from data-ext/test.txt\n",
      "Processing 2 prepared prompts (after splitting large ones)\n",
      "Total tokens to process: 537\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc3c4219b8d43b7a848efe90bf937ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing tokens:   0%|          | 0/537 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Router logits shape: torch.Size([16, 1, 266, 64])\n",
      "Expert indices shape: torch.Size([16, 1, 266, 8])\n",
      "Router logits shape: torch.Size([16, 1, 271, 64])\n",
      "Expert indices shape: torch.Size([16, 1, 271, 8])\n",
      "Expert token counts saved to data-ext/test_expert_data.pt\n",
      "Analysis completed for data-ext/test.txt\n"
     ]
    }
   ],
   "source": [
    "file_path = \"data-ext/test.txt\"\n",
    "df = analyze_text_file_routing(model, tokenizer, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wk/sgrd2bsj1msgt6bs3kfgj9hw0000gn/T/ipykernel_19424/61385568.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(\"data-ext/test_expert_data.pt\")\n"
     ]
    }
   ],
   "source": [
    "data = torch.load(\"data-ext/test_expert_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 65.,  70.,  20., 204., 154.,  25., 129.,  45.,   9.,   4.,  67., 524.,\n",
       "         83.,  13.,  11.,  26.,  35., 226.,  19., 105.,  33.,   8.,  37.,   9.,\n",
       "         46.,  27.,  60.,  13., 235.,  28.,  33.,  54.,  36.,  37.,   9.,  43.,\n",
       "         52.,  42.,  25.,  19.,  70., 120.,  66.,  36.,  55.,  83., 132., 101.,\n",
       "         82.,  40.,  80., 225.,  12.,  44.,  29.,  47.,  81., 112.,   3.,  16.,\n",
       "         73.,  43.,  53.,  13.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_graph_all_tokens_paper(expert_data, layer_number, tokenizer, domain=None):\n",
    "    \"\"\"\n",
    "    Visualizes expert distribution for all tokens in a file for a specific layer.\n",
    "    \n",
    "    Args:\n",
    "        expert_data: Dictionary with layer numbers as keys and tensor of expert counts as values\n",
    "                     or path to PyTorch file with this data\n",
    "        layer_number: Layer to analyze (1-27)\n",
    "        domain: Optional domain name for title (e.g., 'GSM8K', 'Math', etc.)\n",
    "    \n",
    "    Returns:\n",
    "        fig: Plotly figure object\n",
    "    \"\"\"\n",
    "    # Load data if a file path is provided\n",
    "    if isinstance(expert_data, str):\n",
    "        txt_file_path = expert_data.replace(\"data-ext/pt/\", \"data-ext/\").replace(\"_expert_data.pt\", \".txt\").replace(\"_expert_data_chat.pt\", \".txt\").replace(\"_expert_data_base.pt\", \".txt\")\n",
    "        \n",
    "        num_tokens = None\n",
    "        if os.path.exists(txt_file_path):\n",
    "            # Try different encodings\n",
    "            encodings = ['utf-8', 'latin-1', 'utf-16']\n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    with open(txt_file_path, 'r', encoding=encoding) as f:\n",
    "                        text_content = f.read()\n",
    "                        tokens = tokenizer(text_content, return_tensors=\"pt\")\n",
    "                        num_tokens = tokens.input_ids.numel()\n",
    "                        print(f\"Total tokens in {txt_file_path}: {num_tokens}\")\n",
    "                        break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            else:\n",
    "                print(f\"Could not read {txt_file_path} with any of the attempted encodings\")\n",
    "        \n",
    "        expert_data = torch.load(expert_data)\n",
    "    else:\n",
    "        # If expert_data is already loaded (not a string path)\n",
    "        num_tokens = None\n",
    "    \n",
    "    # Validate layer number is in the data\n",
    "    if layer_number not in expert_data:\n",
    "        raise ValueError(f\"Layer {layer_number} not found in expert data\")\n",
    "    \n",
    "    # Get counts for the specified layer\n",
    "    expert_counts = expert_data[layer_number].numpy()\n",
    "        \n",
    "    # Compute percentages\n",
    "    total_tokens = num_tokens\n",
    "    if total_tokens == 0:\n",
    "        print(\"No tokens found for this layer\")\n",
    "        return\n",
    "    \n",
    "    print(f\"total_tokens: {total_tokens}\")\n",
    "    print(f\"num_tokens: {num_tokens}\")\n",
    "    percentages = (expert_counts / total_tokens) * 100\n",
    "    \n",
    "    # Set discrete opacity based on 9.375% threshold\n",
    "    # 9.375% is 6 times the expected uniform distribution (1/64 = 1.5625%)\n",
    "    threshold = 12.5\n",
    "    opacities = np.where(percentages >= threshold, 1.0, 0.3)\n",
    "    \n",
    "    # Create plotly figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add bar trace with discrete color and opacity\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=list(range(64)),\n",
    "        y=percentages,\n",
    "        marker=dict(\n",
    "            color='#636EFA',  # Blue color for all bars\n",
    "            opacity=opacities\n",
    "        ),\n",
    "        hovertemplate='Expert ID: %{x}<br>Tokens: %{text}<br>Percentage: %{y:.2f}%<extra></extra>',\n",
    "        text=[f\"{int(count)}\" for count in expert_counts],\n",
    "        textposition='none'  # Ensure no text is displayed on the bars\n",
    "    ))\n",
    "    \n",
    "    # Add horizontal line at threshold\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=-0.5,\n",
    "        x1=63.5,\n",
    "        y0=threshold,\n",
    "        y1=threshold,\n",
    "        line=dict(\n",
    "            color=\"red\",\n",
    "            width=2,\n",
    "            dash=\"dash\",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add annotation for the threshold line\n",
    "    fig.add_annotation(\n",
    "        x=63,\n",
    "        y=threshold,\n",
    "        text=f\"{threshold}% threshold\",\n",
    "        showarrow=False,\n",
    "        yshift=10,\n",
    "        font=dict(color=\"red\")\n",
    "    )\n",
    "    \n",
    "    routed_tokens = expert_counts.sum()\n",
    "    \n",
    "    # Domain label for title\n",
    "    domain_label = f\" - {domain}\" if domain else \"\"\n",
    "    token_info = f\" (No. of Tokens: {num_tokens}, Routed Tokens: {int(routed_tokens)})\" if num_tokens else f\" (Routed Tokens: {int(routed_tokens)})\"\n",
    "    \n",
    "    # Calculate y-axis max based on the max percentage or default to 100 if num_tokens is None\n",
    "    y_max = 100\n",
    "    \n",
    "    # Update layout with white background\n",
    "    fig.update_layout(\n",
    "        title=f'Expert Usage Distribution - Layer {layer_number}{domain_label}{token_info}',\n",
    "        xaxis_title='Expert ID',\n",
    "        yaxis_title='Usage Percentage (%)',\n",
    "        yaxis_range=[0, y_max],  # Dynamic y-axis based on data\n",
    "        xaxis=dict(tickmode='linear', tick0=0, dtick=4),\n",
    "        showlegend=False,\n",
    "        width=1000,\n",
    "        height=600,\n",
    "        plot_bgcolor='white',\n",
    "        paper_bgcolor='white'\n",
    "    )\n",
    "    \n",
    "    # Add gridlines with lighter color\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='rgba(128, 128, 128, 0.1)')\n",
    "    fig.update_xaxes(showgrid=False)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in data-ext/test.txt: 538\n",
      "total_tokens: 538\n",
      "num_tokens: 538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wk/sgrd2bsj1msgt6bs3kfgj9hw0000gn/T/ipykernel_19424/473425529.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expert_data = torch.load(expert_data)\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Expert ID: %{x}<br>Tokens: %{text}<br>Percentage: %{y:.2f}%<extra></extra>",
         "marker": {
          "color": "#636EFA",
          "opacity": [
           0.3,
           0.3,
           1,
           1,
           0.3,
           0.3,
           0.3,
           1,
           1,
           0.3,
           1,
           0.3,
           0.3,
           0.3,
           0.3,
           1,
           0.3,
           0.3,
           0.3,
           1,
           0.3,
           0.3,
           0.3,
           0.3,
           0.3,
           0.3,
           1,
           1,
           0.3,
           0.3,
           0.3,
           0.3,
           0.3,
           0.3,
           0.3,
           0.3,
           0.3,
           1,
           0.3,
           1,
           1,
           0.3,
           0.3,
           0.3,
           1,
           0.3,
           1,
           1,
           1,
           1,
           0.3,
           0.3,
           0.3,
           0.3,
           0.3,
           0.3,
           0.3,
           0.3,
           0.3,
           0.3,
           0.3,
           1,
           0.3,
           1
          ]
         },
         "text": [
          "45",
          "40",
          "100",
          "90",
          "37",
          "62",
          "4",
          "357",
          "498",
          "27",
          "71",
          "41",
          "15",
          "40",
          "0",
          "82",
          "57",
          "48",
          "42",
          "213",
          "57",
          "2",
          "13",
          "31",
          "42",
          "1",
          "109",
          "152",
          "3",
          "25",
          "8",
          "43",
          "64",
          "0",
          "42",
          "11",
          "8",
          "110",
          "51",
          "246",
          "78",
          "40",
          "35",
          "21",
          "82",
          "54",
          "119",
          "186",
          "70",
          "71",
          "54",
          "47",
          "33",
          "54",
          "44",
          "50",
          "63",
          "47",
          "3",
          "66",
          "25",
          "90",
          "5",
          "72"
         ],
         "textposition": "none",
         "type": "bar",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63
         ],
         "y": [
          8.364312171936035,
          7.434944152832031,
          18.587360382080078,
          16.72862434387207,
          6.877323150634766,
          11.524164199829102,
          0.7434943914413452,
          66.35688018798828,
          92.56505584716797,
          5.018587589263916,
          13.197025299072266,
          7.620818138122559,
          2.7881040573120117,
          7.434944152832031,
          0,
          15.241636276245117,
          10.594795227050781,
          8.9219331741333,
          7.806691646575928,
          39.5910758972168,
          10.594795227050781,
          0.3717471957206726,
          2.4163568019866943,
          5.762082099914551,
          7.806691646575928,
          0.1858735978603363,
          20.260223388671875,
          28.252788543701172,
          0.5576208233833313,
          4.6468400955200195,
          1.4869887828826904,
          7.992564678192139,
          11.895910263061523,
          0,
          7.806691646575928,
          2.044609785079956,
          1.4869887828826904,
          20.446096420288086,
          9.479554176330566,
          45.72490692138672,
          14.498141288757324,
          7.434944152832031,
          6.5055766105651855,
          3.903345823287964,
          15.241636276245117,
          10.037175178527832,
          22.118959426879883,
          34.57249069213867,
          13.011153221130371,
          13.197025299072266,
          10.037175178527832,
          8.73606014251709,
          6.133829116821289,
          10.037175178527832,
          8.178439140319824,
          9.293680191040039,
          11.710037231445312,
          8.73606014251709,
          0.5576208233833313,
          12.267658233642578,
          4.6468400955200195,
          16.72862434387207,
          0.9293680191040039,
          13.38290023803711
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "color": "red"
          },
          "showarrow": false,
          "text": "12.5% threshold",
          "x": 63,
          "y": 12.5,
          "yshift": 10
         }
        ],
        "height": 600,
        "paper_bgcolor": "white",
        "plot_bgcolor": "white",
        "shapes": [
         {
          "line": {
           "color": "red",
           "dash": "dash",
           "width": 2
          },
          "type": "line",
          "x0": -0.5,
          "x1": 63.5,
          "y0": 12.5,
          "y1": 12.5
         }
        ],
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Expert Usage Distribution - Layer 15 (No. of Tokens: 538, Routed Tokens: 4296)"
        },
        "width": 1000,
        "xaxis": {
         "dtick": 4,
         "showgrid": false,
         "tick0": 0,
         "tickmode": "linear",
         "title": {
          "text": "Expert ID"
         }
        },
        "yaxis": {
         "gridcolor": "rgba(128, 128, 128, 0.1)",
         "gridwidth": 1,
         "range": [
          0,
          100
         ],
         "showgrid": true,
         "title": {
          "text": "Usage Percentage (%)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = bar_graph_all_tokens_paper(\"data-ext/test_expert_data.pt\", layer_number=15, tokenizer=tokenizer)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
