{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        # use_flash_attention_2=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model(\"deepseek-ai/deepseek-moe-16b-base\")\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, tokenizer, txt_file_path, device=device, domain=\"code\", top_k=2):\n",
    "    \"\"\"\n",
    "    Calculate perplexity using the DeepSeek model with configurable number of experts per token.\n",
    "    \n",
    "    Args:\n",
    "        model: The DeepSeek model\n",
    "        tokenizer: The tokenizer to use\n",
    "        txt_file_path: Path to the input text file containing samples\n",
    "        device: The device to run on (cuda/cpu)\n",
    "        domain: Domain name for output file (\"code\", \"text\", etc)\n",
    "        top_k: Number of experts to select per token (1-6)\n",
    "    \"\"\"\n",
    "    # Validate top_k parameter\n",
    "    if not 1 <= top_k <= 6:\n",
    "        raise ValueError(\"top_k must be between 1 and 6\")\n",
    "    \n",
    "    # Configure MoE layers to use specified number of experts\n",
    "    for layer in model.model.layers:\n",
    "        if hasattr(layer.mlp, 'experts'):  # Check if it's an MoE layer\n",
    "            layer.mlp.num_experts_per_tok = top_k\n",
    "            if hasattr(layer.mlp, 'gate'):\n",
    "                layer.mlp.gate.top_k = top_k\n",
    "\n",
    "    # Read text file and get samples\n",
    "    with open(txt_file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Process the text file based on its content\n",
    "    if 'github.txt' in txt_file_path:\n",
    "        import re\n",
    "        # Find all code blocks using the file pattern\n",
    "        file_pattern = re.compile(r'.*\\b\\w+\\.(js|py|c|cpp|java|ts|rb|go|rs|cs|swift|kt|php)$', re.MULTILINE)\n",
    "        \n",
    "        # Find all matches (file headers)\n",
    "        matches = list(file_pattern.finditer(content))\n",
    "        \n",
    "        # Extract code blocks between file headers\n",
    "        samples = []\n",
    "        for i in range(len(matches)):\n",
    "            start_pos = matches[i].start()\n",
    "            # If this is the last match, go to the end of the file\n",
    "            if i == len(matches) - 1:\n",
    "                end_pos = len(content)\n",
    "            else:\n",
    "                end_pos = matches[i+1].start()\n",
    "            \n",
    "            # Extract the code block including the file header\n",
    "            code_block = content[start_pos:end_pos].strip()\n",
    "            samples.append(code_block)\n",
    "    else:\n",
    "        # Regular text file processing (one prompt per line)\n",
    "        samples = [line.strip() for line in content.split('\\n') if line.strip()]\n",
    "\n",
    "    # Calculate perplexity for each sample\n",
    "    perplexities = []\n",
    "    file_name = f\"{domain}_perplexity_top{top_k}.csv\"\n",
    "    \n",
    "    # Count total chunks for progress bar\n",
    "    total_chunks = 0\n",
    "    for sample in samples:\n",
    "        if not sample.strip():\n",
    "            continue\n",
    "        encodings = tokenizer(sample.strip(), return_tensors='pt')\n",
    "        seq_len = encodings.input_ids.size(1)\n",
    "        chunks = (seq_len + 2047) // 2048  # Ceiling division\n",
    "        total_chunks += max(1, chunks)\n",
    "    \n",
    "    progress_bar = tqdm(total=total_chunks, desc=f\"Processing {domain} samples (top-k={top_k})\")\n",
    "    \n",
    "    with open(file_name, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        if csvfile.tell() == 0:\n",
    "            writer.writerow([f'{domain}_num', 'perplexity', 'chunks'])\n",
    "            \n",
    "        for i, sample in enumerate(samples):\n",
    "            if not sample.strip():\n",
    "                continue\n",
    "                \n",
    "            # Tokenize sample\n",
    "            encodings = tokenizer(sample.strip(), return_tensors='pt')\n",
    "            input_ids = encodings.input_ids.to(device)\n",
    "            \n",
    "            # Check if we need to chunk the sequence\n",
    "            seq_len = input_ids.size(1)\n",
    "            if seq_len > 2048:\n",
    "                # Split into chunks of 2048 tokens\n",
    "                chunks = []\n",
    "                for start_idx in range(0, seq_len, 2048):\n",
    "                    end_idx = min(start_idx + 2048, seq_len)\n",
    "                    chunks.append(input_ids[:, start_idx:end_idx])\n",
    "                \n",
    "                # Calculate perplexity for each chunk and average\n",
    "                chunk_losses = []\n",
    "                for chunk_idx, chunk in enumerate(chunks):\n",
    "                    target_ids = chunk.clone()\n",
    "                    \n",
    "                    # Initialize loss function\n",
    "                    loss_fct = CrossEntropyLoss(reduction='none')\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(chunk)\n",
    "                        logits = outputs.logits\n",
    "                        \n",
    "                        # Clean up CUDA memory\n",
    "                        del outputs\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "                        # Shift logits and target_ids for next-token prediction\n",
    "                        shift_logits = logits[..., :-1, :].contiguous()\n",
    "                        shift_target_ids = target_ids[..., 1:].contiguous()\n",
    "                        \n",
    "                        # Calculate loss\n",
    "                        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), \n",
    "                                      shift_target_ids.view(-1)).cpu()\n",
    "                        \n",
    "                        chunk_losses.append(loss)\n",
    "                        del shift_logits, shift_target_ids, loss\n",
    "                        \n",
    "                    progress_bar.update(1)\n",
    "                \n",
    "                # Combine losses from all chunks\n",
    "                combined_loss = torch.cat(chunk_losses)\n",
    "                avg_nll = combined_loss.mean()\n",
    "                ppl = torch.exp(avg_nll).item()\n",
    "                del combined_loss, avg_nll, chunk_losses\n",
    "                \n",
    "                progress_bar.set_postfix({'Perplexity': f'{ppl:.2f}', 'Chunks': len(chunks)})\n",
    "                perplexities.append((i+1, ppl))\n",
    "                \n",
    "                writer.writerow([i+1, ppl, len(chunks)])\n",
    "                \n",
    "            else:\n",
    "                # Process as a single chunk\n",
    "                target_ids = input_ids.clone()\n",
    "                \n",
    "                # Initialize loss function\n",
    "                loss_fct = CrossEntropyLoss(reduction='none')\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model(input_ids)\n",
    "                    logits = outputs.logits\n",
    "                    \n",
    "                    # Clean up CUDA memory\n",
    "                    del outputs\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "                    # Shift logits and target_ids for next-token prediction\n",
    "                    shift_logits = logits[..., :-1, :].contiguous()\n",
    "                    shift_target_ids = target_ids[..., 1:].contiguous()\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), \n",
    "                                  shift_target_ids.view(-1)).cpu()\n",
    "                    del shift_logits, shift_target_ids\n",
    "                    \n",
    "                    # Calculate perplexity\n",
    "                    avg_nll = loss.mean()\n",
    "                    ppl = torch.exp(avg_nll).item()\n",
    "                    del loss, avg_nll\n",
    "                    \n",
    "                    progress_bar.update(1)\n",
    "                    progress_bar.set_postfix({'Perplexity': f'{ppl:.2f}', 'Chunks': 1})\n",
    "                    perplexities.append((i+1, ppl))\n",
    "                    \n",
    "                    writer.writerow([i+1, ppl, 1])\n",
    "    \n",
    "    return perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file_path = \"data-ext/gsm8k.txt\"\n",
    "\n",
    "for k in range(6, 0, -1):\n",
    "    ppl = calculate_perplexity(model=model,\n",
    "                              tokenizer=tokenizer,\n",
    "                              txt_file_path=text_file_path,\n",
    "                              domain=\"gsm8k\",\n",
    "                              top_k=k)\n",
    "    print(f\"Perplexity (top-{k}): {ppl}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('code_perplexity_top6.csv')\n",
    "\n",
    "# Print column names to verify\n",
    "print(\"Available columns:\", df.columns.tolist())\n",
    "\n",
    "# Create the line plot using plotly\n",
    "fig = px.line(df, x='code_num', y='perplexity', \n",
    "              markers=True,\n",
    "              title='Perplexity by Code Number')\n",
    "\n",
    "# Customize the plot\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Code Number\",\n",
    "    yaxis_title=\"Perplexity\",\n",
    "    xaxis=dict(showgrid=True),\n",
    "    yaxis=dict(showgrid=True)\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "# Read all CSV files and calculate means of log perplexities\n",
    "files = [f'code_perplexity_top{k}.csv' for k in range(1,7)]\n",
    "labels = [f'Top {k}' for k in range(1,7)]\n",
    "log_means = []\n",
    "perplexities = []\n",
    "\n",
    "for file in files:\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(file)\n",
    "    # Calculate mean of log perplexities\n",
    "    log_perplexity = np.log(df['perplexity'])\n",
    "    log_mean = log_perplexity.mean()\n",
    "    log_means.append(log_mean)\n",
    "    perplexities.append(np.exp(log_mean))\n",
    "    \n",
    "    # Print both log and perplexity space results for reference\n",
    "    k = file.split('top')[1].split('.')[0]\n",
    "    print(f\"{k}:\")\n",
    "    print(f\"  Mean log perplexity: {log_mean:.3f}\")\n",
    "    print(f\"  Equivalent perplexity: {np.exp(log_mean):.3f}\")\n",
    "\n",
    "# Create DataFrame for saving\n",
    "results_df = pd.DataFrame({\n",
    "    'Selection': labels,\n",
    "    'Log_Perplexity': log_means,\n",
    "    'Perplexity': perplexities\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('mean_perplexities.csv', index=False)\n",
    "\n",
    "# Create line plot\n",
    "fig = px.line(results_df, \n",
    "              x='Selection', \n",
    "              y='Log_Perplexity',\n",
    "              markers=True,\n",
    "              text=[f'{v:.3f}' for v in log_means],\n",
    "              title='Mean Log Perplexity by Top-k Selection')\n",
    "# Customize the plot\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Selection Method\",\n",
    "    yaxis_title=\"Mean Log Perplexity\", \n",
    "    xaxis=dict(showgrid=True)\n",
    ")\n",
    "\n",
    "# Update marker and text positions\n",
    "fig.update_traces(textposition=\"middle right\")\n",
    "\n",
    "# Display the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_perplexity_by_domain(domains):\n",
    "    \"\"\"\n",
    "    Analyze perplexity across different domains and plot them together.\n",
    "    \n",
    "    Args:\n",
    "        domains: List of domain names to analyze\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for domain in domains:\n",
    "        # Read all CSV files and calculate means of log perplexities\n",
    "        files = [f'data-ext/pplx-csv/{domain}_perplexity_top{k}.csv' for k in range(1,7)]\n",
    "        labels = [f'Top {k}' for k in range(1,7)]\n",
    "        log_means = []\n",
    "        perplexities = []\n",
    "\n",
    "        for file in files:\n",
    "            # Read CSV file\n",
    "            df = pd.read_csv(file)\n",
    "            # Calculate mean of log perplexities\n",
    "            log_perplexity = np.log(df['perplexity'])\n",
    "            log_mean = log_perplexity.mean()\n",
    "            log_means.append(log_mean)\n",
    "            perplexities.append(np.exp(log_mean))\n",
    "            \n",
    "            # Print both log and perplexity space results for reference\n",
    "            k = file.split('top')[1].split('.')[0]\n",
    "            print(f\"{domain} - {k}:\")\n",
    "            print(f\"  Mean log perplexity: {log_mean:.3f}\")\n",
    "            print(f\"  Equivalent perplexity: {np.exp(log_mean):.3f}\")\n",
    "\n",
    "        # Create DataFrame for this domain\n",
    "        results_df = pd.DataFrame({\n",
    "            'Selection': labels,\n",
    "            'Log_Perplexity': log_means,\n",
    "            'Perplexity': perplexities,\n",
    "            'Domain': domain\n",
    "        })\n",
    "        \n",
    "        all_results.append(results_df)\n",
    "        \n",
    "        # Save individual domain results to CSV\n",
    "        results_df.to_csv(f'pplx-data/{domain}_mean_perplexities.csv', index=False)\n",
    "    \n",
    "    # Combine all domain results\n",
    "    combined_results = pd.concat(all_results)\n",
    "    \n",
    "    # Save combined results\n",
    "    combined_results.to_csv('pplx-data/all_domains_mean_perplexities.csv', index=False)\n",
    "    \n",
    "    # Normalize log perplexities to 0-1 scale for each domain\n",
    "    normalized_results = combined_results.copy()\n",
    "    \n",
    "    # Group by domain and normalize log perplexity within each domain\n",
    "    for domain in domains:\n",
    "        domain_data = normalized_results[normalized_results['Domain'] == domain]\n",
    "        min_val = domain_data['Log_Perplexity'].min()\n",
    "        max_val = domain_data['Log_Perplexity'].max()\n",
    "        \n",
    "        # Avoid division by zero if min and max are the same\n",
    "        if max_val > min_val:\n",
    "            normalized_results.loc[normalized_results['Domain'] == domain, 'Normalized_Log_Perplexity'] = (\n",
    "                (normalized_results.loc[normalized_results['Domain'] == domain, 'Log_Perplexity'] - min_val) / \n",
    "                (max_val - min_val)\n",
    "            )\n",
    "        else:\n",
    "            normalized_results.loc[normalized_results['Domain'] == domain, 'Normalized_Log_Perplexity'] = 0.5\n",
    "    \n",
    "    # Create line plot with all domains using normalized values\n",
    "    fig = px.line(normalized_results, \n",
    "                  x='Selection', \n",
    "                  y='Normalized_Log_Perplexity',\n",
    "                  color='Domain',\n",
    "                  markers=True,\n",
    "                  text='Normalized_Log_Perplexity',\n",
    "                  title='Normalized Log Perplexity by Top-k Selection Across Domains (0-1 Scale)')\n",
    "    \n",
    "    # Customize the plot\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Selection Method\",\n",
    "        yaxis_title=\"Normalized Log Perplexity (0-1 Scale)\", \n",
    "        xaxis=dict(showgrid=True),\n",
    "        yaxis=dict(range=[-0.1, 1.1])\n",
    "    )\n",
    "    \n",
    "    # Update marker and text positions\n",
    "    fig.update_traces(textposition=\"middle right\", texttemplate='%{text:.3f}')\n",
    "    \n",
    "    # Display the plot\n",
    "    fig.show()\n",
    "\n",
    "# Example usage\n",
    "analyze_perplexity_by_domain(['french-qa', 'english', 'arxiv', 'aime-math', 'chinese', 'arxiv-title-abs', 'github'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
