{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        # use_flash_attention_2=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model(\"deepseek-ai/deepseek-moe-16b-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_tokens(hidden_states: torch.Tensor, lm_head: torch.nn.Linear, tokenizer, k: int = 5) -> List[Tuple[str, float]]:\n",
    "    \"\"\" get topk tokens from hidden states using lm head \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Ensure hidden_states has at least 2 dimensions (batch_size, num_tokens, hidden_dim)\n",
    "        if hidden_states.dim() == 2:\n",
    "            hidden_states = hidden_states.unsqueeze(0)  # Add batch dimension\n",
    "            # Apply RMS normalization like in DeepseekRMSNorm\n",
    "            variance = hidden_states.pow(2).mean(-1, keepdim=True) \n",
    "            hidden_states = hidden_states * torch.rsqrt(variance + 1e-6)\n",
    "\n",
    "        logits = lm_head(hidden_states)  # (batch_size, num_tokens, vocab_size)\n",
    "    \n",
    "    # Get top-k tokens\n",
    "    scores, token_ids = torch.topk(logits, k=k, dim=-1)  # (batch_size, num_tokens, k)\n",
    "    \n",
    "    # Decode tokens and collect results for each position\n",
    "    results = []\n",
    "    for pos in range(scores.size(1)):  # Iterate over token positions\n",
    "        pos_results = []\n",
    "        for i in range(k):\n",
    "            token = tokenizer.decode(token_ids[0, pos, i])  # Decode token for this position\n",
    "            score = scores[0, pos, i].item()  # Get score for this position\n",
    "            pos_results.append((token, score))\n",
    "        results.append(pos_results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepseekLayerAnalyzer:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.layer_outputs = defaultdict(list)\n",
    "        self.moe_gate_outputs = defaultdict(list)\n",
    "        self.moe_combined_outputs = defaultdict(list)\n",
    "        self.expert_outputs = defaultdict(lambda: defaultdict(list))\n",
    "        self.shared_expert_outputs = defaultdict(list)\n",
    "        self.hooks = []\n",
    "        \n",
    "    def register_hooks(self):\n",
    "        \"\"\"Register hooks for layer outputs and MoE combination points\"\"\"\n",
    "        \n",
    "        def layer_output_hook(layer_idx):\n",
    "            def hook(module, inputs, outputs):\n",
    "                \"\"\"Hook for capturing layer outputs\"\"\"\n",
    "                hidden_states = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "                self.layer_outputs[layer_idx].append(hidden_states.detach())\n",
    "            return hook\n",
    "\n",
    "        def moe_gate_hook(layer_idx):\n",
    "            def hook(module, inputs, outputs):\n",
    "                \"\"\"Hook for capturing MoE gate outputs before expert computation\"\"\"\n",
    "                # Capture topk_idx, topk_weight, and aux_loss from gate outputs\n",
    "                if isinstance(outputs, tuple):\n",
    "                    topk_idx, topk_weight, aux_loss = outputs\n",
    "                    self.moe_gate_outputs[layer_idx].append({\n",
    "                        'topk_idx': topk_idx.detach(),\n",
    "                        'topk_weight': topk_weight.detach(),\n",
    "                        'aux_loss': aux_loss.detach() if aux_loss is not None else None\n",
    "                    })\n",
    "            return hook\n",
    "\n",
    "        def expert_hook(layer_idx, expert_idx):\n",
    "            def hook(module, inputs, outputs):\n",
    "                \"\"\"Hook for capturing expert outputs\"\"\"\n",
    "                # Get the latest gate outputs for this layer\n",
    "                if not self.moe_gate_outputs[layer_idx]:\n",
    "                    return print(f'no gate outputs for layer {layer_idx}')\n",
    "            \n",
    "                gate_data = self.moe_gate_outputs[layer_idx][-1]\n",
    "                \n",
    "                # Handle 2D or 3D tensor shapes\n",
    "                if len(gate_data['topk_idx'].shape) == 2:\n",
    "                    batch_size = 1\n",
    "                    seq_len, top_k = gate_data['topk_idx'].shape\n",
    "                else:\n",
    "                    batch_size, seq_len, top_k = gate_data['topk_idx'].shape\n",
    "                \n",
    "                # Get mask for tokens where this expert was selected\n",
    "                expert_mask = (gate_data['topk_idx'] == expert_idx)                \n",
    "                # Flatten and find positions where this expert was selected\n",
    "                selected_positions = torch.nonzero(expert_mask, as_tuple=True)\n",
    "                # If no tokens selected this expert, skip\n",
    "                if selected_positions[0].numel() == 0:\n",
    "                    return\n",
    "                    \n",
    "                # Get the actual inputs routed to this expert\n",
    "                # Inputs[0] shape: (total_selected_tokens, hidden_dim)\n",
    "                total_selected = inputs[0].shape[0] \n",
    "                # Validate we're processing the correct number of tokens\n",
    "                expected_selected = expert_mask.sum().item()\n",
    "                if total_selected != expected_selected:\n",
    "                    print(f\" expert {expert_idx} processed {total_selected} tokens but expected {expected_selected}\")\n",
    "                    return\n",
    "                    \n",
    "                # Get the full hidden states from outputs\n",
    "                # outputs shape: (total_selected_tokens, hidden_dim)\n",
    "                hidden_states = outputs\n",
    "                if isinstance(outputs, tuple):\n",
    "                    hidden_states = outputs[0]\n",
    "                    \n",
    "                # Record data for each selected position\n",
    "                for pos_idx, pos in enumerate(selected_positions[0]):\n",
    "                    token_data = {\n",
    "                        'position': pos.item(),\n",
    "                        'input': inputs[0][pos_idx].detach(),\n",
    "                        'output': outputs[pos_idx].detach(),\n",
    "                        'hidden_state': hidden_states[pos_idx].detach()  # Store full hidden state\n",
    "                    }\n",
    "                    \n",
    "                    # Get the corresponding gate weight for this position\n",
    "                    # Find which expert slot (in top_k) this expert was selected for this position\n",
    "                    expert_slots = (gate_data['topk_idx'][pos.item()] == expert_idx).nonzero(as_tuple=True)[0]\n",
    "                    if len(expert_slots) > 0:\n",
    "                        token_data['gate_weight'] = gate_data['topk_weight'][pos.item()][expert_slots[0]].item()\n",
    "                    \n",
    "                    self.expert_outputs[layer_idx][expert_idx].append(token_data)\n",
    "            return hook\n",
    "        \n",
    "        def shared_expert_hook(layer_idx):\n",
    "            def hook(module, inputs, outputs):\n",
    "                \"\"\"Hook for capturing shared expert outputs\"\"\"\n",
    "                self.shared_expert_outputs[layer_idx].append({\n",
    "                    'input': inputs[0].detach(),\n",
    "                    'output': outputs.detach()\n",
    "                })\n",
    "            return hook\n",
    "\n",
    "        def moe_combine_hook(layer_idx):\n",
    "            def hook(module, inputs, outputs):\n",
    "                \"\"\"Hook for capturing final combined MoE outputs\"\"\"\n",
    "                # For DeepseekMoE, this captures the weighted sum of expert outputs\n",
    "                self.moe_combined_outputs[layer_idx].append({\n",
    "                    'combined_output': outputs.detach(),\n",
    "                    'input': inputs[0].detach()  # Original input before MoE\n",
    "                })\n",
    "            return hook\n",
    "\n",
    "        # Register hooks for each layer\n",
    "        for layer_idx, layer in enumerate(self.model.model.layers):\n",
    "            # Hook for layer output\n",
    "            hook = layer.register_forward_hook(layer_output_hook(layer_idx))\n",
    "            self.hooks.append(hook)\n",
    "            \n",
    "            # If it's an MoE layer, add MoE-specific hooks\n",
    "            if hasattr(layer.mlp, 'experts'):\n",
    "                # Hook for gate mechanism\n",
    "                gate_hook = layer.mlp.gate.register_forward_hook(moe_gate_hook(layer_idx))\n",
    "                self.hooks.append(gate_hook)\n",
    "                \n",
    "                # Hook for each expert\n",
    "                for expert_idx, expert in enumerate(layer.mlp.experts):\n",
    "                    expert_hook_fn = expert.register_forward_hook(expert_hook(layer_idx, expert_idx))\n",
    "                    self.hooks.append(expert_hook_fn)\n",
    "\n",
    "                # Hook for shared expert if it exists\n",
    "                if hasattr(layer.mlp, 'shared_experts'):\n",
    "                    shared_hook = layer.mlp.shared_experts.register_forward_hook(shared_expert_hook(layer_idx))\n",
    "                    self.hooks.append(shared_hook)\n",
    "                \n",
    "                # Hook for final combined output\n",
    "                combine_hook = layer.mlp.register_forward_hook(moe_combine_hook(layer_idx))\n",
    "                self.hooks.append(combine_hook)\n",
    "\n",
    "    def analyze_tokens(self, input_ids: torch.Tensor, return_hidden_states: bool = False) -> Dict:\n",
    "        \"\"\" run inference and analyze tokens at each layer and expert combination point \"\"\"\n",
    "\n",
    "        self.moe_combined_outputs.clear()\n",
    "        self.expert_outputs.clear()\n",
    "        self.shared_expert_outputs.clear()\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids)\n",
    "        \n",
    "        results = {\n",
    "            'layer_predictions': {},\n",
    "            'moe_analysis': {},\n",
    "            'hidden_states': {} if return_hidden_states else None\n",
    "        }\n",
    "        \n",
    "        # Analyze layer outputs\n",
    "        for layer_idx, outputs in self.layer_outputs.items():\n",
    "            if not outputs:  # Skip if no outputs captured\n",
    "                continue\n",
    "            hidden_states = outputs[-1]  # Get last captured output\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # Get token predictions for this layer\n",
    "            top_tokens = get_top_k_tokens(hidden_states, self.model.lm_head, self.tokenizer)\n",
    "            results['layer_predictions'][layer_idx] = top_tokens\n",
    "            \n",
    "            if return_hidden_states:\n",
    "                results['hidden_states'][f'layer_{layer_idx}'] = hidden_states\n",
    "        \n",
    "        # Analyze MoE layers\n",
    "        for layer_idx in self.moe_gate_outputs.keys():\n",
    "            if not self.moe_gate_outputs[layer_idx]:\n",
    "                continue\n",
    "                \n",
    "            gate_data = self.moe_gate_outputs[layer_idx][-1]  # Get last captured data\n",
    "            combined_data = self.moe_combined_outputs[layer_idx][-1]\n",
    "            \n",
    "            # Initialize predictions dictionary by position\n",
    "            expert_predictions_by_pos = defaultdict(dict)\n",
    "            expert_hidden_states_by_pos = defaultdict(dict)\n",
    "            \n",
    "            # Process expert outputs by position\n",
    "            for expert_idx, data_list in self.expert_outputs[layer_idx].items():\n",
    "                for data in data_list:\n",
    "                    position = data['position']\n",
    "                    # Get predictions for this expert's output at this position\n",
    "                    predictions = get_top_k_tokens(\n",
    "                        data['output'].unsqueeze(0),\n",
    "                        self.model.lm_head,\n",
    "                        self.tokenizer\n",
    "                    )\n",
    "                    expert_predictions_by_pos[position][expert_idx] = predictions[0]  # [0] because we only have one prediction set\n",
    "\n",
    "                    # Store hidden states\n",
    "                    expert_hidden_states_by_pos[position][expert_idx] = {\n",
    "                    'hidden_state': data['hidden_state'].tolist(),\n",
    "                    'gate_weight': data.get('gate_weight', None)\n",
    "                }\n",
    "            \n",
    "            # Get predictions for shared expert if it exists\n",
    "            if self.shared_expert_outputs[layer_idx]:\n",
    "                shared_expert_predictions = get_top_k_tokens(\n",
    "                    self.shared_expert_outputs[layer_idx][-1]['output'],\n",
    "                    self.model.lm_head,\n",
    "                    self.tokenizer\n",
    "                )\n",
    "            \n",
    "            # Update experts_analysis dictionary to include hidden states\n",
    "            experts_analysis = {\n",
    "            'selected_experts': gate_data['topk_idx'].tolist(),\n",
    "            'expert_weights': gate_data['topk_weight'].tolist(),\n",
    "            'aux_loss': gate_data['aux_loss'].item() if gate_data['aux_loss'] is not None else None,\n",
    "            'expert_predictions_by_position': dict(expert_predictions_by_pos),\n",
    "            'expert_hidden_states_by_position': dict(expert_hidden_states_by_pos),\n",
    "            'shared_expert_predictions': shared_expert_predictions if self.shared_expert_outputs[layer_idx] else None\n",
    "        }\n",
    "            \n",
    "            # Get token predictions from combined output\n",
    "            combined_tokens = get_top_k_tokens(\n",
    "                combined_data['combined_output'], \n",
    "                self.model.lm_head,\n",
    "                self.tokenizer\n",
    "            )\n",
    "            \n",
    "            experts_analysis['combined_output_tokens'] = combined_tokens\n",
    "            results['moe_analysis'][layer_idx] = experts_analysis\n",
    "\n",
    "        return results\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"remove all registered hooks\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks.clear()\n",
    "\n",
    "def analyze_deepseek_moe(model, tokenizer, input_text: str, return_hidden_states: bool = False):\n",
    "    \"\"\" analyze DeepSeek MoE model behavior for given input text \"\"\"\n",
    "    analyzer = DeepseekLayerAnalyzer(model, tokenizer)\n",
    "    analyzer.register_hooks()\n",
    "    \n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    try:\n",
    "        results = analyzer.analyze_tokens(input_ids, return_hidden_states=return_hidden_states)\n",
    "        return results\n",
    "    finally:\n",
    "        analyzer.cleanup()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''One might expect language modeling performance to depend on model architecture, the size of neural models, the computing power used to train them, and the data available for this'''\n",
    "\n",
    "analysis = analyze_deepseek_moe(\n",
    "    model, \n",
    "    tokenizer,\n",
    "    input_text=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(text=\"venir\", return_tensors=\"pt\",).input_ids\n",
    "print(f'input_ids : {input_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_idx = 1  # example layer\n",
    "# token_pos = 4\n",
    "# expert_idx = 3\n",
    "# hidden_state_list = analysis['moe_analysis'][layer_idx]['expert_hidden_states_by_position'][token_pos][expert_idx]['hidden_state']\n",
    "# print(f'hidden_state_list : {len(hidden_state_list)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_attn_ln_inputs(model, tokenizer, text):\n",
    "    \"\"\"places a hook on the post attention layernorm to retrieve its inputs\"\"\"\n",
    "    # Store inputs in a dict mapping layer idx -> inputs\n",
    "    post_attn_ln_inputs = {}\n",
    "    hooks = []\n",
    "    \n",
    "    def hook_post_attn_ln(module, input, output, layer_idx):\n",
    "        if layer_idx not in post_attn_ln_inputs:\n",
    "            post_attn_ln_inputs[layer_idx] = []\n",
    "        post_attn_ln_inputs[layer_idx].append([x.detach() for x in input])\n",
    "    \n",
    "    # Register hooks on post attention layernorm for each layer\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        hooks.append(\n",
    "            layer.post_attention_layernorm.register_forward_hook(\n",
    "                lambda m, i, o, idx=i: hook_post_attn_ln(m, i, o, idx)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        # Run inference\n",
    "        input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "        model(input_ids)\n",
    "        \n",
    "        return post_attn_ln_inputs\n",
    "        \n",
    "    finally:\n",
    "        # Clean up hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_attn_ln_inputs = get_post_attn_ln_inputs(model, tokenizer, text=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_moe_equation(model, tokenizer, layer_idx, text=\"the quick brown fox\", tolerance=1e-5):\n",
    "    \"\"\"\n",
    "    Verifies that MoE activations add up correctly:\n",
    "    final_layer_output = residual_stream_hidden_states_before_experts + moe_output\n",
    "    \"\"\"\n",
    "    analyzer = DeepseekLayerAnalyzer(model, tokenizer)\n",
    "    analyzer.register_hooks()\n",
    "    \n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "    try:\n",
    "        # Run the analysis to populate data structures\n",
    "        analyzer.analyze_tokens(input_ids)\n",
    "        # Get moe outputs and residual\n",
    "        if layer_idx not in analyzer.moe_combined_outputs or not analyzer.moe_combined_outputs[layer_idx]:\n",
    "            print(\"No combined output data found\")\n",
    "            return False\n",
    "\n",
    "        residual = post_attn_ln_inputs[layer_idx][-1][0][0]\n",
    "        print(f\"Residual shape: {residual.shape}\")\n",
    "        \n",
    "        # Get the combined MoE output directly from the hook\n",
    "        moe_output = analyzer.moe_combined_outputs[layer_idx][-1]['combined_output'][0]\n",
    "        print(f\"MoE output shape: {moe_output.shape}\")\n",
    "\n",
    "        # Get the final layer output\n",
    "        final_layer_output = analyzer.layer_outputs[layer_idx][-1][0]\n",
    "        print(f\"Final layer output shape: {final_layer_output.shape}\")\n",
    "        \n",
    "        # Full equation: final_output = residual + moe_output\n",
    "        lhs = residual + moe_output\n",
    "        print(f\"lhs shape: {lhs.shape}\")\n",
    "        print(f'lhs {lhs}')\n",
    "        rhs = final_layer_output\n",
    "        print(f\"rhs shape: {rhs.shape}\")\n",
    "        print(f'rhs {rhs}')\n",
    "        # Check if close\n",
    "        is_close = torch.allclose(lhs, rhs, rtol=tolerance, atol=tolerance)\n",
    "        print(f'is_close {is_close}')\n",
    "        if not is_close:\n",
    "            max_diff = (lhs - rhs).abs().max().item()\n",
    "            print(f\"Maximum difference: {max_diff:.6f}\")\n",
    "            print(\"\\nDetailed component analysis:\")\n",
    "            print(f\"Residual max value: {residual.abs().max().item():.6f}\")\n",
    "            print(f\"MoE output max value: {moe_output.abs().max().item():.6f}\")\n",
    "            print(f\"Final output max value: {final_layer_output.abs().max().item():.6f}\")\n",
    "            \n",
    "        return is_close\n",
    "        \n",
    "    finally:\n",
    "        analyzer.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verify equation for a specific layer\n",
    "# is_valid = verify_moe_equation(model, tokenizer, layer_idx=26, )\n",
    "# print(f\"MoE equation holds: {is_valid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access layer predictions\n",
    "for layer_idx, preds in analysis['layer_predictions'].items():\n",
    "    print(f\"Layer {layer_idx} predictions:\", preds)\n",
    "\n",
    "# Access MoE analysis\n",
    "for layer_idx, moe_data in analysis['moe_analysis'].items():\n",
    "    print(f\"\\nMoE Layer {layer_idx}:\")\n",
    "    print(\"Selected experts:\", moe_data['selected_experts'])\n",
    "    print(\"Expert weights:\", moe_data['expert_weights'])\n",
    "    print(\"Combined output tokens:\", moe_data['combined_output_tokens'])\n",
    "    print(\"Expert predictions by position:\", moe_data['expert_predictions_by_position'])\n",
    "    print(\"Shared expert predictions:\", moe_data['shared_expert_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_predictions_for_token(results: dict, layer_idx: int, token_pos: int) -> list:\n",
    "    # Ensure the layer exists in the results\n",
    "    if layer_idx not in results['layer_predictions']:\n",
    "        raise ValueError(f\"Layer {layer_idx} not found in the results.\")\n",
    "\n",
    "    # Get the layer predictions for the specified token position\n",
    "    layer_predictions = results['layer_predictions'][layer_idx][token_pos]\n",
    "    \n",
    "    # Return top 5 predictions\n",
    "    return layer_predictions[:5]\n",
    "\n",
    "def get_shared_expert_predictions_for_token(results: dict, layer_idx: int, token_pos: int) -> list:\n",
    "    # Ensure the layer exists in the results\n",
    "    if layer_idx not in results['moe_analysis']:\n",
    "        raise ValueError(f\"Layer {layer_idx} not found in the results.\")\n",
    "\n",
    "    # Get the MoE analysis for the specified layer\n",
    "    moe_analysis = results['moe_analysis'][layer_idx]\n",
    "\n",
    "    # Get the shared expert predictions for the specified token position\n",
    "    shared_predictions = moe_analysis['shared_expert_predictions'][token_pos]\n",
    "    \n",
    "    # Return top 5 predictions\n",
    "    return shared_predictions[:5]\n",
    "\n",
    "def get_expert_preds(results: dict, layer_idx: int, token_pos: int) -> list:\n",
    "    # Ensure the layer exists in the results\n",
    "    if layer_idx not in results['moe_analysis']:\n",
    "        raise ValueError(f\"Layer {layer_idx} not found in the MoE analysis results.\")\n",
    "        \n",
    "    # Get the MoE analysis for the specified layer\n",
    "    expert_preds = []\n",
    "    moe_analysis = results['moe_analysis'][layer_idx]\n",
    "    expert_predictions = moe_analysis['expert_predictions_by_position']\n",
    "    expert_toks = expert_predictions[token_pos]\n",
    "\n",
    "    for expert_idx, preds in expert_toks.items():\n",
    "        print(f'expert {expert_idx} : {preds}')\n",
    "        expert_preds.append((int(expert_idx), preds))\n",
    "\n",
    "    expert_preds = sorted(expert_preds, key=lambda x: x[0])\n",
    "        \n",
    "    return expert_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highest_pred(results: dict, model, tokenizer, layer_idx: int, token_pos: int, k: int = 5) -> list:\n",
    "    \"\"\"gets predictions from expert with highest weight after adding residual \"\"\"\n",
    "    # Ensure layer exists in results\n",
    "    if layer_idx not in results['moe_analysis']:\n",
    "        raise ValueError(f\"Layer {layer_idx} not found in the MoE analysis results.\")\n",
    "        \n",
    "    # Get MoE analysis for the layer\n",
    "    moe_analysis = results['moe_analysis'][layer_idx]\n",
    "    \n",
    "    # Get expert weights for this token\n",
    "    expert_weights = {}\n",
    "    selected_experts = moe_analysis['selected_experts'][token_pos]\n",
    "    expert_weights_list = moe_analysis['expert_weights'][token_pos]\n",
    "    \n",
    "    if isinstance(selected_experts, int):\n",
    "        # Handle case where only one expert is selected\n",
    "        expert_weights[selected_experts] = 1.0\n",
    "    else:\n",
    "        # Handle case where multiple experts are selected with weights\n",
    "        for expert_idx, weight in zip(selected_experts, expert_weights_list):\n",
    "            expert_weights[expert_idx] = weight\n",
    "                \n",
    "    # Find expert with highest weight\n",
    "    # max_weight_expert = max(expert_weights.items(), key=lambda x: x[1])[0]\n",
    "    max_weight_expert = sorted(expert_weights.items(), key=lambda x: x[1], reverse=True)[k][0]\n",
    "    print(f'max_weight_expert {max_weight_expert}')\n",
    "    \n",
    "    # Get hidden state from this expert\n",
    "    expert_hidden_state = moe_analysis['expert_hidden_states_by_position'][token_pos][max_weight_expert]['hidden_state']\n",
    "    \n",
    "    # Get residual stream\n",
    "    residual = post_attn_ln_inputs[layer_idx][-1][0][0][token_pos]\n",
    "    \n",
    "    # Convert expert hidden state to tensor if it's a list\n",
    "    if isinstance(expert_hidden_state, list):\n",
    "        expert_hidden_state = torch.tensor(expert_hidden_state, dtype=torch.float16)\n",
    "    \n",
    "    # Add residual to expert hidden state\n",
    "    combined = residual + expert_hidden_state\n",
    "    \n",
    "    # Get logits\n",
    "    logits = model.lm_head(combined.unsqueeze(0))\n",
    "    \n",
    "    # Get top 5 predictions\n",
    "    topk = torch.topk(logits[0], k=5)\n",
    "    scores = topk.values.tolist()\n",
    "    tokens = topk.indices.tolist()\n",
    "    \n",
    "    # Convert to token strings\n",
    "    predictions = []\n",
    "    for score, token in zip(scores, tokens):\n",
    "        token_str = tokenizer.decode(token)\n",
    "        predictions.append((token_str, score))\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highest_pred_combined(results: dict, model, tokenizer, res_stream: bool, layer_idx: int, token_pos: int, k: int = 6) -> list:\n",
    "    \"\"\"gets predictions from combining top k experts weighted by their router weights, excluding shared experts\"\"\"\n",
    "    # Ensure layer exists in results\n",
    "    if layer_idx not in results['moe_analysis']:\n",
    "        raise ValueError(f\"Layer {layer_idx} not found in the MoE analysis results.\")\n",
    "        \n",
    "    # Get MoE analysis for the layer\n",
    "    moe_analysis = results['moe_analysis'][layer_idx]\n",
    "    \n",
    "    # Get expert weights for this token\n",
    "    expert_weights = {}\n",
    "    selected_experts = moe_analysis['selected_experts'][token_pos]\n",
    "    expert_weights_list = moe_analysis['expert_weights'][token_pos]\n",
    "    \n",
    "    if isinstance(selected_experts, int):\n",
    "        # Handle case where only one expert is selected\n",
    "        expert_weights[selected_experts] = 1.0\n",
    "    else:\n",
    "        # Handle case where multiple experts are selected with weights\n",
    "        for expert_idx, weight in zip(selected_experts, expert_weights_list):\n",
    "            # Skip shared experts\n",
    "            if expert_idx < 0:  # Shared experts typically have negative indices\n",
    "                continue\n",
    "            expert_weights[expert_idx] = weight\n",
    "                \n",
    "    # Get top k experts by weight\n",
    "    top_k_experts = sorted(expert_weights.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    print(f'Using top {k} experts: {[expert[0] for expert in top_k_experts]}')\n",
    "    \n",
    "    # Initialize combined hidden state\n",
    "    combined_hidden_state = None\n",
    "    \n",
    "    # Combine hidden states from top k experts, weighted by their router weights\n",
    "    for expert_idx, weight in top_k_experts:\n",
    "        expert_hidden_state = moe_analysis['expert_hidden_states_by_position'][token_pos][expert_idx]['hidden_state']\n",
    "        \n",
    "        # Convert expert hidden state to tensor if it's a list\n",
    "        if isinstance(expert_hidden_state, list):\n",
    "            expert_hidden_state = torch.tensor(expert_hidden_state, dtype=torch.float16)\n",
    "            \n",
    "        # Weight the expert's hidden state by its router weight\n",
    "        weighted_hidden_state = expert_hidden_state * weight\n",
    "        \n",
    "        if combined_hidden_state is None:\n",
    "            combined_hidden_state = weighted_hidden_state\n",
    "        else:\n",
    "            combined_hidden_state += weighted_hidden_state\n",
    "    \n",
    "    # to include residual stream in the combined hidden state\n",
    "    if res_stream == True:\n",
    "        # Get residual stream\n",
    "        residual = post_attn_ln_inputs[layer_idx][-1][0][0][token_pos]\n",
    "        \n",
    "        # Add residual to combined expert hidden states\n",
    "        combined = combined_hidden_state + residual\n",
    "    else:\n",
    "        combined = combined_hidden_state\n",
    "    \n",
    "    # Get logits\n",
    "    logits = model.lm_head(combined.unsqueeze(0))\n",
    "    \n",
    "    # Get top 5 predictions\n",
    "    topk = torch.topk(logits[0], k=5)\n",
    "    scores = topk.values.tolist()\n",
    "    tokens = topk.indices.tolist()\n",
    "    \n",
    "    # Convert to token strings\n",
    "    predictions = []\n",
    "    for score, token in zip(scores, tokens):\n",
    "        token_str = tokenizer.decode(token)\n",
    "        predictions.append((token_str, score))\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n : 33\n",
      "Using top 4 experts: [42, 40, 21, 63]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('æ”¿', 0.0418701171875),\n",
       " ('rivia', 0.03973388671875),\n",
       " ('icacy', 0.03839111328125),\n",
       " ('ilar', 0.0374755859375),\n",
       " (' Grac', 0.036895751953125)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(tokenizer.encode(prompt))\n",
    "print(f'n : {n}')\n",
    "layer_idx = 16\n",
    "token_pos = n-1\n",
    "k =4 # which max expert to use\n",
    "get_highest_pred_combined(analysis, model, tokenizer, res_stream = False, layer_idx=layer_idx, token_pos=token_pos, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = 25\n",
    "token_pos = 2\n",
    "layer_preds = get_layer_predictions_for_token(analysis, layer_idx=layer_idx, token_pos=token_pos)\n",
    "shared_preds = get_shared_expert_predictions_for_token(analysis, layer_idx=layer_idx, token_pos=token_pos)\n",
    "expert_preds = get_expert_preds(analysis, layer_idx=layer_idx, token_pos=token_pos)\n",
    "\n",
    "print(f'layer_preds : {layer_preds}')\n",
    "print(f'shared_preds : {shared_preds}')\n",
    "print(f'expert_preds : {expert_preds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logit_lens_viz(analysis_results, token_pos=0, color='sunset', layers_to_plot=[]):\n",
    "    \"\"\"Creates a modern heatmap visualization of predictions across model layers.\n",
    "    \n",
    "    Args:\n",
    "        analysis_results: Analysis results dictionary\n",
    "        token_pos: Position of token to analyze (default: 0)\n",
    "        layers_to_plot: List of layer numbers to plot. If empty, plots all layers (default: [])\n",
    "    \"\"\"\n",
    "    # Initialize data structures\n",
    "    n_layers = max(analysis_results['layer_predictions'].keys()) + 1\n",
    "    data = []\n",
    "    all_values = []\n",
    "    \n",
    "    # Process each layer\n",
    "    for layer in range(n_layers):\n",
    "        if layers_to_plot and layer not in layers_to_plot:\n",
    "            continue\n",
    "            \n",
    "        row = {\n",
    "            'layer': layer,\n",
    "            'l_out': None,\n",
    "            'expert_1': None,\n",
    "            'expert_2': None,\n",
    "            'expert_3': None,\n",
    "            'expert_4': None,\n",
    "            'expert_5': None,\n",
    "            'expert_6': None,\n",
    "            'shared': None,\n",
    "            'expert_residual': None,\n",
    "            'top_6': None,\n",
    "            'top_6_res': None,\n",
    "            'predictions': {\n",
    "                'l_out': [],\n",
    "                'expert_1': [],\n",
    "                'expert_2': [],\n",
    "                'expert_3': [],\n",
    "                'expert_4': [],\n",
    "                'expert_5': [],\n",
    "                'expert_6': [],\n",
    "                'shared': [],\n",
    "                'expert_residual': [],\n",
    "                'top_6': [],\n",
    "                'top_6_res': []\n",
    "            },\n",
    "            'expert_ids': {},\n",
    "            'top_tokens': {\n",
    "                'l_out': '',\n",
    "                'expert_1': '',\n",
    "                'expert_2': '',\n",
    "                'expert_3': '',\n",
    "                'expert_4': '',\n",
    "                'expert_5': '',\n",
    "                'expert_6': '',\n",
    "                'shared': '',\n",
    "                'expert_residual': '',\n",
    "                'top_6': '',\n",
    "                'top_6_res': ''\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Layer predictions\n",
    "        layer_preds = get_layer_predictions_for_token(analysis_results, layer, token_pos)\n",
    "        if layer_preds:\n",
    "            row['l_out'] = layer_preds[0][1]\n",
    "            row['predictions']['l_out'] = layer_preds\n",
    "            row['top_tokens']['l_out'] = layer_preds[0][0]\n",
    "            all_values.append(layer_preds[0][1])\n",
    "        \n",
    "        # Expert predictions\n",
    "        if layer in analysis_results['moe_analysis']:\n",
    "            expert_preds = get_expert_preds(analysis_results, layer, token_pos)\n",
    "            moe_data = analysis_results['moe_analysis'][layer]\n",
    "            \n",
    "            expert_weights = {}\n",
    "            for pos_idx, experts in enumerate(moe_data['selected_experts']):\n",
    "                if pos_idx == token_pos:\n",
    "                    for expert_idx, weight in zip(experts, moe_data['expert_weights'][pos_idx]):\n",
    "                        expert_weights[expert_idx] = weight\n",
    "            \n",
    "            expert_logits = []\n",
    "            for expert_idx in expert_weights:\n",
    "                expert_data = next((p for e, p in expert_preds if e == expert_idx), None)\n",
    "                if expert_data and expert_data[0]:\n",
    "                    expert_logits.append((expert_idx, expert_data[0][1]))\n",
    "            \n",
    "            sorted_experts = sorted(expert_logits, key=lambda x: x[1], reverse=True)[:6]\n",
    "            \n",
    "            for i, (expert_idx, _) in enumerate(sorted_experts, 1):\n",
    "                expert_data = next((p for e, p in expert_preds if e == expert_idx), None)\n",
    "                if expert_data:\n",
    "                    expert_key = f'expert_{i}'\n",
    "                    row[expert_key] = expert_data[0][1]\n",
    "                    row['predictions'][expert_key] = expert_data\n",
    "                    row['expert_ids'][expert_key] = expert_idx\n",
    "                    row['top_tokens'][expert_key] = expert_data[0][0]\n",
    "                    all_values.append(expert_data[0][1])\n",
    "            \n",
    "            # Shared expert predictions\n",
    "            shared_preds = get_shared_expert_predictions_for_token(analysis_results, layer, token_pos)\n",
    "            if shared_preds:\n",
    "                row['shared'] = shared_preds[0][1]\n",
    "                row['predictions']['shared'] = shared_preds\n",
    "                row['top_tokens']['shared'] = shared_preds[0][0]\n",
    "                all_values.append(shared_preds[0][1])\n",
    "\n",
    "            # Expert + Residual predictions\n",
    "            expert_residual_preds = get_highest_pred(analysis_results, model, tokenizer, layer_idx=layer, token_pos=token_pos, k=0)\n",
    "            if expert_residual_preds:\n",
    "                row['expert_residual'] = expert_residual_preds[0][1]\n",
    "                row['predictions']['expert_residual'] = expert_residual_preds\n",
    "                row['top_tokens']['expert_residual'] = expert_residual_preds[0][0]\n",
    "                all_values.append(expert_residual_preds[0][1])\n",
    "\n",
    "            # Top 6 experts combined predictions without residual\n",
    "            top_6_preds = get_highest_pred_combined(analysis_results, model, tokenizer, res_stream=False, layer_idx=layer, token_pos=token_pos, k=6)\n",
    "            if top_6_preds:\n",
    "                row['top_6'] = top_6_preds[0][1]\n",
    "                row['predictions']['top_6'] = top_6_preds\n",
    "                row['top_tokens']['top_6'] = top_6_preds[0][0]\n",
    "                all_values.append(top_6_preds[0][1])\n",
    "\n",
    "            # Top 6 experts combined predictions with residual\n",
    "            top_6_res_preds = get_highest_pred_combined(analysis_results, model, tokenizer, res_stream=True, layer_idx=layer, token_pos=token_pos, k=6)\n",
    "            if top_6_res_preds:\n",
    "                row['top_6_res'] = top_6_res_preds[0][1]\n",
    "                row['predictions']['top_6_res'] = top_6_res_preds\n",
    "                row['top_tokens']['top_6_res'] = top_6_res_preds[0][0]\n",
    "                all_values.append(top_6_res_preds[0][1])\n",
    "\n",
    "        data.append(row)\n",
    "    \n",
    "    valid_values = [v for v in all_values if v is not None]\n",
    "    vmin = min(valid_values) if valid_values else 0\n",
    "    vmax = max(valid_values) if valid_values else 1\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    columns = ['l_out'] + [f'expert_{i}' for i in range(1, 7)] + ['shared', 'expert_residual', 'top_6', 'top_6_res']\n",
    "    x_positions = list(range(len(columns)))\n",
    "    \n",
    "    # Use Plotly's built-in colorscale\n",
    "    colorscale = color\n",
    "\n",
    "    # Create y positions with increased spacing\n",
    "    y_positions = list(range(len(data)))\n",
    "    \n",
    "    # Create the base heatmap for all columns at once\n",
    "    z_matrix = []\n",
    "    for row in data:\n",
    "        z_row = []\n",
    "        for col in columns:\n",
    "            # For layer 0, set expert columns to None to make them grey\n",
    "            if row['layer'] == 0 and (col.startswith('expert_') or col == 'shared' or col == 'expert_residual' or col == 'top_6' or col == 'top_6_res'):\n",
    "                z_row.append(float('nan'))\n",
    "            else:\n",
    "                z_row.append(row[col] if row[col] is not None else float('nan'))\n",
    "        z_matrix.append(z_row)\n",
    "    # Add main heatmap\n",
    "    fig.add_trace(go.Heatmap(\n",
    "        z=z_matrix,\n",
    "        x=x_positions,\n",
    "        y=y_positions,\n",
    "        colorscale=colorscale,\n",
    "        showscale=True,\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        colorbar=dict(\n",
    "            title='logit value',\n",
    "            titleside='right',\n",
    "            y=0.5,\n",
    "            thickness=20,\n",
    "            len=0.5,\n",
    "            tickfont=dict(family='JetBrains Mono', size=12),\n",
    "            titlefont=dict(family='JetBrains Mono', size=14)\n",
    "        ),\n",
    "        hoverongaps=False\n",
    "    ))\n",
    "    \n",
    "    # Add text overlays\n",
    "    for i, row in enumerate(data):\n",
    "        for j, col in enumerate(columns):\n",
    "            # Skip text overlay for experts in layer 0\n",
    "            if row['layer'] == 0 and (col.startswith('expert_') or col == 'shared' or col == 'expert_residual' or col == 'top_6' or col == 'top_6_res'):\n",
    "                continue\n",
    "                \n",
    "            if row[col] is not None:\n",
    "                # Calculate text color based on background\n",
    "                normalized_value = (row[col] - vmin) / (vmax - vmin) if vmax != vmin else 0\n",
    "                text_color = 'white' if normalized_value > 0.5 else 'black'\n",
    "                \n",
    "                # Add expert ID if applicable\n",
    "                if col.startswith('expert_') and col != 'expert_residual':\n",
    "                    expert_id = str(row['expert_ids'].get(col, ''))\n",
    "                    fig.add_trace(go.Scatter(\n",
    "                        x=[j - 0.4],  # Increased x offset to move expert ID more to the left\n",
    "                        y=[i + 0.35],  # Removed y offset to center vertically\n",
    "                        mode='text',\n",
    "                        text=[expert_id],\n",
    "                        textposition='middle center',\n",
    "                        textfont=dict(family='JetBrains Mono', color=text_color, size=12),  # Increased from 12\n",
    "                        hoverinfo='skip',\n",
    "                        showlegend=False\n",
    "                    ))\n",
    "                # Add token text\n",
    "                token_text = row['top_tokens'][col]\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=[j],\n",
    "                    y=[i],\n",
    "                    mode='text',\n",
    "                    text=[token_text],\n",
    "                    textposition='middle center',\n",
    "                    textfont=dict(family='JetBrains Mono', color=text_color, size=14),  # Increased from 12\n",
    "                    hoverinfo='skip',\n",
    "                    showlegend=False\n",
    "                ))\n",
    "    \n",
    "    # Calculate dimensions with improved scaling\n",
    "    n_rows = len(data)\n",
    "    base_width = 1400  # Increased width\n",
    "    min_height_per_row = 80  # Significantly increased height per row\n",
    "    plot_height = max(min_height_per_row * n_rows, 600)  # Ensure minimum height\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(\n",
    "            ticktext=['layer output'] + [f'exp {i}' for i in range(1, 7)] + ['shared', 'exp + residual', 'top 6', 'top 6 + residual'],\n",
    "            tickvals=x_positions,\n",
    "            tickangle=0,\n",
    "            showgrid=False,\n",
    "            zeroline=False,\n",
    "            title='',\n",
    "            color='black',\n",
    "            range=[-0.5, len(columns) - 0.5],\n",
    "            constrain='domain',\n",
    "            tickfont=dict(family='JetBrains Mono', size=14),\n",
    "            side='bottom'  # Move x-axis labels to bottom\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            autorange='reversed',\n",
    "            showgrid=False,\n",
    "            zeroline=False,\n",
    "            title=dict(\n",
    "                text='Layer',\n",
    "                font=dict(family='JetBrains Mono', size=16)\n",
    "            ),\n",
    "            color='black',\n",
    "            range=[-0.5, len(data) - 0.5],\n",
    "            ticktext=[str(row['layer']) for row in data],\n",
    "            tickvals=y_positions,\n",
    "            tickfont=dict(family='JetBrains Mono', size=14),\n",
    "            constrain='domain'\n",
    "        ),\n",
    "        plot_bgcolor='white',\n",
    "        paper_bgcolor='white',\n",
    "        width=base_width,\n",
    "        height=plot_height,\n",
    "        margin=dict(l=80, r=120, t=50, b=100),  # Adjusted margins for bottom x-axis\n",
    "        hovermode='closest'\n",
    "    )\n",
    "    \n",
    "    # Add cell borders\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(columns)):\n",
    "            fig.add_shape(\n",
    "                type=\"rect\",\n",
    "                x0=j-0.5,\n",
    "                y0=i-0.5,\n",
    "                x1=j+0.5,\n",
    "                y1=i+0.5,\n",
    "                # line=dict(color=\"black\", width=4),\n",
    "                fillcolor=\"rgba(0,0,0,0)\",\n",
    "                layer=\"above\"  # Changed from \"below\" to \"above\"\n",
    "            )\n",
    "    \n",
    "    # Add hover text\n",
    "    for i, row in enumerate(data):\n",
    "        for j, col in enumerate(columns):\n",
    "            # Skip hover for experts in layer 0\n",
    "            if row['layer'] == 0 and (col.startswith('expert_') or col == 'shared' or col == 'expert_residual' or col == 'top_6' or col == 'top_6_res'):\n",
    "                continue\n",
    "                \n",
    "            if row[col] is not None:\n",
    "                preds = row['predictions'][col]\n",
    "                if preds:\n",
    "                    pred_text = \"<br>\".join([f\"{token}: {score:.3f}\" for token, score in preds[:5]])\n",
    "                    if col.startswith('expert_') and col != 'expert_residual':\n",
    "                        real_id = row['expert_ids'].get(col)\n",
    "                        hover_text = f\"Layer {row['layer']} expert {real_id} (logit: {row[col]:.3f}):<br>{pred_text}\"\n",
    "                    else:\n",
    "                        hover_text = f\"Layer {row['layer']} {col} (logit: {row[col]:.3f}):<br>{pred_text}\"\n",
    "                    \n",
    "                    fig.add_trace(go.Scatter(\n",
    "                        x=[j],\n",
    "                        y=[i],\n",
    "                        mode='markers',\n",
    "                        marker=dict(opacity=0),\n",
    "                        hovertext=hover_text,\n",
    "                        hoverinfo='text',\n",
    "                        showlegend=False\n",
    "                    ))\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [2, 6, 10, 14, 22, 25, 27]\n",
    "# l = []\n",
    "x = len(tokenizer.encode(prompt))\n",
    "print(x)\n",
    "fig = create_logit_lens_viz(analysis, \n",
    "                            token_pos=x-1, \n",
    "                            color='blues', \n",
    "                            layers_to_plot=l)\n",
    "# save the fig\n",
    "i = 1\n",
    "while os.path.exists(f'logit_lens_viz_{i}.html'):\n",
    "    i += 1\n",
    "fig.write_html(f'logit_lens_viz_{i}.html')\n",
    "fig.write_image(f'logit_lens_viz_{i}.png')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
