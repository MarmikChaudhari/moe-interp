{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ[\"PYTORCH_TRANSFORMERS_SDP_BACKEND\"] = \"flash\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name=\"allenai/OLMoE-1B-7B-0924\"):\n",
    "    # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_expert_weights(model, layer_idx, expert_idx):\n",
    "    \"\"\"\n",
    "    Print the weights of a specific expert MLP at a given layer.\n",
    "    \n",
    "    Args:\n",
    "        model: The OLMoE model\n",
    "        layer_idx: Index of the layer containing the expert\n",
    "        expert_idx: Index of the expert within the layer\n",
    "    \"\"\"\n",
    "    gate_proj = f'model.layers.{layer_idx}.mlp.experts.{expert_idx}.gate_proj.weight'\n",
    "    up_proj = f'model.layers.{layer_idx}.mlp.experts.{expert_idx}.up_proj.weight'\n",
    "    down_proj = f'model.layers.{layer_idx}.mlp.experts.{expert_idx}.down_proj.weight'\n",
    "    \n",
    "    print(\"\\nGate Projection:\")\n",
    "    print(model.state_dict()[gate_proj])\n",
    "    print(\"\\nUp Projection:\") \n",
    "    print(model.state_dict()[up_proj])\n",
    "    print(\"\\nDown Projection:\")\n",
    "    print(model.state_dict()[down_proj])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_input(file_path, chunk_size=1000, tokenizer=None):\n",
    "    \"\"\"    \n",
    "    args :\n",
    "        file_path (str): Path to the input text file\n",
    "        chunk_size (int): Number of tokens per chunk\n",
    "        tokenizer: HuggingFace tokenizer (if None, will split on whitespace)\n",
    "        \n",
    "    output : List of text chunks of approximately chunk_size tokens\n",
    "    \"\"\"\n",
    "    device = 'cpu'\n",
    "    \n",
    "    # Read the full text file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    if tokenizer:\n",
    "        # Tokenize the full text\n",
    "        tokens = tokenizer.encode(text)\n",
    "        tokens_tensor = torch.tensor(tokens).to(device)\n",
    "        \n",
    "        # Split into chunks\n",
    "        chunks = []\n",
    "        for i in range(0, len(tokens), chunk_size):\n",
    "            chunk_tokens = tokens_tensor[i:i + chunk_size]\n",
    "            # Move to CPU for decoding\n",
    "            chunk_tokens = chunk_tokens.cpu()\n",
    "            # Decode tokens back to text\n",
    "            chunk_text = tokenizer.decode(chunk_tokens)\n",
    "            chunks.append(chunk_text)\n",
    "            \n",
    "    else:\n",
    "        # Simple whitespace tokenization\n",
    "        words = text.split()\n",
    "        \n",
    "        # Split into chunks\n",
    "        chunks = []\n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_router_logits(model, input_text: str, k: int = 1):\n",
    "    \"\"\"\n",
    "    args :\n",
    "        model: OlmoeForCausalLM model\n",
    "        input_text: Text string to analyze\n",
    "        k: Number of top experts to return per token\n",
    "        \n",
    "    output : dictionary mapping layer indices to lists of [token_text, expert_index, router_probability] for each token in that layer\n",
    "    \"\"\"\n",
    "    device = \"cpu\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Forward pass with router logits enabled\n",
    "    outputs = model(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        output_router_logits=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    \n",
    "    # Get router logits for all layers\n",
    "    router_logits = outputs.router_logits\n",
    "    \n",
    "    all_layer_results = {}\n",
    "    for layer_idx, layer_router_logits in enumerate(router_logits):\n",
    "        # Apply softmax to get probabilities\n",
    "        probs = torch.nn.functional.softmax(layer_router_logits.detach(), dim=-1)\n",
    "        # Reshape to [seq_len, num_experts] since batch_size=1\n",
    "        probs = probs.reshape(inputs['input_ids'].size(1), -1)\n",
    "        # Get top k probabilities and indices for each token\n",
    "        top_probs, top_indices = torch.topk(probs, k=k)\n",
    "        \n",
    "        # Move tensors to CPU for post-processing\n",
    "        top_probs = top_probs.cpu()\n",
    "        top_indices = top_indices.cpu()\n",
    "        \n",
    "        # Convert token IDs to text\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].cpu())\n",
    "        \n",
    "        # Create list of [token, expert, prob] for each token\n",
    "        layer_tokens = []\n",
    "        for i in range(len(tokens)):\n",
    "            for j in range(k):\n",
    "                # Clean special characters from token text\n",
    "                clean_token = tokens[i].replace('Ä ', '')\n",
    "                layer_tokens.append([\n",
    "                    clean_token,\n",
    "                    top_indices[i][j].item(),\n",
    "                    top_probs[i][j].item()\n",
    "                ])\n",
    "        \n",
    "        all_layer_results[layer_idx] = layer_tokens\n",
    "    \n",
    "    return all_layer_results # Dictionary mapping layer index to list of [token, expert_number, probability]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_router_logits_json(results, domain, device='cpu'):\n",
    "    \"\"\"\n",
    "    args :\n",
    "        results: Dictionary mapping layer index to list of [token, expert_number, probability]\n",
    "        domain: String indicating the domain (e.g., 'arxiv', 'code')\n",
    "        device: Device to use for tensor operations ('cuda' or 'cpu')\n",
    "    output : updated json file with new tokens\n",
    "    \"\"\"\n",
    "    if domain == 'arxiv':\n",
    "        json_path = 'arxiv_all_layers.json'\n",
    "    elif domain == 'github':\n",
    "        json_path = 'github_all_layers.json'\n",
    "    elif domain == 'math':\n",
    "        json_path = 'math_all_layers.json'\n",
    "    elif domain == 'physics':\n",
    "        json_path = 'physics_all_layers.json'\n",
    "    elif domain == 'biology':\n",
    "        json_path = 'biology_all_layers.json'\n",
    "    elif domain == 'legal':\n",
    "        json_path = 'legal_all_layers.json'\n",
    "    elif domain == 'swap':\n",
    "        json_path = 'swap_all_layers.json'\n",
    "\n",
    "    \n",
    "    # Initialize an empty dictionary for existing results\n",
    "    existing_results = {}\n",
    "    \n",
    "    if os.path.exists(json_path):\n",
    "        # Load existing results\n",
    "        with open(json_path, 'r') as f:\n",
    "            try:\n",
    "                existing_results = json.load(f)\n",
    "                # Convert string keys to integers\n",
    "                existing_results = {int(k): v for k, v in existing_results.items()}\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: {json_path} is empty or corrupted. Starting with an empty dictionary.\")\n",
    "    \n",
    "    # Move results to GPU if available\n",
    "    if torch.cuda.is_available() and device == 'cuda':\n",
    "        for layer_idx, layer_tokens in results.items():\n",
    "            # Convert lists to tensors and move to GPU\n",
    "            tokens_tensor = torch.tensor([[t[0], t[1], t[2]] for t in layer_tokens]).cuda()\n",
    "            results[layer_idx] = tokens_tensor.tolist()\n",
    "    \n",
    "    # Combine existing and new results for each layer\n",
    "    for layer_idx, layer_tokens in results.items():\n",
    "        if layer_idx in existing_results:\n",
    "            existing_results[layer_idx].extend(layer_tokens)\n",
    "        else:\n",
    "            existing_results[layer_idx] = layer_tokens\n",
    "    \n",
    "    # Save updated results with integer keys\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(existing_results, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "    return existing_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_expert_distribution(layer_idx, domain, device='cpu'):\n",
    "    \"\"\"    \n",
    "    args :\n",
    "        json_path: Path to the JSON file containing expert counts\n",
    "        device: Device to use for tensor operations ('cuda' or 'cpu')\n",
    "    output : plot of the expert distribution for a particular layer\n",
    "    \"\"\"\n",
    "    if domain == 'arxiv':\n",
    "        json_path = 'arxiv_all_layers.json'\n",
    "    elif domain == 'github':\n",
    "        json_path = 'github_all_layers.json'\n",
    "    elif domain == 'math':\n",
    "        json_path = 'math_all_layers.json'\n",
    "    elif domain == 'physics':\n",
    "        json_path = 'physics_all_layers.json'\n",
    "    elif domain == 'biology':\n",
    "        json_path = 'biology_all_layers.json'\n",
    "    elif domain == 'legal':\n",
    "        json_path = 'legal_all_layers.json'\n",
    "    elif domain == 'swap':\n",
    "        json_path = 'swap_all_layers.json'\n",
    "\n",
    "    # Read JSON file\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Extract layer results\n",
    "    layer_results = data[str(layer_idx)]\n",
    "    \n",
    "    # Create a dictionary to store expert counts\n",
    "    expert_counts = defaultdict(int)\n",
    "    \n",
    "    # Move data to GPU if available\n",
    "    if torch.cuda.is_available() and device == 'cuda':\n",
    "        layer_results = torch.tensor(layer_results).cuda()\n",
    "        \n",
    "    # Count how many tokens went to each expert\n",
    "    total_assignments = len(layer_results)\n",
    "    print(f'Total assignments: {total_assignments}')\n",
    "    \n",
    "    # Count occurrences of each expert\n",
    "    if torch.cuda.is_available() and device == 'cuda':\n",
    "        # Process on GPU\n",
    "        for _, expert, _ in layer_results.cpu().numpy():\n",
    "            expert_counts[int(expert)] += 1\n",
    "    else:\n",
    "        # Process on CPU\n",
    "        for _, expert, _ in layer_results:\n",
    "            expert_counts[expert] += 1\n",
    "            \n",
    "    print(f'Expert counts: {expert_counts}')\n",
    "    print(f'Total experts: {len(expert_counts)}')\n",
    "    print(f'Expert count for l0', expert_counts[0])\n",
    "    \n",
    "    # Convert to lists for plotting and calculate percentages\n",
    "    experts = [f'{i}' for i in range(64)]\n",
    "    percentages = [expert_counts[i]/total_assignments * 100 for i in range(64)]\n",
    "    \n",
    "    # Create bar chart\n",
    "    fig = go.Figure(data=[\n",
    "        go.Bar(\n",
    "            x=experts,\n",
    "            y=percentages,\n",
    "            textposition='auto',\n",
    "            marker_color='red'  # You can use any color here - hex code, RGB, or color name\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'percentage of total tokens routed to each expert for layer {layer_idx}',\n",
    "        xaxis_title='expert',\n",
    "        yaxis_title='% of total tokens',\n",
    "        yaxis=dict(range=[0, 100]), # Set y-axis range from 0 to 100%\n",
    "        xaxis_tickangle=-45,\n",
    "        bargap=0.2\n",
    "    )\n",
    "    \n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_experts(model, expert_idx, target_layer_idx, source_layer_idx=0, source_expert_idx=0):\n",
    "    \"\"\"\n",
    "    Swap experts between two layers in the OLMoE model.\n",
    "    \n",
    "    Args:\n",
    "        model: The OLMoE model\n",
    "        expert_idx: Index of the expert in target layer to swap with\n",
    "        target_layer_idx: Index of the layer containing the expert to swap with\n",
    "        source_layer_idx: Index of the source layer (default 0)\n",
    "        source_expert_idx: Index of the source expert (default 0)\n",
    "\n",
    "    \"\"\"\n",
    "    # Access the decoder layers\n",
    "    decoder_layers = model.model.layers\n",
    "    print(decoder_layers[0].mlp.experts[0].gate_proj.weight.shape)\n",
    "    \n",
    "    # Verify indices are valid\n",
    "    num_layers = len(decoder_layers)\n",
    "    if target_layer_idx >= num_layers or source_layer_idx >= num_layers:\n",
    "        raise ValueError(f\"Layer index out of range. Model has {num_layers} layers.\")\n",
    "    \n",
    "    # Get the MoE blocks from both layers\n",
    "    source_moe = decoder_layers[source_layer_idx].mlp\n",
    "    target_moe = decoder_layers[target_layer_idx].mlp\n",
    "    \n",
    "    # Verify expert indices are valid\n",
    "    num_experts = len(source_moe.experts)\n",
    "    if expert_idx >= num_experts or source_expert_idx >= num_experts:\n",
    "        raise ValueError(f\"Expert index out of range. Each layer has {num_experts} experts.\")\n",
    "        \n",
    "    # Swap the expert weights\n",
    "    source_expert = source_moe.experts[source_expert_idx]\n",
    "    target_expert = target_moe.experts[expert_idx]\n",
    "    \n",
    "    # Swap gate projection weights\n",
    "    source_expert.gate_proj.weight, target_expert.gate_proj.weight = \\\n",
    "        target_expert.gate_proj.weight, source_expert.gate_proj.weight\n",
    "        \n",
    "    # Swap up projection weights\n",
    "    source_expert.up_proj.weight, target_expert.up_proj.weight = \\\n",
    "        target_expert.up_proj.weight, source_expert.up_proj.weight\n",
    "        \n",
    "    # Swap down projection weights  \n",
    "    source_expert.down_proj.weight, target_expert.down_proj.weight = \\\n",
    "        target_expert.down_proj.weight, source_expert.down_proj.weight\n",
    "    \n",
    "    return {\n",
    "        'swapped_experts': {\n",
    "            'source': {\n",
    "                'layer': source_layer_idx,\n",
    "                'expert': source_expert_idx\n",
    "            },\n",
    "            'target': {\n",
    "                'layer': target_layer_idx,\n",
    "                'expert': expert_idx\n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists of experts to swap\n",
    "top_experts_list = [0, 36, 38, 41, 50, 59, 22, 10] # layer 15\n",
    "bottom_experts_list = [1, 2, 3, 6, 7, 8, 9, 18] # layer 15\n",
    "\n",
    "top_layer_idx = 0\n",
    "bottom_layer_idx = 7\n",
    "# Swap experts at each index\n",
    "for i in range(len(top_experts_list)):\n",
    "    swap_experts(model, expert_idx=top_experts_list[i], target_layer_idx=top_layer_idx, source_layer_idx=bottom_layer_idx, source_expert_idx=bottom_experts_list[i])\n",
    "    print(f\"Swapped experts {top_experts_list[i]} and {bottom_experts_list[i]} in layers {top_layer_idx} and {bottom_layer_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and chunk input file\n",
    "file_path = 'data/physics_arxiv_200k.txt'\n",
    "domain = 'physics'\n",
    "chunks = prepare_text_input(file_path, chunk_size=1024, tokenizer=tokenizer)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)  # Move model to GPU if available\n",
    "\n",
    "# Process all chunks\n",
    "all_results = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f'Processing chunk {i+1}/{len(chunks)}')\n",
    "    # print(f'Sample text: {chunk[:10]}...')  \n",
    "    \n",
    "    # Get router logits for the chunk\n",
    "    with torch.cuda.amp.autocast():  # Enable automatic mixed precision\n",
    "        results = get_router_logits(model, chunk)\n",
    "    all_results.append(results)\n",
    "    \n",
    "    # Save intermediate results \n",
    "    update_router_logits_json(results, domain=domain)\n",
    "    \n",
    "    # Clear GPU cache periodically\n",
    "    if (i + 1) % 10 == 0:  # Every 10 chunks\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'physics'\n",
    "num_layers = 16  # OLMoE has 16 layers\n",
    "\n",
    "# Combine results from all chunks\n",
    "combined_results = []\n",
    "for layer_idx in range(len(all_results[0])):  # For each layer\n",
    "    layer_combined = []\n",
    "    for chunk_result in all_results:\n",
    "        # Skip if chunk_result[layer_idx] is not iterable\n",
    "        if not isinstance(chunk_result[layer_idx], (list, tuple)):\n",
    "            continue\n",
    "            \n",
    "        # Move data to GPU if available \n",
    "        if torch.cuda.is_available():\n",
    "            chunk_result = [\n",
    "                (token.cuda() if torch.is_tensor(token) else token,\n",
    "                 expert.cuda() if torch.is_tensor(expert) else expert,\n",
    "                 prob.cuda() if torch.is_tensor(prob) else prob)\n",
    "                for token, expert, prob in chunk_result[layer_idx]\n",
    "            ]\n",
    "        layer_combined.extend(chunk_result)\n",
    "    combined_results.append(layer_combined)\n",
    "\n",
    "# Analyze and plot routing for all layers\n",
    "for layer_to_plot in range(num_layers):\n",
    "    print(f\"\\nRouting for first 5 tokens in layer {layer_to_plot}: \")\n",
    "    layer_results = combined_results[layer_to_plot]\n",
    "    \n",
    "    # Skip if no valid results for this layer\n",
    "    if not layer_results:\n",
    "        print(f\"No valid results for layer {layer_to_plot}\")\n",
    "        continue\n",
    "        \n",
    "    for token_info in layer_results[:5]:  # Limit to first 5 tokens\n",
    "        # Skip if token_info is not a tuple/list\n",
    "        if not isinstance(token_info, (tuple, list)):\n",
    "            continue\n",
    "            \n",
    "        token, expert, prob = token_info\n",
    "        # Move to CPU for printing\n",
    "        if torch.cuda.is_available():\n",
    "            prob = prob.cpu()\n",
    "        print(f\"Token: {token}, Expert: {expert}, Probability: {prob:.3f}\")\n",
    "\n",
    "    # Plot expert distribution for all processed data\n",
    "    with torch.cuda.amp.autocast():  # Enable automatic mixed precision\n",
    "        fig = plot_expert_distribution(layer_idx=layer_to_plot, domain=domain)\n",
    "    fig.show()\n",
    "\n",
    "    # Save plot as HTML and image\n",
    "    fig.write_html(f'plots/physics/{domain}_layer{layer_to_plot}_expert_dist.html')\n",
    "    fig.write_image(f'plots/physics/{domain}_layer{layer_to_plot}_expert_dist.png')\n",
    "    \n",
    "    # Clear GPU cache after each layer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
