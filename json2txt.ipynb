{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arc-easy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_arc = load_dataset(\"allenai/ai2_arc\", \"ARC-Easy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 2251 training examples\n",
      "Test set size: 2376 examples\n",
      "Validation set size: 570 examples\n",
      "{'id': 'Mercury_7220990', 'question': 'Which factor will most likely cause a person to develop a fever?', 'choices': {'text': ['a leg muscle relaxing after exercise', 'a bacterial population in the bloodstream', 'several viral particles on the skin', 'carbohydrates being digested in the stomach'], 'label': ['A', 'B', 'C', 'D']}, 'answerKey': 'B'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset loaded with {len(dataset_arc['train'])} training examples\")\n",
    "print(f\"Test set size: {len(dataset_arc['test'])} examples\")\n",
    "print(f\"Validation set size: {len(dataset_arc['validation'])} examples\")\n",
    "\n",
    "print(dataset_arc['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save questions from all splits of ARC-Easy dataset to a text file\n",
    "with open('dataset/arc_easy_question.txt', 'w') as f:\n",
    "    # Write training set questions\n",
    "    for item in dataset_arc['train']:\n",
    "        f.write(item['question'] + '\\n')\n",
    "    \n",
    "    # Write validation set questions \n",
    "    for item in dataset_arc['validation']:\n",
    "        f.write(item['question'] + '\\n')\n",
    "        \n",
    "    # Write test set questions\n",
    "    for item in dataset_arc['test']:\n",
    "        f.write(item['question'] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### piqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_piqa = load_dataset(\"ybisk/piqa\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 16113 training examples\n",
      "Test set size: 3084 examples\n",
      "Validation set size: 1838 examples\n",
      "\n",
      "{'goal': \"When boiling butter, when it's ready, you can\", 'sol1': 'Pour it onto a plate', 'sol2': 'Pour it into a jar', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset loaded with {len(dataset_piqa['train'])} training examples\")\n",
    "print(f\"Test set size: {len(dataset_piqa['test'])} examples\")\n",
    "print(f\"Validation set size: {len(dataset_piqa['validation'])} examples\\n\")\n",
    "\n",
    "print(dataset_piqa['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/piqa_goal.txt', 'w') as f:\n",
    "    for item in dataset_piqa['train']:\n",
    "        f.write(item['goal'] + '\\n')\n",
    "    for item in dataset_piqa['validation']:\n",
    "        f.write(item['goal'] + '\\n')\n",
    "    for item in dataset_piqa['test']:\n",
    "        f.write(item['goal'] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gsm8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_gsm8k = load_dataset(\"openai/gsm8k\", \"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 7473 training examples\n",
      "Test set size: 1319 examples\n",
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset loaded with {len(dataset_gsm8k['train'])} training examples\")\n",
    "print(f\"Test set size: {len(dataset_gsm8k['test'])} examples\")\n",
    "\n",
    "print(dataset_gsm8k['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/gsm8k_question.txt', 'w') as f:\n",
    "    for item in dataset_gsm8k['train']:\n",
    "        f.write(item['question'] + '\\n')\n",
    "    for item in dataset_gsm8k['test']:\n",
    "        f.write(item['question'] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### human eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_humaneval = load_dataset(\"openai/openai_humaneval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 164 examples\n",
      "{'task_id': 'HumanEval/0', 'prompt': 'from typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n', 'canonical_solution': '    for idx, elem in enumerate(numbers):\\n        for idx2, elem2 in enumerate(numbers):\\n            if idx != idx2:\\n                distance = abs(elem - elem2)\\n                if distance < threshold:\\n                    return True\\n\\n    return False\\n', 'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\\n    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\\n\\n\", 'entry_point': 'has_close_elements'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test set size: {len(dataset_humaneval['test'])} examples\")\n",
    "\n",
    "print(dataset_humaneval['test'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/humaneval_prompt.txt', 'w') as f:\n",
    "    for item in dataset_humaneval['test']:\n",
    "        f.write(item['prompt'] + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
