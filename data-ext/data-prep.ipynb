{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-moe-16b-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd604824188942a3a171cadb1ba4d397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6cb637c4ea44dc79cea02e37b268041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to gsm8k_dataset.txt with 7473 training examples and 1319 test examples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       " 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gsm8k\n",
    "\n",
    "dataset = load_dataset(\"openai/gsm8k\", 'main')\n",
    "\n",
    "# Create a txt file with all the dataset in the specified format\n",
    "with open(\"gsm8k_dataset.txt\", \"w\") as f:\n",
    "    # # Process training data\n",
    "    # for item in dataset[\"train\"]:\n",
    "    #     f.write(item[\"question\"] + \"\\n\")\n",
    "    #     f.write(item[\"answer\"] + \"\\n\\n\")\n",
    "    \n",
    "    # Process test data\n",
    "    for item in dataset[\"test\"]:\n",
    "        f.write(item[\"question\"] + \"\\n\")\n",
    "        f.write(item[\"answer\"] + \"\\n\\n\")\n",
    "\n",
    "print(f\"Dataset saved to gsm8k_dataset.txt with {len(dataset['train'])} training examples and {len(dataset['test'])} test examples\")\n",
    "\n",
    "# Display a sample\n",
    "dataset[\"train\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252002\n"
     ]
    }
   ],
   "source": [
    "with open(\"gsm8k_dataset.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "    tokens = tokenizer.encode(text)\n",
    "    print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIME problems extracted to aime_problems.txt with 933 problems\n"
     ]
    }
   ],
   "source": [
    "# for aime 1983 - 2024\n",
    "# form csv file to txt and the format is ID,Year,Problem Number,Question,Answer,Part\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the AIME CSV file\n",
    "# Assuming the file is in the current directory or you need to specify the path\n",
    "aime_csv_path = \"AIME_Dataset_1983_2024 (1).csv\"  # Update this path if needed\n",
    "\n",
    "if os.path.exists(aime_csv_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(aime_csv_path)\n",
    "    \n",
    "    # Create a text file with just the problems\n",
    "    with open(\"aime_problems.txt\", \"w\") as f:\n",
    "        for _, row in df.iterrows():\n",
    "            # Write the question to the file\n",
    "            f.write(f\"{row['Question']}\\n\")\n",
    "    \n",
    "    print(f\"AIME problems extracted to aime_problems.txt with {len(df)} problems\")\n",
    "else:\n",
    "    print(f\"CSV file not found at {aime_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105050\n"
     ]
    }
   ],
   "source": [
    "with open(\"aime_problems.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "    tokens = tokenizer.encode(text)\n",
    "    print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers in dataset: 2694879\n",
      "Completed processing 500 random papers. Output saved to arxiv_title_abstract.txt\n",
      "Number of tokens in the arXiv title+abstract file: 111068\n"
     ]
    }
   ],
   "source": [
    "# Process the arXiv metadata JSON file to extract titles and abstracts (random 10k papers)\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Path to the arXiv metadata JSON file\n",
    "arxiv_json_path = \"arxiv-metadata-oai-snapshot.json\"\n",
    "output_txt_path = \"arxiv_title_abstract.txt\"\n",
    "\n",
    "# First, count total number of papers in the file\n",
    "total_papers = 0\n",
    "with open(arxiv_json_path, \"r\") as json_file:\n",
    "    for _ in json_file:\n",
    "        total_papers += 1\n",
    "print(f\"Total papers in dataset: {total_papers}\")\n",
    "\n",
    "# Generate 10k random indices without repetition\n",
    "sample_size = min(500, total_papers)\n",
    "selected_indices = set(random.sample(range(total_papers), sample_size))\n",
    "\n",
    "# Process the JSON file, only extracting papers at the selected indices\n",
    "with open(arxiv_json_path, \"r\") as json_file, open(output_txt_path, \"w\") as txt_file:\n",
    "    processed_count = 0\n",
    "    for idx, line in enumerate(json_file):\n",
    "        if idx in selected_indices:\n",
    "            try:\n",
    "                # Parse the JSON object\n",
    "                paper = json.loads(line.strip())\n",
    "                \n",
    "                # Extract title and abstract\n",
    "                title = paper.get(\"title\", \"\").strip()\n",
    "                abstract = paper.get(\"abstract\", \"\").strip()\n",
    "                \n",
    "                # Write to the output file\n",
    "                if title and abstract:\n",
    "                    txt_file.write(f\"{title}\\n{abstract}\\n\\n\")\n",
    "                \n",
    "                processed_count += 1\n",
    "                if processed_count % 1000 == 0:\n",
    "                    print(f\"Processed {processed_count} papers\")\n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error parsing JSON at index {idx}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing paper at index {idx}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "print(f\"Completed processing {processed_count} random papers. Output saved to {output_txt_path}\")\n",
    "\n",
    "# Check the token count of the resulting file\n",
    "with open(output_txt_path, \"r\") as f:\n",
    "    text = f.read()\n",
    "    tokens = tokenizer.encode(text)\n",
    "    print(f\"Number of tokens in the arXiv title+abstract file: {len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_arxiv = load_dataset(\"armanc/scientific_papers\", \"arxiv\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'abstract', 'section_names'],\n",
      "        num_rows: 203037\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'abstract', 'section_names'],\n",
      "        num_rows: 6436\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'abstract', 'section_names'],\n",
      "        num_rows: 6440\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(load_arxiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving test articles: 100%|██████████| 25/25 [00:00<00:00, 10174.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 25 articles from the test set to arxiv_samples/arxiv_test_1000.txt\n",
      "Number of tokens in the saved test articles: 197352\n"
     ]
    }
   ],
   "source": [
    "# Save 1000 articles from the test set\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the output directory and file\n",
    "output_dir = \"arxiv_samples\"\n",
    "output_file = os.path.join(output_dir, \"arxiv_test_1000.txt\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get the test set\n",
    "test_set = load_arxiv[\"test\"]\n",
    "\n",
    "\n",
    "sample_size = 25\n",
    "test_samples = test_set.select(range(min(sample_size, len(test_set))))\n",
    "\n",
    "# Save the articles to a text file\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, article in enumerate(tqdm(test_samples, desc=\"Saving test articles\")):\n",
    "        article_text = article.get(\"article\", \"\").strip()\n",
    "        abstract = article.get(\"abstract\", \"\").strip()\n",
    "        \n",
    "        if article_text and abstract:\n",
    "            f.write(f\"Article: {article_text}\\n\\n\")\n",
    "print(f\"Saved {sample_size} articles from the test set to {output_file}\")\n",
    "\n",
    "# Check the token count of the resulting file\n",
    "with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "    tokens = tokenizer.encode(text)\n",
    "    print(f\"Number of tokens in the saved test articles: {len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': 'entanglement @xcite in a composite system refers to certain implicit correlation between the subsystems arising from their interaction .\\nit is the key resource of quantum computation and quantum information processing @xcite .\\ndue to recent advances in this field , entanglement has generated renewed interest .\\nthere have been different approaches to understand and to quantify entanglement @xcite .\\nbut so far the entanglement , only in a bipartite pure state has been investigated very extensively .\\nthe von neumann entropy @xcite of either of the subsystems provides a good measure of entanglement in this case @xcite .\\nthis is the quantum partner of the shannon s entropy @xcite in classical information theory and is defined as @xcite @xmath1 where @xmath2 . here , @xmath3 is the reduced density operator of the subsystem @xmath4 and is given by @xmath5 where @xmath6 is the density operator of the composite system under consideration and @xmath7 , @xmath8 .\\nin general , the quantities @xmath9 satisfy the following inequality ( due to araki and lieb ) @xcite : @xmath10 where @xmath11 is the joint entropy of the composite system comprising @xmath12 and @xmath13 .\\nthe second part of the above inequality is known as subadditivity inequality @xcite . for a pure state , @xmath14 and thus @xmath15 .\\nthe equality sign in the above relation holds good if and only if the composite density matrix @xmath6 can be written as a tensor product of its two reduced density matrices @xmath16 and @xmath17 , i.e. , for a disentangled state .\\none can define the index of correlation @xmath18 given by the expression @xmath19 @xcite , which can also be interpreted as information entropy in quantum information point of view .\\nwe note that kim _\\net al . _ have calculated the entropies of different kinds of pure states including two - mode fock states and squeezed states @xcite .\\nfurther , the above relation for entropy has been studied in the context of entangled gaussian states @xcite .\\nso far we have discussed about the measurement of entanglement in a bipartite pure state .\\nif the composite system is in a mixed state ( defined by the density operator @xmath20 ) , the entanglement of formation @xmath21 can be defined in terms of the average von neumann entropies of the pure states of the decompositions @xcite .\\nwootters has shown the quantity @xmath21 to be an explicit function of @xmath20 @xcite .\\nhe has introduced the notion of concurrence in this context .\\nwe further notice that from the schmidt decomposition of a pure bipartite state , one can properly identify the entanglement present in the state @xcite .\\nthis is also very useful to study bipartite continuous systems @xcite . on the other hand , for a mixed state @xmath20 , the separability criterion has been proposed @xcite to study entanglement .\\nthis is based on positive partial transpose mapping of @xmath20 .\\nthus the negativity ( entanglement monotone ) of the eigenvalues of the partial transpose of @xmath20 could be a measure of entanglement in a mixed bipartite system @xcite .\\nthe concept of negativity as an entanglement measure has been used in context of interaction of atoms with thermal field @xcite .\\nthe separability criterion has been extended to continuous systems @xcite also .    despite many approaches to define entanglement for a bipartite system ,\\nthere have been only a few approaches to quantify entanglement in the composite systems of three or more particles @xcite .\\nwe note that a generalization of schmidt decomposition in multipartite systems in pure states has been introduced @xcite .\\n@xcite proposed a measurement of entanglement in a tripartite system in terms of concurrences of the pairs of subsystems .\\nthis measure is invariant under permutations of the subsystems .\\nan average entanglement in a four - partite entangled state has been defined in terms of von neumann entropies of the pairs of subsystems @xcite .\\nvery recently , yukalov has addressed the question more generally and quantified multipartite entanglement @xcite in terms of the ratio of norms of an entangling operator and of a disentangling operator in the relevant disentangled hilbert space .    in this paper\\n, we put forward a possible measurement of entanglement of a four - particle system by studying the entropy of the reduced three - particle system .\\nas mentioned above , the von neumann entropy is a good measure for entanglement in a bipartite system . for a tripartite composite state ,\\nthis entropy satisfies a strong subadditivity inequality ( ssi ) @xcite , which has many important implications in the subject of quantum information theory . in this paper\\n, we study the properties of a four - particle entangled state through the three - particle entropy and the ssi .\\nthe structure of the paper is as follows . in sec .\\nii , we provide a brief discussion on strong subadditivity inequality from the quantum information point of view . in sec .\\niii , we describe a physical model and show the preparation of a four - particle entangled state . in sec .\\niv , we study the validity of the ssi in the present context and provide a physical explanation of the results .\\nwe conclude this paper by proposing a measurement of the corresponding four - particle entanglement .\\nwe have already mentioned that for a bipartite composite system of two particles @xmath12 and @xmath13 , the joint entropy @xmath11 satisfies the subadditivity inequality ( [ ineq ] ) . for a composite system of three particles a , b , and c , this inequality can be extended to the following form @xcite : @xmath22 this inequality is known as strong subadditivity inequality . the most obvious situation that the equality sign holds in ( [ lieb ] ) is when\\nthe composite density matrix @xmath23 can be written as the tensor product of its three reduced density matrices as @xmath24 , i.e. , when the system is in a disentangled state .\\nhowever , the more stringent condition for this reads as @xcite @xmath25    there have been numerous implications of the above inequality ( [ lieb ] ) in quantum information theory @xcite .\\nfirstly , it refers to the fact that the conditioning on the subsystem always reduces the entropy , i.e. , @xmath26 , where , @xmath27 is the entropy of a conditional on knowing the state of b. secondly , the above inequality implies that discarding a quantum system never increases mutual information , i.e. , @xmath28 , where , @xmath29 is the mutual information of the subsystems a and b. thirdly , quantum operations never increase mutual information of two subsystems .\\nthis means that if the mutual information of the two subsystems a and b becomes @xmath30 after trace - preserving operation on b , then @xmath31 .\\nfurther , this inequality ( [ lieb ] ) implies that the conditional entropy of the subsystems a , b , and c is also subadditive , i.e. , @xmath32 .    to verify ssi\\n, one needs to calculate the entropies like @xmath33 which clearly requires a three - particle mixed state which we can produce using a pure four - particle entangled state @xcite . in the next section ,\\nwe discuss how one can prepare a pure four - particle entangled state so that we can study ssi for the first time for a system realizable using cavity qed methods .\\nwe consider two three - level atoms ( a and b ) with relevant energy levels in @xmath0-configuration ( see fig .  [ fig1 ] ) interacting with a two - mode high quality optical cavity .\\nthe specified annihilation operators for the cavity modes are @xmath34 and @xmath35 .\\nthe atoms are interacting with the cavity mode @xmath34 in @xmath36 transition and with the mode @xmath35 in @xmath37 transition .\\nthe hamiltonian for the system under rotating wave approximation can be written as    @xmath38\\\\;,\\\\ ] ]    where , @xmath39 is the atomic transition frequency between the levels @xmath40 and @xmath41 , @xmath42 @xmath43 is the respective frequency of the cavity modes @xmath34 and @xmath35 , @xmath44 @xmath43 provides the atom - cavity coupling .\\nwe assume @xmath44 s to be real and function of time .\\nwe start with the initial state @xmath45 , where @xmath46 and @xmath47 are the initial numbers of photons in the cavity modes @xmath34 and @xmath35 , respectively and the two atoms are in @xmath48 state .\\nthe state of the system can be expanded in terms of the relevant basis states in the following way : @xmath49 from the schrdinger equation we find the following equations of the corresponding probability amplitudes : @xmath50\\\\;,\\\\\\\\ \\\\dot{d}_6&=&-i(\\\\sqrt{\\\\mu + 2}g_{2a}d_4+\\\\sqrt{\\\\mu + 2}g_{2b}d_7)\\\\;,\\\\nonumber\\\\\\\\ \\\\dot{d}_7&=&-i(\\\\sqrt{\\\\mu + 1}g_{2a}d_5+\\\\sqrt{\\\\mu + 2}g_{2b}d_6+\\\\delta d_7+\\\\sqrt{n-1}g_{1b}d _ 8)\\\\;,\\\\nonumber\\\\\\\\ \\\\dot{d}_8&=&-i(\\\\sqrt{n-1}g_{1b}d_7+\\\\sqrt{\\\\mu + 1}g_{2a}d_9)\\\\;,\\\\nonumber\\\\\\\\ \\\\dot{d}_9&=&-i(\\\\sqrt{n}g_{1a}d_1+\\\\sqrt{n-1}g_{1b}d_5+\\\\sqrt{\\\\mu + 1}g_{2a}d_8+\\\\delta d_9)\\\\;,\\\\nonumber\\\\end{aligned}\\\\ ] ] where , we have used the following transformations : @xmath51 where , @xmath52 , @xmath53 is the one - photon detuning of the cavity modes for the @xmath54-th atom .\\nhere we have assumed that the cavity modes are in two - photon resonance and @xmath55 .    writing these equations ( [ doteq ] ) in the matrix form @xmath56=-i[m][d_i]$ ] , we find that one of the eigenvalues of the matrix @xmath57 $ ] is zero .\\nthe corresponding eigenstate is @xmath58\\\\ ; , \\\\label{dark}\\\\ ] ]    where @xmath59 clearly , this state is an entangled state of four particles , namely , the atoms a and b , and the two modes @xmath34 and @xmath35 . using appropriate time - dependence of the pulses ,\\nthe four - particle system can be prepared in this state , as discussed in the next section .\\nrecently , there have a few experimental demonstrations of preparation of four - particle entangled state @xcite and performance of a c - not gate @xcite .\\ninterestingly , the state @xmath60 is a two - atom two - mode multipartite coherent population trapping ( cpt ) state which is a counterpart of the well - known cpt state for a single atom interacting with two coherent fields @xcite .\\nwe next discuss how the state @xmath60 can be prepared by using raman adiabatic passage technique .\\nwe assume that both the atoms are initially in @xmath48 state .\\nwe further assume the time - dependence of the rabi frequencies @xmath44 of the two modes as @xmath61 here , @xmath62 ( @xmath63 ) is the amplitude of the respective pulse , @xmath64 and @xmath65 are the width and time - separation respectively , of the two pulses .\\nnote that the pulses are applied in counterintuitive sequence . under this condition\\n, the atom - cavity system follows the evolution of the state @xmath60 adiabatically .\\nthis state is a zero eigenvalue eigenstate ( adiabatic state ) of the hamiltonian ( [ hamil1 ] ) . during this process , known as stimulated raman adiabatic technique ( stirap ) @xcite ,\\nthe atom - cavity system remains in this state for all times . in the present case , at the end of the evolution ,\\nthe population of both the atoms are simultaneously transferred to the state @xmath66 . however , if the atoms are not in one - photon resonance , i.e. , if @xmath67\\n, then this transfer process is not complete .\\nthis happens because the system does not remain confined in the null adiabatic state @xmath60 for @xmath67 @xcite .\\nwe now investigate the validity of ssi for any trio of quantum systems in the present process .\\nwe can express this inequality for any three particles , namely , atom a , atom b , and cavity mode @xmath34 with number @xmath46 of photons out of the four - particle system under consideration as @xmath68 here , @xmath69 defines the joint von neumann entropy of the relevant subsystems [ see eq .\\n( [ entropy ] ) ] . this can be calculated from the state ( [ wavefunc ] ) by tracing over the other subsystems , e.g. , @xmath70 where , @xmath6 is the reduced density matrix of the atoms a and b and is given by @xmath71 we show the time variation of @xmath72 in fig .\\nclearly , @xmath73 never becomes negative during the evolution and thus the ssi ( [ ssi ] ) holds for all times .    from fig .\\n[ fig2 ] , one clearly sees that for @xmath74 , in long time limit , @xmath72 becomes zero .\\nthis means that the subsystems ( a , b , and the mode @xmath34 with photon number @xmath46 ) become disentangled .\\nthis happens because of complete adiabatic transfer of population to the level @xmath66 of both the atoms at long time limit .\\nthe entire process can be written as @xmath75 we have shown the time - variation of the coefficients @xmath76 , @xmath77 , @xmath78 , and @xmath79 [ see eq .  ( [ norm ] ) ] in fig .  [ fig3 ] .\\nthis figure reveals the above evolution according to the state @xmath60 under the action of the pulses ( [ pulse ] ) .\\nbut for @xmath80 , since complete population transfer does not occur , the system remains entangled in the state @xmath81 at long time limit .\\nthis is clear from the dashed curve of fig .\\n[ fig2 ] , as the equality @xmath82 no longer holds at this time limit .\\nthus we can recognize the expression @xmath72 [ see eq .\\n( [ ssi ] ) ] as a measure of four - particle entanglement in the present process .\\nprecisely , @xmath83 , where the equality sign holds good for the disentangled states .\\nan increase in value of @xmath72 refers to increase in entanglement .\\nthus , during the evolution , the system gets more entangled for @xmath74 than for @xmath80 .\\nhowever , at the end of the evolution , the entanglement persists for nonzero @xmath84 .\\nwe must emphasize here that , the present definition of entanglement measurement satisfies all the relevant criteria , namely , ( a ) it is semipositive , i.e. , @xmath83 , ( b ) @xmath82 for an disentangled state , and ( c ) the function @xmath73 is continuous in time domain .\\nusing our four - particle entanglement , we can also study the inequality ( [ ineq ] ) involving the entropies of two particles , say , atoms a and b. they remain strongly correlated during the evolution , as @xmath18 remains much larger than zero . at the end of the evolution , @xmath18 becomes zero for @xmath85 which means that the subsystems become uncorrelated .\\nin other words , the entanglement between them vanishes .\\nwe note in passing that for a four - particle ghz state defined by @xmath86 of four qubits a , b , c , and d , one is led to a three - particle mixed state @xmath23 defined by @xmath87 in this case , @xmath88 .\\ntherefore , the parameter @xmath72 in this case becomes zero , as from eq .\\n( [ ssi ] ) .\\nso we have a counter - example , in which the equality sign in ( [ lieb ] ) holds for an entangled state , too .\\nhowever , we note that the above state ( [ mix ] ) satisfies the condition ( [ must ] ) and thus the said equality .\\nin conclusion , we have shown for the first time the role of strong subadditivity inequality for entropies in a four - particle composite system .\\nthe stimulated raman adiabatic technique has been used to prepare the four - particle entangled state using two three - level atoms initially in their ground states in a two - mode cavity .\\nwe further show that the parameter @xmath72 could serve as a possible measurement of entanglement in the four - particle entangled state under consideration .              c. h. bennett , g. brassard , s. popescu , b. schumacher , j. a. smolin , and w. k. wootters , phys .\\nlett . * 76 * , 722 ( 1996 ) ; c. h. bennett , d. p. divincenzo , j. smolin , and w. k. wootters , phys . rev .\\na * 54 * , 3824 ( 1996 ) .\\na. peres , phys .\\nlett . * 77 * , 1413 ( 1996 ) ; m. horodecki , p. horodecki , and r. horodecki , phys .\\na * 223 * , 1 ( 1996 ) ; p. horodecki , _ ibid . _ * 232 * , 333 ( 1997 ) ; v. giovannetti , s. mancini , d. vitali , and p. tombesi , phys . rev .\\na * 67 * , 022320 - 1 ( 2003 ) .                          c. a. sackett , d. kielpinski , b. e. king , c. langer , v. meyer , c. j. myatt , m. rowe , q. a. turchette , w. m. itano , d. j. wineland , and c. monroe , nature ( london ) * 404 * , 256 ( 2000 ) ; m. eibl , s. gaertner , m. bourennane , c. kurtsiefer , m. ukowski , and h. weinfurter , phys .\\nlett . * 90 * , 200403 ( 2003 ) .',\n",
       " 'abstract': ' strong subadditivity inequality for a three - particle composite system is an important inequality in quantum information theory which can be studied via a four - particle entangled state . \\n we use two three - level atoms in @xmath0 configuration interacting with a two - mode cavity and the raman adiabatic passage technique for the production of the four - particle entangled state . using this four - particle entanglement \\n , we study for the first time various aspects of the strong subadditivity inequality . ',\n",
       " 'section_names': '[sec:intro]introduction\\nstrong subadditivity inequality\\n[sec:basic]preparation of four-particle entangled state\\nstudy of strong subadditivity inequality\\nconclusions'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_arxiv[\"test\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 200 entries to code_samples.txt\n"
     ]
    }
   ],
   "source": [
    "# Load data from the parquet file and save content to a text file\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "# https://huggingface.co/datasets/codeparrot/github-code\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"/Users/idhantgulati/Documents/moe-interp/data-ext/full-datasets/train-00000-of-01126.parquet\"\n",
    "output_file = \"code_samples.txt\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Load the parquet file\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Limit to first 1000 samples\n",
    "    df = df.head(200)\n",
    "    \n",
    "    # Save path and content to a text file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for index, row in df.iterrows():\n",
    "            if 'path' in df.columns and 'content' in df.columns:\n",
    "                path = row['path']\n",
    "                content = row['content']\n",
    "                f.write(f\"{path}\\n{content}\\n\\n\")\n",
    "    \n",
    "    print(f\"Saved {len(df)} entries to {output_file}\")\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384815\n"
     ]
    }
   ],
   "source": [
    "with open(\"code_samples.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "    tokens = tokenizer.encode(text)\n",
    "    print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to context_question_pairs.txt\n"
     ]
    }
   ],
   "source": [
    "# Process the valid.json file to create a context-question format text file\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Define the input and output file paths\n",
    "valid_json_path = \"valid.json\"\n",
    "output_file = \"context_question_pairs.txt\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(valid_json_path):\n",
    "    # Load the JSON file\n",
    "    with open(valid_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Open the output file for writing\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        # Iterate through the data\n",
    "        for item in data.get(\"data\", []):\n",
    "            title = item.get(\"title\", \"\")\n",
    "            \n",
    "            for paragraph in item.get(\"paragraphs\", []):\n",
    "                context = paragraph.get(\"context\", \"\")\n",
    "                \n",
    "                # Write the context\n",
    "                f.write(f\"\\n{context}\\n\")\n",
    "                \n",
    "                # Process questions and answers\n",
    "                for qa in paragraph.get(\"qas\", []):\n",
    "                    question = qa.get(\"question\", \"\")\n",
    "                    \n",
    "                    # Write the question\n",
    "                    f.write(f\"{question}\\n\")\n",
    "                    \n",
    "    \n",
    "    print(f\"Processed data saved to {output_file}\")\n",
    "else:\n",
    "    print(f\"File not found: {valid_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (289517 > 16384). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289517\n"
     ]
    }
   ],
   "source": [
    "with open(\"french-qa.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "    tokens = tokenizer.encode(text)\n",
    "    print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 rows saved to parquet_sample.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# chinese\n",
    "# https://huggingface.co/datasets/opencsg/chinese-fineweb-edu\n",
    "\n",
    "# Load the parquet file\n",
    "try:\n",
    "    df = pd.read_parquet(\"00001.parquet\")\n",
    "    \n",
    "    # Get the first 100 rows\n",
    "    df_sample = df.head(275)\n",
    "    \n",
    "    # Save to a text file\n",
    "    output_file = \"parquet_sample.txt\"\n",
    "    \n",
    "    # Write to text file with double newlines between each row\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for index, row in df_sample.iterrows():\n",
    "            f.write('\\n\\n'.join([str(value) for value in row.values]))\n",
    "            f.write('\\n\\n')\n",
    "    \n",
    "    print(f\"First 100 rows saved to {output_file}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found: 00001.parquet\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading parquet file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310000\n"
     ]
    }
   ],
   "source": [
    "with open(\"parquet_sample.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "    tokens = tokenizer.encode(text)\n",
    "    print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 text entries saved to first_100_texts.json\n",
      "\n",
      "DataFrame Info:\n",
      "Columns: ['TEXT', 'SOURCE', 'METADATA']\n",
      "Total entries: 1305\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# https://huggingface.co/datasets/sedthh/gutenberg_english\n",
    "\n",
    "# Try to read the parquet file and save the first 100 text entries to a json file\n",
    "try:\n",
    "    # Load the parquet file\n",
    "    # df = pd.read_parquet(\"train-00000-of-00037-f5fce855b93d2d02.parquet\")\n",
    "    df = pd.read_parquet(\"train-00001-of-00037-9f227d74fc154ce9.parquet\")\n",
    "    \n",
    "    # Get the first 100 rows, skipping the 9th\n",
    "    df_sample = df.head(5)\n",
    "    \n",
    "    # Save TEXT column to a json file\n",
    "    output_file = \"first_100_texts.json\"\n",
    "    \n",
    "    # Write to json file with each text entry as one line\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for text in df_sample['TEXT']:\n",
    "            json_line = json.dumps({\"text\": str(text)})\n",
    "            f.write(json_line + '\\n')  # Each JSON object on a separate line\n",
    "    \n",
    "    print(f\"First 100 text entries saved to {output_file}\")\n",
    "    \n",
    "    # Print the dataframe info for reference\n",
    "    print(\"\\nDataFrame Info:\")\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    print(f\"Total entries: {len(df)}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"File not found: train-00000-of-00037-f5fce855b93d2d02.parquet\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading parquet file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "745010\n"
     ]
    }
   ],
   "source": [
    "with open(\"first_100_texts1.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "    tokens = tokenizer.encode(text)\n",
    "    print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 2 entries in the dataset:\n",
      "                                                TEXT     SOURCE  \\\n",
      "0  HONORINE\\r\\n\\r\\n    \\r\\n\\r\\n\\r\\n      By Honor...  gutenberg   \n",
      "1  THE MYSTERY OF “THE YELLOW ROOM”\\r\\n\\r\\n    \\r...  gutenberg   \n",
      "\n",
      "                                            METADATA  \n",
      "0  {\"language\": \"en\", \"text_id\": 1683, \"title\": \"...  \n",
      "1  {\"language\": \"en\", \"text_id\": 1685, \"title\": \"...  \n"
     ]
    }
   ],
   "source": [
    "    # Load the Gutenberg English dataset\n",
    "    # Format: Parquet file with columns including 'TEXT' containing book content\n",
    "    # The dataset is split into multiple files (37 shards)\n",
    "    # Each file contains thousands of text entries from public domain books\n",
    "    df = pd.read_parquet(\"train-00001-of-00037-9f227d74fc154ce9.parquet\")\n",
    "    \n",
    "    # Print the first 2 entries to understand the format\n",
    "    print(\"First 2 entries in the dataset:\")\n",
    "    print(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
