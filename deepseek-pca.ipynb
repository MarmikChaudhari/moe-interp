{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48459c5b5a244a17b39b0d7bf6182c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_model(model_name):\n",
    "    # print(f\"Default torch dtype: {torch.get_default_dtype()}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        # use_flash_attention_2=True,\n",
    "    )\n",
    "    # Print model parameters dtype\n",
    "    # first_param = next(model.parameters())\n",
    "    # print(f\"Model parameters dtype: {first_param.dtype}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model(\"deepseek-ai/deepseek-moe-16b-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, input_text, max_length=70):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    \n",
    "    # outputs = model(input_ids, output_router_logits=True)\n",
    "    outputs = model(input_ids,)\n",
    "    \n",
    "    output = model.generate(input_ids, max_length=max_length, use_cache=True, pad_token_id=tokenizer.pad_token_id)\n",
    "    print(\"generated text :\")\n",
    "    for token in output[0]:\n",
    "        print(tokenizer.decode(token, skip_special_tokens=True), end='', flush=True)\n",
    "    print()\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_router_logits(model, input_ids):\n",
    "    \"\"\"get router logits from model forward pass\"\"\"\n",
    "    # Get hidden states for each layer\n",
    "    outputs = model(input_ids, output_hidden_states=True)\n",
    "    hidden_states = outputs.hidden_states\n",
    "    \n",
    "    # Initialize tensor to store all router logits\n",
    "    num_layers = len(model.model.layers)\n",
    "    first_layer = next(layer for layer in model.model.layers if hasattr(layer.mlp, 'gate'))\n",
    "    num_experts = first_layer.mlp.gate.weight.shape[0]\n",
    "    bsz, seq_len, hidden_dim = hidden_states[0].shape\n",
    "    router_logits = torch.zeros((num_layers, bsz * seq_len, num_experts))\n",
    "    \n",
    "    # For each MoE layer, calculate router logits\n",
    "    layer_count = 0\n",
    "    for layer_idx, layer in enumerate(model.model.layers):\n",
    "        # Check if this layer uses MoE\n",
    "        if hasattr(layer.mlp, 'gate'):\n",
    "            # Get hidden states before MoE layer\n",
    "            layer_hidden = hidden_states[layer_idx]\n",
    "            \n",
    "            # Calculate router logits using layer's gate\n",
    "            flat_hidden = layer_hidden.view(-1, hidden_dim)\n",
    "            logits = torch.nn.functional.linear(flat_hidden, layer.mlp.gate.weight)\n",
    "            \n",
    "            # Store logits in the tensor\n",
    "            router_logits[layer_count] = logits\n",
    "            layer_count += 1\n",
    "                \n",
    "    router_logits = router_logits[:layer_count]  \n",
    "    print(f\"router logits shape : {router_logits.shape}\")\n",
    "    return router_logits\n",
    "\n",
    "def get_last_token_router_probs(router_logits, layer_idx):\n",
    "    \"\"\"Get router probabilities for the last token in a specified layer\"\"\"\n",
    "    layer_logits = router_logits[layer_idx]  # Shape: [sequence_length, num_experts]\n",
    "    last_token_logits = layer_logits[-1]  # get last token logits\n",
    "    routing_probs = torch.nn.functional.softmax(last_token_logits, dim=-1)\n",
    "    return routing_probs\n",
    "\n",
    "def topk(router_probs, k):\n",
    "    \"\"\"zero out all components except top k router probabilities\"\"\"\n",
    "    values, indices = torch.topk(router_probs, k)\n",
    "    zeroed_probs = torch.zeros_like(router_probs)\n",
    "    zeroed_probs[indices] = values\n",
    "    return zeroed_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_router_probs_matrix(model, prompts, k=8):\n",
    "    \"\"\"get router probs matrix for all tokens and last token for multiple inputs across all layers\n",
    "    shape of all_router_logits : [num_prompts, num_layers-1, max_seq_len, num_experts]\n",
    "    shape of last_token_prob_matrix : [num_layers-1, num_prompts, num_experts]\n",
    "    \"\"\"\n",
    "    num_prompts = len(prompts)\n",
    "    num_experts = 64\n",
    "    num_layers = 28\n",
    "    last_token_prob_matrix = torch.zeros((num_layers-1, num_prompts, num_experts))\n",
    "    \n",
    "    # Store router logits for all tokens\n",
    "    all_router_logits = []\n",
    "    max_seq_len = 0\n",
    "    \n",
    "    # First pass to get max sequence length\n",
    "    for prompt in prompts:\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        seq_len = input_ids.shape[1]\n",
    "        max_seq_len = max(max_seq_len, seq_len)\n",
    "    \n",
    "    # Get router probs for each prompt\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        router_logits = get_router_logits(model, input_ids)\n",
    "        \n",
    "        # Pad router logits to max sequence length\n",
    "        curr_seq_len = router_logits.shape[1]\n",
    "        if curr_seq_len < max_seq_len:\n",
    "            padding = torch.zeros((router_logits.shape[0], max_seq_len - curr_seq_len, num_experts))\n",
    "            router_logits = torch.cat([router_logits, padding], dim=1)\n",
    "            \n",
    "        all_router_logits.append(router_logits)\n",
    "        \n",
    "        # Get probs for each layer's last token\n",
    "        for layer_idx in range(1,num_layers):\n",
    "            probs = get_last_token_router_probs(router_logits, layer_idx-1)\n",
    "            top_probs = topk(probs, k=k)\n",
    "            last_token_prob_matrix[layer_idx-1, i] = top_probs\n",
    "            \n",
    "    # Stack all router logits into single tensor\n",
    "    all_router_logits = torch.stack(all_router_logits)\n",
    "\n",
    "    return last_token_prob_matrix, all_router_logits\n",
    "\n",
    "\n",
    "def get_last_token(prompt):\n",
    "    \"\"\"get the last token of a prompt using the tokenizer\"\"\"\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    last_token = tokenizer.decode([tokens[-1]])\n",
    "    return last_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_visualize(prompts):\n",
    "    \"\"\"perform PCA visualization on router probabilities for a list of prompts\"\"\"\n",
    "    \n",
    "    # Get router probability matrix using helper functions\n",
    "    last_token_prob_matrix, all_router_logits = get_router_probs_matrix(model, prompts, k=64)\n",
    "    router_prob_matrix_np = last_token_prob_matrix.detach().numpy()\n",
    "\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=3)\n",
    "    pca_result = pca.fit_transform(router_prob_matrix_np)\n",
    "\n",
    "    print(\"\\nPCA results:\")\n",
    "    print(f\"explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "    print(f\"cumulative explained variance: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "    print(\"\\nPCA transformed data shape:\", pca_result.shape)\n",
    "\n",
    "    # Get last token of each prompt\n",
    "    last_tokens = [get_last_token(prompt) for prompt in prompts]\n",
    "\n",
    "    # Create 3D scatter plot\n",
    "    fig = go.Figure(data=[go.Scatter3d(\n",
    "        x=pca_result[:, 0],\n",
    "        y=pca_result[:, 1],\n",
    "        z=pca_result[:, 2],\n",
    "        mode='markers+text',\n",
    "        text=last_tokens,\n",
    "        textposition=\"top center\",\n",
    "        marker=dict(\n",
    "            size=10,\n",
    "            opacity=0.8\n",
    "        )\n",
    "    )])\n",
    "\n",
    "    # Update layout for 3D\n",
    "    fig.update_layout(\n",
    "        title='3D PCA of Router Probabilities',\n",
    "        scene=dict(\n",
    "            xaxis_title='first principal component',\n",
    "            yaxis_title='second principal component', \n",
    "            zaxis_title='third principal component',\n",
    "            xaxis=dict(showgrid=True, gridwidth=1, gridcolor='LightGray'),\n",
    "            yaxis=dict(showgrid=True, gridwidth=1, gridcolor='LightGray'),\n",
    "            zaxis=dict(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
    "        ),\n",
    "        width=1000,\n",
    "        height=800,\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(router_prob_matrix, idx1, idx2):\n",
    "\n",
    "    # Get the probability vectors for the two tokens\n",
    "    vec1 = router_prob_matrix[idx1]\n",
    "    vec2 = router_prob_matrix[idx2]\n",
    "    \n",
    "    # Check if inputs are already torch tensors\n",
    "    if not isinstance(vec1, torch.Tensor):\n",
    "        vec1 = torch.from_numpy(vec1).float()\n",
    "    if not isinstance(vec2, torch.Tensor):\n",
    "        vec2 = torch.from_numpy(vec2).float()\n",
    "    \n",
    "    # Compute cosine similarity using torch.nn.functional\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(vec1.unsqueeze(0), vec2.unsqueeze(0))\n",
    "    \n",
    "    return cos_sim.item()\n",
    "\n",
    "# print(f\"Cosine similarity between tokens 2 and 3: {compute_cosine_similarity(router_prob_matrix, 2, 3):.4f}\")\n",
    "# print(f\"Cosine similarity between tokens 0 and 5: {compute_cosine_similarity(router_prob_matrix, 0, 5):.4f}\")\n",
    "# print(f\"Cosine similarity between tokens 10 and 15: {compute_cosine_similarity(router_prob_matrix, 10, 15):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompts_from_txt(txt_file_path,  domain = 'english', output_path=f'english.json'):\n",
    "    \"\"\" read prompts from a txt file and save them in json format. \"\"\"\n",
    "    \n",
    "    with open(txt_file_path, 'r', encoding='utf-8') as f:\n",
    "        prompts = [line.strip() for line in f.readlines() if line.strip()]\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump({f\"{domain}\": prompts}, f, indent=4)\n",
    "        \n",
    "    print(f\"{domain} prompts saved to {output_path}\")\n",
    "    return prompts\n",
    "\n",
    "def parse_code_blocks(txt_file_path, output_path='code.json', domain='code'):\n",
    "    \"\"\"parse code blocks between ``` markers from a text file and save them in json format.\"\"\"\n",
    "    code_blocks = []\n",
    "    current_block = []\n",
    "    in_block = False\n",
    "    \n",
    "    with open(txt_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip().startswith('```'):\n",
    "                if in_block:\n",
    "                    # Current block is complete, save it and start new block\n",
    "                    if current_block:\n",
    "                        code_blocks.append('\\n'.join(current_block))\n",
    "                    current_block = []\n",
    "                # Always start a new block since ``` only indicates start\n",
    "                in_block = True\n",
    "                current_block = []\n",
    "            elif in_block:\n",
    "                # Add line to current block\n",
    "                current_block.append(line.rstrip())\n",
    "    \n",
    "    # Save final block if exists\n",
    "    if current_block:\n",
    "        code_blocks.append('\\n'.join(current_block))\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump({domain: code_blocks}, f, indent=4)\n",
    "        \n",
    "    print(f\"code blocks saved to {output_path}\")\n",
    "    return code_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompts_from_json(json_file_path):\n",
    "    \"\"\"load prompts from a json file and return them as a list.\"\"\"\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    # get the first (and only) value from the dictionary\n",
    "    # since the json structure is {\"domain\": [prompts]}\n",
    "    prompts = list(data.values())[0]\n",
    "    return prompts\n",
    "\n",
    "def prepare_multi_domain_prompts(domain_files, output_path='all_domain_prompts.json'):\n",
    "    \"\"\"\n",
    "    prepare a json file containing prompts from multiple domains.\n",
    "    \n",
    "    args:\n",
    "        domain_files: Dict mapping domain names to lists of tuples (file_path, parser_func)\n",
    "            where parser_func is a function that takes a file path and returns a list of prompts\n",
    "            \n",
    "    example:\n",
    "        domain_files = {\n",
    "            'code': [('code.txt', parse_code_blocks)], \n",
    "            'english': [('english.txt', prepare_prompts_from_txt)]\n",
    "        }\n",
    "    \"\"\"\n",
    "    all_prompts = {}\n",
    "    \n",
    "    for domain, file_list in domain_files.items():\n",
    "        domain_prompts = []\n",
    "        for _, prompts in file_list:\n",
    "            # Use load_prompts_from_json if prompts is a dict\n",
    "            if not isinstance(prompts, list):\n",
    "                prompts = load_prompts_from_json(prompts)\n",
    "            domain_prompts.extend(prompts)\n",
    "                \n",
    "        all_prompts[domain] = domain_prompts\n",
    "        \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_prompts, f, indent=4)\n",
    "        \n",
    "    print(f\"all domain prompts saved to {output_path}\")\n",
    "    return all_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_all_to_list(all_prompts):\n",
    "    \"\"\"\n",
    "    combines prompts from all domains into a single list of domain-specific prompt lists.\n",
    "    returns a list in the format [[domain1_prompts], [domain2_prompts], ...].\n",
    "    \"\"\"\n",
    "    # Create list of domain-specific prompt lists\n",
    "    combined_prompts = [\n",
    "        prompts for prompts in all_prompts.values()\n",
    "    ]\n",
    "        \n",
    "    total_prompts = sum(len(prompts) for prompts in combined_prompts)\n",
    "    print(f\"total prompts: {total_prompts}\")\n",
    "    print(f\"prompts per domain:\")\n",
    "    for domain, prompts in zip(all_prompts.keys(), combined_prompts):\n",
    "        print(f\"  {domain}: {len(prompts)}\")\n",
    "        \n",
    "    return combined_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_visualize_all_domains(router_prob_matrix,combined_prompts, layer_id=None):\n",
    "    \"\"\"\n",
    "    perform PCA visualization on router probability matrix for all domains together.\n",
    "    colors points based on which domain list they came from.\n",
    "    \n",
    "    args:\n",
    "        combined_prompts: List of lists of prompts, where each inner list represents a domain\n",
    "                         [code_prompts, english_prompts, french_prompts, ...]\n",
    "        layer_idx: if None, visualizes all layers but has to be >0 ALWAYS if used\n",
    "    \"\"\"\n",
    "    # Flatten prompts list while tracking domain indices\n",
    "    all_prompts = []\n",
    "    domain_colors = []\n",
    "    \n",
    "    # Generate enough distinct colors for all domains\n",
    "    num_domains = len(combined_prompts)\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, num_domains))\n",
    "    \n",
    "    for domain_idx, domain_prompts in enumerate(combined_prompts):\n",
    "        all_prompts.extend(domain_prompts)\n",
    "        domain_colors.extend([colors[domain_idx]] * len(domain_prompts))\n",
    "    \n",
    "    # If layer_idx is None, visualize all layers\n",
    "    if layer_id is None:\n",
    "        # num_layers = router_prob_matrix.shape[0]\n",
    "        # print(f\"num layers : {num_layers}\")\n",
    "        \n",
    "        for layer_idx in range(1,28):\n",
    "            print(f\"layer {layer_idx} router probs shape : {router_prob_matrix[layer_idx-1].shape}\")\n",
    "            # Perform PCA for current layer\n",
    "            pca = PCA(n_components=3)\n",
    "            layer_probs = router_prob_matrix[layer_idx-1].detach().numpy()\n",
    "            pca_result = pca.fit_transform(layer_probs)\n",
    "\n",
    "            print(f\"\\nPCA results for layer {layer_idx}:\")\n",
    "            print(f\"explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "            print(f\"cumulative explained variance: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "            print(\"\\nPCA transformed data shape:\", pca_result.shape)\n",
    "\n",
    "            # Get last tokens for each prompt\n",
    "            last_tokens = [get_last_token(prompt) for prompt in all_prompts]\n",
    "\n",
    "            # Create 3D scatter plot with domain-specific colors\n",
    "            fig = go.Figure(data=[go.Scatter3d(\n",
    "                x=pca_result[:, 0],\n",
    "                y=pca_result[:, 1], \n",
    "                z=pca_result[:, 2],\n",
    "                mode='markers+text',\n",
    "                text=last_tokens,\n",
    "                textposition=\"top center\",\n",
    "                marker=dict(\n",
    "                    size=10,\n",
    "                    opacity=0.8,\n",
    "                    color=[f'rgb({int(c[0]*255)},{int(c[1]*255)},{int(c[2]*255)})' for c in domain_colors]\n",
    "                )\n",
    "            )])\n",
    "\n",
    "            # Update layout for better 3D visualization\n",
    "            fig.update_layout(\n",
    "                title=f'PCA visualization of router probabilities across domains for layer {layer_idx}',\n",
    "                scene=dict(\n",
    "                    xaxis_title='PC1',\n",
    "                    yaxis_title='PC2', \n",
    "                    zaxis_title='PC3'\n",
    "                ),\n",
    "                width=1000,\n",
    "                height=800,\n",
    "                showlegend=False\n",
    "            )\n",
    "\n",
    "            # Save figure as HTML and show plot\n",
    "            os.makedirs('pca', exist_ok=True)\n",
    "            fig.write_html(f'pca/pca_visualization_layer_{layer_idx}.html')\n",
    "            fig.show()\n",
    "    else:\n",
    "        # Original single layer visualization\n",
    "        layer_probs = router_prob_matrix[layer_id-1].detach().numpy()\n",
    "        pca = PCA(n_components=3)\n",
    "        pca_result = pca.fit_transform(layer_probs)\n",
    "\n",
    "        print(\"\\nPCA results:\")\n",
    "        print(f\"explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "        print(f\"cumulative explained variance: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "        print(\"\\nPCA transformed data shape:\", pca_result.shape)\n",
    "\n",
    "        # Get last tokens for each prompt\n",
    "        last_tokens = [get_last_token(prompt) for prompt in all_prompts]\n",
    "\n",
    "        # Create 3D scatter plot with domain-specific colors\n",
    "        fig = go.Figure(data=[go.Scatter3d(\n",
    "            x=pca_result[:, 0],\n",
    "            y=pca_result[:, 1], \n",
    "            z=pca_result[:, 2],\n",
    "            mode='markers+text',\n",
    "            text=last_tokens,\n",
    "            textposition=\"top center\",\n",
    "            marker=dict(\n",
    "                size=10,\n",
    "                opacity=0.8,\n",
    "                color=[f'rgb({int(c[0]*255)},{int(c[1]*255)},{int(c[2]*255)})' for c in domain_colors]\n",
    "            )\n",
    "        )])\n",
    "\n",
    "        # Update layout for better 3D visualization\n",
    "        fig.update_layout(\n",
    "            title=f'PCA visualization of router probabilities across domains for layer {layer_id}',\n",
    "            scene=dict(\n",
    "                xaxis_title='PC1',\n",
    "                yaxis_title='PC2', \n",
    "                zaxis_title='PC3'\n",
    "            ),\n",
    "            width=1000,\n",
    "            height=800,\n",
    "            showlegend=False\n",
    "        )\n",
    "\n",
    "        fig.show()\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_graph_visualize(router_probs_matrix, layer_id=0, domain=0):\n",
    "    total_tokens = router_probs_matrix.shape[1]  # 600 tokens total\n",
    "    tokens_per_domain = total_tokens // 3  # 200 tokens per domain\n",
    "    num_experts = router_probs_matrix.shape[2]  # 64 experts\n",
    "    \n",
    "    # Calculate start and end indices for this domain's tokens\n",
    "    start_idx = domain * tokens_per_domain  # 0 for code, 200 for English, 400 for French\n",
    "    end_idx = start_idx + tokens_per_domain\n",
    "    # Get probabilities for specified layer and domain tokens\n",
    "    domain_probs = router_probs_matrix[layer_id, start_idx:end_idx, :]\n",
    "\n",
    "    # Initialize expert counts\n",
    "    expert_counts = defaultdict(int)\n",
    "    \n",
    "    # For each token in this domain, get top 7 experts and increment their counts\n",
    "    for token_probs in domain_probs:\n",
    "        # print(f\"token probs shape : {token_probs.shape}\")\n",
    "        top_experts = torch.topk(token_probs, k=7).indices\n",
    "        for expert in top_experts:\n",
    "            expert_counts[int(expert)] += 1\n",
    "            \n",
    "    # Calculate percentage of domain tokens that were routed to each expert\n",
    "    expert_percentages = [expert_counts[i]/tokens_per_domain * 100 for i in range(num_experts)]\n",
    "    \n",
    "    # Create bar plot using plotly\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        x=list(range(num_experts)),\n",
    "        y=expert_percentages,\n",
    "        marker_color='red'\n",
    "    ))\n",
    "    \n",
    "    domain_name = 'code' if domain==0 else 'English' if domain==1 else 'French'\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f'Percentage of total tokens from {domain_name} domain routed to each expert (top 7 per token) for layer {layer_id}',\n",
    "        xaxis_title='Expert',\n",
    "        yaxis_title='% of domain tokens',\n",
    "        yaxis=dict(range=[0, 100]),  # Set y-axis range from 0 to 100%\n",
    "        xaxis_tickangle=-45,\n",
    "        bargap=0.2,\n",
    "        plot_bgcolor='white',\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    # Add grid\n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='rgba(128, 128, 128, 0.2)')\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='rgba(128, 128, 128, 0.2)')\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english prompts saved to english.json\n",
      "french prompts saved to french.json\n",
      "code blocks saved to code.json\n",
      "total code prompts : 200\n",
      "total english prompts : 200\n",
      "code blocks saved to code.json\n",
      "english prompts saved to english.json\n",
      "french prompts saved to french.json\n",
      "all domain prompts saved to all_prompts.json\n",
      "total domains : 3\n",
      "total prompts: 600\n",
      "prompts per domain:\n",
      "  code: 200\n",
      "  english: 200\n",
      "  french: 200\n"
     ]
    }
   ],
   "source": [
    "prepare_prompts_from_txt('interp-data/engl-lit.txt', domain='english', output_path='english.json')\n",
    "prepare_prompts_from_txt('interp-data/french.txt', domain='french', output_path='french.json')\n",
    "parse_code_blocks('interp-data/code.txt', 'code.json', domain='code')\n",
    "\n",
    "code_prompts = load_prompts_from_json(json_file_path='code.json')\n",
    "print(f\"total code prompts : {len(code_prompts)}\")\n",
    "english_prompts = load_prompts_from_json(json_file_path='english.json')\n",
    "print(f'total english prompts : {len(english_prompts)}')\n",
    "\n",
    "\n",
    "domain_files = {\n",
    "    'code': [('interp-data/code.txt', parse_code_blocks(txt_file_path='interp-data/code.txt', output_path='code.json', domain='code'))],\n",
    "    'english': [('interp-data/engl-lit.txt', prepare_prompts_from_txt(txt_file_path='interp-data/engl-lit.txt', output_path='english.json', domain='english'))],\n",
    "    'french': [('interp-data/french.txt', prepare_prompts_from_txt(txt_file_path='interp-data/french.txt', output_path='french.json', domain='french'))]\n",
    "}\n",
    "all_prompts = prepare_multi_domain_prompts(domain_files, output_path='all_prompts.json')\n",
    "print(f\"total domains : {len(all_prompts)}\")\n",
    "\n",
    "# convert all prompts to a single list of domain-specific prompt lists\n",
    "combined_prompts = convert_all_to_list(all_prompts)\n",
    "\n",
    "# # # Create test version with only first 5 prompts per domain\n",
    "# test_combined_prompts = [\n",
    "#     domain_prompts[:2] for domain_prompts in combined_prompts\n",
    "# ]\n",
    "\n",
    "# print(test_combined_prompts[0]) # Print first 5 prompts from first domain\n",
    "# print(f' total prompts in first domain : {len(test_combined_prompts[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in PCA visualizations layers are 0 indexed but in the code they are 1 indexed because of the router probs matrix !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get router probabilities for all prompts\n",
    "combined_prompts = combined_prompts\n",
    "k = 64\n",
    "last_token_prob_matrix, all_router_logits = get_router_probs_matrix(model, prompts=[p for domain in combined_prompts for p in domain],k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_text = \"what is principal component analysis ?\"\n",
    "# outputs = generate_text(model, tokenizer, input_text)\n",
    "\n",
    "# test_prompts = [\n",
    "#     \"The quick brown fox\",\n",
    "#     \"1+1=\",\n",
    "#     \"the grey cat\",\n",
    "#     \"the grey elephant\",\n",
    "#     \"2*8\",\n",
    "#     \"def hello_world() : \\n    print('hello world')\",\n",
    "#     \"what is principal component analysis\",\n",
    "#     'what is capital of india',\n",
    "#     'sqrt 16',\n",
    "#     'void bubbleSort(int arr[], int n) {',\n",
    "#     'def is_prime(n):',\n",
    "#     'if n <= 1:',\n",
    "#     'return False',\n",
    "#     'for i in range(2, int(n**0.5) + 1):',\n",
    "#     'if n % i == 0:',\n",
    "#     'return False',\n",
    "#     'return True',\n",
    "#     \"china\",\n",
    "#     \"the united states of america\",\n",
    "#     \"london\",\n",
    "#     \"tokyo\",\n",
    "#     'paris'\n",
    "# ]\n",
    "\n",
    "# layer_idx = 0\n",
    "# k = 64\n",
    "# router_logits = get_router_logits(model, input_ids=tokenizer.encode(input_text, return_tensors=\"pt\"))\n",
    "# probs = get_last_token_router_probs(router_logits, layer_idx=layer_idx)\n",
    "# print(f\"router probs shape: {probs.shape}, sum: {probs.sum():.2f}\")\n",
    "# print(f'router probs : {probs}')\n",
    "# top_probs = topk(probs, k=k) \n",
    "# print(f\"top {k} probs : {top_probs}\")\n",
    "# print(f\"top {k} probs sum : {top_probs.sum():.2f}\")\n",
    "\n",
    "\n",
    "# router_prob_matrix = get_router_probs_matrix(model, prompts = test_prompts,k=k)\n",
    "# print(f\"router probability matrix shape : {router_prob_matrix.shape}\")\n",
    "# print(\"\\nrouter probs matrix :\")\n",
    "# print(router_prob_matrix[:])\n",
    "# print(f\"\\nverify each row sums to 1 : {router_prob_matrix.sum(dim=1)}\")\n",
    "\n",
    "\n",
    "# last_tokens = [get_last_token(prompt) for prompt in test_prompts]\n",
    "# print(last_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
