{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import OlmoeForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15ad069ec9d4edb935a55fda4c817f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_model(model_name=\"allenai/OLMoE-1B-7B-0924\"):\n",
    "    model = OlmoeForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_from_dataset(tokenizer, dataset_name, category, split=\"test\"):\n",
    "    \"\"\"\n",
    "    Load dataset from HuggingFace datasets, tokenize it and save as pkl file.\n",
    "    Args:\n",
    "        tokenizer : Tokenizer for the model\n",
    "        dataset_name : Name of dataset on HuggingFace hub (e.g. \"wikitext\", \"c4\")\n",
    "        category : Category/subset of the dataset (e.g. \"abstract_algebra\" for MMLU)\n",
    "        split : Dataset split to load (default: \"train\")\n",
    "    Returns :\n",
    "        Tuple (output_path, inputs)\n",
    "        output_path : Path to the pkl file containing tokenized dataset\n",
    "        inputs : Dictionary with input_ids and attention_mask tensors for the entire dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load dataset with category\n",
    "    dataset = load_dataset(dataset_name, category)\n",
    "    \n",
    "    # Check if split exists in dataset\n",
    "    if split not in dataset:\n",
    "        available_splits = list(dataset.keys())\n",
    "        raise ValueError(f\"Split '{split}' not found in dataset. Available splits: {available_splits}\")\n",
    "    \n",
    "    # Get text field (most datasets use 'text' as the field name)\n",
    "    texts = dataset[split]['text'] if 'text' in dataset[split].features else list(dataset[split][next(iter(dataset[split].features))])\n",
    "    \n",
    "    # Tokenize all texts at once with padding\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Save tokenized inputs with sanitized path\n",
    "    # Replace forward slashes with underscores to avoid directory issues\n",
    "    safe_dataset_name = dataset_name.replace('/', '_')\n",
    "    output_path = f\"{safe_dataset_name}_{category}_{split}_tokenized.pkl\"\n",
    "    torch.save(inputs, output_path)\n",
    "    \n",
    "    return output_path, inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: {'input_ids': tensor([[   34, 15411,  8479,  ...,     1,     1,     1],\n",
      "        [   34,   346,    69,  ...,     1,     1,     1],\n",
      "        [ 7371,   273,   253,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [43228,   273, 28833,  ...,     1,     1,     1],\n",
      "        [  688,  1821,    13,  ...,     1,     1,     1],\n",
      "        [ 7371,   273,   253,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "tokenized_path, inputs = get_tokens_from_dataset(tokenizer, dataset_name=\"cais/mmlu\", category=\"anatomy\", split=\"test\")\n",
    "print(f'inputs: {inputs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset dict_keys(['test', 'validation', 'dev'])\n",
      "{'question': 'What is true for a type-Ia (\"type one-a\") supernova?', 'subject': 'astronomy', 'choices': ['This type occurs in binary systems.', 'This type occurs in young galaxies.', 'This type produces gamma-ray bursts.', 'This type produces high amounts of X-rays.'], 'answer': 0}\n",
      "Test set: 152 examples\n",
      "Validation set: 16 examples\n",
      "dev set: {'question': 'Say the pupil of your eye has a diameter of 5 mm and you have a telescope with an aperture of 50 cm. How much more light can the telescope gather than your eye?', 'subject': 'astronomy', 'choices': ['10000 times more', '100 times more', '1000 times more', '10 times more'], 'answer': 0} examples\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"cais/mmlu\", \"astronomy\")\n",
    "print(f'dataset {dataset.keys()}')\n",
    "\n",
    "print(dataset['test'][0])\n",
    "print(f\"Test set: {len(dataset['test'])} examples\")\n",
    "print(f\"Validation set: {len(dataset['validation'])} examples\")\n",
    "print(f'dev set: {(dataset[\"dev\"][2])} examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_router_logits(model, input_text: str, layer_idx: Optional[int] = None, k: int = 1):\n",
    "    \"\"\"\n",
    "    Get router logits for each token in the input text.\n",
    "    \n",
    "    Args:\n",
    "        model: OlmoeForCausalLM model\n",
    "        input_text: Text string to analyze\n",
    "        layer_idx: Optional int specifying which layer to analyze. If None, analyze all layers.\n",
    "        k: Number of top experts to return per token\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries, one per text, each containing:\n",
    "        - tokens: List of tokens\n",
    "        - router_probs: List of top-k router probabilities for each token in specified layer(s)\n",
    "        - router_indices: List of top-k expert indices for each token in specified layer(s)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Forward pass with router logits enabled\n",
    "    outputs = model(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        output_router_logits=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    \n",
    "    # Process router logits for each layer\n",
    "    layer_probs = []\n",
    "    layer_indices = []\n",
    "    \n",
    "    # Get router logits for the specified layer(s)\n",
    "    router_logits = outputs.router_logits\n",
    "    if layer_idx is not None:\n",
    "        router_logits = [router_logits[layer_idx]]\n",
    "        \n",
    "    for layer_router_logits in router_logits:\n",
    "        # Apply softmax to get probabilities\n",
    "        probs = torch.nn.functional.softmax(layer_router_logits.detach(), dim=-1)\n",
    "        # Reshape to [seq_len, num_experts] since batch_size=1\n",
    "        probs = probs.reshape(inputs['input_ids'].size(1), -1)\n",
    "        # Get top k probabilities and indices for each token\n",
    "        top_probs, top_indices = torch.topk(probs, k=k)\n",
    "        \n",
    "        layer_probs.append(top_probs.tolist())\n",
    "        layer_indices.append(top_indices.tolist())\n",
    "        \n",
    "    results.append({\n",
    "        \"tokens\": inputs,\n",
    "        \"router_probs\": layer_probs,\n",
    "        \"router_indices\": layer_indices\n",
    "    })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_text: What is true for a type-Ia (\"type one-a\") supernova?\n",
      "results: [{'tokens': {'input_ids': tensor([[ 1276,   310,  2032,   323,   247,  1511,    14,    42,    66,  5550,\n",
      "           881,   581,    14,    66,  2807, 13708,  8947,    32]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'router_probs': [[[0.1212511956691742], [0.08246838301420212], [0.0790310949087143], [0.10655879229307175], [0.08738203346729279], [0.09577271342277527], [0.12028155475854874], [0.21959717571735382], [0.27170330286026], [0.08207882195711136], [0.10128765553236008], [0.13019609451293945], [0.17984884977340698], [0.13717882335186005], [0.08026150614023209], [0.0579301081597805], [0.07837120443582535], [0.051885128021240234]]], 'router_indices': [[[57], [14], [48], [50], [6], [32], [55], [27], [35], [6], [18], [55], [1], [27], [4], [49], [26], [6]]]}]\n"
     ]
    }
   ],
   "source": [
    "input_text = dataset['test'][0]['question'] # retrieve question from dataset\n",
    "print(f'input_text: {input_text}')\n",
    "results = get_router_logits(model, input_text, layer_idx=0)\n",
    "print(f'results: {results}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
