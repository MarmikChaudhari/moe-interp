{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import OlmoeForCausalLM, AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6627cf7de5f34ecbb4b28546d9a0faec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_model(model_name=\"allenai/OLMoE-1B-7B-0924\"):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split text file into tokens for model's context length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_input(file_path, chunk_size=1000, tokenizer=None):\n",
    "    \"\"\"    \n",
    "    args :\n",
    "        file_path (str): Path to the input text file\n",
    "        chunk_size (int): Number of tokens per chunk\n",
    "        tokenizer: HuggingFace tokenizer (if None, will split on whitespace)\n",
    "        \n",
    "    output : List of text chunks of approximately chunk_size tokens\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Read the full text file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    if tokenizer:\n",
    "        # Tokenize the full text\n",
    "        tokens = tokenizer.encode(text)\n",
    "        tokens_tensor = torch.tensor(tokens).to(device)\n",
    "        \n",
    "        # Split into chunks\n",
    "        chunks = []\n",
    "        for i in range(0, len(tokens), chunk_size):\n",
    "            chunk_tokens = tokens_tensor[i:i + chunk_size]\n",
    "            # Move to CPU for decoding\n",
    "            chunk_tokens = chunk_tokens.cpu()\n",
    "            # Decode tokens back to text\n",
    "            chunk_text = tokenizer.decode(chunk_tokens)\n",
    "            chunks.append(chunk_text)\n",
    "            \n",
    "    else:\n",
    "        # Simple whitespace tokenization\n",
    "        words = text.split()\n",
    "        \n",
    "        # Split into chunks\n",
    "        chunks = []\n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the router logits for each token across all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_router_logits(model, input_text: str, k: int = 1):\n",
    "    \"\"\"\n",
    "    args :\n",
    "        model: OlmoeForCausalLM model\n",
    "        input_text: Text string to analyze\n",
    "        k: Number of top experts to return per token\n",
    "        \n",
    "    output : dictionary mapping layer indices to lists of [token_text, expert_index, router_probability] for each token in that layer\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Forward pass with router logits enabled\n",
    "    outputs = model(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        output_router_logits=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    \n",
    "    # Get router logits for all layers\n",
    "    router_logits = outputs.router_logits\n",
    "    \n",
    "    all_layer_results = {}\n",
    "    for layer_idx, layer_router_logits in enumerate(router_logits):\n",
    "        # Apply softmax to get probabilities\n",
    "        probs = torch.nn.functional.softmax(layer_router_logits.detach(), dim=-1)\n",
    "        # Reshape to [seq_len, num_experts] since batch_size=1\n",
    "        probs = probs.reshape(inputs['input_ids'].size(1), -1)\n",
    "        # Get top k probabilities and indices for each token\n",
    "        top_probs, top_indices = torch.topk(probs, k=k)\n",
    "        \n",
    "        # Move tensors to CPU for post-processing\n",
    "        top_probs = top_probs.cpu()\n",
    "        top_indices = top_indices.cpu()\n",
    "        \n",
    "        # Convert token IDs to text\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].cpu())\n",
    "        \n",
    "        # Create list of [token, expert, prob] for each token\n",
    "        layer_tokens = []\n",
    "        for i in range(len(tokens)):\n",
    "            for j in range(k):\n",
    "                # Clean special characters from token text\n",
    "                clean_token = tokens[i].replace('Ä ', '')\n",
    "                layer_tokens.append([\n",
    "                    clean_token,\n",
    "                    top_indices[i][j].item(),\n",
    "                    top_probs[i][j].item()\n",
    "                ])\n",
    "        \n",
    "        all_layer_results[layer_idx] = layer_tokens\n",
    "    \n",
    "    return all_layer_results # Dictionary mapping layer index to list of [token, expert_number, probability]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### update/create the router logits json file with new tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_router_logits_json(results, domain, device='cuda'):\n",
    "    \"\"\"\n",
    "    args :\n",
    "        results: Dictionary mapping layer index to list of [token, expert_number, probability]\n",
    "        domain: String indicating the domain (e.g., 'arxiv', 'code')\n",
    "        device: Device to use for tensor operations ('cuda' or 'cpu')\n",
    "    output : updated json file with new tokens\n",
    "    \"\"\"\n",
    "    if domain == 'arxiv':\n",
    "        json_path = 'arxiv_all_layers.json'\n",
    "    elif domain == 'github':\n",
    "        json_path = 'github_all_layers.json'\n",
    "    elif domain == 'math':\n",
    "        json_path = 'math_all_layers.json'\n",
    "    elif domain == 'physics':\n",
    "        json_path = 'physics_all_layers.json'\n",
    "    elif domain == 'biology':\n",
    "        json_path = 'biology_all_layers.json'\n",
    "    elif domain == 'legal':\n",
    "        json_path = 'legal_all_layers.json'\n",
    "\n",
    "    \n",
    "    # Initialize an empty dictionary for existing results\n",
    "    existing_results = {}\n",
    "    \n",
    "    if os.path.exists(json_path):\n",
    "        # Load existing results\n",
    "        with open(json_path, 'r') as f:\n",
    "            try:\n",
    "                existing_results = json.load(f)\n",
    "                # Convert string keys to integers\n",
    "                existing_results = {int(k): v for k, v in existing_results.items()}\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: {json_path} is empty or corrupted. Starting with an empty dictionary.\")\n",
    "    \n",
    "    # Move results to GPU if available\n",
    "    if torch.cuda.is_available() and device == 'cuda':\n",
    "        for layer_idx, layer_tokens in results.items():\n",
    "            # Convert lists to tensors and move to GPU\n",
    "            tokens_tensor = torch.tensor([[t[0], t[1], t[2]] for t in layer_tokens]).cuda()\n",
    "            results[layer_idx] = tokens_tensor.tolist()\n",
    "    \n",
    "    # Combine existing and new results for each layer\n",
    "    for layer_idx, layer_tokens in results.items():\n",
    "        if layer_idx in existing_results:\n",
    "            existing_results[layer_idx].extend(layer_tokens)\n",
    "        else:\n",
    "            existing_results[layer_idx] = layer_tokens\n",
    "    \n",
    "    # Save updated results with integer keys\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(existing_results, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "    return existing_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot the expert distribution for a particular layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_expert_distribution(layer_idx, domain, device='cuda'):\n",
    "    \"\"\"    \n",
    "    args :\n",
    "        json_path: Path to the JSON file containing expert counts\n",
    "        device: Device to use for tensor operations ('cuda' or 'cpu')\n",
    "    output : plot of the expert distribution for a particular layer\n",
    "    \"\"\"\n",
    "    if domain == 'arxiv':\n",
    "        json_path = 'arxiv_all_layers.json'\n",
    "    elif domain == 'github':\n",
    "        json_path = 'github_all_layers.json'\n",
    "    elif domain == 'math':\n",
    "        json_path = 'math_all_layers.json'\n",
    "    elif domain == 'physics':\n",
    "        json_path = 'physics_all_layers.json'\n",
    "    elif domain == 'biology':\n",
    "        json_path = 'biology_all_layers.json'\n",
    "    elif domain == 'legal':\n",
    "        json_path = 'legal_all_layers.json'\n",
    "\n",
    "    # Read JSON file\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Extract layer results\n",
    "    layer_results = data[str(layer_idx)]\n",
    "    \n",
    "    # Create a dictionary to store expert counts\n",
    "    expert_counts = defaultdict(int)\n",
    "    \n",
    "    # Move data to GPU if available\n",
    "    if torch.cuda.is_available() and device == 'cuda':\n",
    "        layer_results = torch.tensor(layer_results).cuda()\n",
    "        \n",
    "    # Count how many tokens went to each expert\n",
    "    total_assignments = len(layer_results)\n",
    "    print(f'Total assignments: {total_assignments}')\n",
    "    \n",
    "    # Count occurrences of each expert\n",
    "    if torch.cuda.is_available() and device == 'cuda':\n",
    "        # Process on GPU\n",
    "        for _, expert, _ in layer_results.cpu().numpy():\n",
    "            expert_counts[int(expert)] += 1\n",
    "    else:\n",
    "        # Process on CPU\n",
    "        for _, expert, _ in layer_results:\n",
    "            expert_counts[expert] += 1\n",
    "            \n",
    "    print(f'Expert counts: {expert_counts}')\n",
    "    print(f'Total experts: {len(expert_counts)}')\n",
    "    print(f'Expert count for l0', expert_counts[0])\n",
    "    \n",
    "    # Convert to lists for plotting and calculate percentages\n",
    "    experts = [f'{i}' for i in range(64)]\n",
    "    percentages = [expert_counts[i]/total_assignments * 100 for i in range(64)]\n",
    "    \n",
    "    # Create bar chart\n",
    "    fig = go.Figure(data=[\n",
    "        go.Bar(\n",
    "            x=experts,\n",
    "            y=percentages,\n",
    "            textposition='auto',\n",
    "            marker_color='red'  # You can use any color here - hex code, RGB, or color name\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'percentage of total tokens routed to each expert for layer {layer_idx}',\n",
    "        xaxis_title='expert',\n",
    "        yaxis_title='% of total tokens',\n",
    "        yaxis=dict(range=[0, 100]), # Set y-axis range from 0 to 100%\n",
    "        xaxis_tickangle=-45,\n",
    "        bargap=0.2\n",
    "    )\n",
    "    \n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### expert distribution for a single text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read and chunk input file\n",
    "# file_path = 'arxiv.txt'\n",
    "# domain = 'arxiv'\n",
    "# chunks = prepare_text_input(file_path, chunk_size=4096, tokenizer=tokenizer)\n",
    "# print(f'Number of chunks: {len(chunks)}')\n",
    "# # Process first chunk\n",
    "# first_chunk = chunks[3]\n",
    "# # print(f'Processing text : {first_chunk[:100]}...')  # Print first 100 chars\n",
    "\n",
    "# # Get router logits for the chunk\n",
    "# results = get_router_logits(model, first_chunk)\n",
    "\n",
    "# # Save results for analysis\n",
    "# update_router_logits_json(results, domain=domain)\n",
    "\n",
    "# # Save results to parquet for visualization\n",
    "# layer_to_plot = 0  # Analyze first layer\n",
    "\n",
    "# # Plot expert distribution\n",
    "# fig = plot_expert_distribution(layer_idx=layer_to_plot, domain=domain)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### expert distribution for all text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/2\n",
      "Warning: github_router_logits_all_layers.json is empty or corrupted. Starting with an empty dictionary.\n",
      "Processing chunk 2/2\n"
     ]
    }
   ],
   "source": [
    "# Read and chunk input text file\n",
    "file_path = 'github.txt'\n",
    "domain = 'github'\n",
    "chunks = prepare_text_input(file_path, chunk_size=4096, tokenizer=tokenizer)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)  # Move model to GPU if available\n",
    "\n",
    "# Process all chunks\n",
    "all_results = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f'Processing chunk {i+1}/{len(chunks)}')\n",
    "    # print(f'Sample text: {chunk[:10]}...')  \n",
    "    \n",
    "    # Get router logits for the chunk\n",
    "    with torch.cuda.amp.autocast():  # Enable automatic mixed precision\n",
    "        results = get_router_logits(model, chunk)\n",
    "    all_results.append(results)\n",
    "    \n",
    "    # Save intermediate results \n",
    "    update_router_logits_json(results, domain=domain)\n",
    "    \n",
    "    # Clear GPU cache periodically\n",
    "    if (i + 1) % 10 == 0:  # Every 10 chunks\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Routing for first 5 tokens in layer 0: \n",
      "Token: import, Expert: 57, Probability: 0.241\n",
      "Token: *, Expert: 8, Probability: 0.218\n",
      "Token: as, Expert: 21, Probability: 0.204\n",
      "Token: THREE, Expert: 21, Probability: 0.212\n",
      "Token: from, Expert: 25, Probability: 0.113\n",
      "Total assignments: 5197\n",
      "Expert counts: defaultdict(<class 'int'>, {57: 2, 8: 32, 21: 1218, 25: 82, 6: 41, 26: 8, 14: 43, 2: 147, 3: 26, 54: 284, 53: 98, 4: 6, 27: 60, 42: 92, 15: 442, 44: 70, 29: 195, 36: 109, 52: 285, 37: 42, 62: 40, 16: 27, 17: 63, 55: 59, 48: 22, 33: 89, 56: 70, 61: 56, 59: 68, 32: 19, 13: 51, 30: 54, 60: 122, 41: 255, 49: 86, 18: 44, 31: 231, 40: 65, 47: 159, 35: 48, 50: 56, 45: 8, 58: 30, 10: 48, 24: 10, 20: 5, 1: 20, 5: 16, 22: 6, 63: 6, 7: 7, 51: 56, 11: 4, 43: 14, 19: 1})\n",
      "Total experts: 55\n",
      "Expert count for l0 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "red"
         },
         "textposition": "auto",
         "type": "bar",
         "x": [
          "0",
          "1",
          "2",
          "3",
          "4",
          "5",
          "6",
          "7",
          "8",
          "9",
          "10",
          "11",
          "12",
          "13",
          "14",
          "15",
          "16",
          "17",
          "18",
          "19",
          "20",
          "21",
          "22",
          "23",
          "24",
          "25",
          "26",
          "27",
          "28",
          "29",
          "30",
          "31",
          "32",
          "33",
          "34",
          "35",
          "36",
          "37",
          "38",
          "39",
          "40",
          "41",
          "42",
          "43",
          "44",
          "45",
          "46",
          "47",
          "48",
          "49",
          "50",
          "51",
          "52",
          "53",
          "54",
          "55",
          "56",
          "57",
          "58",
          "59",
          "60",
          "61",
          "62",
          "63"
         ],
         "y": [
          0,
          0.38483740619588225,
          2.828554935539734,
          0.5002886280546469,
          0.11545122185876468,
          0.3078699249567058,
          0.7889166827015586,
          0.13469309216855876,
          0.6157398499134116,
          0,
          0.9236097748701174,
          0.07696748123917645,
          0,
          0.9813353857994996,
          0.8274004233211469,
          8.504906676928996,
          0.519530498364441,
          1.212237829517029,
          0.846642293630941,
          0.01924187030979411,
          0.09620935154897056,
          23.43659803732923,
          0.11545122185876468,
          0,
          0.19241870309794112,
          1.5778333654031171,
          0.1539349624783529,
          1.1545122185876466,
          0,
          3.752164710409852,
          1.039060996728882,
          4.44487204156244,
          0.3655955358860881,
          1.712526457571676,
          0,
          0.9236097748701174,
          2.097363863767558,
          0.8081585530113526,
          0,
          0,
          1.2507215701366172,
          4.9066769289974985,
          1.7702520685010583,
          0.26938618433711753,
          1.3469309216855878,
          0.1539349624783529,
          0,
          3.059457379257264,
          0.4233211468154705,
          1.6548008466422939,
          1.0775447373484701,
          1.0775447373484701,
          5.4839330382913225,
          1.885703290359823,
          5.4646911679815275,
          1.1352703482778526,
          1.3469309216855878,
          0.03848374061958822,
          0.5772561092938233,
          1.3084471810659997,
          2.347508177794882,
          1.0775447373484701,
          0.7696748123917645,
          0.11545122185876468
         ]
        }
       ],
       "layout": {
        "bargap": 0.2,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "percentage of total tokens routed to each expert for layer 0"
        },
        "xaxis": {
         "tickangle": -45,
         "title": {
          "text": "expert"
         }
        },
        "yaxis": {
         "range": [
          0,
          100
         ],
         "title": {
          "text": "% of total tokens"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer_to_plot = 0  # Analyze first layer\n",
    "\n",
    "# Combine results from all chunks\n",
    "combined_results = []\n",
    "for layer_idx in range(len(all_results[0])):  # For each layer\n",
    "    layer_combined = []\n",
    "    for chunk_result in all_results:\n",
    "        # Move data to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            chunk_result = [\n",
    "                (token.cuda() if torch.is_tensor(token) else token,\n",
    "                 expert.cuda() if torch.is_tensor(expert) else expert, \n",
    "                 prob.cuda() if torch.is_tensor(prob) else prob)\n",
    "                for token, expert, prob in chunk_result[layer_idx]\n",
    "            ]\n",
    "        layer_combined.extend(chunk_result)\n",
    "    combined_results.append(layer_combined)\n",
    "\n",
    "# Analyze routing for first few tokens in layer 0\n",
    "print(\"\\nRouting for first 5 tokens in layer 0: \")\n",
    "layer_results = combined_results[layer_to_plot]  # Layer 0\n",
    "for token_info in layer_results[:5]:  # Limit to first 5 tokens\n",
    "    token, expert, prob = token_info\n",
    "    # Move to CPU for printing\n",
    "    if torch.cuda.is_available():\n",
    "        prob = prob.cpu()\n",
    "    print(f\"Token: {token}, Expert: {expert}, Probability: {prob:.3f}\")\n",
    "\n",
    "# Plot expert distribution for all processed data\n",
    "with torch.cuda.amp.autocast():  # Enable automatic mixed precision\n",
    "    fig = plot_expert_distribution(layer_idx=layer_to_plot, domain=domain)\n",
    "fig.show()\n",
    "\n",
    "# Clear GPU cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
