{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import OlmoeForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15daeee3603e4feab2578c3300ba21d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_model(model_name=\"allenai/OLMoE-1B-7B-0924\"):\n",
    "    model = OlmoeForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split text file into tokens for model's context length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_input(file_path, chunk_size=1000, tokenizer=None):\n",
    "    \"\"\"    \n",
    "    args :\n",
    "        file_path (str): Path to the input text file\n",
    "        chunk_size (int): Number of tokens per chunk\n",
    "        tokenizer: HuggingFace tokenizer (if None, will split on whitespace)\n",
    "        \n",
    "    output : List of text chunks of approximately chunk_size tokens\n",
    "    \"\"\"\n",
    "    # Read the full text file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    if tokenizer:\n",
    "        # Tokenize the full text\n",
    "        tokens = tokenizer.encode(text)\n",
    "        \n",
    "        # Split into chunks\n",
    "        chunks = []\n",
    "        for i in range(0, len(tokens), chunk_size):\n",
    "            chunk_tokens = tokens[i:i + chunk_size]\n",
    "            # Decode tokens back to text\n",
    "            chunk_text = tokenizer.decode(chunk_tokens)\n",
    "            chunks.append(chunk_text)\n",
    "            \n",
    "    else :\n",
    "        # Simple whitespace tokenization\n",
    "        words = text.split()\n",
    "        \n",
    "        # Split into chunks\n",
    "        chunks = []\n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the router logits for each token across all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_router_logits(model, input_text: str, k: int = 1):\n",
    "    \"\"\"\n",
    "    args :\n",
    "        model: OlmoeForCausalLM model\n",
    "        input_text: Text string to analyze\n",
    "        k: Number of top experts to return per token\n",
    "        \n",
    "    output : dictionary mapping layer indices to lists of [token_text, expert_index, router_probability] for each token in that layer\n",
    "    \"\"\"\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Forward pass with router logits enabled\n",
    "    outputs = model(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        output_router_logits=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    \n",
    "    # Get router logits for all layers\n",
    "    router_logits = outputs.router_logits\n",
    "    \n",
    "    all_layer_results = {}\n",
    "    for layer_idx, layer_router_logits in enumerate(router_logits):\n",
    "        # Apply softmax to get probabilities\n",
    "        probs = torch.nn.functional.softmax(layer_router_logits.detach(), dim=-1)\n",
    "        # Reshape to [seq_len, num_experts] since batch_size=1\n",
    "        probs = probs.reshape(inputs['input_ids'].size(1), -1)\n",
    "        # Get top k probabilities and indices for each token\n",
    "        top_probs, top_indices = torch.topk(probs, k=k)\n",
    "        \n",
    "        # Convert token IDs to text\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "        \n",
    "        # Create list of [token, expert, prob] for each token\n",
    "        layer_tokens = []\n",
    "        for i in range(len(tokens)):\n",
    "            for j in range(k):\n",
    "                # Clean special characters from token text\n",
    "                clean_token = tokens[i].replace('Ä ', '')\n",
    "                layer_tokens.append([\n",
    "                    clean_token,\n",
    "                    top_indices[i][j].item(),\n",
    "                    top_probs[i][j].item()\n",
    "                ])\n",
    "        \n",
    "        all_layer_results[layer_idx] = layer_tokens\n",
    "    \n",
    "    return all_layer_results # Dictionary mapping layer index to list of [token, expert_number, probability]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### update/create the router logits json file with new tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_router_logits_json(results, json_path='router_logits_all_layers.json'):\n",
    "    \"\"\"\n",
    "    args :\n",
    "        results: Dictionary mapping layer index to list of [token, expert_number, probability]\n",
    "        json_path: Path to the JSON file\n",
    "    output : updated json file with new tokens\n",
    "    \"\"\"\n",
    "    if os.path.exists(json_path):\n",
    "        # Load existing results\n",
    "        with open(json_path, 'r') as f :\n",
    "            existing_results = json.load(f)\n",
    "            # Convert string keys to integers\n",
    "            existing_results = {int(k): v for k, v in existing_results.items()}\n",
    "            \n",
    "        # Combine existing and new results for each layer\n",
    "        for layer_idx, layer_tokens in results.items():\n",
    "            if layer_idx in existing_results:\n",
    "                existing_results[layer_idx].extend(layer_tokens)\n",
    "            else:\n",
    "                existing_results[layer_idx] = layer_tokens\n",
    "                \n",
    "        combined_results = existing_results\n",
    "    else :\n",
    "        # Create new JSON with results\n",
    "        combined_results = results\n",
    "    \n",
    "    # Save updated results with integer keys\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(combined_results, f)\n",
    "        \n",
    "    return combined_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the router logits for all tokens to a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_parquet(results, layer_idx):\n",
    "    \"\"\"    \n",
    "    args :\n",
    "        layer_results : List of lists containing [token, expert_number, probability] for a specific layer\n",
    "        layer_idx : Index of the layer being processed\n",
    "        output_path : Path to save the Parquet file\n",
    "    \"\"\"\n",
    "    output_path=f'expert_counts_layer_{layer_idx}.parquet'\n",
    "\n",
    "    layer_results = results[layer_idx]\n",
    "    # Create DataFrame from new results\n",
    "    new_df = pd.DataFrame(layer_results, columns=['token', 'expert_number', 'probability'])\n",
    "    \n",
    "    # Add heading\n",
    "    new_df.columns.name = f'Layer {layer_idx} Router Logits'\n",
    "    \n",
    "    # Convert types explicitly\n",
    "    new_df['token'] = new_df['token'].astype(str)\n",
    "    new_df['expert_number'] = new_df['expert_number'].astype(int)\n",
    "    new_df['probability'] = new_df['probability'].astype(float)\n",
    "    \n",
    "    if os.path.exists(output_path):\n",
    "        # Read existing dataframe and append new results\n",
    "        existing_df = pd.read_parquet(output_path)\n",
    "        combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "    else:\n",
    "        # Create new dataframe if file doesn't exist\n",
    "        combined_df = new_df\n",
    "        \n",
    "    # Save to Parquet\n",
    "    combined_df.to_parquet(output_path, index=False)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot the expert distribution for a particular layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a dictionary to store expert counts\n",
    "# expert_counts = defaultdict(int)\n",
    "\n",
    "# # Count how many tokens went to each expert\n",
    "# total_tokens = len(set(token for token, _, _ in results))\n",
    "# # unpacking the results list into token, expert, prob\n",
    "# for token, expert, prob in results:\n",
    "#     # print(f'token: {token}, expert: {expert}, prob: {prob}')\n",
    "#     expert_counts[expert] += 1\n",
    "\n",
    "\n",
    "def plot_expert_distribution(layer_idx):\n",
    "    \"\"\"    \n",
    "    args :\n",
    "        parquet_path: Path to the Parquet file containing expert counts\n",
    "    output : plot of the expert distribution for a particular layer\n",
    "    \"\"\"\n",
    "    parquet_path=f'expert_counts_layer_{layer_idx}.parquet'\n",
    "    # Read parquet file\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    \n",
    "    # Create a dictionary to store expert counts\n",
    "    expert_counts = defaultdict(int)\n",
    "    \n",
    "    # Count how many tokens went to each expert\n",
    "    total_tokens = len(set(df['token']))\n",
    "    \n",
    "    # Count occurrences of each expert\n",
    "    for expert in df['expert_number']:\n",
    "        expert_counts[expert] += 1\n",
    "    \n",
    "    # Convert to lists for plotting and calculate percentages\n",
    "    experts = [f'{i}' for i in range(64)]\n",
    "    percentages = [expert_counts[i]/total_tokens * 100 for i in range(64)]\n",
    "    # Create bar chart\n",
    "    fig = go.Figure(data=[\n",
    "        go.Bar(\n",
    "            x=experts,\n",
    "            y=percentages,\n",
    "            textposition='auto',\n",
    "            marker_color='red'  # You can use any color here - hex code, RGB, or color name\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'percentage of total tokens routed to each expert for layer {layer_idx}',\n",
    "        xaxis_title='expert',\n",
    "        yaxis_title='% of total tokens',\n",
    "        yaxis=dict(range=[0, 100]), # Set y-axis range from 0 to 100%\n",
    "        xaxis_tickangle=-45,\n",
    "        bargap=0.2\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### expert distribution for a single text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and chunk input file\n",
    "file_path = 'github_oss_with_stack_texts.txt'\n",
    "chunks = prepare_text_input(file_path, chunk_size=4096, tokenizer=tokenizer)\n",
    "\n",
    "# Process first chunk\n",
    "first_chunk = chunks[0]\n",
    "print(f'Processing text : {first_chunk[:100]}...')  # Print first 100 chars\n",
    "\n",
    "# Get router logits for the chunk\n",
    "results = get_router_logits(model, first_chunk)\n",
    "\n",
    "# Save results for analysis\n",
    "update_router_logits_json(results)\n",
    "\n",
    "# Analyze routing for first few tokens in layer 0\n",
    "print(\"\\nRouting for first 5 tokens in layer 0: \")\n",
    "layer_results = results[0]  # Layer 0\n",
    "for token_info in layer_results[:5]:\n",
    "    token, expert, prob = token_info\n",
    "    print(f\"Token: {token}, Expert: {expert}, Probability: {prob:.3f}\")\n",
    "\n",
    "# Save results to parquet for visualization\n",
    "layer_to_plot = 0  # Analyze first layer\n",
    "save_to_parquet(results, layer_idx=layer_to_plot)\n",
    "\n",
    "# Plot expert distribution\n",
    "fig = plot_expert_distribution(layer_idx=layer_to_plot)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### expert distribution for all text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and chunk input file\n",
    "file_path = 'github_oss_with_stack_texts.txt'\n",
    "chunks = prepare_text_input(file_path, chunk_size=4096, tokenizer=tokenizer)\n",
    "\n",
    "# Process all chunks\n",
    "all_results = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f'Processing chunk {i+1}/{len(chunks)}')\n",
    "    print(f'Sample text: {chunk[:100]}...')  # Print first 100 chars\n",
    "    \n",
    "    # Get router logits for the chunk\n",
    "    results = get_router_logits(model, chunk)\n",
    "    all_results.append(results)\n",
    "    \n",
    "    # Save intermediate results\n",
    "    update_router_logits_json(results)\n",
    "\n",
    "# Combine results from all chunks\n",
    "combined_results = []\n",
    "for layer_idx in range(len(all_results[0])):  # For each layer\n",
    "    layer_combined = []\n",
    "    for chunk_result in all_results:\n",
    "        layer_combined.extend(chunk_result[layer_idx])\n",
    "    combined_results.append(layer_combined)\n",
    "\n",
    "# Analyze routing for first few tokens in layer 0\n",
    "print(\"\\nRouting for first 5 tokens in layer 0: \")\n",
    "layer_results = combined_results[0]  # Layer 0\n",
    "for token_info in layer_results[:5]:\n",
    "    token, expert, prob = token_info\n",
    "    print(f\"Token: {token}, Expert: {expert}, Probability: {prob:.3f}\")\n",
    "\n",
    "# Save combined results to parquet for visualization\n",
    "layer_to_plot = 0  # Analyze first layer\n",
    "save_to_parquet(combined_results, layer_idx=layer_to_plot)\n",
    "\n",
    "# Plot expert distribution for all processed data\n",
    "fig = plot_expert_distribution(layer_idx=layer_to_plot)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
