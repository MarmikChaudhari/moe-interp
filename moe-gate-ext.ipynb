{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if DEVICE.type == \"cuda\":\n",
    "    # Print CUDA details\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated()/1024**2:.2f}MB\")\n",
    "    print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved()/1024**2:.2f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    # model = AutoModelForCausalLM.from_pretrained(\n",
    "    #     model_name,\n",
    "    #     torch_dtype=torch.float16,\n",
    "    #     trust_remote_code=True,\n",
    "    #     # use_flash_attention_2=True,\n",
    "    # )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # model.to(DEVICE)\n",
    "    return tokenizer #model, tokenizer\n",
    "\n",
    "tokenizer = load_model(\"deepseek-ai/deepseek-moe-16b-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepseekForCausalLM(\n",
       "  (model): DeepseekModel(\n",
       "    (embed_tokens): Embedding(102400, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0): DeepseekDecoderLayer(\n",
       "        (self_attn): DeepseekSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): DeepseekRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): DeepseekMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=10944, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=10944, bias=False)\n",
       "          (down_proj): Linear(in_features=10944, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): DeepseekRMSNorm()\n",
       "        (post_attention_layernorm): DeepseekRMSNorm()\n",
       "      )\n",
       "      (1-27): 27 x DeepseekDecoderLayer(\n",
       "        (self_attn): DeepseekSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): DeepseekRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): DeepseekMoE(\n",
       "          (experts): ModuleList(\n",
       "            (0-63): 64 x DeepseekMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
       "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (gate): MoEGate()\n",
       "          (shared_experts): DeepseekMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=2816, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=2816, bias=False)\n",
       "            (down_proj): Linear(in_features=2816, out_features=2048, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): DeepseekRMSNorm()\n",
       "        (post_attention_layernorm): DeepseekRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): DeepseekRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=102400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moe_metadata(model, input_ids):\n",
    "    \"\"\"Get both router logits and expert indices for all MoE layers.\"\"\"\n",
    "    router_logits_list = []\n",
    "    expert_indices_list = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        # output contains: (topk_idx, topk_weight, aux_loss)\n",
    "        hidden_states = input[0]\n",
    "        \n",
    "        logits = torch.matmul(hidden_states, module.weight.T)\n",
    "        router_logits_list.append(logits.detach())\n",
    "        \n",
    "        # store expert indices actually used for routing\n",
    "        expert_indices_list.append(output[0].detach())\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    hooks = []\n",
    "    for layer_idx, layer in enumerate(model.model.layers):\n",
    "        if layer.mlp.__class__.__name__ == 'DeepseekMoE':\n",
    "            hook = layer.mlp.gate.register_forward_hook(hook_fn)\n",
    "            hooks.append(hook)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model(input_ids)\n",
    "    \n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    moe_metadata = {\n",
    "        'router_logits': torch.stack(router_logits_list) if router_logits_list else None,\n",
    "        'expert_indices': torch.stack(expert_indices_list) if expert_indices_list else None\n",
    "    }\n",
    "    \n",
    "    if moe_metadata['router_logits'] is not None:\n",
    "        print(f\"Router logits shape: {moe_metadata['router_logits'].shape}\")\n",
    "    if moe_metadata['expert_indices'] is not None:\n",
    "        print(f\"Expert indices shape: {moe_metadata['expert_indices'].shape}\")\n",
    "    \n",
    "    return moe_metadata\n",
    "\n",
    "def prepare_prompt(prompt, tokenizer, max_tokens=2048):\n",
    "    \"\"\"\n",
    "    Prepare a prompt for processing, splitting if necessary to fit within model context.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The text prompt to prepare\n",
    "        tokenizer: The model's tokenizer\n",
    "        max_tokens: Maximum number of tokens per chunk (default: 2048)\n",
    "        \n",
    "    Returns:\n",
    "        List of prompts that fit within token limit\n",
    "    \"\"\"\n",
    "    # Tokenize the prompt to get token count\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    \n",
    "    # If prompt is small enough, return as is\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return [prompt]\n",
    "    \n",
    "    # Split into manageable chunks\n",
    "    prepared_prompts = []\n",
    "    \n",
    "    # Decode tokens into chunks\n",
    "    start_idx = 0\n",
    "    while start_idx < len(tokens):\n",
    "        end_idx = min(start_idx + max_tokens, len(tokens))\n",
    "        chunk_tokens = tokens[start_idx:end_idx]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens)\n",
    "        prepared_prompts.append(chunk_text)\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    print(f\"Long prompt detected! Split into {len(prepared_prompts)} chunks.\")\n",
    "    return prepared_prompts\n",
    "\n",
    "def process_text_file_for_expert_counts(file_path, model, tokenizer, output_path=None, max_tokens=4096):\n",
    "    \"\"\"\n",
    "    Process a text file to analyze MoE routing and count tokens per expert in each layer.\n",
    "    Saves a PyTorch file with expert token counts.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to text file with prompts (one per line)\n",
    "        model: DeepSeek MoE model\n",
    "        tokenizer: DeepSeek tokenizer\n",
    "        output_path: Path to save PyTorch results (default: based on input filename)\n",
    "        max_tokens: Maximum tokens per prompt chunk\n",
    "    \"\"\"\n",
    "    # Load the text file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        raw_prompts = [line.strip() for line in f.readlines() if line.strip()]\n",
    "    \n",
    "    print(f\"Loaded {len(raw_prompts)} raw prompts from {file_path}\")\n",
    "    \n",
    "    # Prepare prompts (handle large prompts by splitting)\n",
    "    prompts = []\n",
    "    for raw_prompt in raw_prompts:\n",
    "        prepared_chunks = prepare_prompt(raw_prompt, tokenizer, max_tokens)\n",
    "        prompts.extend(prepared_chunks)\n",
    "    \n",
    "    print(f\"Processing {len(prompts)} prepared prompts (after splitting large ones)\")\n",
    "    \n",
    "    # Set default output path if not provided\n",
    "    if output_path is None:\n",
    "        output_path = file_path.replace('.txt', '_expert_data.pt')\n",
    "    \n",
    "    # Initialize counter for expert usage\n",
    "    # Structure: {layer_num: {expert_id: count}}\n",
    "    expert_counts = {}\n",
    "    \n",
    "    # Calculate total tokens for progress bar\n",
    "    total_tokens = 0\n",
    "    for prompt in prompts:\n",
    "        tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        total_tokens += tokens.size(1)\n",
    "    \n",
    "    print(f\"Total tokens to process: {total_tokens}\")\n",
    "    \n",
    "    # Initialize progress bar\n",
    "    pbar = tqdm(total=total_tokens, desc=\"Processing tokens\")\n",
    "    processed_tokens = 0\n",
    "    \n",
    "    # Process each prompt\n",
    "    for prompt in prompts:\n",
    "        # Tokenize the prompt\n",
    "        tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        seq_len = tokens.size(1)\n",
    "        \n",
    "        # Get MoE routing metadata\n",
    "        moe_metadata = get_moe_metadata(model, tokens)\n",
    "        \n",
    "        if moe_metadata['expert_indices'] is None:\n",
    "            print(\"No MoE layers detected or no routing information available\")\n",
    "            processed_tokens += seq_len\n",
    "            pbar.update(seq_len)\n",
    "            continue\n",
    "        \n",
    "        # Extract expert indices\n",
    "        expert_indices = moe_metadata['expert_indices']  # shape: [num_layers, seq_len, top_k]\n",
    "        num_moe_layers = expert_indices.size(0)\n",
    "        \n",
    "        # Initialize counter for this batch if needed\n",
    "        for layer_idx in range(num_moe_layers):\n",
    "            layer_num = layer_idx + 1  # 1-based layer indexing\n",
    "            if layer_num not in expert_counts:\n",
    "                expert_counts[layer_num] = {}\n",
    "        \n",
    "        # Count token routing for each layer\n",
    "        for layer_idx in range(num_moe_layers):\n",
    "            layer_num = layer_idx + 1  # 1-based layer indexing\n",
    "            \n",
    "            # Process each token in sequence\n",
    "            for token_idx in range(seq_len):\n",
    "                # Get experts selected for this token in this layer\n",
    "                selected_experts = expert_indices[layer_idx, token_idx].cpu().numpy().tolist()\n",
    "                \n",
    "                # Count each expert\n",
    "                for expert_id in selected_experts:\n",
    "                    if expert_id not in expert_counts[layer_num]:\n",
    "                        expert_counts[layer_num][expert_id] = 0\n",
    "                    expert_counts[layer_num][expert_id] += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        processed_tokens += seq_len\n",
    "        pbar.update(seq_len)\n",
    "    \n",
    "    # Close progress bar\n",
    "    pbar.close()\n",
    "    \n",
    "    # Convert counts to a simple tensor format for saving\n",
    "    expert_token_counts = {}\n",
    "    for layer_num in sorted(expert_counts.keys()):\n",
    "        layer_data = expert_counts[layer_num]\n",
    "        # Create a tensor with counts for each expert (assuming 64 experts)\n",
    "        counts = torch.zeros(64)\n",
    "        for expert_id, count in layer_data.items():\n",
    "            counts[expert_id] = count\n",
    "        expert_token_counts[layer_num] = counts\n",
    "    \n",
    "    # Save just the token counts per expert\n",
    "    torch.save(expert_token_counts, output_path)\n",
    "    print(f\"Expert token counts saved to {output_path}\")\n",
    "    \n",
    "    # Create a DataFrame for visualization purposes\n",
    "    rows = []\n",
    "    for layer_num in sorted(expert_counts.keys()):\n",
    "        layer_data = expert_counts[layer_num]\n",
    "        for expert_id in range(64):  # Assuming 64 experts\n",
    "            count = layer_data.get(expert_id, 0)\n",
    "            rows.append({\n",
    "                'layer': layer_num,\n",
    "                'expert_id': expert_id,\n",
    "                'token_count': count\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "def visualize_expert_counts(df, file_base, output_dir=None):\n",
    "    \"\"\"\n",
    "    Visualize the token counts per expert for each layer using Plotly.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with expert count data\n",
    "        file_base: Base name for plot titles\n",
    "        output_dir: Directory to save visualizations\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import plotly.graph_objects as go\n",
    "    \n",
    "    # Set default output directory\n",
    "    if output_dir is None:\n",
    "        output_dir = \".\"\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Extract unique layers\n",
    "    layers = sorted(df['layer'].unique())\n",
    "    \n",
    "    # Create a plot for each layer\n",
    "    for layer in layers:\n",
    "        layer_df = df[df['layer'] == layer]\n",
    "        \n",
    "        # Sort by expert_id for consistent visualization\n",
    "        layer_df = layer_df.sort_values('expert_id')\n",
    "        \n",
    "        # Create plotly figure\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Add bar trace\n",
    "        fig.add_trace(go.Bar(\n",
    "            x=layer_df['expert_id'],\n",
    "            y=layer_df['token_count'],\n",
    "            marker_color='steelblue'\n",
    "        ))\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f'{file_base} - Layer {layer} Expert Usage',\n",
    "            xaxis_title='Expert ID',\n",
    "            yaxis_title='Token Count',\n",
    "            yaxis_gridcolor='rgba(0,0,0,0.1)',\n",
    "            width=1000,\n",
    "            height=600\n",
    "        )\n",
    "        \n",
    "        # Save the plot\n",
    "        # fig.write_image(f\"{output_dir}/{file_base}_layer{layer}_expert_usage.png\")\n",
    "    \n",
    "    print(f\"Expert usage visualizations saved to {output_dir}\")\n",
    "\n",
    "def analyze_text_file_routing(model, tokenizer, file_path):\n",
    "    \"\"\"\n",
    "    Main function to analyze MoE routing for a text file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to text file with prompts (one per line)\n",
    "        model_name: Name of DeepSeek MoE model to use\n",
    "    \"\"\"\n",
    "    # Process the file for expert counts and save as PyTorch file\n",
    "    df = process_text_file_for_expert_counts(file_path, model, tokenizer)\n",
    "    \n",
    "    # Visualize the results\n",
    "    file_base = os.path.basename(file_path).replace('.txt', '')\n",
    "    visualize_expert_counts(df, file_base)\n",
    "    \n",
    "    print(f\"Analysis completed for {file_path}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data-ext/test.txt\"\n",
    "df = analyze_text_file_routing(model, tokenizer, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wk/sgrd2bsj1msgt6bs3kfgj9hw0000gn/T/ipykernel_24289/4036518352.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(\"data-ext/gsm8k_expert_data.pt\")\n"
     ]
    }
   ],
   "source": [
    "data = torch.load(\"data-ext/gsm8k_expert_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([28550.,  8051., 20614.,  6439.,  8464., 15529.,  8803., 17602.,  7979.,\n",
       "         9239.,  9358.,  6246.,  6327.,  9828., 10541.,  6800., 16189.,  6640.,\n",
       "        11420.,  5676.,  8199.,  9876., 11074.,  4400.,  4386.,  6602., 24165.,\n",
       "         6705.,  6997.,  8640.,  8214.,  4198.,  4410., 11428.,  5912.,  8226.,\n",
       "         7188.,  6626.,  6126.,  5993., 10989.,  5436.,  6459., 11866., 19479.,\n",
       "        11095., 19071.,  9194.,  6852.,  8344.,  5369.,  5105.,  6272.,  9522.,\n",
       "         4824.,  2773.,  6093.,  6270.,  6276.,  6767., 13202.,  9643.,  5591.,\n",
       "         7132.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14720.,  7030.,  8374.,  6724., 17667.,  5873.,  5287.,  6760.,  5480.,\n",
       "         9770.,  4613.,  8404., 10488.,  6823., 10313., 20137.,  5645., 11405.,\n",
       "         6030.,  3382., 30181.,  3400.,  6523.,  8812.,  3522.,  9980.,  4335.,\n",
       "        19292.,  5431.,  5154.,  5922.,  6534.,  7084.,  4476.,  5080.,  5149.,\n",
       "         4994.,  7212.,  5670., 21702.,  9197.,  8657.,  6235.,  3531.,  5372.,\n",
       "         7736., 16348., 12341.,  8304.,  5506., 11602., 27963.,  5269., 19684.,\n",
       "         6344.,  4766., 11658.,  5978.,  6281.,  8249., 15812., 13785., 12228.,\n",
       "         5060.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_graph_all_tokens_paper(expert_data, layer_number, tokenizer, domain=None):\n",
    "    \"\"\"\n",
    "    Visualizes expert distribution for all tokens in a file for a specific layer.\n",
    "    \n",
    "    Args:\n",
    "        expert_data: Dictionary with layer numbers as keys and tensor of expert counts as values\n",
    "                     or path to PyTorch file with this data\n",
    "        layer_number: Layer to analyze (1-27)\n",
    "        domain: Optional domain name for title (e.g., 'GSM8K', 'Math', etc.)\n",
    "    \n",
    "    Returns:\n",
    "        fig: Plotly figure object\n",
    "    \"\"\"\n",
    "    # Load data if a file path is provided\n",
    "    if isinstance(expert_data, str):\n",
    "        # Get the corresponding text file path before loading the data\n",
    "        txt_file_path = expert_data.replace(\"_expert_data.pt\", \".txt\")\n",
    "        \n",
    "        # Now load the data\n",
    "        expert_data = torch.load(expert_data)\n",
    "        \n",
    "        # Count the number of tokens in the text file using the tokenizer\n",
    "        if os.path.exists(txt_file_path):\n",
    "            with open(txt_file_path, 'r', encoding='utf-8') as f:\n",
    "                text_content = f.read()\n",
    "                # Use tokenizer to count tokens\n",
    "                tokens = tokenizer(text_content, return_tensors=\"pt\")\n",
    "                num_tokens = tokens.input_ids.numel()\n",
    "                print(f\"Total tokens in {txt_file_path}: {num_tokens}\")\n",
    "        else:\n",
    "            print(f\"Text file {txt_file_path} not found\")\n",
    "            num_tokens = None\n",
    "    else:\n",
    "        # If expert_data is already loaded (not a string path)\n",
    "        num_tokens = None\n",
    "    \n",
    "    # Validate layer number is in the data\n",
    "    if layer_number not in expert_data:\n",
    "        raise ValueError(f\"Layer {layer_number} not found in expert data\")\n",
    "    \n",
    "    # Get counts for the specified layer\n",
    "    expert_counts = expert_data[layer_number].numpy()\n",
    "    \n",
    "    # Compute percentages\n",
    "    total_tokens = num_tokens\n",
    "    if total_tokens == 0:\n",
    "        print(\"No tokens found for this layer\")\n",
    "        return\n",
    "    \n",
    "    percentages = (expert_counts / total_tokens) * 100\n",
    "    \n",
    "    # Set discrete opacity based on 9.375% threshold\n",
    "    # 9.375% is 6 times the expected uniform distribution (1/64 = 1.5625%)\n",
    "    threshold = 9.375\n",
    "    opacities = np.where(percentages >= threshold, 1.0, 0.3)\n",
    "    \n",
    "    # Create plotly figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add bar trace with discrete color and opacity\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=list(range(64)),\n",
    "        y=percentages,\n",
    "        marker=dict(\n",
    "            color='#636EFA',  # Blue color for all bars\n",
    "            opacity=opacities\n",
    "        ),\n",
    "        hovertemplate='Expert ID: %{x}<br>Tokens: %{text}<br>Percentage: %{y:.2f}%<extra></extra>',\n",
    "        text=[f\"{int(count)}\" for count in expert_counts],\n",
    "        textposition='none'  # Ensure no text is displayed on the bars\n",
    "    ))\n",
    "    \n",
    "    # Add horizontal line at threshold\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=-0.5,\n",
    "        x1=63.5,\n",
    "        y0=threshold,\n",
    "        y1=threshold,\n",
    "        line=dict(\n",
    "            color=\"red\",\n",
    "            width=2,\n",
    "            dash=\"dash\",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add annotation for the threshold line\n",
    "    fig.add_annotation(\n",
    "        x=63,\n",
    "        y=threshold,\n",
    "        text=f\"{threshold}% threshold\",\n",
    "        showarrow=False,\n",
    "        yshift=10,\n",
    "        font=dict(color=\"red\")\n",
    "    )\n",
    "    \n",
    "    routed_tokens = expert_counts.sum()\n",
    "    \n",
    "    # Domain label for title\n",
    "    domain_label = f\" - {domain}\" if domain else \"\"\n",
    "    token_info = f\" (No. of Tokens: {num_tokens}, Routed Tokens: {int(routed_tokens)})\" if num_tokens else f\" (Routed Tokens: {int(routed_tokens)})\"\n",
    "    \n",
    "    # Calculate y-axis max based on the max percentage or default to 100 if num_tokens is None\n",
    "    y_max = 100\n",
    "    \n",
    "    # Update layout with white background\n",
    "    fig.update_layout(\n",
    "        title=f'Expert Usage Distribution - Layer {layer_number}{domain_label}{token_info}',\n",
    "        xaxis_title='Expert ID',\n",
    "        yaxis_title='Usage Percentage (%)',\n",
    "        yaxis_range=[0, y_max],  # Dynamic y-axis based on data\n",
    "        xaxis=dict(tickmode='linear', tick0=0, dtick=4),\n",
    "        showlegend=False,\n",
    "        width=1000,\n",
    "        height=600,\n",
    "        plot_bgcolor='white',\n",
    "        paper_bgcolor='white'\n",
    "    )\n",
    "    \n",
    "    # Add gridlines with lighter color\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='rgba(128, 128, 128, 0.1)')\n",
    "    fig.update_xaxes(showgrid=False)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wk/sgrd2bsj1msgt6bs3kfgj9hw0000gn/T/ipykernel_25852/2687242160.py:20: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in data-ext/arxiv_title_abstract.txt: 103197\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Expert ID: %{x}<br>Tokens: %{text}<br>Percentage: %{y:.2f}%<extra></extra>",
         "marker": {
          "color": "#636EFA",
          "opacity": [
           0.3,
           0.3,
           0.3,
           0.3,
           0.3,
           1,
           1,
           1,
           0.3,
           1,
           0.3,
           0.3,
           0.3,
           0.3,
           1,
           0.3,
           0.3,
           0.3,
           1,
           0.3,
           1,
           0.3,
           0.3,
           1,
           1,
           1,
           0.3,
           0.3,
           1,
           0.3,
           0.3,
           1,
           1,
           0.3,
           0.3,
           1,
           1,
           0.3,
           1,
           0.3,
           0.3,
           1,
           0.3,
           1,
           0.3,
           0.3,
           0.3,
           0.3,
           0.3,
           1,
           0.3,
           0.3,
           1,
           0.3,
           1,
           1,
           1,
           0.3,
           0.3,
           0.3,
           1,
           0.3,
           1,
           0.3
          ]
         },
         "text": [
          "7126",
          "6877",
          "8066",
          "4339",
          "4854",
          "13391",
          "11882",
          "16531",
          "5285",
          "14186",
          "5593",
          "831",
          "6134",
          "8810",
          "10087",
          "7034",
          "8726",
          "5934",
          "10359",
          "4962",
          "27258",
          "2844",
          "8413",
          "18077",
          "11545",
          "9844",
          "3905",
          "3430",
          "10229",
          "8906",
          "6656",
          "12570",
          "11157",
          "5953",
          "8510",
          "16566",
          "16271",
          "9312",
          "18433",
          "5040",
          "5322",
          "11906",
          "8960",
          "10728",
          "1761",
          "6632",
          "4171",
          "4268",
          "6151",
          "10311",
          "9546",
          "1788",
          "23040",
          "5540",
          "10532",
          "18223",
          "35685",
          "7122",
          "8415",
          "9578",
          "13325",
          "7357",
          "18312",
          "1577"
         ],
         "textposition": "none",
         "type": "bar",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63
         ],
         "y": [
          6.905239493396126,
          6.663953409498339,
          7.816118685620705,
          4.204579590491972,
          4.70362510538097,
          12.976152407531227,
          11.513900597885597,
          16.018876517728227,
          5.121272905220113,
          13.74652363925308,
          5.419731193736252,
          0.8052559667432193,
          5.943971239474015,
          8.53706987606229,
          9.774508948903552,
          6.816089615008187,
          8.455672161012433,
          5.750167156021977,
          10.038082502398325,
          4.808279310445071,
          26.413558533678305,
          2.755894066687985,
          8.152368770409993,
          17.516982082812486,
          11.187340717268913,
          9.539036987509327,
          3.7840247294010485,
          3.3237400312024574,
          9.9121098481545,
          8.630095836119267,
          6.449799897283835,
          12.18058664496061,
          10.811360795371959,
          5.768578543949921,
          8.24636375088423,
          16.052792232332337,
          15.766931209240578,
          9.023518125526905,
          17.861953351357112,
          4.883862902991367,
          5.15712666065874,
          11.537157087899843,
          8.682422938651317,
          10.395651036367335,
          1.7064449547951974,
          6.426543407269591,
          4.041784160392259,
          4.135779140866498,
          5.960444586567439,
          9.991569522369836,
          9.25026890316579,
          1.7326085060612229,
          22.326230413674818,
          5.368373111621462,
          10.205723034584338,
          17.658459063732472,
          34.57949358992994,
          6.901363411727085,
          8.154306811244513,
          9.281277556518114,
          12.912197059992053,
          7.1290832097832295,
          17.74470188086863,
          1.5281451980193224
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "color": "red"
          },
          "showarrow": false,
          "text": "9.375% threshold",
          "x": 63,
          "y": 9.375,
          "yshift": 10
         }
        ],
        "height": 600,
        "paper_bgcolor": "white",
        "plot_bgcolor": "white",
        "shapes": [
         {
          "line": {
           "color": "red",
           "dash": "dash",
           "width": 2
          },
          "type": "line",
          "x0": -0.5,
          "x1": 63.5,
          "y0": 9.375,
          "y1": 9.375
         }
        ],
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Expert Usage Distribution - Layer 12 (No. of Tokens: 103197, Routed Tokens: 616176)"
        },
        "width": 1000,
        "xaxis": {
         "dtick": 4,
         "showgrid": false,
         "tick0": 0,
         "tickmode": "linear",
         "title": {
          "text": "Expert ID"
         }
        },
        "yaxis": {
         "gridcolor": "rgba(128, 128, 128, 0.1)",
         "gridwidth": 1,
         "range": [
          0,
          100
         ],
         "showgrid": true,
         "title": {
          "text": "Usage Percentage (%)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = bar_graph_all_tokens_paper(\"data-ext/arxiv_title_abstract_expert_data.pt\", layer_number=12, tokenizer=tokenizer)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
