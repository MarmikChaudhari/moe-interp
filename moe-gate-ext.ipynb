{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if DEVICE.type == \"cuda\":\n",
    "    # Print CUDA details\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated()/1024**2:.2f}MB\")\n",
    "    print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved()/1024**2:.2f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87afa376e2c94230a2b3ece5af7ccb4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_model(model_name):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        # use_flash_attention_2=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model.to(DEVICE)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model(\"deepseek-ai/deepseek-moe-16b-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepseekForCausalLM(\n",
       "  (model): DeepseekModel(\n",
       "    (embed_tokens): Embedding(102400, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0): DeepseekDecoderLayer(\n",
       "        (self_attn): DeepseekSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): DeepseekRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): DeepseekMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=10944, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=10944, bias=False)\n",
       "          (down_proj): Linear(in_features=10944, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): DeepseekRMSNorm()\n",
       "        (post_attention_layernorm): DeepseekRMSNorm()\n",
       "      )\n",
       "      (1-27): 27 x DeepseekDecoderLayer(\n",
       "        (self_attn): DeepseekSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): DeepseekRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): DeepseekMoE(\n",
       "          (experts): ModuleList(\n",
       "            (0-63): 64 x DeepseekMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
       "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (gate): MoEGate()\n",
       "          (shared_experts): DeepseekMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=2816, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=2816, bias=False)\n",
       "            (down_proj): Linear(in_features=2816, out_features=2048, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): DeepseekRMSNorm()\n",
       "        (post_attention_layernorm): DeepseekRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): DeepseekRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=102400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moe_metadata(model, input_ids):\n",
    "    \"\"\"Get both router logits and expert indices for all MoE layers.\"\"\"\n",
    "    router_logits_list = []\n",
    "    expert_indices_list = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        # output contains: (topk_idx, topk_weight, aux_loss)\n",
    "        hidden_states = input[0]\n",
    "        \n",
    "        logits = torch.matmul(hidden_states, module.weight.T)\n",
    "        router_logits_list.append(logits.detach())\n",
    "        \n",
    "        # store expert indices actually used for routing\n",
    "        expert_indices_list.append(output[0].detach())\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    hooks = []\n",
    "    for layer_idx, layer in enumerate(model.model.layers):\n",
    "        if layer.mlp.__class__.__name__ == 'DeepseekMoE':\n",
    "            hook = layer.mlp.gate.register_forward_hook(hook_fn)\n",
    "            hooks.append(hook)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model(input_ids)\n",
    "    \n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    moe_metadata = {\n",
    "        'router_logits': torch.stack(router_logits_list) if router_logits_list else None,\n",
    "        'expert_indices': torch.stack(expert_indices_list) if expert_indices_list else None\n",
    "    }\n",
    "    \n",
    "    if moe_metadata['router_logits'] is not None:\n",
    "        print(f\"Router logits shape: {moe_metadata['router_logits'].shape}\")\n",
    "    if moe_metadata['expert_indices'] is not None:\n",
    "        print(f\"Expert indices shape: {moe_metadata['expert_indices'].shape}\")\n",
    "    \n",
    "    return moe_metadata\n",
    "\n",
    "def prepare_prompt(prompt, tokenizer, max_tokens=2048):\n",
    "    \"\"\"\n",
    "    Prepare a prompt for processing, splitting if necessary to fit within model context.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The text prompt to prepare\n",
    "        tokenizer: The model's tokenizer\n",
    "        max_tokens: Maximum number of tokens per chunk (default: 2048)\n",
    "        \n",
    "    Returns:\n",
    "        List of prompts that fit within token limit\n",
    "    \"\"\"\n",
    "    # Tokenize the prompt to get token count\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    \n",
    "    # If prompt is small enough, return as is\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return [prompt]\n",
    "    \n",
    "    # Split into manageable chunks\n",
    "    prepared_prompts = []\n",
    "    \n",
    "    # Decode tokens into chunks\n",
    "    start_idx = 0\n",
    "    while start_idx < len(tokens):\n",
    "        end_idx = min(start_idx + max_tokens, len(tokens))\n",
    "        chunk_tokens = tokens[start_idx:end_idx]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens)\n",
    "        prepared_prompts.append(chunk_text)\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    print(f\"Long prompt detected! Split into {len(prepared_prompts)} chunks.\")\n",
    "    return prepared_prompts\n",
    "\n",
    "def process_text_file_for_expert_counts(file_path, model, tokenizer, output_path=None, max_tokens=4096):\n",
    "    \"\"\"\n",
    "    Process a text file to analyze MoE routing and count tokens per expert in each layer.\n",
    "    Saves a PyTorch file with expert token counts.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to text file with prompts (one per line)\n",
    "        model: DeepSeek MoE model\n",
    "        tokenizer: DeepSeek tokenizer\n",
    "        output_path: Path to save PyTorch results (default: based on input filename)\n",
    "        max_tokens: Maximum tokens per prompt chunk\n",
    "    \"\"\"\n",
    "    # Load the text file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        raw_prompts = [line.strip() for line in f.readlines() if line.strip()]\n",
    "    \n",
    "    print(f\"Loaded {len(raw_prompts)} raw prompts from {file_path}\")\n",
    "    \n",
    "    # Prepare prompts (handle large prompts by splitting)\n",
    "    prompts = []\n",
    "    for raw_prompt in raw_prompts:\n",
    "        prepared_chunks = prepare_prompt(raw_prompt, tokenizer, max_tokens)\n",
    "        prompts.extend(prepared_chunks)\n",
    "    \n",
    "    print(f\"Processing {len(prompts)} prepared prompts (after splitting large ones)\")\n",
    "    \n",
    "    # Set default output path if not provided\n",
    "    if output_path is None:\n",
    "        output_path = file_path.replace('.txt', '_expert_data.pt')\n",
    "    \n",
    "    # Initialize counter for expert usage\n",
    "    # Structure: {layer_num: {expert_id: count}}\n",
    "    expert_counts = {}\n",
    "    \n",
    "    # Calculate total tokens for progress bar\n",
    "    total_tokens = 0\n",
    "    for prompt in prompts:\n",
    "        tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        total_tokens += tokens.size(1)\n",
    "    \n",
    "    print(f\"Total tokens to process: {total_tokens}\")\n",
    "    \n",
    "    # Initialize progress bar\n",
    "    pbar = tqdm(total=total_tokens, desc=\"Processing tokens\")\n",
    "    processed_tokens = 0\n",
    "    \n",
    "    # Process each prompt\n",
    "    for prompt in prompts:\n",
    "        # Tokenize the prompt\n",
    "        tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        seq_len = tokens.size(1)\n",
    "        \n",
    "        # Get MoE routing metadata\n",
    "        moe_metadata = get_moe_metadata(model, tokens)\n",
    "        \n",
    "        if moe_metadata['expert_indices'] is None:\n",
    "            print(\"No MoE layers detected or no routing information available\")\n",
    "            processed_tokens += seq_len\n",
    "            pbar.update(seq_len)\n",
    "            continue\n",
    "        \n",
    "        # Extract expert indices\n",
    "        expert_indices = moe_metadata['expert_indices']  # shape: [num_layers, seq_len, top_k]\n",
    "        num_moe_layers = expert_indices.size(0)\n",
    "        \n",
    "        # Initialize counter for this batch if needed\n",
    "        for layer_idx in range(num_moe_layers):\n",
    "            layer_num = layer_idx + 1  # 1-based layer indexing\n",
    "            if layer_num not in expert_counts:\n",
    "                expert_counts[layer_num] = {}\n",
    "        \n",
    "        # Count token routing for each layer\n",
    "        for layer_idx in range(num_moe_layers):\n",
    "            layer_num = layer_idx + 1  # 1-based layer indexing\n",
    "            \n",
    "            # Process each token in sequence\n",
    "            for token_idx in range(seq_len):\n",
    "                # Get experts selected for this token in this layer\n",
    "                selected_experts = expert_indices[layer_idx, token_idx].cpu().numpy().tolist()\n",
    "                \n",
    "                # Count each expert\n",
    "                for expert_id in selected_experts:\n",
    "                    if expert_id not in expert_counts[layer_num]:\n",
    "                        expert_counts[layer_num][expert_id] = 0\n",
    "                    expert_counts[layer_num][expert_id] += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        processed_tokens += seq_len\n",
    "        pbar.update(seq_len)\n",
    "    \n",
    "    # Close progress bar\n",
    "    pbar.close()\n",
    "    \n",
    "    # Convert counts to a simple tensor format for saving\n",
    "    expert_token_counts = {}\n",
    "    for layer_num in sorted(expert_counts.keys()):\n",
    "        layer_data = expert_counts[layer_num]\n",
    "        # Create a tensor with counts for each expert (assuming 64 experts)\n",
    "        counts = torch.zeros(64)\n",
    "        for expert_id, count in layer_data.items():\n",
    "            counts[expert_id] = count\n",
    "        expert_token_counts[layer_num] = counts\n",
    "    \n",
    "    # Save just the token counts per expert\n",
    "    torch.save(expert_token_counts, output_path)\n",
    "    print(f\"Expert token counts saved to {output_path}\")\n",
    "    \n",
    "    # Create a DataFrame for visualization purposes\n",
    "    rows = []\n",
    "    for layer_num in sorted(expert_counts.keys()):\n",
    "        layer_data = expert_counts[layer_num]\n",
    "        for expert_id in range(64):  # Assuming 64 experts\n",
    "            count = layer_data.get(expert_id, 0)\n",
    "            rows.append({\n",
    "                'layer': layer_num,\n",
    "                'expert_id': expert_id,\n",
    "                'token_count': count\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "def visualize_expert_counts(df, file_base, output_dir=None):\n",
    "    \"\"\"\n",
    "    Visualize the token counts per expert for each layer using Plotly.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with expert count data\n",
    "        file_base: Base name for plot titles\n",
    "        output_dir: Directory to save visualizations\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import plotly.graph_objects as go\n",
    "    \n",
    "    # Set default output directory\n",
    "    if output_dir is None:\n",
    "        output_dir = \".\"\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Extract unique layers\n",
    "    layers = sorted(df['layer'].unique())\n",
    "    \n",
    "    # Create a plot for each layer\n",
    "    for layer in layers:\n",
    "        layer_df = df[df['layer'] == layer]\n",
    "        \n",
    "        # Sort by expert_id for consistent visualization\n",
    "        layer_df = layer_df.sort_values('expert_id')\n",
    "        \n",
    "        # Create plotly figure\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Add bar trace\n",
    "        fig.add_trace(go.Bar(\n",
    "            x=layer_df['expert_id'],\n",
    "            y=layer_df['token_count'],\n",
    "            marker_color='steelblue'\n",
    "        ))\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f'{file_base} - Layer {layer} Expert Usage',\n",
    "            xaxis_title='Expert ID',\n",
    "            yaxis_title='Token Count',\n",
    "            yaxis_gridcolor='rgba(0,0,0,0.1)',\n",
    "            width=1000,\n",
    "            height=600\n",
    "        )\n",
    "        \n",
    "        # Save the plot\n",
    "        # fig.write_image(f\"{output_dir}/{file_base}_layer{layer}_expert_usage.png\")\n",
    "    \n",
    "    print(f\"Expert usage visualizations saved to {output_dir}\")\n",
    "\n",
    "def analyze_text_file_routing(model, tokenizer, file_path):\n",
    "    \"\"\"\n",
    "    Main function to analyze MoE routing for a text file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to text file with prompts (one per line)\n",
    "        model_name: Name of DeepSeek MoE model to use\n",
    "    \"\"\"\n",
    "    # Process the file for expert counts and save as PyTorch file\n",
    "    df = process_text_file_for_expert_counts(file_path, model, tokenizer)\n",
    "    \n",
    "    # Visualize the results\n",
    "    file_base = os.path.basename(file_path).replace('.txt', '')\n",
    "    visualize_expert_counts(df, file_base)\n",
    "    \n",
    "    print(f\"Analysis completed for {file_path}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 raw prompts from data-ext/test.txt\n",
      "Processing 6 prepared prompts (after splitting large ones)\n",
      "Total tokens to process: 517\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cce45517a4647cf8aa2b5c24f290ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing tokens:   0%|          | 0/517 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Router logits shape: torch.Size([27, 1, 68, 64])\n",
      "Expert indices shape: torch.Size([27, 68, 6])\n",
      "Router logits shape: torch.Size([27, 1, 67, 64])\n",
      "Expert indices shape: torch.Size([27, 67, 6])\n",
      "Router logits shape: torch.Size([27, 1, 45, 64])\n",
      "Expert indices shape: torch.Size([27, 45, 6])\n",
      "Router logits shape: torch.Size([27, 1, 254, 64])\n",
      "Expert indices shape: torch.Size([27, 254, 6])\n",
      "Router logits shape: torch.Size([27, 1, 50, 64])\n",
      "Expert indices shape: torch.Size([27, 50, 6])\n",
      "Router logits shape: torch.Size([27, 1, 33, 64])\n",
      "Expert indices shape: torch.Size([27, 33, 6])\n",
      "Expert token counts saved to data-ext/test_expert_data.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert usage visualizations saved to .\n",
      "Analysis completed for data-ext/test.txt\n"
     ]
    }
   ],
   "source": [
    "file_path = \"data-ext/test.txt\"\n",
    "df = analyze_text_file_routing(model, tokenizer, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wk/sgrd2bsj1msgt6bs3kfgj9hw0000gn/T/ipykernel_16425/4266300188.py:2: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = torch.load(\"data-ext/test_expert_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 68.,  58.,  62.,  24.,  58.,  98.,  51., 130.,  67.,  55.,  30.,  21.,\n",
       "         37.,  55., 106.,  26., 171.,  27.,  60.,  20.,  66.,  40.,  51.,  46.,\n",
       "        100.,  29.,  48.,  39.,  51.,  25.,  36.,  13.,  19.,  45.,  57.,  24.,\n",
       "         20.,  41.,  17.,  26.,  74.,  16.,  46.,  76.,  45.,  47., 104.,  21.,\n",
       "         52., 115.,  22.,  56.,  33.,  43.,   9.,  13.,  27.,  44.,  27.,  42.,\n",
       "         64.,  19.,  21.,  69.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([146.,  84.,  41.,  58.,  31.,  34.,  37.,  37.,  38., 151.,  23.,  48.,\n",
       "         25.,  72.,  35.,  16.,  55.,  50.,  15.,   6.,  10.,  23.,  31.,  42.,\n",
       "         18.,  34.,  25.,  42.,  78.,  23., 130.,  21.,  63.,  57.,  28.,  15.,\n",
       "         91.,  68.,  26.,  41.,  42.,  24.,  62.,  25.,  38.,  37.,  25.,  22.,\n",
       "         16.,  64.,  31.,  76.,  60.,  58., 117.,  39., 111.,  36.,  19.,  25.,\n",
       "         14.,  79.,  71., 143.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[27]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
