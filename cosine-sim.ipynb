{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "if DEVICE.type == \"cuda\":\n",
    "    # Print CUDA details\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated()/1024**2:.2f}MB\")\n",
    "    print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved()/1024**2:.2f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d09a83d4adb41859c2513df2aae2efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_model(model_name):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        # use_flash_attention_2=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model(\"deepseek-ai/deepseek-moe-16b-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_tokens(hidden_states: torch.Tensor, lm_head: torch.nn.Linear, tokenizer, k: int = 5) -> List[Tuple[str, float]]:\n",
    "    \"\"\" get topk tokens from hidden states using lm head \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Move tensors to global device\n",
    "        hidden_states = hidden_states.to(DEVICE)\n",
    "        lm_head = lm_head.to(DEVICE)\n",
    "        \n",
    "        # Ensure hidden_states has at least 2 dimensions (batch_size, num_tokens, hidden_dim)\n",
    "        if hidden_states.dim() == 2:\n",
    "            hidden_states = hidden_states.unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "        # Compute logits in chunks to avoid OOM\n",
    "        chunk_size = 512  # Adjust based on GPU memory\n",
    "        num_chunks = (hidden_states.size(1) + chunk_size - 1) // chunk_size\n",
    "        all_logits = []\n",
    "        \n",
    "        for i in range(num_chunks):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = min((i + 1) * chunk_size, hidden_states.size(1))\n",
    "            chunk = hidden_states[:, start_idx:end_idx]\n",
    "            chunk_logits = lm_head(chunk)\n",
    "            all_logits.append(chunk_logits)\n",
    "            \n",
    "        logits = torch.cat(all_logits, dim=1)  # (batch_size, num_tokens, vocab_size)\n",
    "    \n",
    "    # Get top-k tokens\n",
    "    scores, token_ids = torch.topk(logits, k=k, dim=-1)  # (batch_size, num_tokens, k)\n",
    "    \n",
    "    # Move to CPU for decoding\n",
    "    scores = scores.cpu()\n",
    "    token_ids = token_ids.cpu()\n",
    "    \n",
    "    # Decode tokens and collect results for each position\n",
    "    results = []\n",
    "    for pos in range(scores.size(1)):  # Iterate over token positions\n",
    "        pos_results = []\n",
    "        for i in range(k):\n",
    "            token = tokenizer.decode(token_ids[0, pos, i])  # Decode token for this position\n",
    "            score = scores[0, pos, i].item()  # Get score for this position\n",
    "            pos_results.append((token, score))\n",
    "        results.append(pos_results)\n",
    "    \n",
    "    # Clear CUDA cache if using GPU\n",
    "    if DEVICE.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepseekLayerAnalyzer:\n",
    "    \"\"\" Analyzes the behavior of a DeepSeek MoE model by capturing and analyzing outputs from different layers and experts.\n",
    "    \n",
    "    Args:\n",
    "        model: The DeepSeek MoE model to analyze\n",
    "        tokenizer: The tokenizer associated with the model\n",
    "        device: Device to run analysis on ('cuda' or 'cpu')\n",
    "        \n",
    "    Attributes:\n",
    "        layer_outputs (defaultdict): Stores outputs from each model layer\n",
    "        moe_gate_outputs (defaultdict): Stores gate outputs from MoE layers\n",
    "        moe_combined_outputs (defaultdict): Stores combined outputs after expert computation\n",
    "        expert_outputs (defaultdict): Stores individual expert outputs per layer and position\n",
    "        shared_expert_outputs (defaultdict): Stores outputs from shared experts if present\n",
    "        hooks (list): List of registered PyTorch hooks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = DEVICE\n",
    "        self.hooks = []\n",
    "        self.reset_state()\n",
    "        \n",
    "        # Check GPU memory if using CUDA\n",
    "        if device == 'cuda' and torch.cuda.is_available():\n",
    "            self.gpu_mem = get_device_properties(0).total_memory\n",
    "            print(f\"GPU Memory Available: {self.gpu_mem / 1e9:.2f} GB\")\n",
    "        \n",
    "    def reset_state(self):\n",
    "        \"\"\"Clear all stored state between runs and free GPU memory\"\"\"\n",
    "        self.layer_outputs = defaultdict(list)\n",
    "        self.moe_gate_outputs = defaultdict(list)\n",
    "        self.moe_combined_outputs = defaultdict(list)\n",
    "        self.expert_outputs = defaultdict(lambda: defaultdict(list))\n",
    "        self.shared_expert_outputs = defaultdict(list)\n",
    "        \n",
    "        # Clear CUDA cache if using GPU\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    def register_hooks(self):\n",
    "        \"\"\"Register hooks for layer outputs and MoE combination points\"\"\"\n",
    "        \n",
    "        def layer_output_hook(layer_idx):\n",
    "            def hook(module, inputs, outputs):\n",
    "                \"\"\"Hook for capturing layer outputs\"\"\"\n",
    "                hidden_states = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "                # Store on CPU to save GPU memory\n",
    "                self.layer_outputs[layer_idx].append(hidden_states.detach().cpu())\n",
    "            return hook\n",
    "\n",
    "        def moe_gate_hook(layer_idx):\n",
    "            def hook(module, inputs, outputs):\n",
    "                \"\"\"Hook for capturing MoE gate outputs before expert computation\"\"\"\n",
    "                # Capture topk_idx, topk_weight, and aux_loss from gate outputs\n",
    "                if isinstance(outputs, tuple):\n",
    "                    topk_idx, topk_weight, aux_loss = outputs\n",
    "                    self.moe_gate_outputs[layer_idx].append({\n",
    "                        'topk_idx': topk_idx.detach().cpu(),\n",
    "                        'topk_weight': topk_weight.detach().cpu(),\n",
    "                        'aux_loss': aux_loss.detach().cpu() if aux_loss is not None else None\n",
    "                    })\n",
    "            return hook\n",
    "\n",
    "        def expert_hook(layer_idx, expert_idx):\n",
    "            def hook(module, inputs, outputs):\n",
    "                \"\"\"Hook for capturing expert outputs\"\"\"\n",
    "                # Get the latest gate outputs for this layer\n",
    "                if not self.moe_gate_outputs[layer_idx]:\n",
    "                    return print(f'no gate outputs for layer {layer_idx}')\n",
    "            \n",
    "                gate_data = self.moe_gate_outputs[layer_idx][-1]\n",
    "                \n",
    "                # Handle 2D or 3D tensor shapes\n",
    "                if len(gate_data['topk_idx'].shape) == 2:\n",
    "                    batch_size = 1\n",
    "                    seq_len, top_k = gate_data['topk_idx'].shape\n",
    "                else:\n",
    "                    batch_size, seq_len, top_k = gate_data['topk_idx'].shape\n",
    "                \n",
    "                # Get mask for tokens where this expert was selected\n",
    "                expert_mask = (gate_data['topk_idx'] == expert_idx)                \n",
    "                # Flatten and find positions where this expert was selected\n",
    "                selected_positions = torch.nonzero(expert_mask, as_tuple=True)\n",
    "                # If no tokens selected this expert, skip\n",
    "                if selected_positions[0].numel() == 0:\n",
    "                    return\n",
    "                    \n",
    "                # Get the actual inputs routed to this expert\n",
    "                # Inputs[0] shape: (total_selected_tokens, hidden_dim)\n",
    "                total_selected = inputs[0].shape[0] \n",
    "                # Validate we're processing the correct number of tokens\n",
    "                expected_selected = expert_mask.sum().item()\n",
    "                if total_selected != expected_selected:\n",
    "                    print(f\" expert {expert_idx} processed {total_selected} tokens but expected {expected_selected}\")\n",
    "                    return\n",
    "                    \n",
    "                # Get the full hidden states from outputs\n",
    "                # outputs shape: (total_selected_tokens, hidden_dim)\n",
    "                hidden_states = outputs\n",
    "                if isinstance(outputs, tuple):\n",
    "                    hidden_states = outputs[0]\n",
    "                    \n",
    "                # Record data for each selected position\n",
    "                for pos_idx, pos in enumerate(selected_positions[0]):\n",
    "                    token_data = {\n",
    "                        'position': pos.item(),\n",
    "                        'input': inputs[0][pos_idx].detach().cpu(),\n",
    "                        'output': outputs[pos_idx].detach().cpu(),\n",
    "                        'hidden_state': hidden_states[pos_idx].detach().cpu()  # Store full hidden state\n",
    "                    }\n",
    "                    \n",
    "                    # Get the corresponding gate weight for this position\n",
    "                    # Find which expert slot (in top_k) this expert was selected for this position\n",
    "                    expert_slots = (gate_data['topk_idx'][pos.item()] == expert_idx).nonzero(as_tuple=True)[0]\n",
    "                    if len(expert_slots) > 0:\n",
    "                        token_data['gate_weight'] = gate_data['topk_weight'][pos.item()][expert_slots[0]].item()\n",
    "                    \n",
    "                    self.expert_outputs[layer_idx][expert_idx].append(token_data)\n",
    "            return hook\n",
    "        \n",
    "        def shared_expert_hook(layer_idx):\n",
    "            def hook(module, inputs, outputs):\n",
    "                \"\"\"Hook for capturing shared expert outputs\"\"\"\n",
    "                self.shared_expert_outputs[layer_idx].append({\n",
    "                    'input': inputs[0].detach().cpu(),\n",
    "                    'output': outputs.detach().cpu()\n",
    "                })\n",
    "            return hook\n",
    "\n",
    "        def moe_combine_hook(layer_idx):\n",
    "            def hook(module, inputs, outputs):\n",
    "                \"\"\"Hook for capturing final combined MoE outputs\"\"\"\n",
    "                # For DeepseekMoE, this captures the weighted sum of expert outputs\n",
    "                self.moe_combined_outputs[layer_idx].append({\n",
    "                    'combined_output': outputs.detach().cpu(),\n",
    "                    'input': inputs[0].detach().cpu()  # Original input before MoE\n",
    "                })\n",
    "            return hook\n",
    "\n",
    "        # Register hooks for each layer\n",
    "        for layer_idx, layer in enumerate(self.model.model.layers):\n",
    "            # Hook for layer output\n",
    "            hook = layer.register_forward_hook(layer_output_hook(layer_idx))\n",
    "            self.hooks.append(hook)\n",
    "            \n",
    "            # If it's an MoE layer, add MoE-specific hooks\n",
    "            if hasattr(layer.mlp, 'experts'):\n",
    "                # Hook for gate mechanism\n",
    "                gate_hook = layer.mlp.gate.register_forward_hook(moe_gate_hook(layer_idx))\n",
    "                self.hooks.append(gate_hook)\n",
    "                \n",
    "                # Hook for each expert\n",
    "                for expert_idx, expert in enumerate(layer.mlp.experts):\n",
    "                    expert_hook_fn = expert.register_forward_hook(expert_hook(layer_idx, expert_idx))\n",
    "                    self.hooks.append(expert_hook_fn)\n",
    "\n",
    "                # Hook for shared expert if it exists\n",
    "                if hasattr(layer.mlp, 'shared_experts'):\n",
    "                    shared_hook = layer.mlp.shared_experts.register_forward_hook(shared_expert_hook(layer_idx))\n",
    "                    self.hooks.append(shared_hook)\n",
    "                \n",
    "                # Hook for final combined output\n",
    "                combine_hook = layer.mlp.register_forward_hook(moe_combine_hook(layer_idx))\n",
    "                self.hooks.append(combine_hook)\n",
    "\n",
    "    def analyze_tokens(self, input_ids: torch.Tensor, return_hidden_states: bool = False) -> Dict:\n",
    "        \"\"\"Run inference and analyze tokens at each layer and expert combination point\"\"\"\n",
    "        # Clear all state before processing new input\n",
    "        self.reset_state()\n",
    "        \n",
    "        # Move input to device\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids)\n",
    "        \n",
    "        results = {\n",
    "            'layer_predictions': {},\n",
    "            'moe_analysis': {},\n",
    "            'hidden_states': {} if return_hidden_states else None\n",
    "        }\n",
    "        \n",
    "        # Analyze layer outputs\n",
    "        for layer_idx, outputs in self.layer_outputs.items():\n",
    "            if not outputs:  # Skip if no outputs captured\n",
    "                continue\n",
    "            hidden_states = outputs[-1].to(self.device)  # Move back to device for processing\n",
    "            \n",
    "            # Get token predictions for this layer\n",
    "            top_tokens = get_top_k_tokens(hidden_states, self.model.lm_head, self.tokenizer)\n",
    "            results['layer_predictions'][layer_idx] = top_tokens\n",
    "            \n",
    "            if return_hidden_states:\n",
    "                results['hidden_states'][f'layer_{layer_idx}'] = hidden_states.cpu()  # Store on CPU\n",
    "        \n",
    "        # Analyze MoE layers\n",
    "        for layer_idx in self.moe_gate_outputs.keys():\n",
    "            if not self.moe_gate_outputs[layer_idx]:\n",
    "                continue\n",
    "                \n",
    "            gate_data = self.moe_gate_outputs[layer_idx][-1]  # Get last captured data\n",
    "            combined_data = self.moe_combined_outputs[layer_idx][-1]\n",
    "            \n",
    "            # Initialize predictions dictionary by position\n",
    "            expert_predictions_by_pos = defaultdict(dict)\n",
    "            expert_hidden_states_by_pos = defaultdict(dict)\n",
    "            \n",
    "            # Process expert outputs by position\n",
    "            for expert_idx, data_list in self.expert_outputs[layer_idx].items():\n",
    "                for data in data_list:\n",
    "                    position = data['position']\n",
    "                    # Move data to device for prediction\n",
    "                    expert_output = data['output'].to(self.device).unsqueeze(0)\n",
    "                    # Get predictions for this expert's output at this position\n",
    "                    predictions = get_top_k_tokens(\n",
    "                        expert_output,\n",
    "                        self.model.lm_head,\n",
    "                        self.tokenizer\n",
    "                    )\n",
    "                    expert_predictions_by_pos[position][expert_idx] = predictions[0]  # [0] because we only have one prediction set\n",
    "\n",
    "                    # Store hidden states\n",
    "                    expert_hidden_states_by_pos[position][expert_idx] = {\n",
    "                        'hidden_state': data['hidden_state'].tolist(),\n",
    "                        'gate_weight': data.get('gate_weight', None)\n",
    "                    }\n",
    "            \n",
    "            # Get predictions for shared expert if it exists\n",
    "            if self.shared_expert_outputs[layer_idx]:\n",
    "                shared_output = self.shared_expert_outputs[layer_idx][-1]['output'].to(self.device)\n",
    "                shared_expert_predictions = get_top_k_tokens(\n",
    "                    shared_output,\n",
    "                    self.model.lm_head,\n",
    "                    self.tokenizer\n",
    "                )\n",
    "            \n",
    "            # Update experts_analysis dictionary to include hidden states\n",
    "            experts_analysis = {\n",
    "                'selected_experts': gate_data['topk_idx'].tolist(),\n",
    "                'expert_weights': gate_data['topk_weight'].tolist(),\n",
    "                'aux_loss': gate_data['aux_loss'].item() if gate_data['aux_loss'] is not None else None,\n",
    "                'expert_predictions_by_position': dict(expert_predictions_by_pos),\n",
    "                'expert_hidden_states_by_position': dict(expert_hidden_states_by_pos),\n",
    "                'shared_expert_predictions': shared_expert_predictions if self.shared_expert_outputs[layer_idx] else None\n",
    "            }\n",
    "            \n",
    "            # Get token predictions from combined output\n",
    "            combined_output = combined_data['combined_output'].to(self.device)\n",
    "            combined_tokens = get_top_k_tokens(\n",
    "                combined_output, \n",
    "                self.model.lm_head,\n",
    "                self.tokenizer\n",
    "            )\n",
    "            \n",
    "            experts_analysis['combined_output_tokens'] = combined_tokens\n",
    "            results['moe_analysis'][layer_idx] = experts_analysis\n",
    "\n",
    "        return results\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Remove all registered hooks and clear state\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks.clear()\n",
    "        self.reset_state()\n",
    "\n",
    "def analyze_deepseek_moe(model, tokenizer, input_text: str, return_hidden_states: bool = False, device='cpu'):\n",
    "    \"\"\"Analyze DeepSeek MoE model behavior for given input text\"\"\"\n",
    "    # Create a fresh analyzer instance for each analysis\n",
    "    analyzer = DeepseekLayerAnalyzer(model, tokenizer, device=device)\n",
    "    analyzer.register_hooks()\n",
    "    \n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    try:\n",
    "        results = analyzer.analyze_tokens(input_ids, return_hidden_states=return_hidden_states)\n",
    "        return results\n",
    "    finally:\n",
    "        analyzer.cleanup()  # Ensure hooks are removed and state is cleared\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prompt and tokenize\n",
    "prompt = \"quick brown fox\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "n = input_ids.shape[1]\n",
    "token_pos = n-1  # Last token\n",
    "\n",
    "analysis = analyze_deepseek_moe(model, tokenizer, prompt, return_hidden_states=True)\n",
    "\n",
    "# Create analyzer and get hidden states\n",
    "analyzer = DeepseekLayerAnalyzer(model, tokenizer)\n",
    "analyzer.register_hooks()\n",
    "\n",
    "\n",
    "results = analyzer.analyze_tokens(input_ids, return_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_attn_ln_inputs(model, tokenizer, text, device=DEVICE):\n",
    "    \"\"\"places a hook on the post attention layernorm to retrieve its inputs\"\"\"\n",
    "    # Determine device\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Store inputs in a dict mapping layer idx -> inputs\n",
    "    post_attn_ln_inputs = {}\n",
    "    hooks = []\n",
    "    \n",
    "    def hook_post_attn_ln(module, input, output, layer_idx):\n",
    "        if layer_idx not in post_attn_ln_inputs:\n",
    "            post_attn_ln_inputs[layer_idx] = []\n",
    "        # Detach and move to CPU to save memory if using CUDA\n",
    "        if device == 'cuda':\n",
    "            post_attn_ln_inputs[layer_idx].append([x.detach().cpu() for x in input])\n",
    "        else:\n",
    "            post_attn_ln_inputs[layer_idx].append([x.detach() for x in input])\n",
    "    \n",
    "    # Register hooks on post attention layernorm for each layer\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        hooks.append(\n",
    "            layer.post_attention_layernorm.register_forward_hook(\n",
    "                lambda m, i, o, idx=i: hook_post_attn_ln(m, i, o, idx)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        # Run inference\n",
    "        input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "        if device == 'cuda':\n",
    "            with torch.cuda.amp.autocast():  # Use mixed precision for CUDA\n",
    "                model(input_ids)\n",
    "            torch.cuda.empty_cache()  # Clear CUDA cache\n",
    "        else:\n",
    "            model(input_ids)\n",
    "        \n",
    "        return post_attn_ln_inputs\n",
    "        \n",
    "    finally:\n",
    "        # Clean up hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_attn_ln_inputs = get_post_attn_ln_inputs(model, tokenizer, text=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expert_hidden_states_by_weight(analysis, post_attn_ln_inputs, token_pos, k=0):\n",
    "    \"\"\"Gets hidden states (with residual added) from the k-th highest weighted expert for all MoE layers\"\"\"\n",
    "    hidden_states_by_layer = {}\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # For each layer that has MoE\n",
    "    for layer_idx in analysis['moe_analysis'].keys():\n",
    "        moe_analysis = analysis['moe_analysis'][layer_idx]\n",
    "        \n",
    "        # Get expert weights for this token\n",
    "        expert_weights = {}\n",
    "        selected_experts = moe_analysis['selected_experts'][token_pos]\n",
    "        expert_weights_list = moe_analysis['expert_weights'][token_pos]\n",
    "        \n",
    "        # Map experts to their weights\n",
    "        for expert_idx, weight in zip(selected_experts, expert_weights_list):\n",
    "            expert_weights[expert_idx] = weight\n",
    "            \n",
    "        # Sort by weight and get k-th highest\n",
    "        sorted_experts = sorted(expert_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "        if k >= len(sorted_experts):\n",
    "            print(f\"Warning: Layer {layer_idx} only has {len(sorted_experts)} experts, but k={k} requested\")\n",
    "            continue\n",
    "            \n",
    "        target_expert = sorted_experts[k][0]  # Get the k-th expert's id\n",
    "        \n",
    "        # Get hidden state for this expert\n",
    "        expert_hidden_state = moe_analysis['expert_hidden_states_by_position'][token_pos][target_expert]['hidden_state']\n",
    "        \n",
    "        # Get residual stream for this token\n",
    "        try:\n",
    "            residual = post_attn_ln_inputs[layer_idx][-1][0][0][token_pos]  # Added [0] index\n",
    "        except (KeyError, IndexError) as e:\n",
    "            print(f\"Warning: Could not get residual for layer {layer_idx}, skipping. Error: {e}\")\n",
    "            continue\n",
    "            \n",
    "        # Convert to tensor if needed and move to appropriate device\n",
    "        if isinstance(expert_hidden_state, list):\n",
    "            expert_hidden_state = torch.tensor(expert_hidden_state, dtype=torch.float16, device=device)\n",
    "        elif isinstance(expert_hidden_state, torch.Tensor):\n",
    "            expert_hidden_state = expert_hidden_state.to(device=device, dtype=torch.float16)\n",
    "            \n",
    "        # Move residual to same device and dtype\n",
    "        residual = residual.to(device=device, dtype=torch.float16)\n",
    "            \n",
    "        # Add residual using mixed precision if on CUDA\n",
    "        if device == 'cuda':\n",
    "            with torch.cuda.amp.autocast():\n",
    "                combined = residual + expert_hidden_state\n",
    "        else:\n",
    "            combined = residual + expert_hidden_state\n",
    "            \n",
    "        hidden_states_by_layer[layer_idx] = combined\n",
    "        \n",
    "    return hidden_states_by_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of final hidden state : tensor([-1.8672, -1.1270,  0.4009,  ..., -1.5059,  0.0342, -2.5078],\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# Get final layer index and hidden state\n",
    "final_layer_idx = max([int(k) for k in results[\"layer_predictions\"].keys()])\n",
    "final_hidden_state = analyzer.layer_outputs[final_layer_idx][-1][0][token_pos]\n",
    "\n",
    "print(\"Shape of final hidden state :\", final_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_states : {1: tensor([ 0.0326, -0.1321,  0.2737,  ..., -0.1681,  0.0611,  0.0240],\n",
      "       dtype=torch.float16), 2: tensor([-0.1166, -0.3586,  0.1572,  ..., -0.1925,  0.0962,  0.0130],\n",
      "       dtype=torch.float16), 3: tensor([-0.1203, -0.3545,  0.1521,  ..., -0.0521,  0.3518, -0.3347],\n",
      "       dtype=torch.float16), 4: tensor([-0.0322, -0.5200,  0.1499,  ..., -0.0738,  0.2411, -0.0245],\n",
      "       dtype=torch.float16), 5: tensor([-0.0688, -0.5557,  0.0140,  ..., -0.0708,  0.3848, -0.2874],\n",
      "       dtype=torch.float16), 6: tensor([-0.1906, -0.4470, -0.2214,  ..., -0.1390,  0.3452, -0.4048],\n",
      "       dtype=torch.float16), 7: tensor([-0.3516, -0.4438, -0.2330,  ..., -0.0925,  0.4446, -0.5254],\n",
      "       dtype=torch.float16), 8: tensor([-0.3779, -0.4673, -0.3933,  ...,  0.1520,  0.3298, -0.2668],\n",
      "       dtype=torch.float16), 9: tensor([-0.5723, -0.3511, -0.4016,  ...,  0.2032,  0.2272, -0.1050],\n",
      "       dtype=torch.float16), 10: tensor([-0.6016, -0.9678, -0.5117,  ...,  0.5195, -0.2944, -0.2246],\n",
      "       dtype=torch.float16), 11: tensor([-0.6494, -0.8921, -0.1545,  ...,  0.5913, -0.2708, -0.4153],\n",
      "       dtype=torch.float16), 12: tensor([-0.8945, -0.8770,  0.2937,  ...,  0.4543,  0.0727, -0.3394],\n",
      "       dtype=torch.float16), 13: tensor([-1.0361, -0.8433,  0.3567,  ...,  0.5161,  0.4148, -0.1565],\n",
      "       dtype=torch.float16), 14: tensor([-1.0498, -0.8232,  0.3818,  ...,  0.5796,  0.2993, -0.1779],\n",
      "       dtype=torch.float16), 15: tensor([-0.8555, -1.0049,  0.6929,  ...,  0.4048,  0.8613, -0.1631],\n",
      "       dtype=torch.float16), 16: tensor([-1.0811, -1.0879,  1.0557,  ...,  0.2429,  0.9570,  0.2197],\n",
      "       dtype=torch.float16), 17: tensor([-0.7461, -1.0293,  1.4336,  ..., -0.1051,  0.8501,  0.2389],\n",
      "       dtype=torch.float16), 18: tensor([-0.8604, -1.0146,  1.5107,  ..., -0.0657,  0.8730,  0.3257],\n",
      "       dtype=torch.float16), 19: tensor([-0.7988, -1.1611,  1.5811,  ...,  0.0265,  0.7397,  0.1335],\n",
      "       dtype=torch.float16), 20: tensor([-0.6489, -1.4785,  1.8252,  ...,  0.2109,  0.7827, -0.0169],\n",
      "       dtype=torch.float16), 21: tensor([-0.9248, -1.3555,  1.6387,  ...,  0.2310,  0.4561,  0.1229],\n",
      "       dtype=torch.float16), 22: tensor([-1.1846, -1.4824,  1.5996,  ...,  0.7402,  0.3604,  0.1964],\n",
      "       dtype=torch.float16), 23: tensor([-1.1396, -1.3184,  1.9102,  ...,  0.2876,  0.1285,  0.4775],\n",
      "       dtype=torch.float16), 24: tensor([-1.1094, -1.4824,  2.1973,  ...,  0.1986,  0.3853,  0.8594],\n",
      "       dtype=torch.float16), 25: tensor([-1.3955, -1.3262,  1.8281,  ..., -0.0820,  0.4097,  1.1289],\n",
      "       dtype=torch.float16), 26: tensor([-2.1211, -1.3867,  1.9209,  ..., -0.5591,  1.1006,  0.9141],\n",
      "       dtype=torch.float16), 27: tensor([-1.8223, -0.9453,  0.5063,  ..., -0.8398,  1.4990, -0.1547],\n",
      "       dtype=torch.float16)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n = len(tokenizer.encode(prompt))\n",
    "token_pos = n-1  # Last token\n",
    "k = 0  # Get highest weighted expert\n",
    "\n",
    "hidden_states = get_expert_hidden_states_by_weight(analysis,\n",
    "                                                   post_attn_ln_inputs=post_attn_ln_inputs,\n",
    "                                                   token_pos=token_pos,\n",
    "                                                   k=k)\n",
    "\n",
    "print(f'hidden_states : {hidden_states}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 similarity: 0.0709\n",
      "Layer 2 similarity: 0.0688\n",
      "Layer 3 similarity: 0.0408\n",
      "Layer 4 similarity: 0.0459\n",
      "Layer 5 similarity: 0.0466\n",
      "Layer 6 similarity: 0.0602\n",
      "Layer 7 similarity: 0.0601\n",
      "Layer 8 similarity: 0.0609\n",
      "Layer 9 similarity: 0.0849\n",
      "Layer 10 similarity: 0.1040\n",
      "Layer 11 similarity: 0.1093\n",
      "Layer 12 similarity: 0.1161\n",
      "Layer 13 similarity: 0.1243\n",
      "Layer 14 similarity: 0.1267\n",
      "Layer 15 similarity: 0.1420\n",
      "Layer 16 similarity: 0.1436\n",
      "Layer 17 similarity: 0.1578\n",
      "Layer 18 similarity: 0.1534\n",
      "Layer 19 similarity: 0.1505\n",
      "Layer 20 similarity: 0.1613\n",
      "Layer 21 similarity: 0.1775\n",
      "Layer 22 similarity: 0.1823\n",
      "Layer 23 similarity: 0.1899\n",
      "Layer 24 similarity: 0.2017\n",
      "Layer 25 similarity: 0.2255\n",
      "Layer 26 similarity: 0.3474\n",
      "Layer 27 similarity: 0.7798\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# cosine similarity for each layer\n",
    "sim = {}\n",
    "for layer_idx, layer_hidden in hidden_states.items():\n",
    "    # Normalize both vectors for cosine similarity\n",
    "    similarity = F.cosine_similarity(final_hidden_state.unsqueeze(0),\n",
    "                                   layer_hidden.unsqueeze(0))\n",
    "    sim[layer_idx] = similarity.item()\n",
    "\n",
    "# Print results sorted by layer\n",
    "for layer_idx in sorted(sim.keys()):\n",
    "    print(f\"Layer {layer_idx} similarity: {sim[layer_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "<b>Layer %{x}</b><br>Cosine Similarity: %{y:.4f}<br><extra></extra>",
         "line": {
          "width": 2
         },
         "marker": {
          "size": 8
         },
         "mode": "lines+markers",
         "name": "Expert rank 0",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27
         ],
         "y": [
          0.07086181640625,
          0.06878662109375,
          0.040802001953125,
          0.045867919921875,
          0.046630859375,
          0.060211181640625,
          0.06011962890625,
          0.0609130859375,
          0.08489990234375,
          0.10400390625,
          0.1092529296875,
          0.11614990234375,
          0.12432861328125,
          0.126708984375,
          0.1419677734375,
          0.1435546875,
          0.1578369140625,
          0.1534423828125,
          0.1505126953125,
          0.1612548828125,
          0.177490234375,
          0.1822509765625,
          0.18994140625,
          0.20166015625,
          0.2254638671875,
          0.347412109375,
          0.77978515625
         ]
        }
       ],
       "layout": {
        "height": 500,
        "hovermode": "x unified",
        "plot_bgcolor": "white",
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Cosine Similarity with Final Hidden State<br><sup>Using 1th highest weighted expert per layer</sup>",
         "x": 0.5,
         "y": 0.95
        },
        "width": 500,
        "xaxis": {
         "dtick": 1,
         "gridcolor": "rgba(128,128,128,0.2)",
         "gridwidth": 1,
         "range": [
          0,
          27.5
         ],
         "showgrid": true,
         "tickmode": "linear",
         "title": {
          "text": "Layer"
         }
        },
        "yaxis": {
         "gridcolor": "rgba(128,128,128,0.2)",
         "gridwidth": 1,
         "range": [
          -0.2,
          1
         ],
         "showgrid": true,
         "title": {
          "text": "Cosine Similarity"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_cosine_similarities(similarities, k):\n",
    "    \"\"\"\n",
    "    Create an interactive line plot of cosine similarities across layers.\n",
    "    \n",
    "    Args:\n",
    "    similarities: dict with layer_idx -> cosine_similarity_value\n",
    "    k: which expert (by weight rank) was used\n",
    "    \"\"\"\n",
    "    # Set plot size\n",
    "    plot_size = 500  # Size in pixels for both width and height\n",
    "    \n",
    "    # Sort layers for x-axis\n",
    "    layers = sorted(similarities.keys())\n",
    "    sim_values = [similarities[layer] for layer in layers]\n",
    "    \n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add line plot\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=layers,\n",
    "        y=sim_values,\n",
    "        mode='lines+markers',\n",
    "        name=f'Expert rank {k}',\n",
    "        hovertemplate=\n",
    "        \"<b>Layer %{x}</b><br>\" +\n",
    "        \"Cosine Similarity: %{y:.4f}<br>\" +\n",
    "        \"<extra></extra>\",  # Removes secondary box\n",
    "        line=dict(width=2),\n",
    "        marker=dict(size=8)\n",
    "    ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=f'Cosine Similarity with Final Hidden State<br><sup>Using {k+1}th highest weighted expert per layer</sup>',\n",
    "            x=0.5,  # Center title\n",
    "            y=0.95\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title='Layer',\n",
    "            gridcolor='rgba(128,128,128,0.2)',\n",
    "            tickmode='linear',\n",
    "            dtick=1,  # Show every layer number\n",
    "            range=[0, 27.5]  # Set x-axis range from 0 to 27\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='Cosine Similarity',\n",
    "            gridcolor='rgba(128,128,128,0.2)',\n",
    "            range=[-0.2, 1]  # Updated range for cosine similarity\n",
    "        ),\n",
    "        plot_bgcolor='white',\n",
    "        hovermode='x unified',  # Shows all points at a given x-coordinate\n",
    "        showlegend=False,\n",
    "        width=plot_size,\n",
    "        height=plot_size\n",
    "    )\n",
    "    \n",
    "    # Add grid\n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1)\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Use the function\n",
    "fig = plot_cosine_similarities(sim, k)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_multiple_samples(model, tokenizer, domain: str, samples: List[str]):\n",
    "    \"\"\"Analyze multiple samples and return average cosine similarities per layer\"\"\"\n",
    "    all_similarities = []\n",
    "    \n",
    "    for i, sample in enumerate(tqdm(samples, desc=\"Analyzing samples\")):\n",
    "        # Get prompt and tokenize\n",
    "        input_ids = tokenizer(sample, return_tensors=\"pt\").input_ids\n",
    "        n = input_ids.shape[1]\n",
    "        token_pos = n-1  # Last token\n",
    "        \n",
    "        # Run analysis\n",
    "        analysis = analyze_deepseek_moe(model, tokenizer, sample, return_hidden_states=True)\n",
    "        \n",
    "        # Get post attention layer norm inputs\n",
    "        post_attn_ln_inputs = get_post_attn_ln_inputs(model, tokenizer, text=sample)\n",
    "        \n",
    "        # Get final layer index and hidden state\n",
    "        final_layer_idx = max([int(k) for k in results[\"layer_predictions\"].keys()])\n",
    "        final_hidden_state = analyzer.layer_outputs[final_layer_idx][-1][0][token_pos]\n",
    "        \n",
    "        # Get hidden states for highest weighted expert (k=0)\n",
    "        hidden_states = get_expert_hidden_states_by_weight(\n",
    "            analysis,\n",
    "            post_attn_ln_inputs=post_attn_ln_inputs,\n",
    "            token_pos=token_pos,\n",
    "            k=0\n",
    "        )\n",
    "        \n",
    "        # Calculate cosine similarities for this sample\n",
    "        similarities = {}\n",
    "        for layer_idx, layer_hidden in hidden_states.items():\n",
    "            similarity = F.cosine_similarity(\n",
    "                final_hidden_state.unsqueeze(0),\n",
    "                layer_hidden.unsqueeze(0)\n",
    "            )\n",
    "            similarities[layer_idx] = similarity.item()\n",
    "            \n",
    "        all_similarities.append(similarities)\n",
    "        \n",
    "        # Save individual sample results to CSV\n",
    "        df = pd.DataFrame({\n",
    "            'layer': list(similarities.keys()),\n",
    "            'cosine_similarity': list(similarities.values())\n",
    "        })\n",
    "        df.to_csv(f'cosine-sim/csv/{domain}_{i+1}_similarities.csv', index=False)\n",
    "    \n",
    "    # Calculate average similarities per layer\n",
    "    avg_similarities = {}\n",
    "    for layer_idx in all_similarities[0].keys():  # Use first sample's layers as reference\n",
    "        layer_values = [sim[layer_idx] for sim in all_similarities]\n",
    "        avg_similarities[layer_idx] = np.mean(layer_values)\n",
    "    \n",
    "    # Save average similarities to CSV\n",
    "    avg_df = pd.DataFrame({\n",
    "        'layer': list(avg_similarities.keys()),\n",
    "        f'average_cosine_similarity_{domain}': list(avg_similarities.values())\n",
    "    })\n",
    "    avg_df.to_csv(f'cosine-sim/csv/average_similarities_{domain}.csv', index=False)\n",
    "        \n",
    "    return avg_similarities, all_similarities\n",
    "\n",
    "async def process_samples(model, tokenizer, domain=\"test\"):\n",
    "    # Read and parse JSON file\n",
    "    with open(f'interp-data/{domain}.json', 'r', encoding='utf-8') as f:\n",
    "        samples_data = json.load(f)\n",
    "    \n",
    "    # Get samples for specified domain\n",
    "    samples = samples_data.get(domain, [])\n",
    "    if not samples:\n",
    "        raise ValueError(f\"No samples found for domain: {domain}\")\n",
    "        \n",
    "    # Run analysis\n",
    "    avg_sims, all_sims = analyze_multiple_samples(model, tokenizer,domain, samples)\n",
    "    \n",
    "    # Print average similarities\n",
    "    print(\"\\nAverage Cosine Similarities per Layer:\")\n",
    "    for layer_idx in sorted(avg_sims.keys()):\n",
    "        print(f\"Layer {layer_idx}: {avg_sims[layer_idx]:.4f}\")\n",
    "        \n",
    "    return avg_sims, all_sims\n",
    "\n",
    "# Optional: Add visualization for individual samples and average\n",
    "def plot_multi_sample_similarities(all_similarities, avg_similarities):\n",
    "    \"\"\"\n",
    "    Create an interactive plot showing individual sample similarities and their average\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Plot individual samples\n",
    "    layers = sorted(avg_similarities.keys())\n",
    "    for i, sample_sims in enumerate(all_similarities):\n",
    "        sim_values = [sample_sims[layer] for layer in layers]\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=layers,\n",
    "            y=sim_values,\n",
    "            mode='lines',\n",
    "            name=f'Sample {i+1}',\n",
    "            opacity=0.3,\n",
    "            hoverinfo='skip'  # Disable hover for individual samples\n",
    "        ))\n",
    "    \n",
    "    # Plot average\n",
    "    avg_values = [avg_similarities[layer] for layer in layers]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=layers,\n",
    "        y=avg_values,\n",
    "        mode='lines+markers',\n",
    "        name='Average',\n",
    "        line=dict(width=3, color='black'),\n",
    "        marker=dict(size=8),\n",
    "        hovertemplate=\"<b>Layer %{x}</b><br>Average Similarity: %{y:.4f}<extra></extra>\"\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Cosine Similarities Across Samples',\n",
    "        xaxis_title='Layer',\n",
    "        yaxis_title='Cosine Similarity',\n",
    "        yaxis_range=[-0.2, 1],\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing samples: 100%|██████████| 3/3 [00:23<00:00,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Cosine Similarities per Layer:\n",
      "Layer 1: 0.0258\n",
      "Layer 2: 0.0366\n",
      "Layer 3: 0.0311\n",
      "Layer 4: 0.0374\n",
      "Layer 5: 0.0284\n",
      "Layer 6: 0.0334\n",
      "Layer 7: 0.0396\n",
      "Layer 8: 0.0328\n",
      "Layer 9: 0.0424\n",
      "Layer 10: 0.0462\n",
      "Layer 11: 0.0485\n",
      "Layer 12: 0.0497\n",
      "Layer 13: 0.0527\n",
      "Layer 14: 0.0572\n",
      "Layer 15: 0.0649\n",
      "Layer 16: 0.0656\n",
      "Layer 17: 0.0932\n",
      "Layer 18: 0.1040\n",
      "Layer 19: 0.1208\n",
      "Layer 20: 0.1245\n",
      "Layer 21: 0.1434\n",
      "Layer 22: 0.1436\n",
      "Layer 23: 0.1643\n",
      "Layer 24: 0.1843\n",
      "Layer 25: 0.2209\n",
      "Layer 26: 0.2225\n",
      "Layer 27: 0.7700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and tokenizer as before\n",
    "analyzer = DeepseekLayerAnalyzer(model, tokenizer)\n",
    "analyzer.register_hooks()\n",
    "domain = \"test\"\n",
    "\n",
    "# Process samples\n",
    "avg_sims, all_sims = await process_samples(model, \n",
    "                                           tokenizer,\n",
    "                                           domain=domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "skip",
         "mode": "lines",
         "name": "Sample 1",
         "opacity": 0.3,
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27
         ],
         "y": [
          0.0513916015625,
          0.05712890625,
          0.052337646484375,
          0.04156494140625,
          0.0128631591796875,
          0.0065765380859375,
          0.0355224609375,
          0.036285400390625,
          0.06011962890625,
          0.054412841796875,
          0.0538330078125,
          0.04638671875,
          0.04949951171875,
          0.063232421875,
          0.07489013671875,
          0.06622314453125,
          0.08612060546875,
          0.09844970703125,
          0.11846923828125,
          0.12005615234375,
          0.1334228515625,
          0.12890625,
          0.146728515625,
          0.1688232421875,
          0.1690673828125,
          0.2666015625,
          0.7421875
         ]
        },
        {
         "hoverinfo": "skip",
         "mode": "lines",
         "name": "Sample 2",
         "opacity": 0.3,
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27
         ],
         "y": [
          -0.003009796142578125,
          0.01221466064453125,
          -0.01222991943359375,
          0.0154876708984375,
          0.0128631591796875,
          0.02362060546875,
          0.01702880859375,
          0.006664276123046875,
          0.0186920166015625,
          0.01337432861328125,
          0.007625579833984375,
          0.00914764404296875,
          0.0021572113037109375,
          0.006137847900390625,
          0.01580810546875,
          0.01971435546875,
          0.036285400390625,
          0.058074951171875,
          0.065673828125,
          0.07672119140625,
          0.08837890625,
          0.098388671875,
          0.118896484375,
          0.1455078125,
          0.2017822265625,
          0.01241302490234375,
          0.79736328125
         ]
        },
        {
         "hoverinfo": "skip",
         "mode": "lines",
         "name": "Sample 3",
         "opacity": 0.3,
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27
         ],
         "y": [
          0.02899169921875,
          0.04046630859375,
          0.053192138671875,
          0.055145263671875,
          0.059539794921875,
          0.070068359375,
          0.06610107421875,
          0.055328369140625,
          0.04833984375,
          0.07086181640625,
          0.08404541015625,
          0.0936279296875,
          0.1064453125,
          0.10223388671875,
          0.1041259765625,
          0.11083984375,
          0.1571044921875,
          0.155517578125,
          0.1783447265625,
          0.1766357421875,
          0.2083740234375,
          0.20361328125,
          0.2271728515625,
          0.2386474609375,
          0.291748046875,
          0.388427734375,
          0.7705078125
         ]
        },
        {
         "hovertemplate": "<b>Layer %{x}</b><br>Average Similarity: %{y:.4f}<extra></extra>",
         "line": {
          "color": "black",
          "width": 3
         },
         "marker": {
          "size": 8
         },
         "mode": "lines+markers",
         "name": "Average",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27
         ],
         "y": [
          0.025791168212890625,
          0.036603291829427086,
          0.031099955240885418,
          0.0373992919921875,
          0.028422037760416668,
          0.033421834309895836,
          0.03955078125,
          0.03275934855143229,
          0.042383829752604164,
          0.046216328938802086,
          0.04850133260091146,
          0.04972076416015625,
          0.05270067850748698,
          0.057201385498046875,
          0.06494140625,
          0.06559244791666667,
          0.093170166015625,
          0.10401407877604167,
          0.12082926432291667,
          0.12447102864583333,
          0.14339192708333334,
          0.14363606770833334,
          0.16426595052083334,
          0.184326171875,
          0.22086588541666666,
          0.22248077392578125,
          0.77001953125
         ]
        }
       ],
       "layout": {
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Cosine Similarities Across Samples"
        },
        "xaxis": {
         "title": {
          "text": "Layer"
         }
        },
        "yaxis": {
         "range": [
          -0.2,
          1
         ],
         "title": {
          "text": "Cosine Similarity"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plot_multi_sample_similarities(all_sims, avg_sims)\n",
    "fig.write_image(f\"cosine-sim/cosine-sim-plot-{domain}.png\")\n",
    "fig.write_html(f\"cosine-sim/cosine-sim-plot-{domain}.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
