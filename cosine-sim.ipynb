{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "import os\n",
    "import csv\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Print CUDA details\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated()/1024**2:.2f}MB\")\n",
    "    print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved()/1024**2:.2f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        # use_flash_attention_2=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model(\"deepseek-ai/deepseek-moe-16b-base\")\n",
    "model.eval()\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "class DeepseekMoEHook:\n",
    "    \"\"\"\n",
    "    Hook into DeepSeek MoE models to capture router logits, hidden states, and expert outputs\n",
    "    for the last token after processing the whole sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, k_experts: int = 6):\n",
    "        \"\"\"\n",
    "        Initialize the hook collector.\n",
    "        \n",
    "        Args:\n",
    "            model: DeepSeek model to hook into\n",
    "            k_experts: Number of top experts to track (default: 6)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.device = DEVICE  # Use the global DEVICE variable\n",
    "        self.k_experts = k_experts\n",
    "        self.hooks = []\n",
    "        \n",
    "        # Containers for collected data\n",
    "        self.router_logits = {}\n",
    "        self.layer_hidden_states = {}\n",
    "        self.top1_expert_hidden_states = {}\n",
    "        self.topk_expert_hidden_states = {}\n",
    "        self.residual_streams = {}  # Add container for residual streams\n",
    "        \n",
    "        # Track expert outputs for matching later\n",
    "        self.expert_outputs = defaultdict(dict)\n",
    "        \n",
    "        # Set up hooks\n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        \"\"\"Register all hooks on the model.\"\"\"\n",
    "        \n",
    "        # Hook to capture router logits and selected experts\n",
    "        def hook_router_logits(layer_idx):\n",
    "            def hook_fn(module, inputs, outputs):\n",
    "                # Get input hidden states\n",
    "                hidden_states = inputs[0]\n",
    "                \n",
    "                # Calculate router logits (handle data types for precision)\n",
    "                if self.device.type == 'cuda':\n",
    "                    # Use half precision for GPU to save memory\n",
    "                    router_logits = torch.matmul(hidden_states.half(), module.weight.T.half())\n",
    "                else:\n",
    "                    router_logits = torch.matmul(hidden_states, module.weight.T)\n",
    "                \n",
    "                # Get the last token router logits\n",
    "                if len(router_logits.shape) == 3:  # [batch, seq_len, num_experts]\n",
    "                    self.router_logits[layer_idx] = router_logits[:, -1, :].detach()\n",
    "                else:  # [seq_len, num_experts]\n",
    "                    self.router_logits[layer_idx] = router_logits[-1:, :].detach()\n",
    "                \n",
    "                # Get expert indices and weights from outputs\n",
    "                topk_idx, topk_weight, _ = outputs\n",
    "                \n",
    "                # Handle different tensor shapes\n",
    "                if len(topk_idx.shape) == 3:  # [batch, seq_len, top_k]\n",
    "                    # Extract data for last token\n",
    "                    self.top1_expert_hidden_states[layer_idx] = {\n",
    "                        'expert_idx': topk_idx[:, -1, 0].detach(),  # Top 1 expert\n",
    "                        'expert_weight': topk_weight[:, -1, 0].detach(),  # Top 1 weight\n",
    "                        'token_idx': hidden_states.size(1) - 1  # Last token position\n",
    "                    }\n",
    "                    \n",
    "                    self.topk_expert_hidden_states[layer_idx] = {\n",
    "                        'expert_idx': topk_idx[:, -1, :].detach(),  # Top k experts\n",
    "                        'expert_weight': topk_weight[:, -1, :].detach(),  # Top k weights\n",
    "                        'token_idx': hidden_states.size(1) - 1  # Last token position\n",
    "                    }\n",
    "                elif len(topk_idx.shape) == 2:  # [seq_len, top_k]\n",
    "                    # Extract data for last token\n",
    "                    self.top1_expert_hidden_states[layer_idx] = {\n",
    "                        'expert_idx': topk_idx[-1, 0].detach().unsqueeze(0),  # Top 1 expert\n",
    "                        'expert_weight': topk_weight[-1, 0].detach().unsqueeze(0),  # Top 1 weight\n",
    "                        'token_idx': hidden_states.size(0) - 1  # Last token position\n",
    "                    }\n",
    "                    \n",
    "                    self.topk_expert_hidden_states[layer_idx] = {\n",
    "                        'expert_idx': topk_idx[-1, :].detach().unsqueeze(0),  # Top k experts\n",
    "                        'expert_weight': topk_weight[-1, :].detach().unsqueeze(0),  # Top k weights\n",
    "                        'token_idx': hidden_states.size(0) - 1  # Last token position\n",
    "                    }\n",
    "                \n",
    "                # Store the residual stream (input to MoE)\n",
    "                self.residual_streams[layer_idx] = hidden_states[:, -1, :].detach()\n",
    "                \n",
    "                return outputs\n",
    "            return hook_fn\n",
    "        \n",
    "        # Hook to capture expert outputs\n",
    "        def hook_expert_output(layer_idx, expert_idx):\n",
    "            def hook_fn(module, inputs, outputs):\n",
    "                # Save the expert's output for later matching\n",
    "                # In DeepseekMoE, each expert receives only the tokens routed to it\n",
    "                # We'll identify which one came from our last token in post-processing\n",
    "                self.expert_outputs[layer_idx][expert_idx] = outputs.detach()\n",
    "                return outputs\n",
    "            return hook_fn\n",
    "        \n",
    "        # Hook to capture layer hidden states (final combined output)\n",
    "        def hook_layer_output(layer_idx):\n",
    "            def hook_fn(module, inputs, outputs):\n",
    "                # Get hidden states\n",
    "                hidden_states = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "                \n",
    "                # Store only the hidden states for the last token\n",
    "                self.layer_hidden_states[layer_idx] = hidden_states[:, -1, :].detach()\n",
    "                return outputs\n",
    "            return hook_fn\n",
    "        \n",
    "        # Hook to capture pre-MoE residual streams\n",
    "        def hook_post_attn_ln(layer_idx):\n",
    "            def hook_fn(module, inputs, outputs):\n",
    "                # Capture the output of post-attention layer norm\n",
    "                # This is the input to the MoE module (the residual stream)\n",
    "                self.residual_streams[layer_idx] = outputs[:, -1, :].detach()\n",
    "                return outputs\n",
    "            return hook_fn\n",
    "        \n",
    "        # Hook to capture final hidden states after MoE processing\n",
    "        def hook_moe_output(layer_idx):\n",
    "            def hook_fn(module, inputs, outputs):\n",
    "                # Get the combined MoE output\n",
    "                combined_output = outputs[:, -1, :].detach()\n",
    "                \n",
    "                # Save the combined output in our tracker dictionaries\n",
    "                if layer_idx in self.top1_expert_hidden_states:\n",
    "                    self.top1_expert_hidden_states[layer_idx]['final_hidden_state'] = combined_output\n",
    "                    self.topk_expert_hidden_states[layer_idx]['final_hidden_state'] = combined_output\n",
    "                \n",
    "                return outputs\n",
    "            return hook_fn\n",
    "        \n",
    "        # Register hooks for each layer\n",
    "        for layer_idx, layer in enumerate(self.model.model.layers):\n",
    "            # Register hook to capture final output of each layer\n",
    "            self.hooks.append(layer.register_forward_hook(hook_layer_output(layer_idx)))\n",
    "            \n",
    "            # If it's an MoE layer, register additional hooks\n",
    "            if hasattr(layer.mlp, 'experts') and layer.mlp.__class__.__name__ == 'DeepseekMoE':\n",
    "                # Hook for post attention layer norm (to get residual stream)\n",
    "                self.hooks.append(layer.post_attention_layernorm.register_forward_hook(\n",
    "                    hook_post_attn_ln(layer_idx)))\n",
    "                \n",
    "                # Hook for router\n",
    "                self.hooks.append(layer.mlp.gate.register_forward_hook(hook_router_logits(layer_idx)))\n",
    "                \n",
    "                # Hook for each expert\n",
    "                for expert_idx, expert in enumerate(layer.mlp.experts):\n",
    "                    self.hooks.append(expert.register_forward_hook(hook_expert_output(layer_idx, expert_idx)))\n",
    "                \n",
    "                # Hook for shared expert if it exists\n",
    "                if hasattr(layer.mlp, 'shared_experts'):\n",
    "                    self.hooks.append(layer.mlp.shared_experts.register_forward_hook(\n",
    "                        hook_expert_output(layer_idx, 'shared')))\n",
    "                \n",
    "                # Hook for final MoE output\n",
    "                self.hooks.append(layer.mlp.register_forward_hook(hook_moe_output(layer_idx)))\n",
    "    def _match_expert_outputs(self):\n",
    "        \"\"\"Match expert outputs with correct weight prioritization.\"\"\"\n",
    "        for layer_idx in self.top1_expert_hidden_states:\n",
    "            if layer_idx not in self.expert_outputs:\n",
    "                continue\n",
    "                \n",
    "            # Get residual stream for this layer\n",
    "            residual = self.residual_streams.get(layer_idx, None)\n",
    "            \n",
    "            # Process top k experts first to identify actual highest weight expert\n",
    "            batch_size = self.topk_expert_hidden_states[layer_idx]['expert_idx'].size(0)\n",
    "            for batch_idx in range(batch_size):\n",
    "                expert_indices = self.topk_expert_hidden_states[layer_idx]['expert_idx'][batch_idx]\n",
    "                expert_weights = self.topk_expert_hidden_states[layer_idx]['expert_weight'][batch_idx]\n",
    "                \n",
    "                # Find the expert with the highest weight\n",
    "                max_weight_idx = torch.argmax(expert_weights).item()\n",
    "                max_weight_expert_idx = expert_indices[max_weight_idx].item()\n",
    "                max_weight = expert_weights[max_weight_idx].item()\n",
    "                \n",
    "                # Update top1_expert_hidden_states with the actual highest weight expert\n",
    "                self.top1_expert_hidden_states[layer_idx]['expert_idx'][batch_idx] = torch.tensor([max_weight_expert_idx], \n",
    "                                                                                             device=self.device)\n",
    "                self.top1_expert_hidden_states[layer_idx]['expert_weight'][batch_idx] = torch.tensor([max_weight], \n",
    "                                                                                                device=self.device)\n",
    "                \n",
    "                # Create dict for expert hidden states if it doesn't exist\n",
    "                if 'expert_hidden_states' not in self.top1_expert_hidden_states[layer_idx]:\n",
    "                    self.top1_expert_hidden_states[layer_idx]['expert_hidden_states'] = {}\n",
    "                \n",
    "                # Store expert hidden states for top 1 expert\n",
    "                if max_weight_expert_idx in self.expert_outputs[layer_idx]:\n",
    "                    expert_output = self.expert_outputs[layer_idx][max_weight_expert_idx]\n",
    "                    if len(expert_output.shape) > 1:\n",
    "                        # Get the expert output\n",
    "                        expert_hidden = expert_output[0].to(self.device)\n",
    "                        \n",
    "                        # Add residual if available\n",
    "                        if residual is not None:\n",
    "                            expert_hidden = expert_hidden + residual[batch_idx].to(self.device)\n",
    "                        \n",
    "                        self.top1_expert_hidden_states[layer_idx]['expert_hidden_states'][batch_idx] = expert_hidden\n",
    "            \n",
    "                # Create dict for all expert hidden states if it doesn't exist\n",
    "                if 'expert_hidden_states' not in self.topk_expert_hidden_states[layer_idx]:\n",
    "                    self.topk_expert_hidden_states[layer_idx]['expert_hidden_states'] = {}\n",
    "                \n",
    "                # Store for each expert in top-k\n",
    "                expert_states = {}\n",
    "                for k_idx, expert_idx in enumerate(expert_indices):\n",
    "                    expert_idx = expert_idx.item()\n",
    "                    if expert_idx in self.expert_outputs[layer_idx]:\n",
    "                        expert_output = self.expert_outputs[layer_idx][expert_idx]\n",
    "                        if len(expert_output.shape) > 1:\n",
    "                            # Get the expert output\n",
    "                            expert_hidden = expert_output[0].to(self.device)\n",
    "                            \n",
    "                            # Add residual if available\n",
    "                            if residual is not None:\n",
    "                                expert_hidden = expert_hidden + residual[batch_idx].to(self.device)\n",
    "                            \n",
    "                            expert_states[k_idx] = expert_hidden\n",
    "                \n",
    "                self.topk_expert_hidden_states[layer_idx]['expert_hidden_states'][batch_idx] = expert_states\n",
    "                \n",
    "                # Now compute the combined/ensemble hidden state from just the top-k experts\n",
    "                # (without shared expert contribution)\n",
    "                if expert_states:  # Only if we have any expert states\n",
    "                    hidden_dim = next(iter(expert_states.values())).shape[-1]\n",
    "                    combined_topk = torch.zeros(hidden_dim, device=self.device)\n",
    "                    total_weight = 0.0\n",
    "                    \n",
    "                    for k_idx, expert_idx in enumerate(expert_indices):\n",
    "                        if k_idx in expert_states:\n",
    "                            expert_state = expert_states[k_idx]\n",
    "                            expert_weight = expert_weights[k_idx]\n",
    "                            combined_topk += expert_weight * expert_state\n",
    "                            total_weight += expert_weight\n",
    "                    \n",
    "                    # Normalize if weights don't sum to 1.0\n",
    "                    if total_weight > 0 and abs(total_weight - 1.0) > 1e-5:\n",
    "                        combined_topk /= total_weight\n",
    "                    \n",
    "                    # Store the combined hidden state\n",
    "                    if 'combined_topk_hidden_state' not in self.topk_expert_hidden_states[layer_idx]:\n",
    "                        self.topk_expert_hidden_states[layer_idx]['combined_topk_hidden_state'] = {}\n",
    "                    \n",
    "                    self.topk_expert_hidden_states[layer_idx]['combined_topk_hidden_state'][batch_idx] = combined_topk\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Run a forward pass through the model and collect all hook data.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token IDs to process\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing all collected data\n",
    "        \"\"\"\n",
    "        # Clear previous data\n",
    "        self.router_logits.clear()\n",
    "        self.layer_hidden_states.clear()\n",
    "        self.top1_expert_hidden_states.clear()\n",
    "        self.topk_expert_hidden_states.clear()\n",
    "        self.expert_outputs.clear()\n",
    "        self.residual_streams.clear()\n",
    "        \n",
    "        # Ensure input is on the correct device\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        \n",
    "        # Run model forward pass with appropriate optimizations\n",
    "        with torch.no_grad():\n",
    "            if self.device.type == 'cuda':\n",
    "                # Use mixed precision on GPU\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = self.model(input_ids)\n",
    "                # Clear cache immediately after forward pass\n",
    "                torch.cuda.empty_cache()\n",
    "            else:\n",
    "                outputs = self.model(input_ids)\n",
    "            \n",
    "        # Match expert outputs to get hidden states\n",
    "        self._match_expert_outputs()\n",
    "        \n",
    "        # Return processed results\n",
    "        return {\n",
    "            'layer_hidden_states': self.layer_hidden_states,\n",
    "            'router_logits': self.router_logits,\n",
    "            'top1_expert': self.top1_expert_hidden_states,\n",
    "            'topk_expert': self.topk_expert_hidden_states,\n",
    "            'residual_streams': self.residual_streams\n",
    "        }\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Remove all hooks from the model.\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        \n",
    "        # Clear CUDA cache if using GPU\n",
    "        if self.device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    def __del__(self):\n",
    "        \"\"\"Cleanup when object is deleted.\"\"\"\n",
    "        self.remove_hooks()\n",
    "\n",
    "def get_last_token_moe_data(model, tokenizer, prompt, k_experts=6):\n",
    "    \"\"\"\n",
    "    Helper function to get MoE data for the last token of a sequence.\n",
    "    \n",
    "    Args:\n",
    "        model: DeepSeek model\n",
    "        tokenizer: DeepSeek tokenizer\n",
    "        prompt: Input text prompt\n",
    "        k_experts: Number of top experts to track (default: 6)\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing all collected MoE data\n",
    "    \"\"\"\n",
    "    # Create hook\n",
    "    hook = DeepseekMoEHook(model, k_experts=k_experts)\n",
    "    \n",
    "    # Prepare input\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
    "    \n",
    "    try:\n",
    "        # Get data\n",
    "        moe_data = hook.forward(input_ids)\n",
    "        \n",
    "        # Add prompt information\n",
    "        moe_data['prompt'] = prompt\n",
    "        moe_data['input_ids'] = input_ids\n",
    "        moe_data['last_token_id'] = input_ids[0, -1].item()\n",
    "        moe_data['last_token'] = tokenizer.decode([moe_data['last_token_id']])\n",
    "        \n",
    "        return moe_data\n",
    "    finally:\n",
    "        # Always remove hooks\n",
    "        hook.remove_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a file of prompts and calculate cosine similarities for all layers\n",
    "def process_prompts_file(file_path, model, tokenizer, output_dir=\"cosine-sim-csv\", device=DEVICE):\n",
    "    # Check if device is specified, otherwise use global DEVICE if available\n",
    "    if device is None:\n",
    "        try:\n",
    "            device = DEVICE\n",
    "        except NameError:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract domain name from file path\n",
    "    domain_name = os.path.basename(file_path).split('.')[0]\n",
    "    \n",
    "    # Read prompts from file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        prompts = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    # Process each prompt\n",
    "    for prompt_idx, prompt in enumerate(tqdm(prompts, desc=\"Processing prompts\")):\n",
    "        # Create a unique CSV file for each prompt with domain name\n",
    "        csv_filename = f\"{domain_name}_prompt_{prompt_idx+1}_cos-sim.csv\"\n",
    "        csv_path = os.path.join(output_dir, csv_filename)\n",
    "        \n",
    "        # Write header to CSV\n",
    "        with open(csv_path, 'w', newline='') as csvfile:\n",
    "            fieldnames = ['layer', 'final_vs_top1', 'final_vs_topk', 'top1_vs_topk']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "        \n",
    "        # Get MoE data for the prompt\n",
    "        moe_data = get_last_token_moe_data(model, tokenizer, prompt)\n",
    "        \n",
    "        # Calculate cosine similarities for each layer\n",
    "        results = []\n",
    "        num_layers = 27  # Assuming 27 layers (1-27)\n",
    "        \n",
    "        for layer_idx in range(1, num_layers + 1):\n",
    "            # Skip if layer doesn't exist in the data\n",
    "            if layer_idx >= len(moe_data['layer_hidden_states']):\n",
    "                continue\n",
    "                \n",
    "            # Get hidden states for this layer\n",
    "            final_hidden_state = moe_data['layer_hidden_states'][layer_idx]\n",
    "            \n",
    "            # Get top-1 expert data\n",
    "            try:\n",
    "                top1_expert_hidden_state = moe_data['top1_expert'][layer_idx]['expert_hidden_states'][0]\n",
    "                \n",
    "                # Get combined top-k expert data\n",
    "                # Check if we need to recompute the combined top-k hidden state\n",
    "                if 'expert_hidden_states' in moe_data['topk_expert'][layer_idx] and 'expert_weights' in moe_data['topk_expert'][layer_idx]:\n",
    "                    # Get expert states and weights\n",
    "                    expert_states = moe_data['topk_expert'][layer_idx]['expert_hidden_states']\n",
    "                    expert_weights = moe_data['topk_expert'][layer_idx]['expert_weights'][0]\n",
    "                    expert_indices = moe_data['topk_expert'][layer_idx].get('expert_indices', list(range(len(expert_states))))\n",
    "                    \n",
    "                    # Compute combined top-k hidden state\n",
    "                    hidden_dim = expert_states[0].shape[-1]\n",
    "                    combined_topk = torch.zeros(hidden_dim, device=device)\n",
    "                    total_weight = 0.0\n",
    "                    \n",
    "                    for k_idx, expert_idx in enumerate(expert_indices):\n",
    "                        if k_idx in expert_states:\n",
    "                            expert_state = expert_states[k_idx]\n",
    "                            expert_weight = expert_weights[k_idx]\n",
    "                            combined_topk += expert_weight * expert_state\n",
    "                            total_weight += expert_weight\n",
    "                    \n",
    "                    # Normalize if weights don't sum to 1.0\n",
    "                    if total_weight > 0 and abs(total_weight - 1.0) > 1e-5:\n",
    "                        combined_topk /= total_weight\n",
    "                        \n",
    "                    combined_topk_hidden_state = combined_topk\n",
    "                else:\n",
    "                    # Use the pre-computed combined top-k hidden state\n",
    "                    combined_topk_hidden_state = moe_data['topk_expert'][layer_idx]['combined_topk_hidden_state'][0]\n",
    "                \n",
    "                # Normalize vectors for cosine similarity\n",
    "                def normalize_vector(vector):\n",
    "                    return vector / vector.norm(p=2, dim=-1, keepdim=True)\n",
    "                \n",
    "                final_hidden_norm = normalize_vector(final_hidden_state)\n",
    "                top1_expert_norm = normalize_vector(top1_expert_hidden_state)\n",
    "                combined_topk_norm = normalize_vector(combined_topk_hidden_state)\n",
    "                \n",
    "                # Calculate cosine similarities - fix the tensor dimension issue\n",
    "                # Make sure tensors are flattened to 1D before computing cosine similarity\n",
    "                cos_sim_final_top1 = F.cosine_similarity(\n",
    "                    final_hidden_norm.flatten().unsqueeze(0), \n",
    "                    top1_expert_norm.flatten().unsqueeze(0)\n",
    "                ).item()\n",
    "                \n",
    "                cos_sim_final_topk = F.cosine_similarity(\n",
    "                    final_hidden_norm.flatten().unsqueeze(0), \n",
    "                    combined_topk_norm.flatten().unsqueeze(0)\n",
    "                ).item()\n",
    "                \n",
    "                cos_sim_top1_topk = F.cosine_similarity(\n",
    "                    top1_expert_norm.flatten().unsqueeze(0), \n",
    "                    combined_topk_norm.flatten().unsqueeze(0)\n",
    "                ).item()\n",
    "                \n",
    "                # Store results\n",
    "                results.append({\n",
    "                    'layer': layer_idx,\n",
    "                    'final_vs_top1': cos_sim_final_top1,\n",
    "                    'final_vs_topk': cos_sim_final_topk,\n",
    "                    'top1_vs_topk': cos_sim_top1_topk\n",
    "                })\n",
    "            except (KeyError, IndexError) as e:\n",
    "                print(f\"Error processing layer {layer_idx} for prompt {prompt_idx+1}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Write results to CSV\n",
    "        with open(csv_path, 'a', newline='') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writerows(results)\n",
    "        \n",
    "        print(f\"Results for prompt {prompt_idx+1} saved to {csv_path}\")\n",
    "        \n",
    "        # Clear CUDA cache if using GPU\n",
    "        if DEVICE == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Clean model memory between prompts\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        gc.collect()  # Force garbage collection\n",
    "        if DEVICE == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Processing complete. Results saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_cosine_similarities(csv_file, prompt_num=1):\n",
    "    \"\"\"\n",
    "    Plot cosine similarities from a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file containing cosine similarity data\n",
    "        prompt_num (int): Prompt number for the title\n",
    "    \n",
    "    Returns:\n",
    "        plotly.graph_objects.Figure: The generated figure\n",
    "    \"\"\"\n",
    "    # Load the CSV data\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Create a plotly figure\n",
    "    fig = make_subplots()\n",
    "    \n",
    "    # Add traces for each metric\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df['layer'], \n",
    "            y=df['final_vs_top1'], \n",
    "            mode='lines+markers',\n",
    "            name='Final vs Top1'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df['layer'], \n",
    "            y=df['final_vs_topk'], \n",
    "            mode='lines+markers',\n",
    "            name='Final vs TopK'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df['layer'], \n",
    "            y=df['top1_vs_topk'], \n",
    "            mode='lines+markers',\n",
    "            name='Top1 vs TopK'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f'Cosine Similarities Across Layers for Prompt {prompt_num}',\n",
    "        xaxis_title='Layer',\n",
    "        yaxis_title='Cosine Similarity',\n",
    "        legend_title='Metrics',\n",
    "        hovermode='x unified',\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    # Ensure x-axis shows all layer numbers\n",
    "    fig.update_xaxes(tickmode='array', tickvals=df['layer'])\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Example usage\n",
    "csv_file = 'cosine-sim-csv/prompt_1_cosine_similarities.csv'\n",
    "fig = plot_cosine_similarities(csv_file)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from glob import glob\n",
    "\n",
    "def average_cosine_similarities(base_name, output_dir=\"cosine-sim-csv-avg\"):\n",
    "    \"\"\"\n",
    "    Averages the cosine similarities across all files with the same base name pattern.\n",
    "    \n",
    "    Args:\n",
    "        base_name (str): The base name pattern to match files (e.g., 'aime-math')\n",
    "        output_dir (str): Directory to save the averaged results\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the averaged cosine similarities\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Find all matching CSV files\n",
    "    pattern = f\"cosine-sim-csv/{base_name}_chunked_prompt_*_cos-sim.csv\"\n",
    "    csv_files = glob(pattern)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No files found matching pattern: {pattern}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} files matching pattern: {pattern}\")\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    all_top1_vs_topk = []\n",
    "    \n",
    "    # Read each CSV file and extract the top1_vs_topk column\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        all_top1_vs_topk.append(df['top1_vs_topk'].values)\n",
    "    \n",
    "    # Convert to numpy array for easier manipulation\n",
    "    all_top1_vs_topk = np.array(all_top1_vs_topk)\n",
    "    \n",
    "    # Calculate average\n",
    "    avg_top1_vs_topk = np.mean(all_top1_vs_topk, axis=0)\n",
    "    \n",
    "    # Create a new DataFrame with the averaged data\n",
    "    result_df = pd.DataFrame({\n",
    "        'layer': df['layer'],  # Using the layer numbers from the last loaded file\n",
    "        'avg_top1_vs_topk': avg_top1_vs_topk\n",
    "    })\n",
    "    \n",
    "    # Save the averaged data to a CSV file\n",
    "    output_file = f\"{output_dir}/{base_name}_averaged_top1_vs_topk.csv\"\n",
    "    result_df.to_csv(output_file, index=False)\n",
    "    print(f\"Averaged data saved to {output_file}\")\n",
    "    \n",
    "    # return result_df\n",
    "\n",
    "def plot_averaged_similarities(base_name, output_dir=\"cosine-sim-csv-avg\"):\n",
    "    \"\"\"\n",
    "    Plots the averaged cosine similarities.\n",
    "    \n",
    "    Args:\n",
    "        base_name (str): The base name pattern used to generate the averaged data\n",
    "        output_dir (str): Directory where the averaged data is saved\n",
    "    \n",
    "    Returns:\n",
    "        plotly.graph_objects.Figure: The plotly figure object\n",
    "    \"\"\"\n",
    "    # Get the averaged data\n",
    "    df = average_cosine_similarities(base_name, output_dir)\n",
    "    \n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df['layer'], \n",
    "            y=df['avg_top1_vs_topk'], \n",
    "            mode='lines+markers',\n",
    "            name='Avg Top1 vs TopK'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f'Average Top1 vs TopK Cosine Similarities Across Layers for {base_name}',\n",
    "        xaxis_title='Layer',\n",
    "        yaxis_title='Average Cosine Similarity',\n",
    "        hovermode='x unified',\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    # Ensure x-axis shows all layer numbers\n",
    "    fig.update_xaxes(tickmode='array', tickvals=df['layer'])\n",
    "    \n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_combined_similarities(base_names, csv_dir=\"cosine-sim-csv-avg\"):\n",
    "    \"\"\"\n",
    "    Plot combined graph of top1_vs_topk similarities for multiple datasets\n",
    "    \n",
    "    Args:\n",
    "        base_names: List of dataset base names\n",
    "        csv_dir: Directory containing the averaged CSV files\n",
    "    \n",
    "    Returns:\n",
    "        Plotly figure object\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import plotly.graph_objects as go\n",
    "    import os\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for base_name in base_names:\n",
    "        # Construct the file path\n",
    "        file_path = os.path.join(csv_dir, f\"{base_name}_averaged_top1_vs_topk.csv\")\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Warning: File {file_path} not found. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Add trace for this dataset\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df['layer'], \n",
    "                y=df['avg_top1_vs_topk'], \n",
    "                mode='lines+markers',\n",
    "                name=base_name\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title='Comparison of Top1 vs TopK Cosine Similarities Across Datasets',\n",
    "        xaxis_title='Layer',\n",
    "        yaxis_title='Average Cosine Similarity',\n",
    "        hovermode='x unified',\n",
    "        template='plotly_white',\n",
    "        legend_title=\"Dataset\"\n",
    "    )\n",
    "    \n",
    "    # Ensure x-axis shows appropriate layer numbers\n",
    "    # Using the last loaded dataframe for x-axis values\n",
    "    if 'df' in locals():\n",
    "        fig.update_xaxes(tickmode='array', tickvals=df['layer'])\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = ['data-ext/aime-math_chunked.txt',\n",
    "             'data-ext/gsm8k_chunked.txt',\n",
    "             'data-ext/github_chunked.txt',\n",
    "             'data-ext/chinese_chunked.txt',\n",
    "             'data-ext/arxiv_title_abstract_chunked.txt',\n",
    "             'data-ext/english_chunked.txt',\n",
    "             'data-ext/french-qa_chunked.txt']\n",
    "for file in file_path:\n",
    "    process_prompts_file(file, model, tokenizer, output_dir=\"cosine-sim-csv\")\n",
    "    print(f\"Processed {file}\")\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()  # Force garbage collection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name = ['aime-math',\n",
    "            'gsm8k',\n",
    "            'github',\n",
    "            'chinese',\n",
    "            'arxiv_title_abstract',\n",
    "            'english',\n",
    "            'french-qa']\n",
    "\n",
    "for base in base_name:\n",
    "    average_cosine_similarities(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot combined graph for all datasets\n",
    "combined_fig = plot_combined_similarities(base_name)\n",
    "combined_fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
