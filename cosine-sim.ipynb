{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "import os\n",
    "import csv\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Print CUDA details\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated()/1024**2:.2f}MB\")\n",
    "    print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved()/1024**2:.2f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1016af4617745d89d72176c2bd6e3ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DeepseekForCausalLM(\n",
       "  (model): DeepseekModel(\n",
       "    (embed_tokens): Embedding(102400, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0): DeepseekDecoderLayer(\n",
       "        (self_attn): DeepseekSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): DeepseekRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): DeepseekMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=10944, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=10944, bias=False)\n",
       "          (down_proj): Linear(in_features=10944, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): DeepseekRMSNorm()\n",
       "        (post_attention_layernorm): DeepseekRMSNorm()\n",
       "      )\n",
       "      (1-27): 27 x DeepseekDecoderLayer(\n",
       "        (self_attn): DeepseekSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): DeepseekRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): DeepseekMoE(\n",
       "          (experts): ModuleList(\n",
       "            (0-63): 64 x DeepseekMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
       "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (gate): MoEGate()\n",
       "          (shared_experts): DeepseekMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=2816, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=2816, bias=False)\n",
       "            (down_proj): Linear(in_features=2816, out_features=2048, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): DeepseekRMSNorm()\n",
       "        (post_attention_layernorm): DeepseekRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): DeepseekRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=102400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_model(model_name):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        # use_flash_attention_2=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model(\"deepseek-ai/deepseek-moe-16b-base\")\n",
    "model.eval()\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "class DeepseekMoEHook:\n",
    "    \"\"\"\n",
    "    Hook into DeepSeek MoE models to capture router logits, hidden states, and expert outputs\n",
    "    for the last token after processing the whole sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, k_experts: int = 6):\n",
    "        \"\"\"\n",
    "        Initialize the hook collector.\n",
    "        \n",
    "        Args:\n",
    "            model: DeepSeek model to hook into\n",
    "            k_experts: Number of top experts to track (default: 6)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.device = DEVICE  # Use the global DEVICE variable\n",
    "        self.k_experts = k_experts\n",
    "        self.hooks = []\n",
    "        \n",
    "        # Containers for collected data\n",
    "        self.router_logits = {}\n",
    "        self.layer_hidden_states = {}\n",
    "        self.top1_expert_hidden_states = {}\n",
    "        self.topk_expert_hidden_states = {}\n",
    "        self.residual_streams = {}  # Add container for residual streams\n",
    "        \n",
    "        # Track expert outputs for matching later\n",
    "        self.expert_outputs = defaultdict(dict)\n",
    "        \n",
    "        # Set up hooks\n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        \"\"\"Register all hooks on the model.\"\"\"\n",
    "        \n",
    "        # Hook to capture router logits and selected experts\n",
    "        def hook_router_logits(layer_idx):\n",
    "            def hook_fn(module, inputs, outputs):\n",
    "                # Get input hidden states\n",
    "                hidden_states = inputs[0]\n",
    "                \n",
    "                # Calculate router logits (handle data types for precision)\n",
    "                if self.device.type == 'cuda':\n",
    "                    # Use half precision for GPU to save memory\n",
    "                    router_logits = torch.matmul(hidden_states.half(), module.weight.T.half())\n",
    "                else:\n",
    "                    router_logits = torch.matmul(hidden_states, module.weight.T)\n",
    "                \n",
    "                # Get the last token router logits\n",
    "                if len(router_logits.shape) == 3:  # [batch, seq_len, num_experts]\n",
    "                    self.router_logits[layer_idx] = router_logits[:, -1, :].detach()\n",
    "                else:  # [seq_len, num_experts]\n",
    "                    self.router_logits[layer_idx] = router_logits[-1:, :].detach()\n",
    "                \n",
    "                # Get expert indices and weights from outputs\n",
    "                topk_idx, topk_weight, _ = outputs\n",
    "                \n",
    "                # Handle different tensor shapes\n",
    "                if len(topk_idx.shape) == 3:  # [batch, seq_len, top_k]\n",
    "                    # Extract data for last token\n",
    "                    self.top1_expert_hidden_states[layer_idx] = {\n",
    "                        'expert_idx': topk_idx[:, -1, 0].detach(),  # Top 1 expert\n",
    "                        'expert_weight': topk_weight[:, -1, 0].detach(),  # Top 1 weight\n",
    "                        'token_idx': hidden_states.size(1) - 1  # Last token position\n",
    "                    }\n",
    "                    \n",
    "                    self.topk_expert_hidden_states[layer_idx] = {\n",
    "                        'expert_idx': topk_idx[:, -1, :].detach(),  # Top k experts\n",
    "                        'expert_weight': topk_weight[:, -1, :].detach(),  # Top k weights\n",
    "                        'token_idx': hidden_states.size(1) - 1  # Last token position\n",
    "                    }\n",
    "                elif len(topk_idx.shape) == 2:  # [seq_len, top_k]\n",
    "                    # Extract data for last token\n",
    "                    self.top1_expert_hidden_states[layer_idx] = {\n",
    "                        'expert_idx': topk_idx[-1, 0].detach().unsqueeze(0),  # Top 1 expert\n",
    "                        'expert_weight': topk_weight[-1, 0].detach().unsqueeze(0),  # Top 1 weight\n",
    "                        'token_idx': hidden_states.size(0) - 1  # Last token position\n",
    "                    }\n",
    "                    \n",
    "                    self.topk_expert_hidden_states[layer_idx] = {\n",
    "                        'expert_idx': topk_idx[-1, :].detach().unsqueeze(0),  # Top k experts\n",
    "                        'expert_weight': topk_weight[-1, :].detach().unsqueeze(0),  # Top k weights\n",
    "                        'token_idx': hidden_states.size(0) - 1  # Last token position\n",
    "                    }\n",
    "                \n",
    "                # Store the residual stream (input to MoE)\n",
    "                self.residual_streams[layer_idx] = hidden_states[:, -1, :].detach()\n",
    "                \n",
    "                return outputs\n",
    "            return hook_fn\n",
    "        \n",
    "        # Hook to capture expert outputs\n",
    "        def hook_expert_output(layer_idx, expert_idx):\n",
    "            def hook_fn(module, inputs, outputs):\n",
    "                # Save the expert's output for later matching\n",
    "                # In DeepseekMoE, each expert receives only the tokens routed to it\n",
    "                # We'll identify which one came from our last token in post-processing\n",
    "                self.expert_outputs[layer_idx][expert_idx] = outputs.detach()\n",
    "                return outputs\n",
    "            return hook_fn\n",
    "        \n",
    "        # Hook to capture layer hidden states (final combined output)\n",
    "        def hook_layer_output(layer_idx):\n",
    "            def hook_fn(module, inputs, outputs):\n",
    "                # Get hidden states\n",
    "                hidden_states = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "                \n",
    "                # Store only the hidden states for the last token\n",
    "                self.layer_hidden_states[layer_idx] = hidden_states[:, -1, :].detach()\n",
    "                return outputs\n",
    "            return hook_fn\n",
    "        \n",
    "        # Hook to capture pre-MoE residual streams\n",
    "        def hook_post_attn_ln(layer_idx):\n",
    "            def hook_fn(module, inputs, outputs):\n",
    "                # Capture the output of post-attention layer norm\n",
    "                # This is the input to the MoE module (the residual stream)\n",
    "                self.residual_streams[layer_idx] = outputs[:, -1, :].detach()\n",
    "                return outputs\n",
    "            return hook_fn\n",
    "        \n",
    "        # Hook to capture final hidden states after MoE processing\n",
    "        def hook_moe_output(layer_idx):\n",
    "            def hook_fn(module, inputs, outputs):\n",
    "                # Get the combined MoE output\n",
    "                combined_output = outputs[:, -1, :].detach()\n",
    "                \n",
    "                # Save the combined output in our tracker dictionaries\n",
    "                if layer_idx in self.top1_expert_hidden_states:\n",
    "                    self.top1_expert_hidden_states[layer_idx]['final_hidden_state'] = combined_output\n",
    "                    self.topk_expert_hidden_states[layer_idx]['final_hidden_state'] = combined_output\n",
    "                \n",
    "                return outputs\n",
    "            return hook_fn\n",
    "        \n",
    "        # Register hooks for each layer\n",
    "        for layer_idx, layer in enumerate(self.model.model.layers):\n",
    "            # Register hook to capture final output of each layer\n",
    "            self.hooks.append(layer.register_forward_hook(hook_layer_output(layer_idx)))\n",
    "            \n",
    "            # If it's an MoE layer, register additional hooks\n",
    "            if hasattr(layer.mlp, 'experts') and layer.mlp.__class__.__name__ == 'DeepseekMoE':\n",
    "                # Hook for post attention layer norm (to get residual stream)\n",
    "                self.hooks.append(layer.post_attention_layernorm.register_forward_hook(\n",
    "                    hook_post_attn_ln(layer_idx)))\n",
    "                \n",
    "                # Hook for router\n",
    "                self.hooks.append(layer.mlp.gate.register_forward_hook(hook_router_logits(layer_idx)))\n",
    "                \n",
    "                # Hook for each expert\n",
    "                for expert_idx, expert in enumerate(layer.mlp.experts):\n",
    "                    self.hooks.append(expert.register_forward_hook(hook_expert_output(layer_idx, expert_idx)))\n",
    "                \n",
    "                # Hook for shared expert if it exists\n",
    "                if hasattr(layer.mlp, 'shared_experts'):\n",
    "                    self.hooks.append(layer.mlp.shared_experts.register_forward_hook(\n",
    "                        hook_expert_output(layer_idx, 'shared')))\n",
    "                \n",
    "                # Hook for final MoE output\n",
    "                self.hooks.append(layer.mlp.register_forward_hook(hook_moe_output(layer_idx)))\n",
    "    \n",
    "    def _match_expert_outputs(self):\n",
    "        \"\"\"Match expert outputs to token indices to get hidden states for top 1 and top k experts.\"\"\"\n",
    "        for layer_idx in self.top1_expert_hidden_states:\n",
    "            if layer_idx not in self.expert_outputs:\n",
    "                continue\n",
    "                \n",
    "            # Get residual stream for this layer\n",
    "            residual = self.residual_streams.get(layer_idx, None)\n",
    "            \n",
    "            # Process top 1 expert\n",
    "            batch_size = self.top1_expert_hidden_states[layer_idx]['expert_idx'].size(0)\n",
    "            for batch_idx in range(batch_size):\n",
    "                expert_idx = self.top1_expert_hidden_states[layer_idx]['expert_idx'][batch_idx].item()\n",
    "                \n",
    "                # If we captured output for this expert\n",
    "                if expert_idx in self.expert_outputs[layer_idx]:\n",
    "                    # Create dict for expert hidden states if it doesn't exist\n",
    "                    if 'expert_hidden_states' not in self.top1_expert_hidden_states[layer_idx]:\n",
    "                        self.top1_expert_hidden_states[layer_idx]['expert_hidden_states'] = {}\n",
    "                    \n",
    "                    # Store expert hidden states - take first row as approximate output\n",
    "                    # In real scenarios, we'd need token mapping which is complex in MoE\n",
    "                    expert_output = self.expert_outputs[layer_idx][expert_idx]\n",
    "                    if len(expert_output.shape) > 1:\n",
    "                        # Get the expert output\n",
    "                        expert_hidden = expert_output[0].to(self.device)\n",
    "                        \n",
    "                        # Add residual if available\n",
    "                        if residual is not None:\n",
    "                            expert_hidden = expert_hidden + residual[batch_idx].to(self.device)\n",
    "                        \n",
    "                        self.top1_expert_hidden_states[layer_idx]['expert_hidden_states'][batch_idx] = expert_hidden\n",
    "            \n",
    "            # Process top k experts\n",
    "            for batch_idx in range(batch_size):\n",
    "                expert_indices = self.topk_expert_hidden_states[layer_idx]['expert_idx'][batch_idx]\n",
    "                expert_weights = self.topk_expert_hidden_states[layer_idx]['expert_weight'][batch_idx]\n",
    "                \n",
    "                # Create dict for all expert hidden states if it doesn't exist\n",
    "                if 'expert_hidden_states' not in self.topk_expert_hidden_states[layer_idx]:\n",
    "                    self.topk_expert_hidden_states[layer_idx]['expert_hidden_states'] = {}\n",
    "                \n",
    "                # Store for each expert in top-k\n",
    "                expert_states = {}\n",
    "                for k_idx, expert_idx in enumerate(expert_indices):\n",
    "                    expert_idx = expert_idx.item()\n",
    "                    if expert_idx in self.expert_outputs[layer_idx]:\n",
    "                        expert_output = self.expert_outputs[layer_idx][expert_idx]\n",
    "                        if len(expert_output.shape) > 1:\n",
    "                            # Get the expert output\n",
    "                            expert_hidden = expert_output[0].to(self.device)\n",
    "                            \n",
    "                            # Add residual if available\n",
    "                            if residual is not None:\n",
    "                                expert_hidden = expert_hidden + residual[batch_idx].to(self.device)\n",
    "                            \n",
    "                            expert_states[k_idx] = expert_hidden\n",
    "                \n",
    "                self.topk_expert_hidden_states[layer_idx]['expert_hidden_states'][batch_idx] = expert_states\n",
    "                \n",
    "                # Now compute the combined/ensemble hidden state from just the top-k experts\n",
    "                # (without shared expert contribution)\n",
    "                if expert_states:  # Only if we have any expert states\n",
    "                    hidden_dim = next(iter(expert_states.values())).shape[-1]\n",
    "                    combined_topk = torch.zeros(hidden_dim, device=self.device)\n",
    "                    total_weight = 0.0\n",
    "                    \n",
    "                    for k_idx, expert_idx in enumerate(expert_indices):\n",
    "                        if k_idx in expert_states:\n",
    "                            expert_state = expert_states[k_idx]\n",
    "                            expert_weight = expert_weights[k_idx]\n",
    "                            combined_topk += expert_weight * expert_state\n",
    "                            total_weight += expert_weight\n",
    "                    \n",
    "                    # Normalize if weights don't sum to 1.0\n",
    "                    if total_weight > 0 and abs(total_weight - 1.0) > 1e-5:\n",
    "                        combined_topk /= total_weight\n",
    "                    \n",
    "                    # Store the combined hidden state\n",
    "                    if 'combined_topk_hidden_state' not in self.topk_expert_hidden_states[layer_idx]:\n",
    "                        self.topk_expert_hidden_states[layer_idx]['combined_topk_hidden_state'] = {}\n",
    "                    \n",
    "                    self.topk_expert_hidden_states[layer_idx]['combined_topk_hidden_state'][batch_idx] = combined_topk\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Run a forward pass through the model and collect all hook data.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token IDs to process\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing all collected data\n",
    "        \"\"\"\n",
    "        # Clear previous data\n",
    "        self.router_logits.clear()\n",
    "        self.layer_hidden_states.clear()\n",
    "        self.top1_expert_hidden_states.clear()\n",
    "        self.topk_expert_hidden_states.clear()\n",
    "        self.expert_outputs.clear()\n",
    "        self.residual_streams.clear()\n",
    "        \n",
    "        # Ensure input is on the correct device\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        \n",
    "        # Run model forward pass with appropriate optimizations\n",
    "        with torch.no_grad():\n",
    "            if self.device.type == 'cuda':\n",
    "                # Use mixed precision on GPU\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = self.model(input_ids)\n",
    "                # Clear cache immediately after forward pass\n",
    "                torch.cuda.empty_cache()\n",
    "            else:\n",
    "                outputs = self.model(input_ids)\n",
    "            \n",
    "        # Match expert outputs to get hidden states\n",
    "        self._match_expert_outputs()\n",
    "        \n",
    "        # Return processed results\n",
    "        return {\n",
    "            'layer_hidden_states': self.layer_hidden_states,\n",
    "            'router_logits': self.router_logits,\n",
    "            'top1_expert': self.top1_expert_hidden_states,\n",
    "            'topk_expert': self.topk_expert_hidden_states,\n",
    "            'residual_streams': self.residual_streams\n",
    "        }\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Remove all hooks from the model.\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        \n",
    "        # Clear CUDA cache if using GPU\n",
    "        if self.device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    def __del__(self):\n",
    "        \"\"\"Cleanup when object is deleted.\"\"\"\n",
    "        self.remove_hooks()\n",
    "\n",
    "def get_last_token_moe_data(model, tokenizer, prompt, k_experts=6):\n",
    "    \"\"\"\n",
    "    Helper function to get MoE data for the last token of a sequence.\n",
    "    \n",
    "    Args:\n",
    "        model: DeepSeek model\n",
    "        tokenizer: DeepSeek tokenizer\n",
    "        prompt: Input text prompt\n",
    "        k_experts: Number of top experts to track (default: 6)\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing all collected MoE data\n",
    "    \"\"\"\n",
    "    # Create hook\n",
    "    hook = DeepseekMoEHook(model, k_experts=k_experts)\n",
    "    \n",
    "    # Prepare input\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
    "    \n",
    "    try:\n",
    "        # Get data\n",
    "        moe_data = hook.forward(input_ids)\n",
    "        \n",
    "        # Add prompt information\n",
    "        moe_data['prompt'] = prompt\n",
    "        moe_data['input_ids'] = input_ids\n",
    "        moe_data['last_token_id'] = input_ids[0, -1].item()\n",
    "        moe_data['last_token'] = tokenizer.decode([moe_data['last_token_id']])\n",
    "        \n",
    "        return moe_data\n",
    "    finally:\n",
    "        # Always remove hooks\n",
    "        hook.remove_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a file of prompts and calculate cosine similarities for all layers\n",
    "def process_prompts_file(file_path, model, tokenizer, output_dir=\"cosine-sim-csv\", device=DEVICE):\n",
    "    # Check if device is specified, otherwise use global DEVICE if available\n",
    "    if device is None:\n",
    "        try:\n",
    "            device = DEVICE\n",
    "        except NameError:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract domain name from file path\n",
    "    domain_name = os.path.basename(file_path).split('.')[0]\n",
    "    \n",
    "    # Read prompts from file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        prompts = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    # Process each prompt\n",
    "    for prompt_idx, prompt in enumerate(tqdm(prompts, desc=\"Processing prompts\")):\n",
    "        # Create a unique CSV file for each prompt with domain name\n",
    "        csv_filename = f\"{domain_name}_prompt_{prompt_idx+1}_cos-sim.csv\"\n",
    "        csv_path = os.path.join(output_dir, csv_filename)\n",
    "        \n",
    "        # Write header to CSV\n",
    "        with open(csv_path, 'w', newline='') as csvfile:\n",
    "            fieldnames = ['layer', 'final_vs_top1', 'final_vs_topk', 'top1_vs_topk']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "        \n",
    "        # Get MoE data for the prompt\n",
    "        moe_data = get_last_token_moe_data(model, tokenizer, prompt)\n",
    "        \n",
    "        # Calculate cosine similarities for each layer\n",
    "        results = []\n",
    "        num_layers = 27  # Assuming 27 layers (1-27)\n",
    "        \n",
    "        for layer_idx in range(1, num_layers + 1):\n",
    "            # Skip if layer doesn't exist in the data\n",
    "            if layer_idx >= len(moe_data['layer_hidden_states']):\n",
    "                continue\n",
    "                \n",
    "            # Get hidden states for this layer\n",
    "            final_hidden_state = moe_data['layer_hidden_states'][layer_idx]\n",
    "            \n",
    "            # Get top-1 expert data\n",
    "            try:\n",
    "                top1_expert_hidden_state = moe_data['top1_expert'][layer_idx]['expert_hidden_states'][0]\n",
    "                \n",
    "                # Get combined top-k expert data\n",
    "                # Check if we need to recompute the combined top-k hidden state\n",
    "                if 'expert_hidden_states' in moe_data['topk_expert'][layer_idx] and 'expert_weights' in moe_data['topk_expert'][layer_idx]:\n",
    "                    # Get expert states and weights\n",
    "                    expert_states = moe_data['topk_expert'][layer_idx]['expert_hidden_states']\n",
    "                    expert_weights = moe_data['topk_expert'][layer_idx]['expert_weights'][0]\n",
    "                    expert_indices = moe_data['topk_expert'][layer_idx].get('expert_indices', list(range(len(expert_states))))\n",
    "                    \n",
    "                    # Compute combined top-k hidden state\n",
    "                    hidden_dim = expert_states[0].shape[-1]\n",
    "                    combined_topk = torch.zeros(hidden_dim, device=device)\n",
    "                    total_weight = 0.0\n",
    "                    \n",
    "                    for k_idx, expert_idx in enumerate(expert_indices):\n",
    "                        if k_idx in expert_states:\n",
    "                            expert_state = expert_states[k_idx]\n",
    "                            expert_weight = expert_weights[k_idx]\n",
    "                            combined_topk += expert_weight * expert_state\n",
    "                            total_weight += expert_weight\n",
    "                    \n",
    "                    # Normalize if weights don't sum to 1.0\n",
    "                    if total_weight > 0 and abs(total_weight - 1.0) > 1e-5:\n",
    "                        combined_topk /= total_weight\n",
    "                        \n",
    "                    combined_topk_hidden_state = combined_topk\n",
    "                else:\n",
    "                    # Use the pre-computed combined top-k hidden state\n",
    "                    combined_topk_hidden_state = moe_data['topk_expert'][layer_idx]['combined_topk_hidden_state'][0]\n",
    "                \n",
    "                # Normalize vectors\n",
    "                final_hidden_norm = normalize_vector(final_hidden_state)\n",
    "                top1_expert_norm = normalize_vector(top1_expert_hidden_state)\n",
    "                combined_topk_norm = normalize_vector(combined_topk_hidden_state)\n",
    "                \n",
    "                # Calculate cosine similarities\n",
    "                cos_sim_final_top1 = F.cosine_similarity(final_hidden_norm.unsqueeze(0), \n",
    "                                                        top1_expert_norm.unsqueeze(0)).item()\n",
    "                \n",
    "                cos_sim_final_topk = F.cosine_similarity(final_hidden_norm.unsqueeze(0), \n",
    "                                                        combined_topk_norm.unsqueeze(0)).item()\n",
    "                \n",
    "                cos_sim_top1_topk = F.cosine_similarity(top1_expert_norm.unsqueeze(0), \n",
    "                                                        combined_topk_norm.unsqueeze(0)).item()\n",
    "                \n",
    "                # Store results\n",
    "                results.append({\n",
    "                    'layer': layer_idx,\n",
    "                    'final_vs_top1': cos_sim_final_top1,\n",
    "                    'final_vs_topk': cos_sim_final_topk,\n",
    "                    'top1_vs_topk': cos_sim_top1_topk\n",
    "                })\n",
    "            except (KeyError, IndexError) as e:\n",
    "                print(f\"Error processing layer {layer_idx} for prompt {prompt_idx+1}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Write results to CSV\n",
    "        with open(csv_path, 'a', newline='') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writerows(results)\n",
    "        \n",
    "        print(f\"Results for prompt {prompt_idx+1} saved to {csv_path}\")\n",
    "        \n",
    "        # Clear CUDA cache if using GPU\n",
    "        if DEVICE == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Clean model memory between prompts\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        gc.collect()  # Force garbage collection\n",
    "        if DEVICE == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Processing complete. Results saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data-ext/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts: 100%|██████████| 1/1 [00:00<00:00,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for prompt 1 saved to cosine-sim-csv/test_prompt_1_cos-sim.csv\n",
      "Processing complete. Results saved to cosine-sim-csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_prompts_file(file_path, model, tokenizer, output_dir=\"cosine-sim-csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "Final vs Top1",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27
         ],
         "y": [
          0.50537109375,
          0.69970703125,
          0.716796875,
          0.7734375,
          0.740234375,
          0.84716796875,
          0.8046875,
          0.85107421875,
          0.83154296875,
          0.85009765625,
          0.89111328125,
          0.861328125,
          0.82275390625,
          0.8505859375,
          0.87255859375,
          0.916015625,
          0.91357421875,
          0.91357421875,
          0.9228515625,
          0.91943359375,
          0.92041015625,
          0.8955078125,
          0.8876953125,
          0.89453125,
          0.90087890625,
          0.78662109375,
          0.74560546875
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Final vs TopK",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27
         ],
         "y": [
          0.5942533016204834,
          0.7041589617729187,
          0.6954962015151978,
          0.7481895089149475,
          0.7528517842292786,
          0.8330900073051453,
          0.8159211277961731,
          0.8610167503356934,
          0.8453664183616638,
          0.8555467128753662,
          0.8913224339485168,
          0.8638167381286621,
          0.8713208436965942,
          0.8590719103813171,
          0.9002463221549988,
          0.9245775938034058,
          0.9097915887832642,
          0.9233456254005432,
          0.931991696357727,
          0.9289020895957948,
          0.9314561486244202,
          0.9189010858535768,
          0.9024813771247864,
          0.9150834083557128,
          0.9130779504776,
          0.7986319661140442,
          0.7676997184753418
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Top1 vs TopK",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27
         ],
         "y": [
          0.9882787466049194,
          0.9995223879814148,
          0.994404673576355,
          0.991480588912964,
          0.9946702122688292,
          0.9915620684623718,
          0.9970455169677734,
          0.9990678429603576,
          0.9959067106246948,
          0.9971661567687988,
          0.9963561296463012,
          0.9952033758163452,
          0.959775686264038,
          0.99681293964386,
          0.9924310445785522,
          0.9931782484054564,
          0.9835963249206544,
          0.9664807319641112,
          0.9923500418663024,
          0.982701539993286,
          0.9782130122184752,
          0.9674322009086608,
          0.987890601158142,
          0.9762208461761476,
          0.98472261428833,
          0.97521710395813,
          0.9726881980895996
         ]
        }
       ],
       "layout": {
        "hovermode": "x unified",
        "legend": {
         "title": {
          "text": "Metrics"
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Cosine Similarities Across Layers for Prompt 1"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "tickmode": "array",
         "tickvals": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27
         ],
         "title": {
          "text": "Layer"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Cosine Similarity"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_cosine_similarities(csv_file, prompt_num=1):\n",
    "    \"\"\"\n",
    "    Plot cosine similarities from a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file containing cosine similarity data\n",
    "        prompt_num (int): Prompt number for the title\n",
    "    \n",
    "    Returns:\n",
    "        plotly.graph_objects.Figure: The generated figure\n",
    "    \"\"\"\n",
    "    # Load the CSV data\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Create a plotly figure\n",
    "    fig = make_subplots()\n",
    "    \n",
    "    # Add traces for each metric\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df['layer'], \n",
    "            y=df['final_vs_top1'], \n",
    "            mode='lines+markers',\n",
    "            name='Final vs Top1'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df['layer'], \n",
    "            y=df['final_vs_topk'], \n",
    "            mode='lines+markers',\n",
    "            name='Final vs TopK'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df['layer'], \n",
    "            y=df['top1_vs_topk'], \n",
    "            mode='lines+markers',\n",
    "            name='Top1 vs TopK'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f'Cosine Similarities Across Layers for Prompt {prompt_num}',\n",
    "        xaxis_title='Layer',\n",
    "        yaxis_title='Cosine Similarity',\n",
    "        legend_title='Metrics',\n",
    "        hovermode='x unified',\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    # Ensure x-axis shows all layer numbers\n",
    "    fig.update_xaxes(tickmode='array', tickvals=df['layer'])\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Example usage\n",
    "csv_file = 'cosine-sim-csv/prompt_1_cosine_similarities.csv'\n",
    "fig = plot_cosine_similarities(csv_file)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
