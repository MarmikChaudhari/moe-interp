{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        # use_flash_attention_2=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model(\"deepseek-ai/deepseek-moe-16b-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_tokens(hidden_states: torch.Tensor, lm_head: torch.nn.Linear, tokenizer, k: int = 5) -> List[Tuple[str, float]]:\n",
    "    \"\"\" get topk tokens from hidden states using lm head \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Ensure hidden_states has at least 2 dimensions (batch_size, num_tokens, hidden_dim)\n",
    "        if hidden_states.dim() == 2:\n",
    "            hidden_states = hidden_states.unsqueeze(0)  # Add batch dimension\n",
    "        logits = lm_head(hidden_states)  # (batch_size, num_tokens, vocab_size)\n",
    "    \n",
    "    # Get top-k tokens\n",
    "    scores, token_ids = torch.topk(logits, k=k, dim=-1)  # (batch_size, num_tokens, k)\n",
    "    \n",
    "    # Decode tokens and collect results for each position\n",
    "    results = []\n",
    "    for pos in range(scores.size(1)):  # Iterate over token positions\n",
    "        pos_results = []\n",
    "        for i in range(k):\n",
    "            token = tokenizer.decode(token_ids[0, pos, i])  # Decode token for this position\n",
    "            score = scores[0, pos, i].item()  # Get score for this position\n",
    "            pos_results.append((token, score))\n",
    "        results.append(pos_results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepseekLayerAnalyzer:\n",
    "    \"\"\" Analyzes the behavior of a DeepSeek MoE model by capturing and analyzing outputs from different layers and experts.\n",
    "    \n",
    "    Args:\n",
    "        model: The DeepSeek MoE model to analyze\n",
    "        tokenizer: The tokenizer associated with the model\n",
    "        \n",
    "    Attributes:\n",
    "        layer_outputs (defaultdict): Stores outputs from each model layer\n",
    "        moe_gate_outputs (defaultdict): Stores gate outputs from MoE layers\n",
    "        moe_combined_outputs (defaultdict): Stores combined outputs after expert computation\n",
    "        expert_outputs (defaultdict): Stores individual expert outputs per layer and position\n",
    "        shared_expert_outputs (defaultdict): Stores outputs from shared experts if present\n",
    "        hooks (list): List of registered PyTorch hooks\n",
    "        \n",
    "    Methods:\n",
    "        register_hooks(): Sets up hooks to capture layer and expert outputs\n",
    "        analyze_tokens(input_ids, return_hidden_states): Runs inference and analyzes token behavior\n",
    "        cleanup(): Removes all registered hooks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.layer_outputs = defaultdict(list)\n",
    "        self.moe_gate_outputs = defaultdict(list)\n",
    "        self.moe_combined_outputs = defaultdict(list)\n",
    "        self.expert_outputs = defaultdict(lambda: defaultdict(list))\n",
    "        self.shared_expert_outputs = defaultdict(list)\n",
    "        self.hooks = []        \n",
    "    def register_hooks(self):\n",
    "        \"\"\"Register hooks for layer outputs and MoE combination points\"\"\"\n",
    "        \n",
    "        def layer_output_hook(layer_idx):\n",
    "            def hook(module, inputs, outputs):\n",
    "                \"\"\"Hook for capturing layer outputs\"\"\"\n",
    "                hidden_states = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "                self.layer_outputs[layer_idx].append(hidden_states.detach())\n",
    "            return hook\n",
    "\n",
    "        def moe_gate_hook(layer_idx):\n",
    "            def hook(module, inputs, outputs):\n",
    "                \"\"\"Hook for capturing MoE gate outputs before expert computation\"\"\"\n",
    "                # Capture topk_idx, topk_weight, and aux_loss from gate outputs\n",
    "                if isinstance(outputs, tuple):\n",
    "                    topk_idx, topk_weight, aux_loss = outputs\n",
    "                    self.moe_gate_outputs[layer_idx].append({\n",
    "                        'topk_idx': topk_idx.detach(),\n",
    "                        'topk_weight': topk_weight.detach(),\n",
    "                        'aux_loss': aux_loss.detach() if aux_loss is not None else None\n",
    "                    })\n",
    "            return hook\n",
    "\n",
    "        def expert_hook(layer_idx, expert_idx):\n",
    "            def hook(module, inputs, outputs):\n",
    "                \"\"\"Hook for capturing expert outputs\"\"\"\n",
    "                # Get the latest gate outputs for this layer\n",
    "                if not self.moe_gate_outputs[layer_idx]:\n",
    "                    return print(f'no gate outputs for layer {layer_idx}')\n",
    "            \n",
    "                gate_data = self.moe_gate_outputs[layer_idx][-1]\n",
    "                \n",
    "                # Handle 2D or 3D tensor shapes\n",
    "                if len(gate_data['topk_idx'].shape) == 2:\n",
    "                    batch_size = 1\n",
    "                    seq_len, top_k = gate_data['topk_idx'].shape\n",
    "                else:\n",
    "                    batch_size, seq_len, top_k = gate_data['topk_idx'].shape\n",
    "                \n",
    "                # Get mask for tokens where this expert was selected\n",
    "                expert_mask = (gate_data['topk_idx'] == expert_idx)                \n",
    "                # Flatten and find positions where this expert was selected\n",
    "                selected_positions = torch.nonzero(expert_mask, as_tuple=True)\n",
    "                # If no tokens selected this expert, skip\n",
    "                if selected_positions[0].numel() == 0:\n",
    "                    return\n",
    "                    \n",
    "                # Get the actual inputs routed to this expert\n",
    "                # Inputs[0] shape: (total_selected_tokens, hidden_dim)\n",
    "                total_selected = inputs[0].shape[0] \n",
    "                # Validate we're processing the correct number of tokens\n",
    "                expected_selected = expert_mask.sum().item()\n",
    "                if total_selected != expected_selected:\n",
    "                    print(f\" expert {expert_idx} processed {total_selected} tokens but expected {expected_selected}\")\n",
    "                    return\n",
    "                    \n",
    "                # Get the full hidden states from outputs\n",
    "                # outputs shape: (total_selected_tokens, hidden_dim)\n",
    "                hidden_states = outputs\n",
    "                if isinstance(outputs, tuple):\n",
    "                    hidden_states = outputs[0]\n",
    "                    \n",
    "                # Record data for each selected position\n",
    "                for pos_idx, pos in enumerate(selected_positions[0]):\n",
    "                    token_data = {\n",
    "                        'position': pos.item(),\n",
    "                        'input': inputs[0][pos_idx].detach(),\n",
    "                        'output': outputs[pos_idx].detach(),\n",
    "                        'hidden_state': hidden_states[pos_idx].detach()  # Store full hidden state\n",
    "                    }\n",
    "                    \n",
    "                    # Get the corresponding gate weight for this position\n",
    "                    # Find which expert slot (in top_k) this expert was selected for this position\n",
    "                    expert_slots = (gate_data['topk_idx'][pos.item()] == expert_idx).nonzero(as_tuple=True)[0]\n",
    "                    if len(expert_slots) > 0:\n",
    "                        token_data['gate_weight'] = gate_data['topk_weight'][pos.item()][expert_slots[0]].item()\n",
    "                    \n",
    "                    self.expert_outputs[layer_idx][expert_idx].append(token_data)\n",
    "            return hook\n",
    "        \n",
    "        def shared_expert_hook(layer_idx):\n",
    "            def hook(module, inputs, outputs):\n",
    "                \"\"\"Hook for capturing shared expert outputs\"\"\"\n",
    "                self.shared_expert_outputs[layer_idx].append({\n",
    "                    'input': inputs[0].detach(),\n",
    "                    'output': outputs.detach()\n",
    "                })\n",
    "            return hook\n",
    "\n",
    "        def moe_combine_hook(layer_idx):\n",
    "            def hook(module, inputs, outputs):\n",
    "                \"\"\"Hook for capturing final combined MoE outputs\"\"\"\n",
    "                # For DeepseekMoE, this captures the weighted sum of expert outputs\n",
    "                self.moe_combined_outputs[layer_idx].append({\n",
    "                    'combined_output': outputs.detach(),\n",
    "                    'input': inputs[0].detach()  # Original input before MoE\n",
    "                })\n",
    "            return hook\n",
    "\n",
    "        # Register hooks for each layer\n",
    "        for layer_idx, layer in enumerate(self.model.model.layers):\n",
    "            # Hook for layer output\n",
    "            hook = layer.register_forward_hook(layer_output_hook(layer_idx))\n",
    "            self.hooks.append(hook)\n",
    "            \n",
    "            # If it's an MoE layer, add MoE-specific hooks\n",
    "            if hasattr(layer.mlp, 'experts'):\n",
    "                # Hook for gate mechanism\n",
    "                gate_hook = layer.mlp.gate.register_forward_hook(moe_gate_hook(layer_idx))\n",
    "                self.hooks.append(gate_hook)\n",
    "                \n",
    "                # Hook for each expert\n",
    "                for expert_idx, expert in enumerate(layer.mlp.experts):\n",
    "                    expert_hook_fn = expert.register_forward_hook(expert_hook(layer_idx, expert_idx))\n",
    "                    self.hooks.append(expert_hook_fn)\n",
    "\n",
    "                # Hook for shared expert if it exists\n",
    "                if hasattr(layer.mlp, 'shared_experts'):\n",
    "                    shared_hook = layer.mlp.shared_experts.register_forward_hook(shared_expert_hook(layer_idx))\n",
    "                    self.hooks.append(shared_hook)\n",
    "                \n",
    "                # Hook for final combined output\n",
    "                combine_hook = layer.mlp.register_forward_hook(moe_combine_hook(layer_idx))\n",
    "                self.hooks.append(combine_hook)\n",
    "\n",
    "    def analyze_tokens(self, input_ids: torch.Tensor, return_hidden_states: bool = False) -> Dict:\n",
    "        \"\"\" run inference and analyze tokens at each layer and expert combination point \"\"\"\n",
    "\n",
    "        self.moe_combined_outputs.clear()\n",
    "        self.expert_outputs.clear()\n",
    "        self.shared_expert_outputs.clear()\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids)\n",
    "        \n",
    "        results = {\n",
    "            'layer_predictions': {},\n",
    "            'moe_analysis': {},\n",
    "            'hidden_states': {} if return_hidden_states else None\n",
    "        }\n",
    "        \n",
    "        # Analyze layer outputs\n",
    "        for layer_idx, outputs in self.layer_outputs.items():\n",
    "            if not outputs:  # Skip if no outputs captured\n",
    "                continue\n",
    "            hidden_states = outputs[-1]  # Get last captured output\n",
    "            \n",
    "            # Get token predictions for this layer\n",
    "            top_tokens = get_top_k_tokens(hidden_states, self.model.lm_head, self.tokenizer)\n",
    "            results['layer_predictions'][layer_idx] = top_tokens\n",
    "            \n",
    "            if return_hidden_states:\n",
    "                results['hidden_states'][f'layer_{layer_idx}'] = hidden_states\n",
    "        \n",
    "        # Analyze MoE layers\n",
    "        for layer_idx in self.moe_gate_outputs.keys():\n",
    "            if not self.moe_gate_outputs[layer_idx]:\n",
    "                continue\n",
    "                \n",
    "            gate_data = self.moe_gate_outputs[layer_idx][-1]  # Get last captured data\n",
    "            combined_data = self.moe_combined_outputs[layer_idx][-1]\n",
    "            \n",
    "            # Initialize predictions dictionary by position\n",
    "            expert_predictions_by_pos = defaultdict(dict)\n",
    "            expert_hidden_states_by_pos = defaultdict(dict)\n",
    "            \n",
    "            # Process expert outputs by position\n",
    "            for expert_idx, data_list in self.expert_outputs[layer_idx].items():\n",
    "                for data in data_list:\n",
    "                    position = data['position']\n",
    "                    # Get predictions for this expert's output at this position\n",
    "                    predictions = get_top_k_tokens(\n",
    "                        data['output'].unsqueeze(0),\n",
    "                        self.model.lm_head,\n",
    "                        self.tokenizer\n",
    "                    )\n",
    "                    expert_predictions_by_pos[position][expert_idx] = predictions[0]  # [0] because we only have one prediction set\n",
    "\n",
    "                    # Store hidden states\n",
    "                    expert_hidden_states_by_pos[position][expert_idx] = {\n",
    "                    'hidden_state': data['hidden_state'].tolist(),\n",
    "                    'gate_weight': data.get('gate_weight', None)\n",
    "                }\n",
    "            \n",
    "            # Get predictions for shared expert if it exists\n",
    "            if self.shared_expert_outputs[layer_idx]:\n",
    "                shared_expert_predictions = get_top_k_tokens(\n",
    "                    self.shared_expert_outputs[layer_idx][-1]['output'],\n",
    "                    self.model.lm_head,\n",
    "                    self.tokenizer\n",
    "                )\n",
    "            \n",
    "            # Update experts_analysis dictionary to include hidden states\n",
    "            experts_analysis = {\n",
    "            'selected_experts': gate_data['topk_idx'].tolist(),\n",
    "            'expert_weights': gate_data['topk_weight'].tolist(),\n",
    "            'aux_loss': gate_data['aux_loss'].item() if gate_data['aux_loss'] is not None else None,\n",
    "            'expert_predictions_by_position': dict(expert_predictions_by_pos),\n",
    "            'expert_hidden_states_by_position': dict(expert_hidden_states_by_pos),\n",
    "            'shared_expert_predictions': shared_expert_predictions if self.shared_expert_outputs[layer_idx] else None\n",
    "        }\n",
    "            \n",
    "            # Get token predictions from combined output\n",
    "            combined_tokens = get_top_k_tokens(\n",
    "                combined_data['combined_output'], \n",
    "                self.model.lm_head,\n",
    "                self.tokenizer\n",
    "            )\n",
    "            \n",
    "            experts_analysis['combined_output_tokens'] = combined_tokens\n",
    "            results['moe_analysis'][layer_idx] = experts_analysis\n",
    "\n",
    "        return results\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"remove all registered hooks\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks.clear()\n",
    "\n",
    "def analyze_deepseek_moe(model, tokenizer, input_text: str, return_hidden_states: bool = False):\n",
    "    \"\"\" analyze DeepSeek MoE model behavior for given input text \"\"\"\n",
    "    analyzer = DeepseekLayerAnalyzer(model, tokenizer)\n",
    "    analyzer.register_hooks()\n",
    "    \n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    try:\n",
    "        results = analyzer.analyze_tokens(input_ids, return_hidden_states=return_hidden_states)\n",
    "        return results\n",
    "    finally:\n",
    "        analyzer.cleanup()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prompt and tokenize\n",
    "prompt = \"translate the following text to french: The best is yet to come = Le meilleur reste à\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "n = input_ids.shape[1]\n",
    "token_pos = n-1  # Last token\n",
    "\n",
    "analysis = analyze_deepseek_moe(model, tokenizer, prompt, return_hidden_states=True)\n",
    "\n",
    "# Create analyzer and get hidden states\n",
    "analyzer = DeepseekLayerAnalyzer(model, tokenizer)\n",
    "analyzer.register_hooks()\n",
    "\n",
    "\n",
    "results = analyzer.analyze_tokens(input_ids, return_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_attn_ln_inputs(model, tokenizer, text):\n",
    "    \"\"\"places a hook on the post attention layernorm to retrieve its inputs\"\"\"\n",
    "    # Store inputs in a dict mapping layer idx -> inputs\n",
    "    post_attn_ln_inputs = {}\n",
    "    hooks = []\n",
    "    \n",
    "    def hook_post_attn_ln(module, input, output, layer_idx):\n",
    "        if layer_idx not in post_attn_ln_inputs:\n",
    "            post_attn_ln_inputs[layer_idx] = []\n",
    "        post_attn_ln_inputs[layer_idx].append([x.detach() for x in input])\n",
    "    \n",
    "    # Register hooks on post attention layernorm for each layer\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        hooks.append(\n",
    "            layer.post_attention_layernorm.register_forward_hook(\n",
    "                lambda m, i, o, idx=i: hook_post_attn_ln(m, i, o, idx)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        # Run inference\n",
    "        input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "        model(input_ids)\n",
    "        \n",
    "        return post_attn_ln_inputs\n",
    "        \n",
    "    finally:\n",
    "        # Clean up hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_attn_ln_inputs = get_post_attn_ln_inputs(model, tokenizer, text=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expert_hidden_states_by_weight(analysis, post_attn_ln_inputs, token_pos, k=0):\n",
    "    \"\"\"Gets hidden states (with residual added) from the k-th highest weighted expert for all MoE layers\"\"\"\n",
    "    hidden_states_by_layer = {}\n",
    "    \n",
    "    # For each layer that has MoE\n",
    "    for layer_idx in analysis['moe_analysis'].keys():\n",
    "        moe_analysis = analysis['moe_analysis'][layer_idx]\n",
    "        \n",
    "        # Get expert weights for this token\n",
    "        expert_weights = {}\n",
    "        selected_experts = moe_analysis['selected_experts'][token_pos]\n",
    "        expert_weights_list = moe_analysis['expert_weights'][token_pos]\n",
    "        \n",
    "        # Map experts to their weights\n",
    "        for expert_idx, weight in zip(selected_experts, expert_weights_list):\n",
    "            expert_weights[expert_idx] = weight\n",
    "            \n",
    "        # Sort by weight and get k-th highest\n",
    "        sorted_experts = sorted(expert_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "        if k >= len(sorted_experts):\n",
    "            print(f\"Warning: Layer {layer_idx} only has {len(sorted_experts)} experts, but k={k} requested\")\n",
    "            continue\n",
    "            \n",
    "        target_expert = sorted_experts[k][0]  # Get the k-th expert's id\n",
    "        \n",
    "        # Get hidden state for this expert\n",
    "        expert_hidden_state = moe_analysis['expert_hidden_states_by_position'][token_pos][target_expert]['hidden_state']\n",
    "        \n",
    "        # Get residual stream for this token\n",
    "        try:\n",
    "            residual = post_attn_ln_inputs[layer_idx][-1][0][0][token_pos]  # Added [0] index\n",
    "        except (KeyError, IndexError) as e:\n",
    "            print(f\"Warning: Could not get residual for layer {layer_idx}, skipping. Error: {e}\")\n",
    "            continue\n",
    "            \n",
    "        # Convert to tensor if needed\n",
    "        if isinstance(expert_hidden_state, list):\n",
    "            expert_hidden_state = torch.tensor(expert_hidden_state, dtype=torch.float16)\n",
    "            \n",
    "        # Add residual\n",
    "        combined = residual + expert_hidden_state\n",
    "        hidden_states_by_layer[layer_idx] = combined\n",
    "        \n",
    "    return hidden_states_by_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of final hidden state : tensor([-2.1328, -2.8848,  0.5552,  ...,  1.1221, -2.6934, -1.1260],\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# Get final layer index and hidden state\n",
    "final_layer_idx = max([int(k) for k in results[\"layer_predictions\"].keys()])\n",
    "final_hidden_state = analyzer.layer_outputs[final_layer_idx][-1][0][token_pos]\n",
    "\n",
    "print(\"Shape of final hidden state :\", final_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_states : {1: tensor([ 0.0231,  0.0817, -0.1342,  ..., -0.0058, -0.0729, -0.0549],\n",
      "       dtype=torch.float16), 2: tensor([ 0.1169,  0.1069, -0.2629,  ...,  0.0451, -0.1361, -0.0201],\n",
      "       dtype=torch.float16), 3: tensor([ 0.1360,  0.1041, -0.3918,  ...,  0.1637, -0.0724, -0.0071],\n",
      "       dtype=torch.float16), 4: tensor([ 0.2000,  0.1331, -0.7676,  ...,  0.1451, -0.1857, -0.0319],\n",
      "       dtype=torch.float16), 5: tensor([-0.0033,  0.0835, -0.7563,  ...,  0.2800, -0.2732,  0.0765],\n",
      "       dtype=torch.float16), 6: tensor([-0.1963,  0.1505, -0.6743,  ...,  0.3457, -0.3904,  0.1470],\n",
      "       dtype=torch.float16), 7: tensor([-0.1293,  0.2632, -0.8887,  ...,  0.3867, -0.1835, -0.1223],\n",
      "       dtype=torch.float16), 8: tensor([ 0.0015,  0.3374, -0.9141,  ...,  0.2864, -0.3770,  0.1615],\n",
      "       dtype=torch.float16), 9: tensor([-0.0732,  0.2460, -0.8462,  ...,  0.4705, -0.7827,  0.3425],\n",
      "       dtype=torch.float16), 10: tensor([ 0.0711,  0.2905, -0.9800,  ...,  0.3428, -0.2551,  0.2808],\n",
      "       dtype=torch.float16), 11: tensor([ 0.0977,  0.1664, -1.1172,  ...,  0.3318, -0.3962,  0.3608],\n",
      "       dtype=torch.float16), 12: tensor([ 0.2512,  0.2498, -1.2188,  ...,  0.4602, -0.2859,  0.3843],\n",
      "       dtype=torch.float16), 13: tensor([ 0.2363, -0.0282, -1.2207,  ...,  0.4402, -0.3601,  0.4956],\n",
      "       dtype=torch.float16), 14: tensor([ 0.2778,  0.0262, -1.4062,  ...,  0.5562, -0.4226,  0.7183],\n",
      "       dtype=torch.float16), 15: tensor([ 0.3599,  0.1049, -1.7168,  ...,  0.3838, -0.5845,  0.5518],\n",
      "       dtype=torch.float16), 16: tensor([-0.0472, -0.4758, -1.9199,  ...,  0.3643,  0.0320,  0.1895],\n",
      "       dtype=torch.float16), 17: tensor([ 0.2271, -0.2314, -1.8213,  ..., -0.0863, -0.1490,  0.0887],\n",
      "       dtype=torch.float16), 18: tensor([-0.7017, -0.3013, -1.7979,  ...,  0.3896, -0.2930,  0.0118],\n",
      "       dtype=torch.float16), 19: tensor([-1.0059, -1.0234, -1.3340,  ...,  0.2876, -0.3386,  0.1338],\n",
      "       dtype=torch.float16), 20: tensor([-0.8384, -1.7705, -1.8447,  ...,  0.4639,  0.0667,  0.1636],\n",
      "       dtype=torch.float16), 21: tensor([-0.9824, -2.4727, -2.2363,  ..., -0.2021,  0.5742,  0.1231],\n",
      "       dtype=torch.float16), 22: tensor([-0.5723, -2.7148, -1.0449,  ..., -0.0682,  1.1504, -0.2039],\n",
      "       dtype=torch.float16), 23: tensor([-0.1890, -3.5293,  0.6245,  ...,  1.5381,  0.3857,  0.9360],\n",
      "       dtype=torch.float16), 24: tensor([ 0.1215, -3.5918,  0.2125,  ...,  1.1338,  0.1829,  1.2129],\n",
      "       dtype=torch.float16), 25: tensor([ 0.2549, -3.4766,  1.4805,  ...,  0.9580, -0.5869,  0.9561],\n",
      "       dtype=torch.float16), 26: tensor([ 0.2866, -3.9922,  2.1172,  ...,  1.5879, -1.7480,  0.9316],\n",
      "       dtype=torch.float16), 27: tensor([-1.0469, -3.8672,  0.8701,  ...,  0.6934, -1.1338,  0.9082],\n",
      "       dtype=torch.float16)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n = len(tokenizer.encode(prompt))\n",
    "token_pos = n-1  # Last token\n",
    "k = 0  # Get highest weighted expert\n",
    "\n",
    "hidden_states = get_expert_hidden_states_by_weight(analysis,\n",
    "                                                   post_attn_ln_inputs=post_attn_ln_inputs,\n",
    "                                                   token_pos=token_pos,\n",
    "                                                   k=k)\n",
    "\n",
    "print(f'hidden_states : {hidden_states}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 similarity: 0.0064\n",
      "Layer 2 similarity: 0.0087\n",
      "Layer 3 similarity: 0.0131\n",
      "Layer 4 similarity: 0.0206\n",
      "Layer 5 similarity: 0.0040\n",
      "Layer 6 similarity: -0.0043\n",
      "Layer 7 similarity: 0.0240\n",
      "Layer 8 similarity: 0.0075\n",
      "Layer 9 similarity: 0.0093\n",
      "Layer 10 similarity: -0.0001\n",
      "Layer 11 similarity: 0.0196\n",
      "Layer 12 similarity: 0.0048\n",
      "Layer 13 similarity: 0.0221\n",
      "Layer 14 similarity: 0.0418\n",
      "Layer 15 similarity: 0.0453\n",
      "Layer 16 similarity: 0.0729\n",
      "Layer 17 similarity: 0.0902\n",
      "Layer 18 similarity: 0.1382\n",
      "Layer 19 similarity: 0.1611\n",
      "Layer 20 similarity: 0.1864\n",
      "Layer 21 similarity: 0.2150\n",
      "Layer 22 similarity: 0.2467\n",
      "Layer 23 similarity: 0.3972\n",
      "Layer 24 similarity: 0.4072\n",
      "Layer 25 similarity: 0.4253\n",
      "Layer 26 similarity: 0.5107\n",
      "Layer 27 similarity: 0.7437\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# cosine similarity for each layer\n",
    "sim = {}\n",
    "for layer_idx, layer_hidden in hidden_states.items():\n",
    "    # Normalize both vectors for cosine similarity\n",
    "    similarity = F.cosine_similarity(final_hidden_state.unsqueeze(0),\n",
    "                                   layer_hidden.unsqueeze(0))\n",
    "    sim[layer_idx] = similarity.item()\n",
    "\n",
    "# Print results sorted by layer\n",
    "for layer_idx in sorted(sim.keys()):\n",
    "    print(f\"Layer {layer_idx} similarity: {sim[layer_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "<b>Layer %{x}</b><br>Cosine Similarity: %{y:.4f}<br><extra></extra>",
         "line": {
          "width": 2
         },
         "marker": {
          "size": 8
         },
         "mode": "lines+markers",
         "name": "Expert rank 0",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27
         ],
         "y": [
          0.00641632080078125,
          0.00872802734375,
          0.0130767822265625,
          0.0205841064453125,
          0.003955841064453125,
          -0.004302978515625,
          0.0240478515625,
          0.007526397705078125,
          0.00931549072265625,
          -0.00009936094284057617,
          0.0196075439453125,
          0.004779815673828125,
          0.0220947265625,
          0.041778564453125,
          0.045257568359375,
          0.0728759765625,
          0.0902099609375,
          0.13818359375,
          0.1611328125,
          0.1864013671875,
          0.2149658203125,
          0.2467041015625,
          0.397216796875,
          0.4072265625,
          0.42529296875,
          0.5107421875,
          0.74365234375
         ]
        }
       ],
       "layout": {
        "height": 500,
        "hovermode": "x unified",
        "plot_bgcolor": "white",
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Cosine Similarity with Final Hidden State<br><sup>Using 1th highest weighted expert per layer</sup>",
         "x": 0.5,
         "y": 0.95
        },
        "width": 500,
        "xaxis": {
         "dtick": 1,
         "gridcolor": "rgba(128,128,128,0.2)",
         "gridwidth": 1,
         "range": [
          0,
          27.5
         ],
         "showgrid": true,
         "tickmode": "linear",
         "title": {
          "text": "Layer"
         }
        },
        "yaxis": {
         "gridcolor": "rgba(128,128,128,0.2)",
         "gridwidth": 1,
         "range": [
          -0.2,
          1
         ],
         "showgrid": true,
         "title": {
          "text": "Cosine Similarity"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_cosine_similarities(similarities, k):\n",
    "    \"\"\"\n",
    "    Create an interactive line plot of cosine similarities across layers.\n",
    "    \n",
    "    Args:\n",
    "    similarities: dict with layer_idx -> cosine_similarity_value\n",
    "    k: which expert (by weight rank) was used\n",
    "    \"\"\"\n",
    "    # Set plot size\n",
    "    plot_size = 500  # Size in pixels for both width and height\n",
    "    \n",
    "    # Sort layers for x-axis\n",
    "    layers = sorted(similarities.keys())\n",
    "    sim_values = [similarities[layer] for layer in layers]\n",
    "    \n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add line plot\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=layers,\n",
    "        y=sim_values,\n",
    "        mode='lines+markers',\n",
    "        name=f'Expert rank {k}',\n",
    "        hovertemplate=\n",
    "        \"<b>Layer %{x}</b><br>\" +\n",
    "        \"Cosine Similarity: %{y:.4f}<br>\" +\n",
    "        \"<extra></extra>\",  # Removes secondary box\n",
    "        line=dict(width=2),\n",
    "        marker=dict(size=8)\n",
    "    ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=f'Cosine Similarity with Final Hidden State<br><sup>Using {k+1}th highest weighted expert per layer</sup>',\n",
    "            x=0.5,  # Center title\n",
    "            y=0.95\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title='Layer',\n",
    "            gridcolor='rgba(128,128,128,0.2)',\n",
    "            tickmode='linear',\n",
    "            dtick=1,  # Show every layer number\n",
    "            range=[0, 27.5]  # Set x-axis range from 0 to 27\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='Cosine Similarity',\n",
    "            gridcolor='rgba(128,128,128,0.2)',\n",
    "            range=[-0.2, 1]  # Updated range for cosine similarity\n",
    "        ),\n",
    "        plot_bgcolor='white',\n",
    "        hovermode='x unified',  # Shows all points at a given x-coordinate\n",
    "        showlegend=False,\n",
    "        width=plot_size,\n",
    "        height=plot_size\n",
    "    )\n",
    "    \n",
    "    # Add grid\n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1)\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Use the function\n",
    "fig = plot_cosine_similarities(sim, k)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
