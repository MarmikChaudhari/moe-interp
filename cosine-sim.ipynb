{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "import glob\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "if DEVICE.type == \"cuda\":\n",
    "    # Print CUDA details\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated()/1024**2:.2f}MB\")\n",
    "    print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved()/1024**2:.2f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        # use_flash_attention_2=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model(\"deepseek-ai/deepseek-moe-16b-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_tokens(hidden_states: torch.Tensor, lm_head: torch.nn.Linear, tokenizer, k: int = 5) -> List[Tuple[str, float]]:\n",
    "    \"\"\" get topk tokens from hidden states using lm head \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Move tensors to device if needed\n",
    "        if hidden_states.device != DEVICE:\n",
    "            hidden_states = hidden_states.to(DEVICE)\n",
    "        if next(lm_head.parameters()).device != DEVICE:\n",
    "            lm_head = lm_head.to(DEVICE)\n",
    "        \n",
    "        # Ensure hidden_states has at least 2 dimensions (batch_size, num_tokens, hidden_dim)\n",
    "        if hidden_states.dim() == 2:\n",
    "            hidden_states = hidden_states.unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "        # Compute logits in chunks to avoid OOM\n",
    "        chunk_size = 128 if DEVICE.type == \"cuda\" else 32  # Smaller chunks for CPU\n",
    "        num_chunks = (hidden_states.size(1) + chunk_size - 1) // chunk_size\n",
    "        all_logits = []\n",
    "        \n",
    "        for i in range(num_chunks):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = min((i + 1) * chunk_size, hidden_states.size(1))\n",
    "            chunk = hidden_states[:, start_idx:end_idx]\n",
    "            chunk_logits = lm_head(chunk)\n",
    "            all_logits.append(chunk_logits)\n",
    "            \n",
    "            # Free memory after each chunk if using CUDA\n",
    "            if DEVICE.type == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "        logits = torch.cat(all_logits, dim=1)  # (batch_size, num_tokens, vocab_size)\n",
    "    \n",
    "        # Get top-k tokens\n",
    "        scores, token_ids = torch.topk(logits, k=k, dim=-1)  # (batch_size, num_tokens, k)\n",
    "    \n",
    "        # Move to CPU for decoding\n",
    "        scores = scores.cpu()\n",
    "        token_ids = token_ids.cpu()\n",
    "        \n",
    "        # Free GPU memory\n",
    "        if DEVICE.type == \"cuda\":\n",
    "            del logits, all_logits\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "        # Decode tokens and collect results for each position\n",
    "        results = []\n",
    "        for pos in range(scores.size(1)):  # Iterate over token positions\n",
    "            pos_results = []\n",
    "            for i in range(k):\n",
    "                token = tokenizer.decode(token_ids[0, pos, i])  # Decode token for this position\n",
    "                score = scores[0, pos, i].item()  # Get score for this position\n",
    "                pos_results.append((token, score))\n",
    "            results.append(pos_results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepseekLayerAnalyzer:\n",
    "    \"\"\" Analyzes the behavior of a DeepSeek MoE model by capturing and analyzing outputs from different layers and experts.\n",
    "    \n",
    "    Args:\n",
    "        model: The DeepSeek MoE model to analyze\n",
    "        tokenizer: The tokenizer associated with the model\n",
    "        device: Device to run analysis on ('cuda' or 'cpu')\n",
    "        \n",
    "    Attributes:\n",
    "        layer_outputs (defaultdict): Stores outputs from each model layer\n",
    "        moe_gate_outputs (defaultdict): Stores gate outputs from MoE layers\n",
    "        moe_combined_outputs (defaultdict): Stores combined outputs after computation\n",
    "        expert_outputs (defaultdict): Stores individual expert outputs per layer and position\n",
    "        shared_expert_outputs (defaultdict): Stores outputs from shared experts if present\n",
    "        hooks (list): List of registered PyTorch hooks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device='cpu'):\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.hooks = []\n",
    "        self.reset_state()\n",
    "        \n",
    "        # Check GPU memory if using CUDA\n",
    "        if self.device == 'cuda' and torch.cuda.is_available():\n",
    "            self.gpu_mem = torch.cuda.get_device_properties(0).total_memory\n",
    "        \n",
    "    def reset_state(self):\n",
    "        \"\"\"Clear all stored state between runs and free GPU memory\"\"\"\n",
    "        self.layer_outputs = defaultdict(list)\n",
    "        self.moe_gate_outputs = defaultdict(list)\n",
    "        self.moe_combined_outputs = defaultdict(list)\n",
    "        self.expert_outputs = defaultdict(lambda: defaultdict(list))\n",
    "        self.shared_expert_outputs = defaultdict(list)\n",
    "        \n",
    "        # Clear CUDA cache if using GPU\n",
    "        if self.device == 'cuda' and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    def register_hooks(self):\n",
    "        \"\"\"Register hooks for layer outputs and MoE combination points\"\"\"\n",
    "        \n",
    "        def layer_output_hook(layer_idx):\n",
    "            def hook(module, inputs, outputs):\n",
    "                \"\"\"Hook for capturing layer outputs\"\"\"\n",
    "                hidden_states = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "                # Store on device\n",
    "                self.layer_outputs[layer_idx].append(hidden_states.detach())\n",
    "            return hook\n",
    "\n",
    "        def moe_gate_hook(layer_idx):\n",
    "            def hook(module, inputs, outputs):\n",
    "                \"\"\"Hook for capturing MoE gate outputs before expert computation\"\"\"\n",
    "                # Capture topk_idx, topk_weight, and aux_loss from gate outputs\n",
    "                if isinstance(outputs, tuple):\n",
    "                    topk_idx, topk_weight, aux_loss = outputs\n",
    "                    self.moe_gate_outputs[layer_idx].append({\n",
    "                        'topk_idx': topk_idx.detach(),\n",
    "                        'topk_weight': topk_weight.detach(),\n",
    "                        'aux_loss': aux_loss.detach() if aux_loss is not None else None\n",
    "                    })\n",
    "            return hook\n",
    "\n",
    "        def expert_hook(layer_idx, expert_idx):\n",
    "            def hook(module, inputs, outputs):\n",
    "                \"\"\"Hook for capturing expert outputs\"\"\"\n",
    "                # Get the latest gate outputs for this layer\n",
    "                if not self.moe_gate_outputs[layer_idx]:\n",
    "                    return print(f'no gate outputs for layer {layer_idx}')\n",
    "            \n",
    "                gate_data = self.moe_gate_outputs[layer_idx][-1]\n",
    "                \n",
    "                # Handle 2D or 3D tensor shapes\n",
    "                if len(gate_data['topk_idx'].shape) == 2:\n",
    "                    batch_size = 1\n",
    "                    seq_len, top_k = gate_data['topk_idx'].shape\n",
    "                else:\n",
    "                    batch_size, seq_len, top_k = gate_data['topk_idx'].shape\n",
    "                \n",
    "                # Get mask for tokens where this expert was selected\n",
    "                expert_mask = (gate_data['topk_idx'] == expert_idx)                \n",
    "                # Flatten and find positions where this expert was selected\n",
    "                selected_positions = torch.nonzero(expert_mask, as_tuple=True)\n",
    "                # If no tokens selected this expert, skip\n",
    "                if selected_positions[0].numel() == 0:\n",
    "                    return\n",
    "                    \n",
    "                # Get the actual inputs routed to this expert\n",
    "                # Inputs[0] shape: (total_selected_tokens, hidden_dim)\n",
    "                total_selected = inputs[0].shape[0] \n",
    "                # Validate we're processing the correct number of tokens\n",
    "                expected_selected = expert_mask.sum().item()\n",
    "                if total_selected != expected_selected:\n",
    "                    print(f\" expert {expert_idx} processed {total_selected} tokens but expected {expected_selected}\")\n",
    "                    return\n",
    "                    \n",
    "                # Get the full hidden states from outputs\n",
    "                # outputs shape: (total_selected_tokens, hidden_dim)\n",
    "                hidden_states = outputs\n",
    "                if isinstance(outputs, tuple):\n",
    "                    hidden_states = outputs[0]\n",
    "                    \n",
    "                # Record data for each selected position\n",
    "                for pos_idx, pos in enumerate(selected_positions[0]):\n",
    "                    token_data = {\n",
    "                        'position': pos.item(),\n",
    "                        'input': inputs[0][pos_idx].detach(),\n",
    "                        'output': outputs[pos_idx].detach(),\n",
    "                        'hidden_state': hidden_states[pos_idx].detach()\n",
    "                    }\n",
    "                    \n",
    "                    # Get the corresponding gate weight for this position\n",
    "                    # Find which expert slot (in top_k) this expert was selected for this position\n",
    "                    expert_slots = (gate_data['topk_idx'][pos.item()] == expert_idx).nonzero(as_tuple=True)[0]\n",
    "                    if len(expert_slots) > 0:\n",
    "                        token_data['gate_weight'] = gate_data['topk_weight'][pos.item()][expert_slots[0]].item()\n",
    "                    \n",
    "                    self.expert_outputs[layer_idx][expert_idx].append(token_data)\n",
    "            return hook\n",
    "        \n",
    "        def shared_expert_hook(layer_idx):\n",
    "            def hook(module, inputs, outputs):\n",
    "                \"\"\"Hook for capturing shared expert outputs\"\"\"\n",
    "                self.shared_expert_outputs[layer_idx].append({\n",
    "                    'input': inputs[0].detach(),\n",
    "                    'output': outputs.detach()\n",
    "                })\n",
    "            return hook\n",
    "\n",
    "        def moe_combine_hook(layer_idx):\n",
    "            def hook(module, inputs, outputs):\n",
    "                \"\"\"Hook for capturing final combined MoE outputs\"\"\"\n",
    "                # For DeepseekMoE, this captures the weighted sum of expert outputs\n",
    "                self.moe_combined_outputs[layer_idx].append({\n",
    "                    'combined_output': outputs.detach(),\n",
    "                    'input': inputs[0].detach()  # Original input before MoE\n",
    "                })\n",
    "            return hook\n",
    "\n",
    "        # Register hooks for each layer\n",
    "        for layer_idx, layer in enumerate(self.model.model.layers):\n",
    "            # Hook for layer output\n",
    "            hook = layer.register_forward_hook(layer_output_hook(layer_idx))\n",
    "            self.hooks.append(hook)\n",
    "            \n",
    "            # If it's an MoE layer, add MoE-specific hooks\n",
    "            if hasattr(layer.mlp, 'experts'):\n",
    "                # Hook for gate mechanism\n",
    "                gate_hook = layer.mlp.gate.register_forward_hook(moe_gate_hook(layer_idx))\n",
    "                self.hooks.append(gate_hook)\n",
    "                \n",
    "                # Hook for each expert\n",
    "                for expert_idx, expert in enumerate(layer.mlp.experts):\n",
    "                    expert_hook_fn = expert.register_forward_hook(expert_hook(layer_idx, expert_idx))\n",
    "                    self.hooks.append(expert_hook_fn)\n",
    "\n",
    "                # Hook for shared expert if it exists\n",
    "                if hasattr(layer.mlp, 'shared_experts'):\n",
    "                    shared_hook = layer.mlp.shared_experts.register_forward_hook(shared_expert_hook(layer_idx))\n",
    "                    self.hooks.append(shared_hook)\n",
    "                \n",
    "                # Hook for final combined output\n",
    "                combine_hook = layer.mlp.register_forward_hook(moe_combine_hook(layer_idx))\n",
    "                self.hooks.append(combine_hook)\n",
    "\n",
    "    def analyze_tokens(self, input_ids: torch.Tensor, return_hidden_states: bool = False) -> Dict:\n",
    "        \"\"\"Run inference and analyze tokens at each layer and expert combination point\"\"\"\n",
    "        # Clear all state before processing new input\n",
    "        self.reset_state()\n",
    "        \n",
    "        # Move input to device\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids)\n",
    "        \n",
    "        results = {\n",
    "            'layer_predictions': {},\n",
    "            'moe_analysis': {},\n",
    "            'hidden_states': {} if return_hidden_states else None\n",
    "        }\n",
    "        \n",
    "        # Analyze layer outputs\n",
    "        for layer_idx, outputs in self.layer_outputs.items():\n",
    "            if not outputs:  # Skip if no outputs captured\n",
    "                continue\n",
    "            hidden_states = outputs[-1]  # Keep on device\n",
    "            \n",
    "            # Get token predictions for this layer\n",
    "            top_tokens = get_top_k_tokens(hidden_states, self.model.lm_head, self.tokenizer)\n",
    "            results['layer_predictions'][layer_idx] = top_tokens\n",
    "            \n",
    "            if return_hidden_states:\n",
    "                results['hidden_states'][f'layer_{layer_idx}'] = hidden_states\n",
    "        \n",
    "        # Analyze MoE layers\n",
    "        for layer_idx in self.moe_gate_outputs.keys():\n",
    "            if not self.moe_gate_outputs[layer_idx]:\n",
    "                continue\n",
    "                \n",
    "            gate_data = self.moe_gate_outputs[layer_idx][-1]\n",
    "            combined_data = self.moe_combined_outputs[layer_idx][-1]\n",
    "            \n",
    "            # Initialize predictions dictionary by position\n",
    "            expert_predictions_by_pos = defaultdict(dict)\n",
    "            expert_hidden_states_by_pos = defaultdict(dict)\n",
    "            \n",
    "            # Process expert outputs by position\n",
    "            for expert_idx, data_list in self.expert_outputs[layer_idx].items():\n",
    "                for data in data_list:\n",
    "                    position = data['position']\n",
    "                    expert_output = data['output'].unsqueeze(0)\n",
    "                    # Get predictions for this expert's output at this position\n",
    "                    predictions = get_top_k_tokens(\n",
    "                        expert_output,\n",
    "                        self.model.lm_head,\n",
    "                        self.tokenizer\n",
    "                    )\n",
    "                    expert_predictions_by_pos[position][expert_idx] = predictions[0]\n",
    "\n",
    "                    expert_hidden_states_by_pos[position][expert_idx] = {\n",
    "                        'hidden_state': data['hidden_state'].tolist(),\n",
    "                        'gate_weight': data.get('gate_weight', None)\n",
    "                    }\n",
    "            \n",
    "            # Get predictions for shared expert if it exists\n",
    "            if self.shared_expert_outputs[layer_idx]:\n",
    "                shared_output = self.shared_expert_outputs[layer_idx][-1]['output']\n",
    "                shared_expert_predictions = get_top_k_tokens(\n",
    "                    shared_output,\n",
    "                    self.model.lm_head,\n",
    "                    self.tokenizer\n",
    "                )\n",
    "            \n",
    "            experts_analysis = {\n",
    "                'selected_experts': gate_data['topk_idx'].tolist(),\n",
    "                'expert_weights': gate_data['topk_weight'].tolist(),\n",
    "                'aux_loss': gate_data['aux_loss'].item() if gate_data['aux_loss'] is not None else None,\n",
    "                'expert_predictions_by_position': dict(expert_predictions_by_pos),\n",
    "                'expert_hidden_states_by_position': dict(expert_hidden_states_by_pos),\n",
    "                'shared_expert_predictions': shared_expert_predictions if self.shared_expert_outputs[layer_idx] else None\n",
    "            }\n",
    "            \n",
    "            # Get token predictions from combined output\n",
    "            combined_output = combined_data['combined_output']\n",
    "            combined_tokens = get_top_k_tokens(\n",
    "                combined_output, \n",
    "                self.model.lm_head,\n",
    "                self.tokenizer\n",
    "            )\n",
    "            \n",
    "            experts_analysis['combined_output_tokens'] = combined_tokens\n",
    "            results['moe_analysis'][layer_idx] = experts_analysis\n",
    "\n",
    "        return results\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Remove all registered hooks and clear state\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks.clear()\n",
    "        self.reset_state()\n",
    "\n",
    "def analyze_deepseek_moe(model, tokenizer, input_text: str, return_hidden_states: bool = False, device='cpu'):\n",
    "    \"\"\"Analyze DeepSeek MoE model behavior for given input text\"\"\"\n",
    "    # Create a fresh analyzer instance for each analysis\n",
    "    analyzer = DeepseekLayerAnalyzer(model, tokenizer, device=device)\n",
    "    analyzer.register_hooks()\n",
    "    \n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    try:\n",
    "        results = analyzer.analyze_tokens(input_ids, return_hidden_states=return_hidden_states)\n",
    "        return results\n",
    "    finally:\n",
    "        analyzer.cleanup()  # Ensure hooks are removed and state is cleared\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prompt and tokenize\n",
    "prompt = \"quick brown fox\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "n = input_ids.shape[1]\n",
    "token_pos = n-1  # Last token\n",
    "\n",
    "analysis = analyze_deepseek_moe(model, tokenizer, prompt, return_hidden_states=True)\n",
    "\n",
    "# Create analyzer and get hidden states\n",
    "analyzer = DeepseekLayerAnalyzer(model, tokenizer)\n",
    "analyzer.register_hooks()\n",
    "\n",
    "\n",
    "results = analyzer.analyze_tokens(input_ids, return_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_attn_ln_inputs(model, tokenizer, text, device=DEVICE):\n",
    "    \"\"\"places a hook on the post attention layernorm to retrieve its inputs\"\"\"\n",
    "    # Store inputs in a dict mapping layer idx -> inputs\n",
    "    post_attn_ln_inputs = {}\n",
    "    hooks = []\n",
    "    \n",
    "    def hook_post_attn_ln(module, input, output, layer_idx):\n",
    "        if layer_idx not in post_attn_ln_inputs:\n",
    "            post_attn_ln_inputs[layer_idx] = []\n",
    "        # Detach and move to CPU to save memory if using CUDA\n",
    "        if device == 'cuda':\n",
    "            post_attn_ln_inputs[layer_idx].append([x.detach().cpu() for x in input])\n",
    "        else:\n",
    "            post_attn_ln_inputs[layer_idx].append([x.detach() for x in input])\n",
    "    \n",
    "    # Register hooks on post attention layernorm for each layer\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        hooks.append(\n",
    "            layer.post_attention_layernorm.register_forward_hook(\n",
    "                lambda m, i, o, idx=i: hook_post_attn_ln(m, i, o, idx)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        # Move input to device\n",
    "        input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "        \n",
    "        # Ensure model is on correct device\n",
    "        model = model.to(device)\n",
    "        \n",
    "        if device == 'cuda':\n",
    "            with torch.cuda.amp.autocast():  # Use mixed precision for CUDA\n",
    "                model(input_ids)\n",
    "            torch.cuda.empty_cache()  # Clear CUDA cache\n",
    "        else:\n",
    "            model(input_ids)\n",
    "        \n",
    "        return post_attn_ln_inputs\n",
    "        \n",
    "    finally:\n",
    "        # Clean up hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_attn_ln_inputs = get_post_attn_ln_inputs(model, tokenizer, text=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expert_hidden_states_by_weight(analysis, post_attn_ln_inputs, token_pos, k=0):\n",
    "    \"\"\"Gets hidden states (with residual added) from the k-th highest weighted expert for all MoE layers\"\"\"\n",
    "    hidden_states_by_layer = {}\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # For each layer that has MoE\n",
    "    for layer_idx in analysis['moe_analysis'].keys():\n",
    "        moe_analysis = analysis['moe_analysis'][layer_idx]\n",
    "        \n",
    "        # Get expert weights for this token\n",
    "        expert_weights = {}\n",
    "        selected_experts = moe_analysis['selected_experts'][token_pos]\n",
    "        expert_weights_list = moe_analysis['expert_weights'][token_pos]\n",
    "        \n",
    "        # Map experts to their weights\n",
    "        for expert_idx, weight in zip(selected_experts, expert_weights_list):\n",
    "            expert_weights[expert_idx] = weight\n",
    "            \n",
    "        # Sort by weight and get k-th highest\n",
    "        sorted_experts = sorted(expert_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "        if k >= len(sorted_experts):\n",
    "            print(f\"Warning: Layer {layer_idx} only has {len(sorted_experts)} experts, but k={k} requested\")\n",
    "            continue\n",
    "            \n",
    "        target_expert = sorted_experts[k][0]  # Get the k-th expert's id\n",
    "        \n",
    "        # Get hidden state for this expert\n",
    "        expert_hidden_state = moe_analysis['expert_hidden_states_by_position'][token_pos][target_expert]['hidden_state']\n",
    "        \n",
    "        # Get residual stream for this token\n",
    "        try:\n",
    "            residual = post_attn_ln_inputs[layer_idx][-1][0][0][token_pos]  # Added [0] index\n",
    "        except (KeyError, IndexError) as e:\n",
    "            print(f\"Warning: Could not get residual for layer {layer_idx}, skipping. Error: {e}\")\n",
    "            continue\n",
    "            \n",
    "        # Convert to tensor if needed and move to appropriate device\n",
    "        if isinstance(expert_hidden_state, list):\n",
    "            expert_hidden_state = torch.tensor(expert_hidden_state, dtype=torch.float16, device=device)\n",
    "        elif isinstance(expert_hidden_state, torch.Tensor):\n",
    "            expert_hidden_state = expert_hidden_state.to(device=device, dtype=torch.float16)\n",
    "            \n",
    "        # Move residual to same device and dtype\n",
    "        residual = residual.to(device=device, dtype=torch.float16)\n",
    "            \n",
    "        # Add residual using mixed precision if on CUDA\n",
    "        if device == 'cuda':\n",
    "            with torch.cuda.amp.autocast():\n",
    "                combined = residual + expert_hidden_state\n",
    "        else:\n",
    "            combined = residual + expert_hidden_state\n",
    "            \n",
    "        hidden_states_by_layer[layer_idx] = combined\n",
    "        \n",
    "    return hidden_states_by_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get final layer index and hidden state\n",
    "final_layer_idx = max([int(k) for k in results[\"layer_predictions\"].keys()])\n",
    "final_hidden_state = analyzer.layer_outputs[final_layer_idx][-1][0][token_pos]\n",
    "\n",
    "print(\"Shape of final hidden state :\", final_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = len(tokenizer.encode(prompt))\n",
    "token_pos = n-1  # Last token\n",
    "k = 0  # Get highest weighted expert\n",
    "\n",
    "hidden_states = get_expert_hidden_states_by_weight(analysis,\n",
    "                                                   post_attn_ln_inputs=post_attn_ln_inputs,\n",
    "                                                   token_pos=token_pos,\n",
    "                                                   k=k)\n",
    "\n",
    "print(f'hidden_states : {hidden_states}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity for each layer\n",
    "sim = {}\n",
    "for layer_idx, layer_hidden in hidden_states.items():\n",
    "    # Normalize both vectors for cosine similarity\n",
    "    similarity = F.cosine_similarity(final_hidden_state.unsqueeze(0),\n",
    "                                   layer_hidden.unsqueeze(0))\n",
    "    sim[layer_idx] = similarity.item()\n",
    "\n",
    "# Print results sorted by layer\n",
    "for layer_idx in sorted(sim.keys()):\n",
    "    print(f\"Layer {layer_idx} similarity: {sim[layer_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cosine_similarities(similarities, k):\n",
    "    \"\"\"\n",
    "    Create an interactive line plot of cosine similarities across layers.\n",
    "    \n",
    "    Args:\n",
    "    similarities: dict with layer_idx -> cosine_similarity_value\n",
    "    k: which expert (by weight rank) was used\n",
    "    \"\"\"\n",
    "    # Set plot size\n",
    "    plot_size = 500  # Size in pixels for both width and height\n",
    "    \n",
    "    # Sort layers for x-axis\n",
    "    layers = sorted(similarities.keys())\n",
    "    sim_values = [similarities[layer] for layer in layers]\n",
    "    \n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add line plot\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=layers,\n",
    "        y=sim_values,\n",
    "        mode='lines+markers',\n",
    "        name=f'Expert rank {k}',\n",
    "        hovertemplate=\n",
    "        \"<b>Layer %{x}</b><br>\" +\n",
    "        \"Cosine Similarity: %{y:.4f}<br>\" +\n",
    "        \"<extra></extra>\",  # Removes secondary box\n",
    "        line=dict(width=2),\n",
    "        marker=dict(size=8)\n",
    "    ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=f'Cosine Similarity with Final Hidden State<br><sup>Using {k+1}th highest weighted expert per layer</sup>',\n",
    "            x=0.5,  # Center title\n",
    "            y=0.95\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title='Layer',\n",
    "            gridcolor='rgba(128,128,128,0.2)',\n",
    "            tickmode='linear',\n",
    "            dtick=1,  # Show every layer number\n",
    "            range=[0, 27.5]  # Set x-axis range from 0 to 27\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='Cosine Similarity',\n",
    "            gridcolor='rgba(128,128,128,0.2)',\n",
    "            range=[-0.2, 1]  # Updated range for cosine similarity\n",
    "        ),\n",
    "        plot_bgcolor='white',\n",
    "        hovermode='x unified',  # Shows all points at a given x-coordinate\n",
    "        showlegend=False,\n",
    "        width=plot_size,\n",
    "        height=plot_size\n",
    "    )\n",
    "    \n",
    "    # Add grid\n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1)\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Use the function\n",
    "fig = plot_cosine_similarities(sim, k)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def analyze_multiple_samples(model, tokenizer, domain: str, samples: List[str], start_idx: int = 0):\n",
    "    \"\"\"Analyze multiple samples and return average cosine similarities per layer\"\"\"\n",
    "    all_similarities = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Convert from 1-based to 0-based indexing\n",
    "    zero_based_start = start_idx - 1 if start_idx > 0 else 0\n",
    "    \n",
    "    # Create output directories if they don't exist\n",
    "    os.makedirs(f'cosine-sim/csv', exist_ok=True)\n",
    "    \n",
    "    for i, sample in enumerate(tqdm(samples[zero_based_start:], desc=\"Analyzing samples\", initial=zero_based_start, total=len(samples))):\n",
    "        try:\n",
    "            # Clear CUDA cache before processing each sample\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "            \n",
    "            # Get prompt and tokenize\n",
    "            input_ids = tokenizer(sample, return_tensors=\"pt\").input_ids.to(device)\n",
    "            n = input_ids.shape[1]\n",
    "            token_pos = n-1  # Last token\n",
    "            \n",
    "            # Run analysis with mixed precision for CUDA\n",
    "            if device.type == \"cuda\":\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    analysis = analyze_deepseek_moe(model, tokenizer, sample, return_hidden_states=True, device=device.type)\n",
    "                    post_attn_ln_inputs = get_post_attn_ln_inputs(model, tokenizer, text=sample, device=device.type)\n",
    "            else:\n",
    "                analysis = analyze_deepseek_moe(model, tokenizer, sample, return_hidden_states=True)\n",
    "                post_attn_ln_inputs = get_post_attn_ln_inputs(model, tokenizer, text=sample)\n",
    "            \n",
    "            # Get final layer index and hidden state\n",
    "            final_layer_idx = max([int(k) for k in analysis[\"layer_predictions\"].keys()])\n",
    "            final_hidden_state = analysis[\"hidden_states\"][f\"layer_{final_layer_idx}\"][0][token_pos]\n",
    "            \n",
    "            # Get hidden states for highest weighted expert (k=0)\n",
    "            hidden_states = get_expert_hidden_states_by_weight(\n",
    "                analysis,\n",
    "                post_attn_ln_inputs=post_attn_ln_inputs,\n",
    "                token_pos=token_pos,\n",
    "                k=0\n",
    "            )\n",
    "            \n",
    "            # Calculate cosine similarities for this sample\n",
    "            similarities = {}\n",
    "            for layer_idx, layer_hidden in hidden_states.items():\n",
    "                # Use mixed precision for CUDA\n",
    "                if device.type == \"cuda\":\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        similarity = F.cosine_similarity(\n",
    "                            final_hidden_state.unsqueeze(0),\n",
    "                            layer_hidden.unsqueeze(0)\n",
    "                        )\n",
    "                else:\n",
    "                    similarity = F.cosine_similarity(\n",
    "                        final_hidden_state.unsqueeze(0),\n",
    "                        layer_hidden.unsqueeze(0)\n",
    "                    )\n",
    "                similarities[layer_idx] = similarity.item()\n",
    "                \n",
    "            all_similarities.append(similarities)\n",
    "            \n",
    "            # Save individual sample results to CSV using 1-based indexing\n",
    "            df = pd.DataFrame({\n",
    "                'layer': list(similarities.keys()),\n",
    "                'cosine_similarity': list(similarities.values())\n",
    "            })\n",
    "            df.to_csv(f'cosine-sim/csv/{domain}_{i+zero_based_start+1}_similarities.csv', index=False)\n",
    "            \n",
    "            # Clear variables to free memory\n",
    "            del analysis, post_attn_ln_inputs, hidden_states, final_hidden_state\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"\\nOOM error on sample {i+zero_based_start+1}. Clearing cache and skipping sample...\")\n",
    "                if device.type == \"cuda\":\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                continue\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    # Calculate average similarities from individual CSV files\n",
    "    avg_similarities = {}\n",
    "    csv_files = glob.glob(f'cosine-sim/csv/{domain}_*_similarities.csv')\n",
    "    \n",
    "    # Read and combine all CSV files\n",
    "    all_dfs = []\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        all_dfs.append(df)\n",
    "    \n",
    "    if all_dfs:\n",
    "        # Combine all dataframes and calculate mean per layer\n",
    "        combined_df = pd.concat(all_dfs)\n",
    "        avg_df = combined_df.groupby('layer')['cosine_similarity'].mean().reset_index()\n",
    "        avg_df.columns = ['layer', f'average_cosine_similarity_{domain}']\n",
    "        \n",
    "        # Convert to dictionary format\n",
    "        avg_similarities = dict(zip(avg_df['layer'], avg_df[f'average_cosine_similarity_{domain}']))\n",
    "        \n",
    "        # Save average similarities to CSV\n",
    "        avg_df.to_csv(f'cosine-sim/csv/average_similarities_{domain}.csv', index=False)\n",
    "    \n",
    "    return avg_similarities, all_similarities\n",
    "\n",
    "def process_samples(model, tokenizer, file_path: str=\"test.txt\", start_from: int = 1):\n",
    "    \"\"\"\n",
    "    Process samples from a text file starting from a 1-based index position.\n",
    "    Args:\n",
    "        model: The model to analyze\n",
    "        tokenizer: The tokenizer to use\n",
    "        file_path: Path to the text file containing samples\n",
    "        start_from: 1-based index of the sample to start from (default: 1 for first sample)\n",
    "    \"\"\"\n",
    "    # Set device to CUDA\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    if device.type == \"cuda\":\n",
    "        print(f\"CUDA Device: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "        print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved()/1024**3:.2f}GB\")\n",
    "    \n",
    "    # Read text file\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            \n",
    "        # Check if this is a GitHub code file\n",
    "        if 'github.txt' in file_path:\n",
    "            import re\n",
    "            # Find all code blocks using the file pattern\n",
    "            file_pattern = re.compile(r'.*\\b\\w+\\.(js|py|c|cpp|java|ts|rb|go|rs|cs|swift|kt|php)$', re.MULTILINE)\n",
    "            \n",
    "            # Find all matches (file headers)\n",
    "            matches = list(file_pattern.finditer(content))\n",
    "            \n",
    "            # Extract code blocks between file headers\n",
    "            samples = []\n",
    "            for i in range(len(matches)):\n",
    "                start_pos = matches[i].start()\n",
    "                # If this is the last match, go to the end of the file\n",
    "                if i == len(matches) - 1:\n",
    "                    end_pos = len(content)\n",
    "                else:\n",
    "                    end_pos = matches[i+1].start()\n",
    "                \n",
    "                # Extract the code block including the file header\n",
    "                code_block = content[start_pos:end_pos].strip()\n",
    "                samples.append(code_block)\n",
    "        else:\n",
    "            # Regular text file processing (one prompt per line)\n",
    "            samples = [line.strip() for line in content.split('\\n') if line.strip()]\n",
    "        \n",
    "        print(f\"Loaded {len(samples)} samples from {file_path}\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        raise ValueError(f\"Could not find file: {file_path}\")\n",
    "    \n",
    "    # Validate start_from parameter (using 1-based indexing)\n",
    "    if start_from < 1:\n",
    "        raise ValueError(\"start_from must be at least 1 (1-based indexing)\")\n",
    "    if start_from > len(samples):\n",
    "        raise ValueError(f\"start_from ({start_from}) must be less than or equal to total samples ({len(samples)})\")\n",
    "    \n",
    "    # Extract domain name from file path\n",
    "    domain = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    \n",
    "    # Make sure model is on the correct device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Run analysis\n",
    "    avg_sims, all_sims = analyze_multiple_samples(model, tokenizer, domain, samples, start_from)\n",
    "    \n",
    "    # Print average similarities\n",
    "    print(\"\\nAverage Cosine Similarities per Layer:\")\n",
    "    for layer_idx in sorted(avg_sims.keys()):\n",
    "        print(f\"Layer {layer_idx}: {avg_sims[layer_idx]:.4f}\")\n",
    "        \n",
    "    return avg_sims, all_sims\n",
    "\n",
    "def plot_multi_sample_similarities(all_similarities, avg_similarities, domain):\n",
    "    \"\"\"\n",
    "    Create an interactive plot showing average cosine similarities across layers\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Read and plot average from CSV\n",
    "    avg_file = f'cosine-sim/csv/average_similarities_{domain}.csv'\n",
    "    if os.path.exists(avg_file):\n",
    "        avg_df = pd.read_csv(avg_file)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=avg_df['layer'],\n",
    "            y=avg_df[f'average_cosine_similarity_{domain}'],\n",
    "            mode='lines+markers',\n",
    "            name='Average',\n",
    "            line=dict(width=3, color='black'),\n",
    "            marker=dict(size=8),\n",
    "            hovertemplate=\"<b>Layer %{x}</b><br>Average Similarity: %{y:.4f}<extra></extra>\"\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'Average Cosine Similarities - {domain.capitalize()}',\n",
    "        xaxis_title='Layer',\n",
    "        yaxis_title='Cosine Similarity',\n",
    "        yaxis_range=[-0.2, 1],\n",
    "        showlegend=True,\n",
    "        hovermode='closest'\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and tokenizer as before\n",
    "analyzer = DeepseekLayerAnalyzer(model, tokenizer)\n",
    "analyzer.register_hooks()\n",
    "domain = \"test\"\n",
    "\n",
    "# Process samples\n",
    "avg_sims, all_sims = process_samples(model,\n",
    "                                    tokenizer,\n",
    "                                    domain=domain,\n",
    "                                    start_from=1) # 1-based indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_multi_sample_similarities(all_sims, avg_sims)\n",
    "fig.write_image(f\"cosine-sim/cosine-sim-plot-{domain}.png\")\n",
    "fig.write_html(f\"cosine-sim/cosine-sim-plot-{domain}.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for compared k experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expert_hidden_state_by_rank(analysis: Dict, post_attn_ln_inputs: Dict, \n",
    "                                 token_pos: int, rank: int = 1, \n",
    "                                 include_residual: bool = True) -> Dict[int, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Gets hidden state from the expert at specified rank (1 = highest weight, 6 = 6th highest weight)\n",
    "    \n",
    "    Args:\n",
    "        analysis: Analysis results from DeepseekLayerAnalyzer\n",
    "        post_attn_ln_inputs: Inputs to post attention layer norm\n",
    "        token_pos: Position of token to analyze\n",
    "        rank: Which expert to select (1 = highest weight, 2 = second highest, etc.)\n",
    "        include_residual: Whether to include residual connection\n",
    "    \"\"\"\n",
    "    hidden_states_by_layer = {}\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # For each layer that has MoE\n",
    "    for layer_idx in analysis['moe_analysis'].keys():\n",
    "        moe_analysis = analysis['moe_analysis'][layer_idx]\n",
    "        \n",
    "        # Get expert weights for this token\n",
    "        expert_weights = {}\n",
    "        selected_experts = moe_analysis['selected_experts'][token_pos]\n",
    "        expert_weights_list = moe_analysis['expert_weights'][token_pos]\n",
    "        \n",
    "        # Map experts to their weights\n",
    "        for expert_idx, weight in zip(selected_experts, expert_weights_list):\n",
    "            expert_weights[expert_idx] = weight\n",
    "            \n",
    "        # Sort by weight and get the expert at specified rank\n",
    "        sorted_experts = sorted(expert_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "        if rank > len(sorted_experts):\n",
    "            print(f\"Warning: Requested rank {rank} but only {len(sorted_experts)} experts available\")\n",
    "            continue\n",
    "            \n",
    "        expert_idx, weight = sorted_experts[rank - 1]  # rank-1 because rank is 1-based\n",
    "        \n",
    "        # Get hidden state for this expert\n",
    "        expert_hidden_state = moe_analysis['expert_hidden_states_by_position'][token_pos][expert_idx]['hidden_state']\n",
    "        \n",
    "        # Convert to tensor if needed and move to correct device\n",
    "        if isinstance(expert_hidden_state, list):\n",
    "            expert_hidden_state = torch.tensor(expert_hidden_state, device=device, dtype=torch.float16)\n",
    "        elif isinstance(expert_hidden_state, torch.Tensor):\n",
    "            expert_hidden_state = expert_hidden_state.to(device=device, dtype=torch.float16)\n",
    "            \n",
    "        # Add residual if requested\n",
    "        if include_residual:\n",
    "            try:\n",
    "                residual = post_attn_ln_inputs[layer_idx][-1][0][0][token_pos]\n",
    "                residual = residual.to(device=device, dtype=torch.float16)\n",
    "                expert_hidden_state = expert_hidden_state + residual\n",
    "            except (KeyError, IndexError) as e:\n",
    "                print(f\"Warning: Could not get residual for layer {layer_idx}, skipping. Error: {e}\")\n",
    "                continue\n",
    "                \n",
    "        hidden_states_by_layer[layer_idx] = expert_hidden_state\n",
    "        \n",
    "    return hidden_states_by_layer\n",
    "\n",
    "def compare_expert_ranks(analysis: Dict, post_attn_ln_inputs: Dict,\n",
    "                        token_pos: int, final_hidden_state: torch.Tensor,\n",
    "                        rank1: int = 1, rank2: int = 6,\n",
    "                        include_residual: bool = True) -> Dict[int, Tuple[float, float, float]]:\n",
    "    \"\"\"\n",
    "    Compare cosine similarities between two experts at different ranks and the final hidden state.\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    final_hidden_state = final_hidden_state.to(device=device, dtype=torch.float16)\n",
    "    \n",
    "    # Get hidden states for both experts\n",
    "    hidden_states_rank1 = get_expert_hidden_state_by_rank(analysis, post_attn_ln_inputs, token_pos, rank1, include_residual=include_residual)\n",
    "    hidden_states_rank2 = get_expert_hidden_state_by_rank(analysis, post_attn_ln_inputs, token_pos, rank2, include_residual=include_residual)\n",
    "    \n",
    "    similarities = {}\n",
    "    \n",
    "    for layer_idx in hidden_states_rank1.keys():\n",
    "        if layer_idx not in hidden_states_rank2:\n",
    "            continue\n",
    "            \n",
    "        # Get hidden states for this layer\n",
    "        h1 = hidden_states_rank1[layer_idx]\n",
    "        h2 = hidden_states_rank2[layer_idx]\n",
    "        \n",
    "        # Calculate similarities\n",
    "        sim_rank1_final = F.cosine_similarity(h1.unsqueeze(0), final_hidden_state.unsqueeze(0)).item()\n",
    "        sim_rank2_final = F.cosine_similarity(h2.unsqueeze(0), final_hidden_state.unsqueeze(0)).item()\n",
    "        sim_rank1_rank2 = F.cosine_similarity(h1.unsqueeze(0), h2.unsqueeze(0)).item()\n",
    "        \n",
    "        similarities[layer_idx] = (sim_rank1_final, sim_rank2_final, sim_rank1_rank2)\n",
    "        \n",
    "    return similarities\n",
    "\n",
    "def plot_expert_rank_comparisons(similarities: Dict[int, Tuple[float, float, float]], \n",
    "                               rank1: int = 1, rank2: int = 6,\n",
    "                               domain: str = \"test\"):\n",
    "    \"\"\"Create interactive plot comparing expert ranks\"\"\"\n",
    "    # Sort layers for x-axis\n",
    "    layers = sorted(similarities.keys())\n",
    "    sim_rank1_final = [similarities[l][0] for l in layers]\n",
    "    sim_rank2_final = [similarities[l][1] for l in layers]\n",
    "    sim_rank1_rank2 = [similarities[l][2] for l in layers]\n",
    "    \n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add traces\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=layers,\n",
    "        y=sim_rank1_final,\n",
    "        mode='lines+markers',\n",
    "        name=f'Rank {rank1} to Final',\n",
    "        line=dict(width=2),\n",
    "        marker=dict(size=8),\n",
    "        hovertemplate=\"<b>Layer %{x}</b><br>\" +\n",
    "                     f\"Rank {rank1} to Final: %{{y:.4f}}<extra></extra>\"\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=layers,\n",
    "        y=sim_rank2_final,\n",
    "        mode='lines+markers',\n",
    "        name=f'Rank {rank2} to Final',\n",
    "        line=dict(width=2),\n",
    "        marker=dict(size=8),\n",
    "        hovertemplate=\"<b>Layer %{x}</b><br>\" +\n",
    "                     f\"Rank {rank2} to Final: %{{y:.4f}}<extra></extra>\"\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=layers,\n",
    "        y=sim_rank1_rank2,\n",
    "        mode='lines+markers',\n",
    "        name=f'Rank {rank1} to Rank {rank2}',\n",
    "        line=dict(width=2),\n",
    "        marker=dict(size=8),\n",
    "        hovertemplate=\"<b>Layer %{x}</b><br>\" +\n",
    "                     f\"Rank {rank1} to Rank {rank2}: %{{y:.4f}}<extra></extra>\"\n",
    "    ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=f'Expert Rank Comparisons - {domain.capitalize()}',\n",
    "            x=0.5,\n",
    "            y=0.95\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title='Layer',\n",
    "            gridcolor='rgba(128,128,128,0.2)',\n",
    "            tickmode='linear',\n",
    "            dtick=1,\n",
    "            range=[0, 27.5]\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='Cosine Similarity',\n",
    "            gridcolor='rgba(128,128,128,0.2)',\n",
    "            range=[-0.2, 1]\n",
    "        ),\n",
    "        plot_bgcolor='white',\n",
    "        hovermode='x unified',\n",
    "        showlegend=True,\n",
    "        width=500,\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    # Add grid\n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1)\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def analyze_expert_ranks_and_plot(model, tokenizer, prompt: str, \n",
    "                                rank1: int = 1, rank2: int = 6,\n",
    "                                domain: str = \"test\",\n",
    "                                include_residual=True,\n",
    "                                device=\"cuda\"):\n",
    "    \"\"\"Wrapper function to perform complete expert rank analysis and plotting\"\"\"\n",
    "    # Set device\n",
    "    device = torch.device(device if torch.cuda.is_available() and device==\"cuda\" else \"cpu\")\n",
    "    \n",
    "    # Get inputs ready\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    token_pos = input_ids.shape[1] - 1  # Last token\n",
    "    \n",
    "    # Run analysis with mixed precision if on CUDA\n",
    "    if device.type == \"cuda\":\n",
    "        with torch.cuda.amp.autocast():\n",
    "            analysis = analyze_deepseek_moe(model, tokenizer, prompt, return_hidden_states=True, device=device.type)\n",
    "            post_attn_ln_inputs = get_post_attn_ln_inputs(model, tokenizer, text=prompt, device=device.type)\n",
    "    else:\n",
    "        analysis = analyze_deepseek_moe(model, tokenizer, prompt, return_hidden_states=True)\n",
    "        post_attn_ln_inputs = get_post_attn_ln_inputs(model, tokenizer, text=prompt)\n",
    "    \n",
    "    # Get final hidden state\n",
    "    final_layer_idx = max([int(k) for k in analysis[\"layer_predictions\"].keys()])\n",
    "    final_hidden_state = analysis[\"hidden_states\"][f\"layer_{final_layer_idx}\"][0][token_pos]\n",
    "    \n",
    "    # Compare expert ranks\n",
    "    similarities = compare_expert_ranks(\n",
    "        analysis=analysis,\n",
    "        post_attn_ln_inputs=post_attn_ln_inputs,\n",
    "        token_pos=token_pos,\n",
    "        final_hidden_state=final_hidden_state,\n",
    "        rank1=rank1,\n",
    "        rank2=rank2,\n",
    "        include_residual=include_residual\n",
    "    )\n",
    "    \n",
    "    # Create output directories if they don't exist\n",
    "    os.makedirs(f'cosine-sim-avg', exist_ok=True)\n",
    "    \n",
    "    # Create and save visualization\n",
    "    fig = plot_expert_rank_comparisons(similarities, rank1=rank1, rank2=rank2, domain=domain)\n",
    "    \n",
    "    # Save plots\n",
    "    fig.write_image(f\"cosine-sim-avg/expert-rank-comparison-{domain}.png\")\n",
    "    fig.write_html(f\"cosine-sim-avg/expert-rank-comparison-{domain}.html\")\n",
    "    \n",
    "    return similarities, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_samples_with_expert_ranks(model, tokenizer, file_path: str = \"test.txt\", start_from: int = 1):\n",
    "    \"\"\"\n",
    "    Process multiple samples from a text file and analyze expert ranks\n",
    "    Args:\n",
    "        model: The model to analyze\n",
    "        tokenizer: The tokenizer to use\n",
    "        file_path: Path to the text file containing samples\n",
    "        start_from: 1-based index of where to start processing samples\n",
    "    \"\"\"\n",
    "    # Set device to CUDA\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    if device.type == \"cuda\":\n",
    "        print(f\"CUDA Device: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "        print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved()/1024**3:.2f}GB\")\n",
    "        \n",
    "        # Set higher precision for better performance\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "    \n",
    "    # Read text file\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            \n",
    "        # Check if this is a GitHub code file\n",
    "        if 'github.txt' in file_path:\n",
    "            import re\n",
    "            # Find all code blocks using the file pattern\n",
    "            file_pattern = re.compile(r'.*\\b\\w+\\.(js|py|c|cpp|java|ts|rb|go|rs|cs|swift|kt|php)$', re.MULTILINE)\n",
    "            \n",
    "            # Find all matches (file headers)\n",
    "            matches = list(file_pattern.finditer(content))\n",
    "            \n",
    "            # Extract code blocks between file headers\n",
    "            samples = []\n",
    "            for i in range(len(matches)):\n",
    "                start_pos = matches[i].start()\n",
    "                # If this is the last match, go to the end of the file\n",
    "                if i == len(matches) - 1:\n",
    "                    end_pos = len(content)\n",
    "                else:\n",
    "                    end_pos = matches[i+1].start()\n",
    "                \n",
    "                # Extract the code block including the file header\n",
    "                code_block = content[start_pos:end_pos].strip()\n",
    "                samples.append(code_block)\n",
    "        else:\n",
    "            # Regular text file processing (one prompt per line)\n",
    "            samples = [line.strip() for line in content.split('\\n') if line.strip()]\n",
    "        \n",
    "        print(f\"Loaded {len(samples)} samples from {file_path}\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        raise ValueError(f\"Could not find file: {file_path}\")\n",
    "    \n",
    "    # Extract domain name from file path\n",
    "    domain = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    \n",
    "    # Make sure model is on the correct device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Convert from 1-based to 0-based indexing\n",
    "    zero_based_start = start_from - 1 if start_from > 0 else 0\n",
    "    \n",
    "    # Store all similarities\n",
    "    all_similarities = []\n",
    "    \n",
    "    # Create output directories if they don't exist\n",
    "    os.makedirs(f'cosine-sim-avg/csv', exist_ok=True)\n",
    "    os.makedirs(f'cosine-sim-avg', exist_ok=True)\n",
    "    \n",
    "    for i, sample in enumerate(tqdm(samples[zero_based_start:], \n",
    "                                  desc=\"Analyzing samples\", \n",
    "                                  initial=zero_based_start, \n",
    "                                  total=len(samples))):\n",
    "        try:\n",
    "            # Clear CUDA cache before processing each sample\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "            \n",
    "            # Process single sample with mixed precision if on CUDA\n",
    "            if device.type == \"cuda\":\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    similarities, _ = analyze_expert_ranks_and_plot(\n",
    "                        model,\n",
    "                        tokenizer,\n",
    "                        prompt=sample,\n",
    "                        rank1=1,\n",
    "                        rank2=6,\n",
    "                        domain=f\"{domain}_sample_{i+zero_based_start+1}\",\n",
    "                        include_residual=True,\n",
    "                        device=device.type\n",
    "                    )\n",
    "            else:\n",
    "                similarities, _ = analyze_expert_ranks_and_plot(\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    prompt=sample,\n",
    "                    rank1=1,\n",
    "                    rank2=6,\n",
    "                    domain=f\"{domain}_sample_{i+zero_based_start+1}\"\n",
    "                )\n",
    "            \n",
    "            # Store results\n",
    "            all_similarities.append(similarities)\n",
    "            \n",
    "            # Save individual sample results to CSV\n",
    "            sample_df = pd.DataFrame({\n",
    "                'layer': list(similarities.keys()),\n",
    "                'rank1_to_final': [s[0] for s in similarities.values()],\n",
    "                'rank6_to_final': [s[1] for s in similarities.values()],\n",
    "                'rank1_to_rank6': [s[2] for s in similarities.values()]\n",
    "            })\n",
    "            sample_df.to_csv(f'cosine-sim-avg/csv/{domain}_expert_ranks_{i+zero_based_start+1}.csv', \n",
    "                           index=False)\n",
    "            \n",
    "            # Print memory stats periodically for monitoring\n",
    "            if device.type == \"cuda\" and i % 5 == 0:\n",
    "                print(f\"\\nCUDA Memory After Sample {i+zero_based_start+1}: \"\n",
    "                      f\"Allocated: {torch.cuda.memory_allocated()/1024**3:.2f}GB, \"\n",
    "                      f\"Reserved: {torch.cuda.memory_reserved()/1024**3:.2f}GB\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"\\nOOM error on sample {i+zero_based_start+1}. Clearing cache and skipping...\")\n",
    "                if device.type == \"cuda\":\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                continue\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    # Calculate average similarities\n",
    "    # First, create a dictionary to store sums and counts for each layer\n",
    "    sums = defaultdict(lambda: [0.0, 0.0, 0.0])  # [sum_rank1_final, sum_rank6_final, sum_rank1_rank6]\n",
    "    counts = defaultdict(int)\n",
    "    \n",
    "    # Sum up all values\n",
    "    for sim_dict in all_similarities:\n",
    "        for layer, (r1f, r6f, r1r6) in sim_dict.items():\n",
    "            sums[layer][0] += r1f\n",
    "            sums[layer][1] += r6f\n",
    "            sums[layer][2] += r1r6\n",
    "            counts[layer] += 1\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_similarities = {}\n",
    "    for layer in sums:\n",
    "        avg_similarities[layer] = tuple(s / counts[layer] for s in sums[layer])\n",
    "    \n",
    "    # Also save raw averages\n",
    "    raw_avg_df = pd.DataFrame({\n",
    "        'layer': list(avg_similarities.keys()),\n",
    "        'rank1_to_final': [s[0] for s in avg_similarities.values()],\n",
    "        'rank6_to_final': [s[1] for s in avg_similarities.values()],\n",
    "        'rank1_to_rank6': [s[2] for s in avg_similarities.values()]\n",
    "    })\n",
    "    raw_avg_df.to_csv(f'cosine-sim-avg/csv/{domain}_expert_ranks_avg.csv', index=False)\n",
    "    \n",
    "    # Create and save average plot\n",
    "    fig = plot_expert_rank_comparisons(avg_similarities, rank1=1, rank2=6, domain=f\"{domain}_average\")\n",
    "    fig.write_image(f\"cosine-sim-avg/expert-rank-comparison-{domain}_average.png\")\n",
    "    fig.write_html(f\"cosine-sim-avg/expert-rank-comparison-{domain}_average.html\")\n",
    "    \n",
    "    return avg_similarities, all_similarities, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for a single prompt\n",
    "prompt = \"One might expect language modeling performance to depend on model architecture, the size of neural models, the computing power used to train them, and the data available for this\"\n",
    "domain = \"test\"\n",
    "similarities, fig = analyze_expert_ranks_and_plot(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompt=prompt,\n",
    "    rank1=1,  # Highest weighted expert\n",
    "    rank2=6,  # 6th highest weighted expert\n",
    "    domain=domain,\n",
    "    include_residual=True\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(f\"expert-rank-comparison-{domain}-2.png\")\n",
    "fig.write_html(f\"expert-rank-comparison-{domain}-2.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize analyzer as before\n",
    "analyzer = DeepseekLayerAnalyzer(model, tokenizer)\n",
    "analyzer.register_hooks()\n",
    "\n",
    "# Process samples from a text file\n",
    "file_path = \"english.txt\"  # or \"code.txt\", etc.\n",
    "avg_sims, all_sims, fig = await process_samples_with_expert_ranks(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    file_path=file_path,\n",
    "    start_from=1  # Start from first sample\n",
    ")\n",
    "\n",
    "# Display the average plot\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
