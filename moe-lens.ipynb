{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d44ff0a30cb4df198e944d6c20a85e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_model(model_name):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        # use_flash_attention_2=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model(\"deepseek-ai/deepseek-moe-16b-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moe_metadata(model, input_ids):\n",
    "    \"\"\"Get both router logits and expert indices for all MoE layers\"\"\"\n",
    "    router_logits_list = []\n",
    "    expert_indices_list = []\n",
    "    hidden_states_list = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        # output contains: (topk_idx, topk_weight, aux_loss)\n",
    "        hidden_states = input[0]\n",
    "        \n",
    "        logits = torch.matmul(hidden_states, module.weight.T)\n",
    "        router_logits_list.append(logits.detach())\n",
    "        \n",
    "        # store expert indices actually used for routing\n",
    "        expert_indices_list.append(output[0].detach())\n",
    "\n",
    "        # store the hidden states\n",
    "        hidden_states_list.append(hidden_states.detach())\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    hooks = []\n",
    "    for layer_idx, layer in enumerate(model.model.layers):\n",
    "        if layer.mlp.__class__.__name__ == 'DeepseekMoE':\n",
    "            hook = layer.mlp.gate.register_forward_hook(hook_fn)\n",
    "            hooks.append(hook)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model(input_ids)\n",
    "    \n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    moe_metadata = {\n",
    "        'router_logits': torch.stack(router_logits_list) if router_logits_list else None,\n",
    "        'expert_indices': torch.stack(expert_indices_list) if expert_indices_list else None,\n",
    "        'hidden_states': torch.stack(hidden_states_list) if hidden_states_list else None\n",
    "    }\n",
    "    \n",
    "    if moe_metadata['router_logits'] is not None:\n",
    "        print(f\"Router logits shape: {moe_metadata['router_logits'].shape}\")\n",
    "    if moe_metadata['expert_indices'] is not None:\n",
    "        print(f\"Expert indices shape: {moe_metadata['expert_indices'].shape}\")\n",
    "    if moe_metadata['hidden_states'] is not None:\n",
    "        print(f\"Hidden states shape: {moe_metadata['hidden_states'].shape}\")\n",
    "    \n",
    "    return moe_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expert_outputs(model, moe_metadata):\n",
    "    \"\"\"Compute expert outputs for top-k selected experts in all MoE layers\"\"\"\n",
    "    expert_outputs = []\n",
    "    num_layers = 27\n",
    "    \n",
    "    # Get metadata dimensions\n",
    "    # num_layers = moe_metadata['expert_indices'].shape[0]\n",
    "    print(f'expert_indices shape: {moe_metadata[\"expert_indices\"].shape}')\n",
    "    num_tokens = moe_metadata['expert_indices'].shape[1]\n",
    "    top_k = moe_metadata['expert_indices'].shape[2]\n",
    "    hidden_dim = moe_metadata['hidden_states'].shape[-1]\n",
    "\n",
    "    # Pre-allocate tensor: [layers, tokens, top_k, hidden_dim]\n",
    "    all_expert_outputs = torch.zeros(\n",
    "        (num_layers, num_tokens, top_k, hidden_dim),\n",
    "        device=model.device\n",
    "    )\n",
    "\n",
    "    for layer_idx in range(num_layers):\n",
    "        # Get MoE components for current layer\n",
    "        expert_module = model.model.layers[layer_idx+1].mlp.experts\n",
    "        layer_hidden_states = moe_metadata['hidden_states'][layer_idx]  # [1, num_tokens, hdim]\n",
    "        layer_expert_indices = moe_metadata['expert_indices'][layer_idx]  # [num_tokens, top_k]\n",
    "\n",
    "        for token_idx in range(num_tokens):\n",
    "            # Get hidden state for this token (remove batch dim)\n",
    "            hidden_state = layer_hidden_states[0, token_idx]  # [hdim]\n",
    "\n",
    "            # Get expert indices for this token\n",
    "            expert_indices = layer_expert_indices[token_idx]\n",
    "\n",
    "            # Process through each selected expert\n",
    "            for expert_pos, expert_idx in enumerate(expert_indices):\n",
    "                expert = expert_module[expert_idx.item()]\n",
    "                \n",
    "                # Add batch dimension for processing\n",
    "                with torch.no_grad():\n",
    "                    expert_out = expert(hidden_state.unsqueeze(0))  # [1, hdim]\n",
    "                \n",
    "                all_expert_outputs[layer_idx, token_idx, expert_pos] = expert_out.squeeze(0)\n",
    "\n",
    "    print(f\"Expert outputs shape: {all_expert_outputs.shape}\")\n",
    "    return all_expert_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_expert_outputs(model, expert_outputs):\n",
    "    \"\"\"\n",
    "    Project expert outputs through LM head while maintaining structure\n",
    "    Returns tensor of shape [num_layers, num_tokens, num_experts, vocab_size]\n",
    "    \"\"\"\n",
    "    # Get model dtype from LM head\n",
    "    model_dtype = model.lm_head.weight.dtype\n",
    "    \n",
    "    # Get original shape details\n",
    "    num_layers, num_tokens, num_experts, hidden_dim = expert_outputs.shape\n",
    "    vocab_size = model.lm_head.out_features\n",
    "    print(f'vocab_size: {vocab_size}')\n",
    "    # Pre-allocate output tensor using model dtype\n",
    "    expert_logits = torch.zeros(\n",
    "        (num_layers, num_tokens, num_experts, vocab_size),\n",
    "        device=model.device,\n",
    "        dtype=model_dtype  # Match model's dtype\n",
    "    )\n",
    "\n",
    "    # Process each layer, token and expert individually\n",
    "    for layer_idx in range(num_layers):\n",
    "        for token_idx in range(num_tokens):\n",
    "            for expert_idx in range(num_experts):\n",
    "                # Get expert output and cast to model dtype\n",
    "                expert_output = expert_outputs[layer_idx, token_idx, expert_idx]\n",
    "                expert_output = expert_output.to(model_dtype)  # <-- CRITICAL CAST\n",
    "                \n",
    "                # Project through LM head\n",
    "                with torch.no_grad():\n",
    "                    logits = model.lm_head(expert_output.unsqueeze(0))\n",
    "                \n",
    "                # Store result\n",
    "                expert_logits[layer_idx, token_idx, expert_idx] = logits.squeeze(0)\n",
    "\n",
    "    print(f\"Expert logits shape: {expert_logits.shape}\")\n",
    "    return expert_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expert_topk_tokens(expert_logits, tokenizer, k=5):\n",
    "    \"\"\"\n",
    "    Get top-k tokens for each expert at each layer and token position\n",
    "    Returns nested dictionary:\n",
    "    {\n",
    "        layer_idx: {\n",
    "            token_idx: {\n",
    "                expert_idx: {\n",
    "                    'tokens': [decoded tokens],\n",
    "                    'scores': [corresponding scores],\n",
    "                    'ids': [token ids]\n",
    "                }, ...\n",
    "            }, ...\n",
    "        }, ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    num_layers, num_tokens, num_experts, _ = expert_logits.shape\n",
    "    results = {}\n",
    "\n",
    "    for layer_idx in range(num_layers):\n",
    "        layer_results = {}\n",
    "        for token_idx in range(num_tokens):\n",
    "            token_results = {}\n",
    "            for expert_idx in range(num_experts):\n",
    "                # Get logits for this expert configuration\n",
    "                expert_logit = expert_logits[layer_idx, token_idx, expert_idx]\n",
    "                \n",
    "                # Get top-k predictions\n",
    "                topk_scores, topk_indices = torch.topk(expert_logit, k)\n",
    "                \n",
    "                # Convert to CPU/numpy for decoding\n",
    "                topk_indices_cpu = topk_indices.cpu().numpy()\n",
    "                topk_scores_cpu = topk_scores.cpu().numpy()\n",
    "                \n",
    "                # Decode tokens\n",
    "                decoded_tokens = tokenizer.batch_decode(topk_indices_cpu)\n",
    "                \n",
    "                token_results[expert_idx] = {\n",
    "                    'tokens': decoded_tokens,\n",
    "                    'scores': topk_scores_cpu.tolist(),\n",
    "                    'ids': topk_indices_cpu.tolist()\n",
    "                }\n",
    "            \n",
    "            layer_results[token_idx] = token_results\n",
    "        results[layer_idx] = layer_results\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Router logits shape: torch.Size([27, 1, 5, 64])\n",
      "Expert indices shape: torch.Size([27, 5, 6])\n",
      "Hidden states shape: torch.Size([27, 1, 5, 2048])\n",
      "expert_indices shape: torch.Size([27, 5, 6])\n",
      "Expert outputs shape: torch.Size([27, 5, 6, 2048])\n",
      "vocab_size: 102400\n",
      "Expert logits shape: torch.Size([27, 5, 6, 102400])\n"
     ]
    }
   ],
   "source": [
    "input_txt = \"the quick brown fox\"\n",
    "input_ids = tokenizer.encode(input_txt, return_tensors=\"pt\")\n",
    "moe_metadata = get_moe_metadata(model, input_ids)\n",
    "expert_outputs = get_expert_outputs(model, moe_metadata)\n",
    "expert_logits = project_expert_outputs(model, expert_outputs)\n",
    "expert_topk_tokens = get_expert_topk_tokens(expert_logits, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'tokens': [' nether', ' outermost', 'ymap', 'ermis', 'ограф'], 'scores': [12.6640625, 12.5, 11.671875, 11.1171875, 10.8984375], 'ids': [90704, 99790, 91064, 97648, 36224]}, 1: {'tokens': ['irat', 'baid', ' Pallars', 'ntic', ' remains'], 'scores': [11.3671875, 11.1953125, 10.609375, 10.5390625, 10.1640625], 'ids': [83977, 62627, 43386, 6466, 7544]}, 2: {'tokens': [' court', ' cause', ' victim', ' amount', 'мани'], 'scores': [10.5390625, 10.421875, 10.375, 10.328125, 10.1640625], 'ids': [6518, 4309, 17180, 3744, 27802]}, 3: {'tokens': ['��', 'odox', 'Sec', 'Co', 'Bind'], 'scores': [7.78515625, 6.84375, 6.78125, 6.640625, 6.33984375], 'ids': [689, 35024, 8508, 8854, 22641]}, 4: {'tokens': [' Braves', ' Warriors', ' Bruins', ' Reds', ' Bears'], 'scores': [18.640625, 18.546875, 18.21875, 17.78125, 17.484375], 'ids': [97762, 51354, 98696, 77886, 50243]}, 5: {'tokens': ['息', 'ipre', 'ivil', ' S', '\\tS'], 'scores': [9.5234375, 9.4453125, 9.3125, 9.1875, 8.609375], 'ids': [3714, 71905, 5525, 324, 54161]}}\n"
     ]
    }
   ],
   "source": [
    "print(expert_topk_tokens[26][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shared_expert_outputs(model, input_ids):\n",
    "    \"\"\"Get outputs from shared experts in all MoE layers\"\"\"\n",
    "    shared_outputs_list = []\n",
    "    hidden_states_list = []\n",
    "    moe_layers = []\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        # Capture hidden states entering the MoE layer\n",
    "        hidden_states = input[0]\n",
    "        hidden_states_list.append(hidden_states.squeeze(0).detach())\n",
    "        return output\n",
    "\n",
    "    hooks = []\n",
    "    # Identify MoE layers and register hooks\n",
    "    for layer in model.model.layers:\n",
    "        if layer.mlp.__class__.__name__ == 'DeepseekMoE':\n",
    "            moe_layers.append(layer.mlp)\n",
    "            hook = layer.mlp.gate.register_forward_hook(hook_fn)\n",
    "            hooks.append(hook)\n",
    "\n",
    "    # Forward pass to collect hidden states\n",
    "    with torch.no_grad():\n",
    "        model(input_ids)\n",
    "    \n",
    "    # Remove hooks after forward pass\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    # Compute shared expert outputs for each MoE layer\n",
    "    for layer_idx, moe_layer in enumerate(moe_layers):\n",
    "        hidden_states = hidden_states_list[layer_idx]\n",
    "        \n",
    "        # Get output from shared experts (which is a single DeepseekMLP)\n",
    "        with torch.no_grad():\n",
    "            expert_out = moe_layer.shared_experts(hidden_states)\n",
    "        layer_shared_outputs = [expert_out]\n",
    "        \n",
    "        # Stack outputs: [num_shared_experts=1, seq_len, hidden_dim] \n",
    "        shared_outputs_list.append(torch.stack(layer_shared_outputs, dim=0))\n",
    "\n",
    "    # Stack all layer outputs to get shape [num_layers, num_shared_experts=1, seq_len, hidden_dim]\n",
    "    shared_outputs_tensor = torch.stack(shared_outputs_list, dim=0)\n",
    "\n",
    "    return {\n",
    "        'shared_expert_outputs': shared_outputs_tensor,\n",
    "        'hidden_states': torch.stack(hidden_states_list) if hidden_states_list else None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 1, 5, 2048])\n",
      "Shared expert outputs : 27\n",
      "First layer shared outputs shape: torch.Size([1, 5, 2048])\n"
     ]
    }
   ],
   "source": [
    "# Get shared expert outputs separately\n",
    "shared_data = get_shared_expert_outputs(model, input_ids)\n",
    "\n",
    "print(shared_data['shared_expert_outputs'].shape)\n",
    "print(f\"Shared expert outputs : {len(shared_data['shared_expert_outputs'])}\")\n",
    "print(f\"First layer shared outputs shape: {shared_data['shared_expert_outputs'][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 102400\n",
      "Expert logits shape: torch.Size([27, 1, 5, 102400])\n"
     ]
    }
   ],
   "source": [
    "expert_logits = project_expert_outputs(model, expert_outputs=shared_data['shared_expert_outputs'])\n",
    "expert_topk_tokens = get_expert_topk_tokens(expert_logits, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['ა', ' <!--[', 'ა�', 'ELY', '\\tandroid'],\n",
       " 'scores': [1.1953125, 1.1025390625, 1.1005859375, 1.0888671875, 1.0205078125],\n",
       " 'ids': [46554, 69586, 56166, 70939, 97199]}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expert_topk_tokens[19][0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
