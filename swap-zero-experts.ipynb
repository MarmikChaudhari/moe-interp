{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ[\"PYTORCH_TRANSFORMERS_SDP_BACKEND\"] = \"flash\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name=\"allenai/OLMoE-1B-7B-0924\"):\n",
    "    # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_expert_weights(model, layer_idx, expert_idx):\n",
    "    \"\"\"\n",
    "    Print the weights of a specific expert MLP at a given layer.\n",
    "    \n",
    "    Args:\n",
    "        model: The OLMoE model\n",
    "        layer_idx: Index of the layer containing the expert\n",
    "        expert_idx: Index of the expert within the layer\n",
    "    \"\"\"\n",
    "    gate_proj = f'model.layers.{layer_idx}.mlp.experts.{expert_idx}.gate_proj.weight'\n",
    "    up_proj = f'model.layers.{layer_idx}.mlp.experts.{expert_idx}.up_proj.weight'\n",
    "    down_proj = f'model.layers.{layer_idx}.mlp.experts.{expert_idx}.down_proj.weight'\n",
    "    \n",
    "    print(f\"\\nGate Projection for expert {expert_idx} in layer {layer_idx}:\")\n",
    "    print(model.state_dict()[gate_proj])\n",
    "    print(f\"\\nUp Projection for expert {expert_idx} in layer {layer_idx}:\") \n",
    "    print(model.state_dict()[up_proj])\n",
    "    print(f\"\\nDown Projection for expert {expert_idx} in layer {layer_idx}:\")\n",
    "    print(model.state_dict()[down_proj])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### printing weights for experts before swapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_expert_weights(model, layer_idx=0, expert_idx=29)\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80 + \"\\n\")  # Separator for readability\n",
    "\n",
    "# print_expert_weights(model, layer_idx=0, expert_idx=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vanilla inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the model with a prompt on vanilla model\n",
    "# prompt = (\"\"\" \n",
    "# Continue the poem naturally and coherently, maintaining consistency with the rhyme scheme, diction and imagery. Match the poem's tone and style precisely.\n",
    "\n",
    "# we measure rainfall in memories now\n",
    "# count droplets like endangered species\n",
    "# my grandmother's garden is underwater\n",
    "# but the roses still bloom, phosphorescent\n",
    "# in depths where submarines chart\n",
    "# the coordinates of lost cities, while above                  \n",
    "# \"\"\")\n",
    "\n",
    "# # Convert the prompt to inputs and run a forward pass\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "# # Generate output (since it's a causal LM, we need to generate text from input)\n",
    "# outputs = model.generate(\n",
    "#     inputs['input_ids'],  # Only provide input_ids to generate\n",
    "#     attention_mask=inputs['attention_mask'],  # Add attention mask to not attend to padding tokens\n",
    "#     max_new_tokens=156,    # Generate 1024 new tokens\n",
    "#     temperature=0.6,       # Control randomness\n",
    "#     # top_k=100,  # Use top-k sampling\n",
    "#     do_sample=True,        # Use sampling instead of greedy decoding\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "#     pad_token_id=tokenizer.eos_token_id  # Set padding token\n",
    "# )\n",
    "\n",
    "# # Decode the generated output\n",
    "# generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# # Print the original prompt and generated response\n",
    "# print(\"Prompt:\", prompt)\n",
    "# print(\"\\nGenerated response :\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### swap the experts of source layer with target layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_experts(model, expert_idx, target_layer_idx, source_layer_idx=0, source_expert_idx=0):\n",
    "    \"\"\"\n",
    "    Swap experts between two layers in the OLMoE model.\n",
    "    \n",
    "    Args :\n",
    "        model: The OLMoE model\n",
    "        expert_idx: Index of the expert in target layer to swap with\n",
    "        target_layer_idx: Index of the layer containing the expert to swap with\n",
    "        source_layer_idx: Index of the source layer (default 0)\n",
    "        source_expert_idx: Index of the source expert (default 0)\n",
    "\n",
    "    \"\"\"\n",
    "    # Access the decoder layers\n",
    "    decoder_layers = model.model.layers\n",
    "    \n",
    "    # Verify indices are valid\n",
    "    num_layers = len(decoder_layers)\n",
    "    if target_layer_idx >= num_layers or source_layer_idx >= num_layers:\n",
    "        raise ValueError(f\"Layer index out of range. Model has {num_layers} layers.\")\n",
    "    \n",
    "    # Get the MoE blocks from both layers\n",
    "    source_moe = decoder_layers[source_layer_idx].mlp\n",
    "    target_moe = decoder_layers[target_layer_idx].mlp\n",
    "    \n",
    "    # Verify expert indices are valid\n",
    "    num_experts = len(source_moe.experts)\n",
    "    if expert_idx >= num_experts or source_expert_idx >= num_experts:\n",
    "        raise ValueError(f\"Expert index out of range. Each layer has {num_experts} experts.\")\n",
    "        \n",
    "    # Swap the expert weights\n",
    "    source_expert = source_moe.experts[source_expert_idx]\n",
    "    target_expert = target_moe.experts[expert_idx]\n",
    "    \n",
    "    # Swap gate projection weights\n",
    "    source_expert.gate_proj.weight, target_expert.gate_proj.weight = \\\n",
    "        target_expert.gate_proj.weight, source_expert.gate_proj.weight\n",
    "        \n",
    "    # Swap up projection weights\n",
    "    source_expert.up_proj.weight, target_expert.up_proj.weight = \\\n",
    "        target_expert.up_proj.weight, source_expert.up_proj.weight\n",
    "        \n",
    "    # Swap down projection weights  \n",
    "    source_expert.down_proj.weight, target_expert.down_proj.weight = \\\n",
    "        target_expert.down_proj.weight, source_expert.down_proj.weight\n",
    "    \n",
    "    return {\n",
    "        'swapped_experts': {\n",
    "            'source': {\n",
    "                'layer': source_layer_idx,\n",
    "                'expert': source_expert_idx\n",
    "            },\n",
    "            'target': {\n",
    "                'layer': target_layer_idx,\n",
    "                'expert': expert_idx\n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 0, expert 29 with expert 1 in layer 2\n",
      "torch.Size([1024, 2048])\n",
      "Swapped experts at layer 0, expert 42 with expert 12 in layer 2\n"
     ]
    }
   ],
   "source": [
    "# lists of experts to swap\n",
    "source_experts = [29,42]  \n",
    "target_experts = [1,12]  \n",
    "\n",
    "source_layer_idx = 0  \n",
    "target_layer_idx = 2  \n",
    "\n",
    "# Swap experts at each index\n",
    "for i in range(len(source_experts)):\n",
    "    swap_experts(model, expert_idx=target_experts[i], target_layer_idx=target_layer_idx, source_layer_idx=source_layer_idx, source_expert_idx=source_experts[i])\n",
    "    print(f\"Swapped experts at layer {source_layer_idx}, expert {source_experts[i]} with expert {target_experts[i]} in layer {target_layer_idx}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### printing weights for experts after swapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_expert_weights(model, layer_idx=0, expert_idx=29)\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80 + \"\\n\")  # Separator for readability\n",
    "\n",
    "# print_expert_weights(model, layer_idx=2, expert_idx=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with a prompt\n",
    "prompt = (\"\"\"    \n",
    "Continue this text in a natural and coherent way, maintaining consistency with the style, \n",
    "terminology, and logical flow of the preceding text.\n",
    "          \n",
    "\\\\title{Quantum Error Mitigation in NISQ Devices}\n",
    "\\\\begin{abstract}\n",
    "We present a novel approach to error mitigation in noisy intermediate-scale quantum (NISQ) devices. \n",
    "Our method introduces a scaling framework for quantum channels that preserves gate fidelity while reducing environmental noise.\n",
    "\\end{abstract}\n",
    "\\section{Introduction}\n",
    "Recent advances in NISQ devices have demonstrated both promise and limitations in quantum computation. \n",
    "The primary challenge remains decoherence, which introduces errors in quantum operations. We propose a channel scaling approach \n",
    "$\\mathcal{N}(\\\\rho) = e^{-\\lambda t}\\\\rho$ \n",
    "that provides a systematic way to\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Convert the prompt to inputs and run a forward pass\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "# Generate output (since it's a causal LM, we need to generate text from input)\n",
    "outputs = model.generate(\n",
    "    inputs['input_ids'],  # Only provide input_ids to generate\n",
    "    attention_mask=inputs['attention_mask'],  # Add attention mask to not attend to padding tokens\n",
    "    max_new_tokens=156,    # Generate 100 new tokens\n",
    "    temperature=0.6,       # Control randomness\n",
    "    do_sample=True,        # Use sampling instead of greedy decoding\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id  # Set padding token\n",
    ")\n",
    "\n",
    "# Decode the generated output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the original prompt and generated response\n",
    "print(\"Prompt :\", prompt)\n",
    "print(\"\\nGenerated response :\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### top 8 and bottom 8 experts for each layer for `inputs.txt` for `OLMoE-1B-7B-0924`\n",
    "Layer `0`\n",
    "First 8 keys: `[0, 6, 36, 41, 52, 10, 49, 21]`\n",
    "Last 8 keys: `[43, 20, 63, 7, 58, 34, 39, 9]`\n",
    "\n",
    "Layer `1`\n",
    "First 8 keys: `[47, 18, 61, 25, 5, 16, 27, 7]`\n",
    "Last 8 keys: `[3, 6, 33, 52, 56, 10, 43, 57]`\n",
    "\n",
    "Layer `2`\n",
    "First 8 keys: `[60, 10, 61, 45, 40, 30, 15, 26]`\n",
    "Last 8 keys: `[53, 4, 55, 41, 2, 8, 51, 59]`\n",
    "\n",
    "Layer `3`\n",
    "First 8 keys: `[35, 9, 6, 62, 15, 51, 19, 43]`\n",
    "Last 8 keys: `[1, 44, 24, 25, 60, 26, 16, 17]`\n",
    "\n",
    "Layer `4`\n",
    "First 8 keys: `[17, 21, 6, 27, 2, 14, 25, 55]`\n",
    "Last 8 keys: `[36, 13, 54, 12, 57, 18, 15, 32]`\n",
    "\n",
    "Layer `5`\n",
    "First 8 keys: `[0, 60, 31, 57, 37, 17, 21, 2]`\n",
    "Last 8 keys: `[32, 47, 51, 24, 49, 63, 15, 26]`\n",
    "\n",
    "Layer `6`\n",
    "First 8 keys: `[57, 62, 18, 36, 52, 40, 4, 31]`\n",
    "Last 8 keys: `[25, 21, 45, 34, 14, 39, 11, 56]`\n",
    "\n",
    "Layer `7`\n",
    "First 8 keys: `[17, 58, 35, 2, 4, 59, 21, 61]`\n",
    "Last 8 keys: `[43, 47, 41, 27, 53, 6, 51, 8]`\n",
    "\n",
    "Layer `8`\n",
    "First 8 keys: `[16, 54, 14, 18, 32, 3, 37, 6]`\n",
    "Last 8 keys: `[9, 25, 33, 4, 47, 62, 19, 11]`\n",
    "\n",
    "Layer `9`\n",
    "First 8 keys: `[5, 8, 6, 4, 28, 14, 7, 20]`\n",
    "Last 8 keys: `[61, 10, 0, 60, 35, 58, 56, 37]`\n",
    "\n",
    "Layer `10`\n",
    "First 8 keys: `[56, 43, 11, 59, 22, 60, 13, 28]`\n",
    "Last 8 keys: `[53, 63, 57, 33, 1, 31, 10, 54]`\n",
    "\n",
    "Layer `11`\n",
    "First 8 keys: `[47, 23, 27, 51, 54, 33, 52, 25]`\n",
    "Last 8 keys: `[55, 29, 49, 14, 53, 19, 17, 59]`\n",
    "\n",
    "Layer `12`\n",
    "First 8 keys: `[43, 55, 59, 38, 31, 58, 47, 44]`\n",
    "Last 8 keys: `[18, 19, 27, 61, 45, 3, 37, 0]`\n",
    "\n",
    "Layer `13`\n",
    "First 8 keys: `[2, 32, 5, 20, 25, 22, 55, 61]`\n",
    "Last 8 keys: `[56, 17, 40, 1, 48, 52, 21, 36]`\n",
    "\n",
    "Layer `14`\n",
    "First 8 keys: `[9, 58, 6, 4, 24, 52, 11, 17]`\n",
    "Last 8 keys: `[41, 3, 26, 13, 43, 25, 27, 55]`\n",
    "\n",
    "Layer `15`\n",
    "First 8 keys: `[17, 1, 34, 44, 50, 45, 30, 54]`\n",
    "Last 8 keys: `[18, 15, 37, 26, 7, 38, 21, 51]`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sorting expert dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_and_bottom_keys(input_dict, n=8):\n",
    "    # Sorting by values in decreasing order\n",
    "    sorted_items = sorted(input_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "    # Extract keys from the sorted items\n",
    "    sorted_keys = [item[0] for item in sorted_items]\n",
    "    # Get the first n keys and the last n keys\n",
    "    first_n_keys = sorted_keys[:n]\n",
    "    last_n_keys = sorted_keys[-n:]\n",
    "\n",
    "    return first_n_keys, last_n_keys\n",
    "\n",
    "# Example usage with layer 3 dict\n",
    "example_dict =  {27: 151, 19: 19, 55: 86, 6: 162, 25: 105, 41: 50, 48: 26, 17: 753, 3: 44, 21: 275, 7: 63, 61: 23, 39: 31, 59: 16, 2: 141, 43: 14, 10: 11, 62: 14, 33: 32, 11: 16, 52: 34, 14: 106, 35: 35, 5: 44, 60: 27, 24: 23, 58: 18, 63: 9, 51: 38, 8: 23, 4: 10, 16: 11, 23: 12, 13: 6, 22: 14, 38: 21, 34: 26, 26: 28, 29: 9, 50: 20, 46: 16, 9: 21, 37: 22, 28: 9, 53: 48, 1: 30, 20: 21, 45: 36, 57: 3, 42: 45, 56: 12, 44: 13, 30: 24, 40: 12, 0: 21, 18: 3, 54: 6, 12: 5, 36: 7, 15: 2, 32: 1}\n",
    "\n",
    "first_8_keys, last_8_keys = get_top_and_bottom_keys(example_dict)\n",
    "print(\"First 8 keys:\", first_8_keys)\n",
    "print(\"Last 8 keys:\", last_8_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zero out experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_expert(model, layer_idx, expert_idx):\n",
    "    \"\"\"\n",
    "    Zero out a specific expert in a specific layer of the OLMoE model.\n",
    "    \n",
    "    Args:\n",
    "        model: The OLMoE model\n",
    "        layer_idx: Index of the layer containing the expert\n",
    "        expert_idx: Index of the expert to zero out\n",
    "    \"\"\"\n",
    "    # Access the decoder layers\n",
    "    decoder_layers = model.model.layers\n",
    "    \n",
    "    # Verify indices are valid\n",
    "    num_layers = len(decoder_layers)\n",
    "    if layer_idx >= num_layers:\n",
    "        raise ValueError(f\"Layer index out of range. Model has {num_layers} layers.\")\n",
    "    \n",
    "    # Get the MoE block\n",
    "    moe = decoder_layers[layer_idx].mlp\n",
    "    \n",
    "    # Verify expert index is valid\n",
    "    num_experts = len(moe.experts)\n",
    "    if expert_idx >= num_experts:\n",
    "        raise ValueError(f\"Expert index out of range. Layer has {num_experts} experts.\")\n",
    "        \n",
    "    # Get the expert\n",
    "    expert = moe.experts[expert_idx]\n",
    "    \n",
    "    # Zero out all weights in the expert\n",
    "    expert.gate_proj.weight.data.zero_()\n",
    "    expert.up_proj.weight.data.zero_()\n",
    "    expert.down_proj.weight.data.zero_()\n",
    "    \n",
    "    return {\n",
    "        'zeroed_expert': {\n",
    "            'layer': layer_idx,\n",
    "            'expert': expert_idx\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_multiple_experts(model, expert_indices_per_layer):\n",
    "    \"\"\"\n",
    "    Zero out multiple experts across different layers.\n",
    "    \n",
    "    Args:\n",
    "        model: The OLMoE model\n",
    "        expert_indices_per_layer: Dict mapping layer indices to lists of expert indices to zero\n",
    "                                e.g. {0: [0,1], 1: [2,3]} zeros experts 0,1 in layer 0 and 2,3 in layer 1\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for layer_idx, expert_indices in expert_indices_per_layer.items():\n",
    "        for expert_idx in expert_indices:\n",
    "            result = zero_expert(model, layer_idx, expert_idx)\n",
    "            results.append(result)\n",
    "            print(f\"Zeroed out expert {expert_idx} in layer {layer_idx}\")\n",
    "            \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
