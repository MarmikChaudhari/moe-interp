Successful predictions are among the most compelling validations of any
model. Extracting falsifiable predictions from nonlinear multiparameter models
is complicated by the fact that such models are commonly sloppy, possessing
sensitivities to different parameter combinations that range over many decades.
Here we discuss how sloppiness affects the sorts of data that best constrain
model predictions, makes linear uncertainty approximations dangerous, and
introduces computational difficulties in Monte-Carlo uncertainty analysis. We
also present a useful test problem and suggest refinements to the standards by
which models are communicated.

In most vertebrate species, the body axis is generated by the formation of
repeated transient structures called somites. This spatial periodicity in
somitogenesis has been related to the temporally sustained oscillations in
certain mRNAs and their associated gene products in the cells forming the
presomatic mesoderm. The mechanism underlying these oscillations have been
identified as due to the delays involved in the synthesis of mRNA and
translation into protein molecules [J. Lewis, Current Biol. {\bf 13}, 1398
(2003)]. In addition, in the zebrafish embryo intercellular Notch signalling
couples these oscillators and a longitudinal positional information signal in
the form of an Fgf8 gradient exists that could be used to transform these
coupled temporal oscillations into the observed spatial periodicity of somites.
Here we consider a simple model based on this known biology and study its
consequences for somitogenesis. Comparison is made with the known properties of
somite formation in the zebrafish embryo . We also study the effects of
localized Fgf8 perturbations on somite patterning.

We have developed a linearization method to investigate the subthreshold
oscillatory behaviors in nonlinear autonomous systems. By considering firstly
the neuronal system as an example, we show that this theoretical approach can
predict quantitatively the subthreshold oscillatory activities, including the
damping coefficients and the oscillatory frequencies which are in good
agreement with those observed in experiments. Then we generalize the
linearization method to an arbitrary autonomous nonlinear system. The detailed
extension of this theoretical approach is also presented and further discussed.

The widespread use of genetic testing in high risk pregnancies has created
strong interest in rapid and accurate molecular diagnostics for common
chromosomal aneuploidies. We show here that digital polymerase chain reaction
(dPCR) can be used for accurate measurement of trisomy 21 (Down's Syndrome),
the most common human aneuploidy. dPCR is generally applicable to any
aneuploidy, does not depend on allelic distribution or gender, and is able to
detect signals in the presence of mosaics or contaminating maternal DNA.

Biologists are leading current research on genome characterization
(sequencing, alignment, transcription), providing a huge quantity of raw data
about many genome organisms. Extracting knowledge from this raw data is an
important process for biologists, using usually data mining approaches.
However, it is difficult to deals with these genomic information using actual
bioinformatics data mining tools, because data are heterogeneous, huge in
quantity and geographically distributed. In this paper, we present a new
approach between data mining and virtual reality visualization, called visual
data mining. Indeed Virtual Reality becomes ripe, with efficient display
devices and intuitive interaction in an immersive context. Moreover, biologists
use to work with 3D representation of their molecules, but in a desktop
context. We present a software solution, Genome3DExplorer, which addresses the
problem of genomic data visualization, of scene management and interaction.
This solution is based on a well-adapted graphical and interaction paradigm,
where local and global topological characteristics of data are easily visible,
on the contrary to traditional genomic database browsers, always focused on the
zoom and details level.

This paper presents a stability test for a class of interconnected nonlinear
systems motivated by biochemical reaction networks. One of the main results
determines global asymptotic stability of the network from the diagonal
stability of a "dissipativity matrix" which incorporates information about the
passivity properties of the subsystems, the interconnection structure of the
network, and the signs of the interconnection terms. This stability test
encompasses the "secant criterion" for cyclic networks presented in our
previous paper, and extends it to a general interconnection structure
represented by a graph. A second main result allows one to accommodate state
products. This extension makes the new stability criterion applicable to a
broader class of models, even in the case of cyclic systems. The new stability
test is illustrated on a mitogen activated protein kinase (MAPK) cascade model,
and on a branched interconnection structure motivated by metabolic networks.
Finally, another result addresses the robustness of stability in the presence
of diffusion terms in a compartmental system made out of identical systems.

Mechanistic home range models are important tools in modeling animal dynamics
in spatially-complex environments. We introduce a class of stochastic models
for animal movement in a habitat of varying preference. Such models interpolate
between spatially-implicit resource selection analysis (RSA) and
advection-diffusion models, possessing these two models as limiting cases. We
find a closed-form solution for the steady-state (equilibrium) probability
distribution u* using a factorization of the redistribution operator into
symmetric and diagonal parts. How space use is controlled by the preference
function w then depends on the characteristic width of the redistribution
kernel: when w changes rapidly compared to this width, u* ~ w, whereas on
global scales large compared to this width, u* ~ w^2. We analyse the behavior
at discontinuities in w which occur at habitat type boundaries. We simulate the
dynamics of space use given two-dimensional prey-availability data and explore
the effect of the redistribution kernel width. Our factorization allows such
numerical simulations to be done extremely fast; we expect this to aid the
computationally-intensive task of model parameter fitting and inverse modeling.

Transcription networks, and other directed networks can be characterized by
some topological observables such as for example subgraph occurrence (network
motifs). In order to perform such kind of analysis, it is necessary to be able
to generate suitable randomized network ensembles. Typically, one considers
null networks with the same degree sequences of the original ones. The commonly
used algorithms sometimes have long convergence times, and sampling problems.
We present here an alternative, based on a variant of the importance sampling
Montecarlo developed by Chen et al. [1].

It is basic question in biology and other fields to identify the char-
acteristic properties that on one hand are shared by structures from a
particular realm, like gene regulation, protein-protein interaction or neu- ral
networks or foodwebs, and that on the other hand distinguish them from other
structures. We introduce and apply a general method, based on the spectrum of
the normalized graph Laplacian, that yields repre- sentations, the spectral
plots, that allow us to find and visualize such properties systematically. We
present such visualizations for a wide range of biological networks and compare
them with those for networks derived from theoretical schemes. The differences
that we find are quite striking and suggest that the search for universal
properties of biological networks should be complemented by an understanding of
more specific features of biological organization principles at different
scales.

The classical attenuation regulation of gene expression in bacteria is
considered. We propose to represent the secondary RNA structure in the leader
region of a gene or an operon by a term, and we give a probabilistic term
rewriting system modeling the whole process of such a regulation.

Escherichia coli is a motile bacterium that moves up a chemoattractant
gradient by performing a biased random walk composed of alternating runs and
tumbles. Previous models of run and tumble chemotaxis neglect one or more
features of the motion, namely (i) a cell cannot directly detect a
chemoattractant gradient but rather makes temporal comparisons of
chemoattractant concentration, (ii) rather than being entirely random, tumbles
exhibit persistence of direction, meaning that the new direction after a tumble
is more likely to be in the forward hemisphere, and (iii) rotational Brownian
motion makes it impossible for an E. coli cell to swim in a straight line
during a run. This paper presents an analytic calculation of the chemotactic
drift velocity taking account of (i), (ii) and (iii), for weak chemotaxis. The
analytic results are verified by Monte Carlo simulation. The results reveal a
synergy between temporal comparisons and persistence that enhances the drift
velocity, while rotational Brownian motion reduces the drift velocity.

An eutactic star, in a n-dimensional space, is a set of N vectors which can
be viewed as the projection of N orthogonal vectors in a N-dimensional space.
By adequately associating a star of vectors to a particular sea urchin we
propose that a measure of the eutacticity of the star constitutes a measure of
the regularity of the sea urchin. Then we study changes of regularity
(eutacticity) in a macroevolutive and taxonomic level of sea urchins belonging
to the Echinoidea Class. An analysis considering changes through geological
time suggests a high degree of regularity in the shape of these organisms
through their evolution. Rare deviations from regularity measured in
Holasteroida order are discussed.

Statistical mechanics is one of the most powerful and elegant tools in the
quantitative sciences. One key virtue of statistical mechanics is that it is
designed to examine large systems with many interacting degrees of freedom,
providing a clue that it might have some bearing on the analysis of the
molecules of living matter. As a result of data on biological systems becoming
increasingly quantitative, there is a concomitant demand that the models set
forth to describe biological systems be themselves quantitative. We describe
how statistical mechanics is part of the quantitative toolkit that is needed to
respond to such data. The power of statistical mechanics is not limited to
traditional physical and chemical problems and there are a host of interesting
ways in which these ideas can be applied in biology. This article reports on
our efforts to teach statistical mechanics to life science students and
provides a framework for others interested in bringing these tools to a
nontraditional audience in the life sciences.

MOTIVATION: Microarray technology makes it possible to measure thousands of
variables and to compare their values under hundreds of conditions. Once
microarray data are quantified, normalized and classified, the analysis phase
is essentially a manual and subjective task based on visual inspection of
classes in the light of the vast amount of information available. Currently,
data interpretation clearly constitutes the bottleneck of such analyses and
there is an obvious need for tools able to fill the gap between data processed
with mathematical methods and existing biological knowledge. RESULTS: THEA
(Tools for High-throughput Experiments Analysis) is an integrated information
processing system allowing convenient handling of data. It allows to
automatically annotate data issued from classification systems with selected
biological information coming from a knowledge base and to either manually
search and browse through these annotations or automatically generate
meaningful generalizations according to statistical criteria (data mining).
AVAILABILITY: The software is available on the website http://thea.unice.fr/

In this paper we address a general parameter estimation methodology for an
extended biokinetic degradation model [1] for poorly degradable
micropollutants. In particular we concentrate on parameter estimation of the
micropollutant degradation sub-model by specialised microorganisms. In this
case we focus on the case when only substrate degradation data are available
and prove the structural identifiability of the model. Further we consider the
problem of practical identifiability and propose experimental and related
numerical methods for unambiguous parameter estimation based on multiple
substrate degradation curves with different initial concentrations. Finally by
means of simulated pseudo-experiments we have found convincing indications that
the proposed algorithm is stable and yields appropriate parameter estimates
even in unfavourable regimes.

Predicting interactions between small molecules and proteins is a crucial
ingredient of the drug discovery process. In particular, accurate predictive
models are increasingly used to preselect potential lead compounds from large
molecule databases, or to screen for side-effects. While classical in silico
approaches focus on predicting interactions with a given specific target, new
chemogenomics approaches adopt cross-target views. Building on recent
developments in the use of kernel methods in bio- and chemoinformatics, we
present a systematic framework to screen the chemical space of small molecules
for interaction with the biological space of proteins. We show that this
framework allows information sharing across the targets, resulting in a
dramatic improvement of ligand prediction accuracy for three important classes
of drug targets: enzymes, GPCR and ion channels.

BACKGROUND: One of the most evident achievements of bioinformatics is the
development of methods that transfer biological knowledge from characterised
proteins to uncharacterised sequences. This mode of protein function assignment
is mostly based on the detection of sequence similarity and the premise that
functional properties are conserved during evolution. Most automatic approaches
developed to date rely on the identification of clusters of homologous proteins
and the mapping of new proteins onto these clusters, which are expected to
share functional characteristics. RESULTS: Here, we inverse the logic of this
process, by considering the mapping of sequences directly to a functional
classification instead of mapping functions to a sequence clustering. In this
mode, the starting point is a database of labelled proteins according to a
functional classification scheme, and the subsequent use of sequence similarity
allows defining the membership of new proteins to these functional classes. In
this framework, we define the Correspondence Indicators as measures of
relationship between sequence and function and further formulate two Bayesian
approaches to estimate the probability for a sequence of unknown function to
belong to a functional class. This approach allows the parametrisation of
different sequence search strategies and provides a direct measure of
annotation error rates. We validate this approach with a database of enzymes
labelled by their corresponding four-digit EC numbers and analyse specific
cases. CONCLUSION: The performance of this method is significantly higher than
the simple strategy consisting in transferring the annotation from the highest
scoring BLAST match and is expected to find applications in automated
functional annotation pipelines.

In many biochemical processes, proteins bound to DNA at distant sites are
brought into close proximity by loops in the underlying DNA. For example, the
function of some gene-regulatory proteins depends on such DNA looping
interactions. We present a new technique for characterizing the kinetics of
loop formation in vitro, as observed using the tethered particle method, and
apply it to experimental data on looping induced by lambda repressor. Our
method uses a modified (diffusive) hidden Markov analysis that directly
incorporates the Brownian motion of the observed tethered bead. We compare
looping lifetimes found with our method (which we find are consistent over a
range of sampling frequencies) to those obtained via the traditional
threshold-crossing analysis (which can vary depending on how the raw data are
filtered in the time domain). Our method does not involve any time filtering
and can detect sudden changes in looping behavior. For example, we show how our
method can identify transitions between long-lived, kinetically distinct states
that would otherwise be difficult to discern.

We propose a general framework for converting global and local similarities
between biological sequences to quasi-metrics. In contrast to previous works,
our formulation allows asymmetric distances, originating from uneven weighting
of strings, that may induce non-trivial partial orders on sets of biosequences.
Furthermore, the $\ell^p$-type distances considered are more general than
traditional generalized string edit distances corresponding to the $\ell^1$
case, and enable conversion of sequence similarities to distances for a much
wider class of scoring schemes. Our constructions require much less restrictive
gap penalties than the ones regularly used. Numerous examples are provided to
illustrate the concepts introduced and their potential applications.

Environment specific substitution tables have been used effectively for
distinguishing structural and functional constraints on proteins and thereby
identify their active sites (Chelliah et al. (2004)). This work explores
whether a similar approach can be used to identify specificity determining
residues (SDRs) responsible for cofactor dependence, substrate specificity or
subtle catalytic variations. We combine structure-sequence information and
functional annotation from various data sources to create structural alignments
for homologous enzymes and functional partitions therein. We develop a scoring
procedure to predict SDRs and assess their accuracy using information from
bound specific ligands and published literature.

Transforming growth factor (TGF) $\beta$ is known to have properties of both
a tumor suppressor and a tumor promoter. While it inhibits cell proliferation,
it also increases cell motility and decreases cell--cell adhesion. Coupling
mathematical modeling and experiments, we investigate the growth and motility
of oncogene--expressing human mammary epithelial cells under exposure to
TGF--$\beta$. We use a version of the well--known Fisher--Kolmogorov equation,
and prescribe a procedure for its parametrization. We quantify the simultaneous
effects of TGF--$\beta$ to increase the tendency of individual cells and cell
clusters to move randomly and to decrease overall population growth. We
demonstrate that in experiments with TGF--$\beta$ treated cells \textit{in
vitro}, TGF--$\beta$ increases cell motility by a factor of 2 and decreases
cell proliferation by a factor of 1/2 in comparison with untreated cells.

Fast, efficient and reliable algorithms for pairwise alignment of protein
structures are in ever increasing demand for analyzing the rapidly growing data
of protein structures. CLePAPS is a tool developed for this purpose. It
distinguishes itself from other existing algorithms by the use of
conformational letters, which are discretized states of 3D segmental structural
states. A letter corresponds to a cluster of combinations of the three angles
formed by C_alpha pseudobonds of four contiguous residues. A substitution
matrix called CLESUM is available to measure similarity between any two such
letters. CLePAPS regards an aligned fragment pair (AFP) as an ungapped string
pair with a high sum of pairwise CLESUM scores. Using CLESUM scores as the
similarity measure, CLePAPS searches for AFPs by simple string comparison. The
transformation which best superimposes a highly similar AFP can be used to
superimpose the structure pairs under comparison. A highly scored AFP which is
consistent with several other AFPs determines an initial alignment. CLePAPS
then joins consistent AFPs guided by their similarity scores to extend the
alignment by several `zoom-in' iteration steps. A follow-up refinement produces
the final alignment. CLePAPS does not implement dynamic programming. The
utility of CLePAPS is tested on various protein structure pairs.

The key to understanding a protein's function often lies in its
conformational dynamics. We develop a coarse-grained variational model to
investigate the interplay between structural transitions, conformational
flexibility and function of N-terminal calmodulin (nCaM) domain. In this model,
two energy basins corresponding to the ``closed'' apo conformation and ``open''
holo conformation of nCaM domain are connected by a uniform interpolation
parameter. The resulting detailed transition route from our model is largely
consistent with the recently proposed EF$\beta$-scaffold mechanism in EF-hand
family proteins. We find that the N-terminal part in calcium binding loops I
and II shows higher flexibility than the C-terminal part which form this
EF$\beta$-scaffold structure. The structural transition of binding loops I and
II are compared in detail. Our model predicts that binding loop II, with higher
flexibility and early structural change than binding loop I, dominates the
conformational transition in nCaM domain.

Various physical properties such as dipole moment, heat of formation and
energy of the most stable formation of nucleotides and bases were calculated by
PM3 (modified neglect of diatomic overlap, parametric method number 3) and AM1
(Austin model 1) methods. As distinct from previous calculations, for
nucleotides the interaction with neighbours is taken into account up to
gradient of convergence equaling 1. The dependences of these variables from the
place in the codon and the determinative degree were obtained. The difference
of these variables for codons and anticodons is shown.

A multitude of measures have been proposed to quantify the similarity between
protein 3-D structure. Among these measures, contact map overlap (CMO)
maximization deserved sustained attention during past decade because it offers
a fine estimation of the natural homology relation between proteins. Despite
this large involvement of the bioinformatics and computer science community,
the performance of known algorithms remains modest. Due to the complexity of
the problem, they got stuck on relatively small instances and are not
applicable for large scale comparison. This paper offers a clear improvement
over past methods in this respect. We present a new integer programming model
for CMO and propose an exact B &B algorithm with bounds computed by solving
Lagrangian relaxation. The efficiency of the approach is demonstrated on a
popular small benchmark (Skolnick set, 40 domains). On this set our algorithm
significantly outperforms the best existing exact algorithms, and yet provides
lower and upper bounds of better quality. Some hard CMO instances have been
solved for the first time and within reasonable time limits. From the values of
the running time and the relative gap (relative difference between upper and
lower bounds), we obtained the right classification for this test. These
encouraging result led us to design a harder benchmark to better assess the
classification capability of our approach. We constructed a large scale set of
300 protein domains (a subset of ASTRAL database) that we have called Proteus
300. Using the relative gap of any of the 44850 couples as a similarity
measure, we obtained a classification in very good agreement with SCOP. Our
algorithm provides thus a powerful classification tool for large structure
databases.

This note discusses a theoretical issue regarding the application of the
"Modular Response Analysis" method to quasi-steady state (rather than
steady-state) data.

By convention, and even more often, as an unintentional consequence of
design, time distributions of latency and infectious durations in stochastic
epidemic simulations are often exponential. The skewed distribtion typically
leads to unrealistically short times. We examine the effects of altering the
distribution latency and infectious times by comparing the key results after
simulation with exponential and gamma distributions in a homogeneous mixing
model aswell as a model with regional divisions connected by a travel intensity
matrix. We show a delay in spread with more realistic latency times and offer
an explanation of the effect.

Sensitivity analysis is an effective tool for systematically identifying
specific perturbations in parameters that have significant effects on the
behavior of a given biosystem, at the scale investigated. In this work, using a
two-dimensional, multiscale non-small cell lung cancer (NSCLC) model, we
examine the effects of perturbations in system parameters which span both
molecular and cellular levels, i.e. across scales of interest. This is achieved
by first linking molecular and cellular activities and then assessing the
influence of parameters at the molecular level on the tumor's spatio-temporal
expansion rate, which serves as the output behavior at the cellular level.
Overall, the algorithm operated reliably over relatively large variations of
most parameters, hence confirming the robustness of the model. However, three
pathway components (proteins PKC, MEK, and ERK) and eleven reaction steps were
determined to be of critical importance by employing a sensitivity coefficient
as an evaluation index. Each of these sensitive parameters exhibited a similar
changing pattern in that a relatively larger increase or decrease in its value
resulted in a lesser influence on the system's cellular performance. This study
provides a novel cross-scaled approach to analyzing sensitivities of
computational model parameters and proposes its application to
interdisciplinary biomarker studies.

Heavy-tailed or power-law distributions are becoming increasingly common in
biological literature. A wide range of biological data has been fitted to
distributions with heavy tails. Many of these studies use simple fitting
methods to find the parameters in the distribution, which can give highly
misleading results. The potential pitfalls that can occur when using these
methods are pointed out, and a step-by-step guide to fitting power-law
distributions and assessing their goodness-of-fit is offered.

Despite recent molecular technique improvements, biological knowledge remains
incomplete. Reasoning on living systems hence implies to integrate
heterogeneous and partial informations. Although current investigations
successfully focus on qualitative behaviors of macromolecular networks, others
approaches show partial quantitative informations like protein concentration
variations over times. We consider that both informations, qualitative and
quantitative, have to be combined into a modeling method to provide a better
understanding of the biological system. We propose here such a method using a
probabilistic-like approach. After its exhaustive description, we illustrate
its advantages by modeling the carbon starvation response in Escherichia coli.
In this purpose, we build an original qualitative model based on available
observations. After the formal verification of its qualitative properties, the
probabilistic model shows quantitative results corresponding to biological
expectations which confirm the interest of our probabilistic approach.

Many complex biological, social, and economical networks show topologies
drastically differing from random graphs. But, what is a complex network, i.e.\
how can one quantify the complexity of a graph? Here the Offdiagonal Complexity
(OdC), a new, and computationally cheap, measure of complexity is defined,
based on the node-node link cross-distribution, whose nondiagonal elements
characterize the graph structure beyond link distribution, cluster coefficient
and average path length. The OdC apporach is applied to the {\sl Helicobacter
pylori} protein interaction network and randomly rewired surrogates thereof. In
addition, OdC is used to characterize the spatial complexity of cell
aggregates. We investigate the earliest embryo development states of
Caenorhabditis elegans. The development states of the premorphogenetic phase
are represented by symmetric binary-valued cell connection matrices with
dimension growing from 4 to 385. These matrices can be interpreted as adjacency
matrix of an undirected graph, or network. The OdC approach allows to describe
quantitatively the complexity of the cell aggregate geometry.

Two recent streams of work suggest that pairwise interactions may be
sufficient to capture the complexity of biological systems ranging from protein
structure to networks of neurons. In one approach, possible amino acid
sequences in a family of proteins are generated by Monte Carlo annealing of a
"Hamiltonian" that forces pairwise correlations among amino acid substitutions
to be close to the observed correlations. In the other approach, the observed
correlations among pairs of neurons are used to construct a maximum entropy
model for the states of the network as a whole. We show that, in certain
limits, these two approaches are mathematically equivalent, and we comment on
open problems suggested by this framework

Given a metabolic network in terms of its metabolites and reactions, our goal
is to efficiently compute the minimal knock out sets of reactions required to
block a given behaviour. We describe an algorithm which improves the
computation of these knock out sets when the elementary modes (minimal
functional subsystems) of the network are given. We also describe an algorithm
which computes both the knock out sets and the elementary modes containing the
blocked reactions directly from the description of the network and whose
worst-case computational complexity is better than the algorithms currently in
use for these problems. Computational results are included.

Over the last decade, a large variety of clustering algorithms have been
developed to detect coregulatory relationships among genes from microarray gene
expression data. Model based clustering approaches have emerged as
statistically well grounded methods, but the properties of these algorithms
when applied to large-scale data sets are not always well understood. An
in-depth analysis can reveal important insights about the performance of the
algorithm, the expected quality of the output clusters, and the possibilities
for extracting more relevant information out of a particular data set. We have
extended an existing algorithm for model based clustering of genes to
simultaneously cluster genes and conditions, and used three large compendia of
gene expression data for S. cerevisiae to analyze its properties. The algorithm
uses a Bayesian approach and a Gibbs sampling procedure to iteratively update
the cluster assignment of each gene and condition. For large-scale data sets,
the posterior distribution is strongly peaked on a limited number of
equiprobable clusterings. A GO annotation analysis shows that these local
maxima are all biologically equally significant, and that simultaneously
clustering genes and conditions performs better than only clustering genes and
assuming independent conditions. A collection of distinct equivalent
clusterings can be summarized as a weighted graph on the set of genes, from
which we extract fuzzy, overlapping clusters using a graph spectral method. The
cores of these fuzzy clusters contain tight sets of strongly coexpressed genes,
while the overlaps exhibit relations between genes showing only partial
coexpression.

The G-protein coupled receptor (GPCR) superfamily is currently the largest
class of therapeutic targets. \textit{In silico} prediction of interactions
between GPCRs and small molecules is therefore a crucial step in the drug
discovery process, which remains a daunting task due to the difficulty to
characterize the 3D structure of most GPCRs, and to the limited amount of known
ligands for some members of the superfamily. Chemogenomics, which attempts to
characterize interactions between all members of a target class and all small
molecules simultaneously, has recently been proposed as an interesting
alternative to traditional docking or ligand-based virtual screening
strategies. We propose new methods for in silico chemogenomics and validate
them on the virtual screening of GPCRs. The methods represent an extension of a
recently proposed machine learning strategy, based on support vector machines
(SVM), which provides a flexible framework to incorporate various information
sources on the biological space of targets and on the chemical space of small
molecules. We investigate the use of 2D and 3D descriptors for small molecules,
and test a variety of descriptors for GPCRs. We show fo instance that
incorporating information about the known hierarchical classification of the
target family and about key residues in their inferred binding pockets
significantly improves the prediction accuracy of our model. In particular we
are able to predict ligands of orphan GPCRs with an estimated accuracy of
78.1%.

A composite, exponential relaxation function, modulated by a periodic
component, was used to fit to an experimental time series of blood glucose
levels. The 11 parameters function that allows for the detection of a possible
rhythm transition was fitted to the experimental time series using a genetic
algorithm. It has been found that the relaxation from a hyperglycemic condition
following a change in the anti-diabetic treatment, can be characterized by a
change from an initial 12 hours ultradian rhythm to a near-24 hours circadian
rhythm.

In many experiments, the aim is to deduce an underlying multi-substate on-off
kinetic scheme (KS) from the statistical properties of a two-state trajectory.
However, the mapping of a KS into a two-state trajectory leads to the loss of
information about the KS, and so, in many cases, more than one KS can be
associated with the data. We recently showed that the optimal way to solve this
problem is to use canonical forms of reduced dimensions (RD). RD forms are
on-off networks with connections only between substates of different states,
where the connections can have non-exponential waiting time probability density
functions (WT-PDFs). In theory, only a single RD form can be associated with
the data. To utilize RD forms in the analysis of the data, a RD form should be
associated with the data. Here, we give a toolbox for building a RD form from a
finite two-state trajectory. The methods in the toolbox are based on known
statistical methods in data analysis, combined with statistical methods and
numerical algorithms designed specifically for the current problem. Our toolbox
is self-contained - it builds a mechanism based only on the information it
extracts from the data, and its implementation on the data is fast (analyzing a
10^6 cycle trajectory from a thirty-parameter mechanism takes a couple of hours
on a PC with a 2.66 GHz processor). The toolbox is automated and is freely
available for academic research upon electronic request.

MOTIVATION: The use of oligonucleotide microarray technology requires a very
detailed attention to the design of specific probes spotted on the solid phase.
These problems are far from being commonplace since they refer to complex
physicochemical constraints. Whereas there are more and more publicly available
programs for microarray oligonucleotide design, most of them use the same
algorithm or criteria to design oligos, with only little variation. RESULTS: We
show that classical approaches used in oligo design software may be inefficient
under certain experimental conditions, especially when dealing with complex
target mixtures. Indeed, our biological model is a human obligate parasite, the
microsporidia Encephalitozoon cuniculi. Targets that are extracted from
biological samples are composed of a mixture of pathogen transcripts and host
cell transcripts. We propose a new approach to design oligonucleotides which
combines good specificity with a potentially high sensitivity. This approach is
original in the biological point of view as well as in the algorithmic point of
view. We also present an experimental validation of this new strategy by
comparing results obtained with standard oligos and with our composite oligos.
A specific E.cuniculi microarray will overcome the difficulty to discriminate
the parasite mRNAs from the host cell mRNAs demonstrating the power of the
microarray approach to elucidate the lifestyle of an intracellular pathogen
using mix mRNAs.

Living cells are the product of gene expression programs that involve the
regulated transcription of thousands of genes. The elucidation of
transcriptional regulatory networks in thus needed to understand the cell's
working mechanism, and can for example be useful for the discovery of novel
therapeutic targets. Although several methods have been proposed to infer gene
regulatory networks from gene expression data, a recent comparison on a
large-scale benchmark experiment revealed that most current methods only
predict a limited number of known regulations at a reasonable precision level.
We propose SIRENE, a new method for the inference of gene regulatory networks
from a compendium of expression data. The method decomposes the problem of gene
regulatory network inference into a large number of local binary classification
problems, that focus on separating target genes from non-targets for each TF.
SIRENE is thus conceptually simple and computationally efficient. We test it on
a benchmark experiment aimed at predicting regulations in E. coli, and show
that it retrieves of the order of 6 times more known regulations than other
state-of-the-art inference methods.

We define the complexity of DNA sequences as the information content per
nucleotide, calculated by means of some Lempel-Ziv data compression algorithm.
It is possible to use the statistics of the complexity values of the functional
regions of different complete genomes to distinguish among genomes of different
domains of life (Archaea, Bacteria and Eukarya). We shall focus on the
distribution function of the complexity of noncoding regions. We show that the
three domains may be plotted in separate regions within the two-dimensional
space where the axes are the skewness coefficient and the curtosis coefficient
of the aforementioned distribution. Preliminary results on 15 genomes are
introduced.

Models of the dynamics of cellular interaction networks have become
increasingly larger in recent years. Formal verification based on model
checking provides a powerful technology to keep up with this increase in scale
and complexity. The application of model-checking approaches is hampered,
however, by the difficulty for non-expert users to formulate appropriate
questions in temporal logic. In order to deal with this problem, we propose the
use of patterns, that is, high-level query templates that capture recurring
biological questions and that can be automatically translated into temporal
logic. The applicability of the developed set of patterns has been investigated
by the analysis of an extended model of the network of global regulators
controlling the carbon starvation response in Escherichia coli.

Combination therapies are often needed for effective clinical outcomes in the
management of complex diseases, but presently they are generally based on
empirical clinical experience. Here we suggest a novel application of search
algorithms, originally developed for digital communication, modified to
optimize combinations of therapeutic interventions. In biological experiments
measuring the restoration of the decline with age in heart function and
exercise capacity in Drosophila melanogaster, we found that search algorithms
correctly identified optimal combinations of four drugs with only one third of
the tests performed in a fully factorial search. In experiments identifying
combinations of three doses of up to six drugs for selective killing of human
cancer cells, search algorithms resulted in a highly significant enrichment of
selective combinations compared with random searches. In simulations using a
network model of cell death, we found that the search algorithms identified the
optimal combinations of 6-9 interventions in 80-90% of tests, compared with
15-30% for an equivalent random search. These findings suggest that modified
search algorithms from information theory have the potential to enhance the
discovery of novel therapeutic drug combinations. This report also helps to
frame a biomedical problem that will benefit from an interdisciplinary effort
and suggests a general strategy for its solution.

Any cutting-edge scientific research project requires a myriad of
computational tools for data generation, management, analysis and
visualization. Python is a flexible and extensible scientific programming
platform that offered the perfect solution in our recent comparative genomics
investigation (J. B. Lucks, D. R. Nelson, G. Kudla, J. B. Plotkin. Genome
landscapes and bacteriophage codon usage, PLoS Computational Biology, 4,
1000001, 2008). In this paper, we discuss the challenges of this project, and
how the combined power of Biopython, Matplotlib and SWIG were utilized for the
required computational tasks. We finish by discussing how python goes beyond
being a convenient programming language, and promotes good scientific practice
by enabling clean code, integration with professional programming techniques
such as unit testing, and strong data provenance.

Summary: In anticipation of the individualized proteomics era and the need to
integrate knowledge from disease studies, we have augmented our peptide
identification software RAId DbS to take into account annotated single amino
acid polymorphisms, post-translational modifications, and their documented
disease associations while analyzing a tandem mass spectrum. To facilitate new
discoveries, RAId DbS allows users to conduct searches permitting novel
polymorphisms. Availability: The webserver link is http://www.ncbi.nlm.nih.gov/
/CBBResearch/qmbp/raid dbs/index.html. The relevant databases and binaries of
RAId DbS for Linux, Windows, and Mac OS X are available from the same web page.
Contact: yyu@ncbi.nlm.nih.gov

Models of reaction chemistry based on the stochastic simulation algorithm
(SSA) have become a crucial tool for simulating complicated biological reaction
networks due to their ability to handle extremely complicated reaction networks
and to represent noise in small-scale chemistry. These methods can, however,
become highly inefficient for stiff reaction systems, those in which different
reaction channels operate on widely varying time scales. In this paper, we
develop two methods for accelerating sampling in SSA models: an exact method
and a scheme allowing for sampling accuracy up to any arbitrary error bound.
Both methods depend on analysis of the eigenvalues of continuous time Markov
model graphs that define the behavior of the SSA. We demonstrate these methods
for the specific application of sampling breakage times for multiply-connected
bond networks, a class of stiff system important to models of self-assembly
processes. We show theoretically and empirically that our eigenvalue methods
provide substantially reduced sampling times for a wide range of network
breakage models. These techniques are also likely to have broad use in
accelerating SSA models so as to apply them to systems and parameter ranges
that are currently computationally intractable.

Statistical inference of genetic regulatory networks is essential for
understanding temporal interactions of regulatory elements inside the cells.
For inferences of large networks, identification of network structure is
typical achieved under the assumption of sparsity of the networks.
  When the number of time points in the expression experiment is not too small,
we propose to infer the parameters in the ordinary differential equations using
the techniques from functional data analysis (FDA) by regarding the observed
time course expression data as continuous-time curves. For networks with a
large number of genes, we take advantage of the sparsity of the networks by
penalizing the linear coefficients with a L_1 norm. The ability of the
algorithm to infer network structure is demonstrated using the cell-cycle time
course data for Saccharomyces cerevisiae.

In Proteomics, only the de novo peptide sequencing approach allows a partial
amino acid sequence of a peptide to be found from a MS/MS spectrum. In this
article a preliminary work is presented to discover a complete protein sequence
from spectral data (MS and MS/MS spectra). For the moment, our approach only
uses MS spectra. A Genetic Algorithm (GA) has been designed with a new
evaluation function which works directly with a complete MS spectrum as input
and not with a mass list like the other methods using this kind of data. Thus
the mono isotopic peak extraction step which needs a human intervention is
deleted. The goal of this approach is to discover the sequence of unknown
proteins and to allow a better understanding of the differences between
experimental proteins and proteins from databases.

Some problems with the mathematical analysis on which the UK Non-Native
Organism Risk Assessment Scheme is based are outlined.

This report presents the implementation of a protein sequence comparison
algorithm specifically designed for speeding up time consuming part on parallel
hardware such as SSE instructions, multicore architectures or graphic boards.
Three programs have been developed: PLAST-P, TPLAST-N and PLAST-X. They provide
equivalent results compared to the NCBI BLAST family programs (BLAST-P,
TBLAST-N and BLAST-X) with a speed-up factor ranging from 5 to 10.

Information theory is a branch of probability and statistics involving the
analysis of communications. Information theory enables us to analyze and
quantify the information content of predictions made in the context of plant
disease management and related disciplines. In this article, some applications
of information theory in plant disease management are outlined.

The functioning of many biochemical networks is often robust -- remarkably
stable under changes in external conditions and internal reaction parameters.
Much recent work on robustness and evolvability has focused on the structure of
neutral spaces, in which system behavior remains invariant to mutations.
Recently we have shown that the collective behavior of multiparameter models is
most often 'sloppy': insensitive to changes except along a few 'stiff'
combinations of parameters, with an enormous sloppy neutral subspace.
Robustness is often assumed to be an emergent evolved property, but the
sloppiness natural to biochemical networks offers an alternative non-adaptive
explanation. Conversely, ideas developed to study evolvability in robust
systems can be usefully extended to characterize sloppy systems.

We review a recent trend in computational systems biology which aims at using
pattern recognition algorithms to infer the structure of large-scale biological
networks from heterogeneous genomic data. We present several strategies that
have been proposed and that lead to different pattern recognition problems and
algorithms. The strenght of these approaches is illustrated on the
reconstruction of metabolic, protein-protein and regulatory networks of model
organisms. In all cases, state-of-the-art performance is reported.

In this short note, we analyze the assumptions made by McDougal et al (2006),
both explicit and implicit, in their estimation of the proportion of "true
recent infections" using the BED CEIA. This enables us to write down
expressions for the sensitivity, short term specificity and long term
specificity of a test for recent infection defined by a BED ODn below a
threshold. We then derive an identity which shows the relationship between
these parameters, allowing the elimination of sensitivity and short term
specificity from an expression relating the proportion of "true recent
infections" to the proportion of seropositive individuals testing below
threshold. This has two important consequences. Firstly, the simplified formula
is substantially more amenable to calibration. Secondly, naively treating the
parameters as independent would lead to an incorrect estimate of uncertainty
due to imperfect calibration.

We provide a complete thermodynamic solution of a 1D hopping model in the
presence of a random potential by obtaining the density of states. Since the
partition function is related to the density of states by a Laplace transform,
the density of states determines completely the thermodynamic behavior of the
system. We have also shown that the transfer matrix technique, or the so-called
dynamic programming, used to obtain the density of states in the 1D hopping
model may be generalized to tackle a long-standing problem in statistical
significance assessment for one of the most important proteomic tasks - peptide
sequencing using tandem mass spectrometry data.

Statistically meaningful comparison/combination of peptide identification
results from various search methods is impeded by the lack of a universal
statistical standard. Providing an E-value calibration protocol, we
demonstrated earlier the feasibility of translating either the score or
heuristic E-value reported by any method into the textbook-defined E-value,
which may serve as the universal statistical standard. This protocol, although
robust, may lose spectrum-specific statistics and might require a new
calibration when changes in experimental setup occur. To mitigate these issues,
we developed a new MS/MS search tool, RAId_aPS, that is able to provide
spectrum-specific E-values for additive scoring functions. Given a selection of
scoring functions out of RAId score, K-score, Hyperscore and XCorr, RAId_aPS
generates the corresponding score histograms of all possible peptides using
dynamic programming. Using these score histograms to assign E-values enables a
calibration-free protocol for accurate significance assignment for each scoring
function. RAId_aPS features four different modes: (i) compute the total number
of possible peptides for a given molecular mass range, (ii) generate the score
histogram given a MS/MS spectrum and a scoring function, (iii) reassign
E-values for a list of candidate peptides given a MS/MS spectrum and the
scoring functions chosen, and (iv) perform database searches using selected
scoring functions. In modes (iii) and (iv), RAId_aPS is also capable of
combining results from different scoring functions using spectrum-specific
statistics. The web link is
http://www.ncbi.nlm.nih.gov/CBBresearch/Yu/raid_aps/index.html. Relevant
binaries for Linux, Windows, and Mac OS X are available from the same page.

Networks of person-person contacts form the substrate along which infectious
diseases spread. Most network-based studies of the spread focus on the impact
of variations in degree (the number of contacts an individual has). However,
other effects such as clustering, variations in infectiousness or
susceptibility, or variations in closeness of contacts may play a significant
role. We develop analytic techniques to predict how these effects alter the
growth rate, probability, and size of epidemics and validate the predictions
with a realistic social network. We find that (for given degree distribution
and average transmissibility) clustering is the dominant factor controlling the
growth rate, heterogeneity in infectiousness is the dominant factor controlling
the probability of an epidemic, and heterogeneity in susceptibility is the
dominant factor controlling the size of an epidemic. Edge weights (measuring
closeness or duration of contacts) have impact only if correlations exist
between different edges. Combined, these effects can play a minor role in
reinforcing one another, with the impact of clustering largest when the
population is maximally heterogeneous or if the closer contacts are also
strongly clustered. Our most significant contribution is a systematic way to
address clustering in infectious disease models, and our results have a number
of implications for the design of interventions.

The Tribolium genome contains 21 nuclear receptors, representing all of the
six known subfamilies. When compared to other species, this first complete set
for a Coleoptera reveals a strong conservation of the number and identity of
nuclear receptors in holometabolous insects. Two novelties are observed: the
atypical NR0 gene knirps is present only in brachyceran flies, while the NR2E6
gene is found only in Tribolium and in Apis. Using a quantitative analysis of
the evolutionary rate, we discovered that nuclear receptors could be divided
into two groups. In one group of 13 proteins, the rates follow the trend of the
Mecopterida genome-wide acceleration. In a second group of five nuclear
receptors, all acting together at the top of the ecdysone cascade, we observed
an overacceleration of the evolutionary rate during the early divergence of
Mecopterida. We thus extended our analysis to the twelve classic ecdysone
transcriptional regulators and found that six of them (ECR, USP, HR3, E75, HR4
and Kr-h1) underwent an overacceleration at the base of the Mecopterida
lineage. By contrast, E74, E93, BR, HR39, FTZ-F1 and E78 do not show this
divergence. We suggest that coevolution occurred within a network of regulators
that control the ecdysone cascade. The advent of Tribolium as a powerful model
should allow a better understanding of this evolution.

In this report we review modern nonlinearity methods that can be used in the
preterm birth analysis. The nonlinear analysis of uterine contraction signals
can provide information regarding physiological changes during the menstrual
cycle and pregnancy. This information can be used both for the preterm birth
prediction and the preterm labor control.
  Keywords: preterm birth, complex data analysis, nonlinear methods

Clustering is a concept used in a huge variety of applications. We review a
conceptually very simple algorithm for hierarchical clustering called in the
following the {\it mutual information clustering} (MIC) algorithm. It uses
mutual information (MI) as a similarity measure and exploits its grouping
property: The MI between three objects X, Y, and Z is equal to the sum of the
MI between X and Y, plus the MI between Z and the combined object (XY). We use
MIC both in the Shannon (probabilistic) version of information theory, where
the "objects" are probability distributions represented by random samples, and
in the Kolmogorov (algorithmic) version, where the "objects" are symbol
sequences. We apply our method to the construction of phylogenetic trees from
mitochondrial DNA sequences and we reconstruct the fetal ECG from the output of
independent components analysis (ICA) applied to the ECG of a pregnant woman.

We discuss the property of a.e. and in mean convergence of the Kohonen
algorithm considered as a stochastic process. The various conditions ensuring
the a.e. convergence are described and the connection with the rate decay of
the learning parameter is analyzed. The rate of convergence is discussed for
different choices of learning parameters. We proof rigorously that the rate of
decay of the learning parameter which is most used in the applications is a
sufficient condition for a.e. convergence and we check it numerically. The aim
of the paper is also to clarify the state of the art on the convergence
property of the algorithm in view of the growing number of applications of the
Kohonen neural networks. We apply our theorem and considerations to the case of
genetic classification which is a rapidly developing field.

We propose that certain patterns (scars) -- theoretically and numerically
predicted to be formed by electrons arranged on a sphere to minimize the
repulsive Coulomb potential (the Thomson problem) and experimentally found in
spherical crystals formed by self-assembled polystyrene beads (an instance of
the {\it generalized} Thomson problem) -- could be relevant to extend the
classic Caspar and Klug construction for icosahedrally-shaped virus capsids.
The main idea is that scars could be produced on the capsid at an intermediate
stage of its evolution and the release of the bending energy present in scars
into stretching energy could allow for shape-changes. The conjecture can be
tested in experiments and/or in numerical simulations.

We apply Markov chain lumping techniques to aggregate codons from an
empirical substitution matrix. The standard genetic code as well as higher
order amino acid substitution groups are identified. Since the aggregates are
derived from first principles they do not rely on system dependent assumptions
made beforehand, e.g. regarding criteria on what should constitute an amino
acid group. We therefore argue that the acquired aggregations more accurately
capture the multi-level structure of the substitution dynamics than alternative
techniques.

We apply the concept of subset seeds proposed in [1] to similarity search in
protein sequences. The main question studied is the design of efficient seed
alphabets to construct seeds with optimal sensitivity/selectivity trade-offs.
We propose several different design methods and use them to construct several
alphabets.We then perform an analysis of seeds built over those alphabet and
compare them with the standard Blastp seeding method [2,3], as well as with the
family of vector seeds proposed in [4]. While the formalism of subset seed is
less expressive (but less costly to implement) than the accumulative principle
used in Blastp and vector seeds, our seeds show a similar or even better
performance than Blastp on Bernoulli models of proteins compatible with the
common BLOSUM62 matrix.

Molecular docking is an essential tool for drug design. It helps the
scientist to rapidly know if two molecules, respectively called ligand and
receptor, can be combined together to obtain a stable complex. We propose a new
multi-objective model combining an energy term and a surface term to gain such
complexes. The aim of our model is to provide complexes with a low energy and
low surface. This model has been validated with two multi-objective genetic
algorithms on instances from the literature dedicated to the docking
benchmarking.

Recent advances in experimental neuroscience allow, for the first time,
non-invasive studies of the white matter tracts in the human central nervous
system, thus making available cutting-edge brain anatomical data describing
these global connectivity patterns. This new, non-invasive, technique uses
magnetic resonance imaging to construct a snap-shot of the cortical network
within the living human brain. Here, we report on the initial success of a new
weighted network communicability measure in distinguishing local and global
differences between diseased patients and controls. This approach builds on
recent advances in network science, where an underlying connectivity structure
is used as a means to measure the ease with which information can flow between
nodes. One advantage of our method is that it deals directly with the
real-valued connectivity data, thereby avoiding the need to discretise the
corresponding adjacency matrix, that is, to round weights up to 1 or down to 0,
depending upon some threshold value. Experimental results indicate that the new
approach is able to highlight biologically relevant features that are not
immediately apparent from the raw connectivity data.

An approach for multiplex qualitative and quantitative microarray-based PCR
analysis has been proposed. The characteristics of PCR executed on a gel-based
oligonucleotide microarray with immobilized forward primers and a single common
reverse primer in solution were investigated for several DNA targets. One-stage
multiplex on-chip PCR was studied for simultaneous amplification of herpes
simplex viruses types 1 and 2, cytomegalovirus DNA, and bacteriophage lambda
DNA as an internal control. Additionally the joint analysis of increased number
of targets (with addition of Chlamydia trachomatis, Mycoplasma hominis, and
Ureaplasma urealyticum DNA) was done in two-stage version of assay: first stage
was in-tube PCR with target-specific primers, while the reverse ones contained
5'-adapter region; the second stage was on-chip amplification with immobilized
target-specific forward primers and adapter as common reverse primer in
solution. The possible application of one-stage reaction for human cDNA
analysis was additionally demonstrated with utilization of a common
poly-T-containing primer in solution. SYBR green I; and Cy-5 labeled dUTP were
used for real-time and end-point detection of specific PCR products. The
efficiencies of both one-stage and two-stage reactions was shown to be strongly
dependent on magnesium and primers concentrations. Quantitative PCR in the both
versions was studied with 10-fold serial dilutions of phage lambda DNA. The
method enabled detection of 6 DNA copies per reaction for both versions of
assay. The quantitative interval for one-stage reaction covered eight orders of
concentration. The revealed significant effect of gel pad size on microarray
PCR effectiveness has been discussed.

The Automated Protein Structure Analysis (APSA) method is used for the
classification of supersecondary structures. Basis for the classification is
the encoding of three-dimensional (3D) residue conformations into a 16-letter
code (3D-1D projection). It is shown that the letter code of the protein makes
it possible to reconstruct its overall shape without ambiguity (1D-3D
translation). Accordingly, the letter code is used for the development of
classification rules that distinguish supersecondary structures by the
properties of their turns and the orientation of the flanking helix or strand
structures. The orientations of turn and flanking structures are collected in
an octant system that helps to specify 196 supersecondary groups for
(alpha,alpha)-, (alpha,beta)-, (beta,alpha)-, (beta,beta)-class. 391 protein
chains leading to 2499 super secondary structures were analyzed. Frequently
occurring super secondary structures are identified with the help of the octant
classification system and explained on the basis of their letter and
classification codes.

A new method for the Automated Protein Structure Analysis (APSA) is derived,
which simplifies the protein backbone to a smooth curve in 3-dimensional space.
For the purpose of obtaining this smooth line each amino acid is represented by
its C$_{\alpha}$ atom, which serves as suitable anchor point for a cubic spline
fit. The backbone line is characterized by arc length $s$, curvature
$\kappa(s)$, and torsion $\tau(s)$. The $\kappa(s)$ and $\tau(s)$ diagrams of
the protein backbone suppress, because of the level of coarse graining applied,
details of the bond framework of the backbone, however reveal accurately all
secondary structure features of a protein. Advantages of APSA are its
quantitative representation and analysis of 3-dimensional structure in form of
2-dimensional curvature and torsion patterns, its easy visualization of
complicated conformational features, and its general applicability. Typical
differences between 3$_{10}$-,$\alpha$-, $\pi$-helices, and $\beta$-strands are
quantified with the help of the $\kappa(s)$ and $\tau(s)$ diagrams. For a test
set of 20 proteins, 63 % of all helical residues and 48.5 % of all extended
residues are identified to be in ideal conformational environments with the
help of APSA. APSA is compared with other methods for protein structure
analysis and its applicability to higher levels of protein structure is
discussed.

We have investigated the binding interaction between the bacteriophage lambda
repressor CI and its target DNA using total internal reflection fluorescence
microscopy. Large, step-wise changes in the intensity of the red fluorescent
protein fused to CI were observed as it associated and dissociated from
individually labeled single molecule DNA targets. The stochastic association
and dissociation were characterized by Poisson statistics. Dark and bright
intervals were measured for thousands of individual events. The exponential
distribution of the intervals allowed direct determination of the association
and dissociation rate constants, ka and kd respectively. We resolved in detail
how ka and kd varied as a function of 3 control parameters, the DNA length L,
the CI dimer concentration, and the binding affinity. Our results show that
although interaction with non-operator DNA sequences are observable, CI binding
to the operator site is not dependent on the length of flanking non-operator
DNA.

This note studies feedforward circuits as models for perfect adaptation to
step signals in biological systems. A global convergence theorem is proved in a
general framework, which includes examples from the literature as particular
cases. A notable aspect of these circuits is that they do not adapt to pulse
signals, because they display a memory phenomenon. Estimates are given of the
magnitude of this effect.

This article is addressing a recurrent problem in biology: mining newly built
large scale networks. Our approach consists in comparing these new networks to
well known ones. The visual backbone of this comparative analysis is provided
by a network classification hierarchy. This method makes sense when dealing
with metabolic networks since comparison could be done using pathways
(clusters). Moreover each network models an organism and it exists organism
classification such as taxonomies. Video demonstration:
http://www.labri.fr/perso/bourqui/video.wmv

Random Threshold Networks (RTNs) are an idealized model of diluted, non
symmetric spin glasses, neural networks or gene regulatory networks. RTNs also
serve as an interesting general example of any coordinated causal system. Here
we study the conditions for maximal information transfer and behavior diversity
in RTNs. These conditions are likely to play a major role in physical and
biological systems, perhaps serving as important selective traits in biological
systems. We show that the pairwise mutual information is maximized in
dynamically critical networks. Also, we show that the correlated behavior
diversity is maximized for slightly chaotic networks, close to the critical
region. Importantly, critical networks maximize coordinated, diverse dynamical
behavior across the network and across time: the information transmission
between source and receiver nodes and the diversity of dynamical behaviors,
when measured with a time delay between the source and receiver, are maximized
for critical networks.

We apply the concept of subset seeds proposed in [1] to similarity search in
protein sequences. The main question studied is the design of efficient seed
alphabets to construct seeds with optimal sensitivity/selectivity trade-offs.
We propose several different design methods and use them to construct several
alphabets. We then perform a comparative analysis of seeds built over those
alphabets and compare them with the standard BLASTP seeding method [2], [3], as
well as with the family of vector seeds proposed in [4]. While the formalism of
subset seeds is less expressive (but less costly to implement) than the
cumulative principle used in BLASTP and vector seeds, our seeds show a similar
or even better performance than BLASTP on Bernoulli models of proteins
compatible with the common BLOSUM62 matrix. Finally, we perform a large-scale
benchmarking of our seeds against several main databases of protein alignments.
Here again, the results show a comparable or better performance of our seeds
vs. BLASTP.

We study a method of seed-based lossless filtration for approximate string
matching and related bioinformatics applications. The method is based on a
simultaneous use of several spaced seeds rather than a single seed as studied
by Burkhardt and K\"arkk\"ainen [1]. We present algorithms to compute several
important parameters of seed families, study their combinatorial properties,
and describe several techniques to construct efficient families. We also report
a large-scale application of the proposed technique to the problem of
oligonucleotide selection for an EST sequence database.

A basic assumption of molecular biology is that proteins sharing close
three-dimensional (3D) structures are likely to share a common function and in
most cases derive from a same ancestor. Computing the similarity between two
protein structures is therefore a crucial task and has been extensively
investigated. Evaluating the similarity of two proteins can be done by finding
an optimal one-to-one matching between their components, which is equivalent to
identifying a maximum weighted clique in a specific "alignment graph". In this
paper we present a new integer programming formulation for solving such clique
problems. The model has been implemented using the ILOG CPLEX Callable Library.
In addition, we designed a dedicated branch and bound algorithm for solving the
maximum cardinality clique problem. Both approaches have been integrated in
VAST (Vector Alignment Search Tool) - a software for aligning protein 3D
structures largely used in NCBI (National Center for Biotechnology
Information). The original VAST clique solver uses the well known Bron and
Kerbosh algorithm (BK). Our computational results on real life protein
alignment instances show that our branch and bound algorithm is up to 116 times
faster than BK for the largest proteins.

CoPreTHi is a Java based web application, which combines the results of
methods that predict the location of transmembrane segments in protein
sequences into a joint prediction histogram. Clearly, the joint prediction
algorithm, produces superior quality results than individual prediction
schemes. The program is available at http://o2.db.uoa.gr/CoPreTHi

We present a novel method that predicts transmembrane domains in proteins
using solely information contained in the sequence itself. The PRED-TMR
algorithm described, refines a standard hydrophobicity analysis with a
detection of potential termini ('edges', starts and ends) of transmembrane
regions. This allows one both to discard highly hydrophobic regions not
delimited by clear start and end configurations and to confirm putative
transmembrane segments not distinguishable by their hydrophobic composition.
The accuracy obtained on a test set of 101 non-homologous transmembrane
proteins with reliable topologies compares well with that of other popular
existing methods. Only a slight decrease in prediction accuracy was observed
when the algorithm was applied to all transmembrane proteins of the SwissProt
database (release 35). A WWW server running the PRED-TMR algorithm is available
at http://o2.db.uoa. gr/PRED-TMR/

Summary : FT is a tool written in C++, which implements the Fourier analysis
method to locate periodicities in aminoacid or DNA sequences. It is provided
for free public use on a WWW server with a Java interface. Availability : The
server address is http://o2.db.uoa.gr/FT Contact : shamodr@atlas.uoa.gr

A cascading system of hierarchical, artificial neural networks (named
PRED-CLASS) is presented for the generalized classification of proteins into
four distinct classes-transmembrane, fibrous, globular, and mixed-from
information solely encoded in their amino acid sequences. The architecture of
the individual component networks is kept very simple, reducing the number of
free parameters (network synaptic weights) for faster training, improved
generalization, and the avoidance of data overfitting. Capturing information
from as few as 50 protein sequences spread among the four target classes (6
transmembrane, 10 fibrous, 13 globular, and 17 mixed), PRED-CLASS was able to
obtain 371 correct predictions out of a set of 387 proteins (success rate
approximately 96%) unambiguously assigned into one of the target classes. The
application of PRED-CLASS to several test sets and complete proteomes of
several organisms demonstrates that such a method could serve as a valuable
tool in the annotation of genomic open reading frames with no functional
assignment or as a preliminary step in fold recognition and ab initio structure
prediction methods. Detailed results obtained for various data sets and
completed genomes, along with a web sever running the PRED-CLASS algorithm, can
be accessed over the World Wide Web at http://o2.biol.uoa.gr/PRED-CLASS

Current research in biology heavily depends on the availability and efficient
use of information. In order to build new knowledge, various sources of
biological data must often be combined. Semantic Web technologies, which
provide a common framework allowing data to be shared and reused between
applications, can be applied to the management of disseminated biological data.
However, due to some specificities of biological data, the application of these
technologies to life science constitutes a real challenge. Through a use case
of biological data integration, we show in this paper that current Semantic Web
technologies start to become mature and can be applied for the development of
large applications. However, in order to get the best from these technologies,
improvements are needed both at the level of tool performance and knowledge
modeling.

This work presents a simple artificial neural network which classifies
proteins into two classes from their sequences alone: the membrane protein
class and the non-membrane protein class. This may be important in the
functional assignment and analysis of open reading frames (ORF's) identified in
complete genomes and, especially, those ORF's that correspond to proteins with
unknown function. The network described here has a simple hierarchical
feed-forward topology and a limited number of neurons which makes it very fast.
By using only information contained in 11 protein sequences, the method was
able to identify, with 100% accuracy, all membrane proteins with reliable
topologies collected from several papers in the literature. Applied to a test
set of 995 globular, water-soluble proteins, the neural network classified
falsely 23 of them in the membrane protein class (97.7% of correct assignment).
The method was also applied to the complete SWISS-PROT database with
considerable success and on ORF's of several complete genomes. The neural
network developed was associated with the PRED-TMR algorithm (Pasquier,C.,
Promponas,V.J., Palaios,G.A., Hamodrakas,J.S. and Hamodrakas,S.J., 1999) in a
new application package called PRED-TMR2. A WWW server running the PRED-TMR2
software is available at http://o2.db.uoa.gr/PRED-TMR2

We propose two ways of estimating the current source density (CSD) from
measurements of voltage on a Cartesian grid with missing recording points using
the inverse CSD method. The simplest approach is to substitute local averages
(LA) in place of missing data. A more elaborate alternative is to estimate a
smaller number of CSD parameters than the actual number of recordings and to
take the least-squares fit (LS). We compare the two approaches in the three
dimensional case on several sets of surrogate and experimental data, for
varying numbers of missing data points, and discuss their advantages and
drawbacks. One can construct CSD distributions for which one or the other
approach is better. However, in general, LA method is to be recommended being
more stable and more robust to variations in the recorded fields.

Graphical analysis methods are widely used in positron emission tomography
quantification because of their simplicity and model independence. But they
may, particularly for reversible kinetics, lead to bias in the estimated
parameters. The source of the bias is commonly attributed to noise in the data.
Assuming a two-tissue compartmental model, we investigate the bias that
originates from model error. This bias is an intrinsic property of the
simplified linear models used for limited scan durations, and it is exaggerated
by random noise and numerical quadrature error. Conditions are derived under
which Logan's graphical method either over- or under-estimates the distribution
volume in the noise-free case. The bias caused by model error is quantified
analytically. The presented analysis shows that the bias of graphical methods
is inversely proportional to the dissociation rate. Furthermore, visual
examination of the linearity of the Logan plot is not sufficient for
guaranteeing that equilibrium has been reached. A new model which retains the
elegant properties of graphical analysis methods is presented, along with a
numerical algorithm for its solution. We perform simulations with the fibrillar
amyloid-beta radioligand [11C] benzothiazole-aniline using published data from
the University of Pittsburgh and Rotterdam groups. The results show that the
proposed method significantly reduces the bias due to model error. Moreover,
the results for data acquired over a 70 minutes scan duration are at least as
good as those obtained using existing methods for data acquired over a 90
minutes scan duration.

Logan's graphical analysis (LGA) is a widely-used approach for quantification
of biochemical and physiological processes from Positron emission tomography
(PET) image data. A well-noted problem associated with the LGA method is the
bias in the estimated parameters. We recently systematically evaluated the bias
associated with the linear model approximation and developed an alternative to
minimize the bias due to model error. In this study, we examined the noise
structure in the equations defining linear quantification methods, including
LGA. The noise structure conflicts with the conditions given by the
Gauss-Markov theorem for the least squares (LS) solution to generate the best
linear unbiased estimator. By carefully taking care of the data error
structure, we propose to use structured total least squares (STLS) to obtain
the solution using a one-dimensional optimization problem. Simulations of PET
data for [11C] benzothiazole-aniline (Pittsburgh Compound-B [PIB]) show that
the proposed method significantly reduces the bias. We conclude that the bias
associated with noise is primarily due to the unusual structure of he
correlated noise and it can be reduced with the proposed STLS method.

Parametric imaging of the cerebral metabolic rate for glucose (CMRGlc) using
[18F]-fluorodeoxyglucose positron emission tomography is considered.
Traditional imaging is hindered due to low signal to noise ratios at individual
voxels. We propose to minimize the total variation of the tracer uptake rates
while requiring good fit of traditional Patlak equations. This minimization
guarantees spatial homogeneity within brain regions and good distinction
between brain regions. Brain phantom simulations demonstrate significant
improvement in quality of images by the proposed method as compared to Patlak
images with post-filtering using Gaussian or median filters.

Knowing which mode of combinatorial regulation (typically, AND or OR logic
operation) that a gene employs is important for determining its function in
regulatory networks. Here, we introduce a dynamic cross-correlation function
between the output of a gene and its upstream regulator concentrations for
signatures of combinatorial regulation in gene expression noise. We find that
the correlation function is always upwards convex for the AND operation whereas
downwards convex for the OR operation, whichever sources of noise (intrinsic or
extrinsic or both). In turn, this fact implies a means for inferring regulatory
synergies from available experimental data. The extensions and applications are
discussed.

The social networks that infectious diseases spread along are typically
clustered. Because of the close relation between percolation and epidemic
spread, the behavior of percolation in such networks gives insight into
infectious disease dynamics. A number of authors have studied clustered
networks, but the networks often contain preferential mixing between high
degree nodes. We introduce a class of random clustered networks and another
class of random unclustered networks with the same preferential mixing. We
analytically show that percolation in the clustered networks reduces the
component sizes and increases the epidemic threshold compared to the
unclustered networks.

Experimental data regarding auxin and venation formation exist at both
macroscopic and molecular scales, and we attempt to unify them into a
comprehensive model for venation formation. We begin with a set of principles
to guide an abstract model of venation formation, from which we show how
patterns in plant development are related to the representation of global
distance information locally as cellular-level signals. Venation formation, in
particular, is a function of distances between cells and their locations. The
first principle, that auxin is produced at a constant rate in all cells, leads
to a (Poisson) reaction-diffusion equation. Equilibrium solutions uniquely
codify information about distances, thereby providing cells with the signal to
begin differentiation from ground to vascular. A uniform destruction hypothesis
and scaling by cell size leads to a more biologically-relevant (Helmholtz)
model, and simulations demonstrate its capability to predict leaf and root
auxin distributions and venation patterns. The mathematical development is
centered on properties of the distance map, and provides a mechanism by which
global information about shape can be presented locally to individual cells.
The principles provide the foundation for an elaboration of these models in a
companion paper \cite{plos-paper2}, and together they provide a framework for
understanding organ- and plant-scale organization.

To support and guide an extensive experimental research into systems biology
of signaling pathways, increasingly more mechanistic models are being developed
with hopes of gaining further insight into biological processes. In order to
analyse these models, computational and statistical techniques are needed to
estimate the unknown kinetic parameters. This chapter reviews methods from
frequentist and Bayesian statistics for estimation of parameters and for
choosing which model is best for modeling the underlying system. Approximate
Bayesian Computation (ABC) techniques are introduced and employed to explore
different hypothesis about the JAK-STAT signaling pathway.

The principles underlying plant development are extended to allow a more
molecular mechanism to elaborate the schema by which ground cells differentiate
into vascular cells. Biophysical considerations dictate that linear dynamics
are not sufficent to capture facilitated auxin transport (e.g., through PIN).
We group these transport facilitators into a non-linear model under the
assumption that they attempt to minimize certain {\em differences} of auxin
concentration. This Constant Gradient Hypothesis greatly increases the
descriptive power of our model to include complex dynamical behaviour.
Specifically, we show how the early pattern of PIN1 expression appears in the
embryo, how the leaf primordium emerges, how convergence points arise on the
leaf margin, how the first loop is formed, and how the intricate pattern of PIN
shifts during the early establishment of vein patterns in incipient leaves of
Arabidopsis. Given our results, we submit that the model provides evidence that
many of the salient structural characteristics that have been described at
various stages of plant development can arise from the uniform application of a
small number of abstract principles.

We introduce an alternative formulation of the exact stochastic simulation
algorithm (SSA) for sampling trajectories of the chemical master equation for a
well-stirred system of coupled chemical reactions. Our formulation is based on
factored-out, partial reaction propensities. This novel exact SSA, called the
partial propensity direct method (PDM), is highly efficient and has a
computational cost that scales at most linearly with the number of chemical
species, irrespective of the degree of coupling of the reaction network. In
addition, we propose a sorting variant, SPDM, which is especially efficient for
multiscale reaction networks.

New technologies and equipment allow for mass treatment of samples and
research teams share acquired data on an always larger scale. In this context
scientists are facing a major data exploitation problem. More precisely, using
these data sets through data mining tools or introducing them in a classical
experimental approach require a preliminary understanding of the information
space, in order to direct the process. But acquiring this grasp on the data is
a complex activity, which is seldom supported by current software tools. The
goal of this paper is to introduce a solution to this scientific data grasp
problem. Illustrated in the Tissue MicroArrays application domain, the proposal
is based on the synthesis notion, which is inspired by Information Retrieval
paradigms. The envisioned synthesis model gives a central role to the study the
researcher wants to conduct, through the task notion. It allows for the
implementation of a task-oriented Information Retrieval prototype system. Cases
studies and user studies were used to validate this prototype system. It opens
interesting prospects for the extension of the model or extensions towards
other application domains.

Fluorescent and luminescent gene reporters allow us to dynamically quantify
changes in molecular species concentration over time on the single cell level.
The mathematical modeling of their interaction through multivariate dynamical
models requires the development of effective statistical methods to calibrate
such models against available data. Given the prevalence of stochasticity and
noise in biochemical systems inference for stochastic models is of special
interest. In this paper we present a simple and computationally efficient
algorithm for the estimation of biochemical kinetic parameters from gene
reporter data. We use the linear noise approximation to model biochemical
reactions through a stochastic dynamic model which essentially approximates a
diffusion model by an ordinary differential equation model with an
appropriately defined noise process. An explicit formula for the likelihood
function can be derived allowing for computationally efficient parameter
estimation. The proposed algorithm is embedded in a Bayesian framework and
inference is performed using Markov chain Monte Carlo. The major advantage of
the method is that in contrast to the more established diffusion approximation
based methods the computationally costly methods of data augmentation are not
necessary. Our approach also allows for unobserved variables and measurement
error. The application of the method to both simulated and experimental data
shows that the proposed methodology provides a useful alternative to diffusion
approximation based methods.

We study the correlation of the occurrence of coronary heart disease (CHD)
with the presence of the single-nucleotide polymorphism (SNP) at the -308
position of the tumor necrosis factor alpha (TNF-$\alpha$) gene. We also
consider the influence of the occurrence of type 2 diabetes (t2DM). Using
Bayesian inference, we first pursue a bottom-up approach to compute the working
hypothesis and the probabilities derivable from the data. We then pursue a
top-down approach by modelling the signal pathway that causally connects the
SNP with the emergence of CHD. We compute the functional form of the
probability of CHD conditional on the presence of the SNP in terms of both the
statistical and biochemical properties of the system. From the probability of
occurrence of a disease conditional on a given risk factor, we explore the
possibility of extracting information on the pathways involved in the
occurrence of the disease. This is a first study that we want to systematise
into a comprehensive formalism to be applied to the inference of the mechanism
connecting the risk factors to the disease.

The animals, in particular insects (Drosophila melanogaster), response
towards odor stimuli in nature can be established by measuring the dynamic of
the odor response. Such an approach is innovative since responses to odors were
tested only at certain time point so far, not in hour intervals for several
hours. The odor attraction to 14 natural and ecologically relevant odor stimuli
such as fruits- ripe and rotten, yeast and vinegar was tested. A mathematical
model to evaluate obtained data is proposed in this study in which the number
of flies caught over several time points is presented as one simple parameter
showing trapping potential of the trap housing particular odor stimuli. The
knowledge concerning dynamic of the odor response in Drosophila melanogaster
may enlighten the principles of flies behavior in context of exposure towards
odor stimulus.

A real-time recording setup combining exhaled breath VOC measurements by
proton transfer reaction mass spectrometry (PTR-MS) with hemodynamic and
respiratory data is presented. Continuous automatic sampling of exhaled breath
is implemented on the basis of measured respiratory flow: a flow-controlled
shutter mechanism guarantees that only end-tidal exhalation segments are drawn
into the mass spectrometer for analysis.
  Exhaled breath concentration profiles of two prototypic compounds, isoprene
and acetone, during several exercise regimes were acquired, reaffirming and
complementing earlier experimental findings regarding the dynamic response of
these compounds reported by Senthilmohan et al. [1] and Karl et al. [2]. While
isoprene tends to react very sensitively to changes in pulmonary ventilation
and perfusion due to its lipophilic behavior and low Henry constant,
hydrophilic acetone shows a rather stable behavior. Characteristic (median)
values for breath isoprene concentration and molar flow, i.e., the amount of
isoprene exhaled per minute are 100 ppb and 29 nmol/min, respectively, with
some intra-individual day-to-day variation. At the onset of exercise breath
isoprene concentration increases drastically, usually by a factor of ~3-4
within about one minute. Due to a simultaneous increase in ventilation, the
associated rise in molar flow is even more pronounced, leading to a ratio
between peak molar flow and molar flow at rest of ~11.
  Our setup holds great potential in capturing continuous dynamics of
non-polar, low-soluble VOCs over a wide measurement range with simultaneous
appraisal of decisive physiological factors affecting exhalation kinetics.

The purpose of this Note is twofold: First, we introduce the general
formalism of evolutionary genetics dynamics involving fitnesses, under both the
deterministic and stochastic setups, and chiefly in discrete-time. In the
process, we particularize it to a one-parameter model where only a selection
parameter is unknown. Then and in a parallel manner, we discuss the estimation
problems of the selection parameter based on a single-generation frequency
distribution shift under both deterministic and stochastic evolutionary
dynamics. In the stochastics, we consider both the celebrated Wright-Fisher and
Moran models.

Calibration of the self-thinning frontier in even-aged monocultures is
hampered by scarce data and by subjective decisions about the proximity of data
to the frontier. We present a simple model that applies to observations of the
full trajectory of stand mean diameter across a range of densities not close to
the frontier. Development of the model is based on a consideration of the slope
s=ln(Nt/Nt 1)/ln(Dt/Dt 1) of a log-transformed plot of stocking Nt and mean
stem diameter Dt at time t. This avoids the need for subjective decisions about
limiting density and allows the use of abundant data further from the
self-thinning frontier. The model can be solved analytically and yields
equations for the stocking and the stand basal area as an explicit function of
stem diameter. It predicts that self-thinning may be regulated by the maximum
basal area with a slope of -2. The significance of other predictor variables
offers an effective test of competing self-thinning theories such Yoda's -3/2
power rule and Reineke's stand density index.

Array-Based Comparative Genomic Hybridization (aCGH) is a method used to
search for genomic regions with copy numbers variations. For a given aCGH
profile, one challenge is to accurately segment it into regions of constant
copy number. Subjects sharing the same disease status, for example a type of
cancer, often have aCGH profiles with similar copy number variations, due to
duplications and deletions relevant to that particular disease. We introduce a
constrained optimization algorithm that jointly segments aCGH profiles of many
subjects. It simultaneously penalizes the amount of freedom the set of profiles
have to jump from one level of constant copy number to another, at genomic
locations known as breakpoints. We show that breakpoints shared by many
different profiles tend to be found first by the algorithm, even in the
presence of significant amounts of noise. The algorithm can be formulated as a
group LARS problem. We propose an extremely fast way to find the solution path,
i.e., a sequence of shared breakpoints in order of importance. For no extra
cost the algorithm smoothes all of the aCGH profiles into piecewise-constant
regions of equal copy number, giving low-dimensional versions of the original
data. These can be shown for all profiles on a single graph, allowing for
intuitive visual interpretation. Simulations and an implementation of the
algorithm on bladder cancer aCGH profiles are provided.

Gene regulatory circuits show significant stochastic fluctuations in their
circuit signals due to the low copy number of transcription factors. When a
gene circuit component is connected to an existing circuit, the dynamic
properties of the existing circuit can be affected by the connected component.
In this paper, we investigate modularity in the dynamics of the gene circuit
based on stochastic fluctuations in the circuit signals. We show that the noise
in the output signal of the existing circuit can be affected significantly when
the output is connected to the input of another circuit component. More
specifically, the output signal noise can show significantly longer
correlations when the two components are connected. This equivalently means
that the noise power spectral density becomes narrower. We define the relative
change in the correlation time or the spectrum bandwidth by stochastic
retroactivity, which is shown to be directly related to the retroactivity
defined in the deterministic framework by del Vecchio et al. This provides an
insight on how to measure retroactivity, by investigating stochastic
fluctuations in gene expression levels, more specifically, by obtaining an
autocorrelation function of the fluctuations. We also provide an interesting
aspect of the frequency response of the circuit. We show that depending on the
magnitude of operating frequencies, different kinds of signals need to be
preferably chosen for circuit description in a modular fashion: at low enough
frequency, expression level of transcription factor that are not bound to their
specific promoter region needs to be chosen, and at high enough frequency, that
of the total transcription factor, both bound and unbound, does.

We define new profiles based on hydropathy properties and point out specific
profiles for regions surrounding splice sites. We built a set T of flanking
regions of genes with 1-3 introns from 21st and 22nd chromosomes. These genes
contained 313 introns and 385 exons and were extracted from GenBank. They were
used in order to define hydropathy profiles. Most human introns, around 99.66%,
are likely to be U2- type introns. They have highly degenerate sequence motifs
and many different sequences can function as U2-type splice sites. Our new
profiles allow to identify regions which have conservative biochemical features
that are essential for recognition by spliceosome. We have also found
differences between hydropathy profiles for U2 or U12-types of introns on sets
of spice sites extracted from SpliceRack database in order to distinguish GT?AG
introns belonging to U2 and U12-types. Indeed, intron type cannot be simply
determined by the dinucleotide termini. We show that there is a similarity of
hydropathy profiles inside intron types. On the one hand, GT?AG and GC?AG
introns belonging to U2-type have resembling hydropathy profiles as well as
AT?AC and GT?AG introns belonging to U12-type. On the other hand, hydropathy
profiles of U2 and U12-types GT?AG introns are completely different. Finally,
we define and compute a pvalue; we compare our profiles with the profiles
provided by a classical method, Pictogram.

One important preprocessing step in the analysis of microarray data is
background subtraction. In high-density oligonucleotide arrays this is
recognized as a crucial step for the global performance of the data analysis
from raw intensities to expression values.
  We propose here an algorithm for background estimation based on a model in
which the cost function is quadratic in a set of fitting parameters such that
minimization can be performed through linear algebra. The model incorporates
two effects: 1) Correlated intensities between neighboring features in the chip
and 2) sequence-dependent affinities for non-specific hybridization fitted by
an extended nearest-neighbor model.
  The algorithm has been tested on 360 GeneChips from publicly available data
of recent expression experiments. The algorithm is fast and accurate. Strong
correlations between the fitted values for different experiments as well as
between the free-energy parameters and their counterparts in aqueous solution
indicate that the model captures a significant part of the underlying physical
chemistry.

We present an approach to computing spatial information based on Fourier
coefficient distributions. The Fourier transform (FT) of an image contains a
complete description of the image, and the values of the FT coefficients are
uniquely associated with that image. For an image where the distribution of
pixels is uncorrelated, the FT coefficients are normally distributed and
uncorrelated. Further, the probability distribution for the FT coefficients of
such an image can readily be obtained by Parseval's theorem. We take advantage
of these properties to compute the spatial information in an image by
determining the probability of each coefficient (both real and imaginary parts)
in the FT, then using the Shannon formalism to calculate information. By using
the probability distribution obtained from Parseval's theorem, an effective
distance from the completely uncorrelated or most uncertain case is obtained.
The resulting quantity is an information computed in k-space (kSI). This
approach provides a robust, facile and highly flexible framework for
quantifying spatial information in images and other types of data (of arbitrary
dimensions). The kSI metric is tested on a 2D Ising ferromagnet, and the
temperature-dependent phase transition is accurately determined from the
spatial information in configurations of the system.

In this work, we introduce the novel technique of in-chip drop on demand,
which consists in dispensing picoliter to nanoliter drops on demand directly in
the liquid-filled channels of a polymer microfluidic chip, at frequencies up to
2.5 kHz and with precise volume control. The technique involves a PDMS chip
with one or several microliter-size chambers driven by piezoelectric actuators.
Individual aqueous microdrops are dispensed from the chamber to a main
transport channel filled with an immiscible fluid, in a process analogous to
atmospheric drop on demand dispensing. In this article, the drop formation
process is characterized with respect to critical dispense parameters such as
the shape and duration of the driving pulse, and the size of both the fluid
chamber and the nozzle. Several features of the in-chip drop on demand
technique with direct relevance to lab on a chip applications are presented and
discussed, such as the precise control of the dispensed volume, the ability to
merge drops of different reagents and the ability to move a drop from the
shooting area of one nozzle to another for multi-step reactions. The
possibility to drive the microfluidic chip with inexpensive audio electronics
instead of research-grade equipment is also examined and verified. Finally, we
show that the same piezoelectric technique can be used to generate a single gas
bubble on demand in a microfluidic chip.

Risk stratification is most directly and informatively summarized as a risk
distribution curve. From this curve the ROC curve, predictiveness curve, and
other curves depicting risk stratification can be derived, demonstrating that
they present similar information. A mathematical expression for the ROC curve
AUC is derived which clarifies how this measure of discrimination quantifies
the overlap between patients who have and don't have events. This expression is
used to define the positive correlation between the dispersion of the risk
distribution curve and the ROC curve AUC. As more disperse risk distributions
and greater separation between patients with and without events characterize
superior risk stratification, the ROC curve AUC provides useful information.

High-throughput data analyses are becoming common in biology, communications,
economics and sociology. The vast amounts of data are usually represented in
the form of matrices and can be considered as knowledge networks. Spectra-based
approaches have proved useful in extracting hidden information within such
networks and for estimating missing data, but these methods are based
essentially on linear assumptions. The physical models of matching, when
applicable, often suggest non-linear mechanisms, that may sometimes be
identified as noise. The use of non-linear models in data analysis, however,
may require the introduction of many parameters, which lowers the statistical
weight of the model. According to the quality of data, a simpler linear
analysis may be more convenient than more complex approaches.
  In this paper, we show how a simple non-parametric Bayesian model may be used
to explore the role of non-linearities and noise in synthetic and experimental
data sets.

Multivariate methods that relate outcomes to risk factors have been adopted
clinically to individualize treatment. This has promoted the belief that
individuals have a true or unique risk.
  The logic of assigning an individual a single risk value has been criticized
since 1866. The reason is that any individual can be simultaneously considered
a member of different groups, with each group having its own risk level (the
reference class problem).
  Lemeshow et al. provided well-documented examples of remarkable discordance
between predictions for an individual by different valid predictive methods
utilizing different risk factors. The prevalence of such discordance is unknown
as it is rarely evaluated, but must be substantial due to the abundance of risk
factors.
  Lemeshow et al. cautioned against using ICU mortality predictions for the
provision of care to individual patients. If individual risk estimates are used
clinically, users should be aware that valid methods may give very different
results.

Frameshift mutations in protein-coding DNA sequences produce a drastic change
in the resulting protein sequence, which prevents classic protein alignment
methods from revealing the proteins' common origin. Moreover, when a large
number of substitutions are additionally involved in the divergence, the
homology detection becomes difficult even at the DNA level. To cope with this
situation, we propose a novel method to infer distant homology relations of two
proteins, that accounts for frameshift and point mutations that may have
affected the coding sequences. We design a dynamic programming alignment
algorithm over memory-efficient graph representations of the complete set of
putative DNA sequences of each protein, with the goal of determining the two
putative DNA sequences which have the best scoring alignment under a powerful
scoring system designed to reflect the most probable evolutionary process. This
allows us to uncover evolutionary information that is not captured by
traditional alignment methods, which is confirmed by biologically significant
examples.

We combine stroboscopic laser excitation with stochastic photoactivation and
super-resolution fluorescence imaging. This makes it possible to record
hundreds of diffusion trajectories of small protein molecules in single
bacterial cells with millisecond time resolution and sub-diffraction limited
spatial precision. We conclude that the small protein mEos2 exhibits normal
diffusion in the bacterial cytoplasm with a diffusion coefficient of 13.1 -+
1.2 \mu m^2 s^(-1). This investigation lays the groundwork for studying
single-molecule binding and dissociation events for a wide range of
intracellular processes.

We describe the fundamental difference between the nature of problems in
traditional physics and that of many problems arising today in systems biology
and other complex settings. The difference hinges on the much larger number of
a priori plausible alternative laws for explaining the phenomena at hand in the
latter case. An approach and a mathematical framework for prediction in this
hypothesis-rich regime are introduced.

Recommended standardized procedures for determining exhaled lower respiratory
nitric oxide and nasal nitric oxide have been developed by task forces of the
European Respiratory Society and the American Thoracic Society. These
recommendations have paved the way for the measurement of nitric oxide to
become a diagnostic tool for specific clinical applications. It would be
desirable to develop similar guidelines for the sampling of other trace gases
in exhaled breath, especially volatile organic compounds (VOCs) which reflect
ongoing metabolism. The concentrations of water-soluble, blood-borne substances
in exhaled breath are influenced by: (i) breathing patterns affecting gas
exchange in the conducting airways; (ii) the concentrations in the
tracheo-bronchial lining fluid; (iii) the alveolar and systemic concentrations
of the compound. The classical Farhi equation takes only the alveolar
concentrations into account. Real-time measurements of acetone in end-tidal
breath under an ergometer challenge show characteristics which cannot be
explained within the Farhi setting. Here we develop a compartment model that
reliably captures these profiles and is capable of relating breath to the
systemic concentrations of acetone. By comparison with experimental data it is
inferred that the major part of variability in breath acetone concentrations
(e.g., in response to moderate exercise or altered breathing patterns) can be
attributed to airway gas exchange, with minimal changes of the underlying blood
and tissue concentrations. Moreover, it is deduced that measured end-tidal
breath concentrations of acetone determined during resting conditions and free
breathing will be rather poor indicators for endogenous levels. Particularly,
the current formulation includes the classical Farhi and the Scheid series
inhomogeneity model as special limiting cases.

In multicellular organisms, patterns of gene expression are established in
response to gradients of signaling molecules. During fly development in early
Drosophila embryos, the Bicoid (Bcd) morphogen gradient is established within
the first hour after fertilization. Bcd acts as a transcription factor,
initiating the expression of a cascade of genes that determine the segmentation
pattern of the embryo, which serves as a blueprint for the future adult
organism. A robust understanding of the mechanisms that govern this
segmentation cascade is still lacking, and a new generation of quantitative
measurements of the spatio-temporal concentration dynamics of the individual
players of this cascade are necessary for further progress. Here we describe a
series of methods that are meant to represent a start of such a quantification
using Bcd as an example. We describe the generation of a transgenic fly line
expressing a Bcd-eGFP fusion protein, and we use this line to carefully analyze
the Bcd concentration dynamics and to measure absolute Bcd expression levels in
living fly embryos using two-photon microscopy. These experiments have proven
to be a fruitful tool generating new insights into the mechanisms that lead to
the establishment and the readout of the Bcd gradient. Generalization of these
methods to other genes in the Drosophila segmentation cascade is
straightforward and should further our understanding of the early patterning
processes and the architecture of the underlying genetic network structure.

Term enrichment analysis facilitates biological interpretation by assigning
to experimentally/computationally obtained data annotation associated with
terms from controlled vocabularies. This process usually involves obtaining
statistical significance for each vocabulary term and using the most
significant terms to describe a given set of biological entities, often
associated with weights. Many existing enrichment methods require selections of
(arbitrary number of) the most significant entities and/or do not account for
weights of entities. Others either mandate extensive simulations to obtain
statistics or assume normal weight distribution. In addition, most methods have
difficulty assigning correct statistical significance to terms with few
entities. Implementing the well-known Lugananni-Rice formula, we have developed
a novel approach, called SaddleSum, that is free from all the aforementioned
constraints and evaluated it against several existing methods. With entity
weights properly taken into account, SaddleSum is internally consistent and
stable with respect to the choice of number of most significant entities
selected. Making few assumptions on the input data, the proposed method is
universal and can thus be applied to areas beyond analysis of microarrays.
Employing asymptotic approximation, SaddleSum provides a term-size dependent
score distribution function that gives rise to accurate statistical
significance even for terms with few entities. As a consequence, SaddleSum
enables researchers to place confidence in its significance assignments to
small terms that are often biologically most specific.

Investigating the relation between the structure and behavior of complex
biological networks often involves posing the following two questions: Is a
hypothesized structure of a regulatory network consistent with the observed
behavior? And can a proposed structure generate a desired behavior? Answering
these questions presupposes that we are able to test the compatibility of
network structure and behavior. We cast these questions into a parameter search
problem for qualitative models of regulatory networks, in particular
piecewise-affine differential equation models. We develop a method based on
symbolic model checking that avoids enumerating all possible parametrizations,
and show that this method performs well on real biological problems, using the
IRMA synthetic network and benchmark experimental data sets. We test the
consistency between the IRMA network structure and the time-series data, and
search for parameter modifications that would improve the robustness of the
external control of the system behavior.

Simulated evolution of biological networks can be used to generate functional
networks as well as investigate hypotheses regarding natural evolution. A
handful of studies have shown how simulated evolution can be used for studying
the functional space spanned by biochemical networks, studying natural
evolution, or designing new synthetic networks. If there was a method for
easily performing such studies, it can allow the community to further
experiment with simulated evolution and explore all of its uses. As a result,
we have developed a library written in the C language that performs all the
basic functions needed to carry out simulated evolution of biological networks.
The library comes with a generic genetic algorithm as well as genetic
algorithms for specifically evolving genetic networks, protein networks, or
mass-action networks. The library also comes with functions for simulating
these networks. A user needs to specify a desired function. A GUI is provided
for users to become oriented with all the options available in the library. The
library is free and open source under the BSD lisence and can be obtained at
evolvenetworks.sourceforge.net. It can be built on all major platforms. The
code can be most conveniently compiled using cross-platform make (CMake).

We introduce the software tool NTRFinder to find the complex repetitive
structure in DNA we call a nested tandem repeat (NTR). An NTR is a recurrence
of two or more distinct tandem motifs interspersed with each other. We propose
that nested tandem repeats can be used as phylogenetic and population markers.
We have tested our algorithm on both real and simulated data, and present some
real nested tandem repeats of interest. We discuss how the NTR found in the
ribosomal DNA of taro (Colocasia esculenta) may assist in determining the
cultivation prehistory of this ancient staple food crop. NTRFinder can be
downloaded from http://www.maths.otago.ac.nz/? aamatroud/.

Although computationally aligning sequence is a crucial step in the vast
majority of comparative genomics studies our understanding of alignment biases
still needs to be improved. To infer true structural or homologous regions
computational alignments need further evaluation. It has been shown that the
accuracy of aligned positions can drop substantially in particular around gaps.
Here we focus on re-evaluation of score-based alignments with affine gap
penalty costs. We exploit their relationships with pair hidden Markov models
and develop efficient algorithms by which to identify gaps which are
significant in terms of length and multiplicity. We evaluate our statistics
with respect to the well-established structural alignments from SABmark and
find that indel reliability substantially increases with their significance in
particular in worst-case twilight zone alignments. This points out that our
statistics can reliably complement other methods which mostly focus on the
reliability of match positions.

We consider here the problem of chaining seeds in ordered trees. Seeds are
mappings between two trees Q and T and a chain is a subset of non overlapping
seeds that is consistent with respect to postfix order and ancestrality. This
problem is a natural extension of a similar problem for sequences, and has
applications in computational biology, such as mining a database of RNA
secondary structures. For the chaining problem with a set of m constant size
seeds, we describe an algorithm with complexity O(m2 log(m)) in time and O(m2)
in space.

We consider the optimal strategy for laboratory testing of biological samples
when we wish to know the results for each sample rather than the average
prevalence of positive samples. If the proportion of positive samples is low
considerable resources may be devoted to testing samples most of which are
negative. An attractive strategy is to pool samples. If the pooled samples test
positive one must then test the individual samples, otherwise they can all be
assumed to be negative. The pool should be big enough to reduce the number of
tests but not so big that the pooled samples are almost all positive. We show
that if the prevalence of positive samples is greater than 30% it is never
worth pooling. From 30% down to 1% pools of size 4 are close to optimal. Below
1% substantial gains can be made by pooling, especially if the samples are
pooled twice. However, with large pools the sensitivity of the test will fall
correspondingly and this must be taken into consideration. We derive simple
expressions for the optimal pool size and for the corresponding proportion of
samples tested.

We consider novel phylogenetic models with rate matrices that arise via the
embedding of a progenitor model on a small number of character states, into a
target model on a larger number of character states. Adapting
representation-theoretic results from recent investigations of Markov
invariants for the general rate matrix model, we give a prescription for
identifying and counting Markov invariants for such `symmetric embedded'
models, and we provide enumerations of these for low-dimensional cases. The
simplest example is a target model on 3 states, constructed from a general 2
state model; the `2->3' embedding. We show that for 2 taxa, there exist two
invariants of quadratic degree, that can be used to directly infer pairwise
distances from observed sequences under this model. A simple simulation study
verifies their theoretical expected values, and suggests that, given the
appropriateness of the model class, they have greater statistical power than
the standard (log) Det invariant (which is of cubic degree for this case).

Yeast glycolysis is considered the prototype of dissipative biochemical
oscillators. In cellular conditions, under sinusoidal source of glucose, the
activity of glycolytic enzymes can display either periodic, quasiperiodic or
chaotic behavior.
  In order to quantify the functional connectivity for the glycolytic enzymes
in dissipative conditions we have analyzed different catalytic patterns using
the non-linear statistical tool of Transfer Entropy. The data were obtained by
means of a yeast glycolytic model formed by three delay differential equations
where the enzymatic speed functions of the irreversible stages have been
explicitly considered. These enzymatic activity functions were previously
modeled and tested experimentally by other different groups. In agreement with
experimental conditions, the studied time series corresponded to a
quasi-periodic route to chaos. The results of the analysis are three-fold:
first, in addition to the classical topological structure characterized by the
specific location of enzymes, substrates, products and feedback regulatory
metabolites, an effective functional structure emerges in the modeled
glycolytic system, which is dynamical and characterized by notable variations
of the functional interactions. Second, the dynamical structure exhibits a
metabolic invariant which constrains the functional attributes of the enzymes.
Finally, in accordance with the classical biochemical studies, our numerical
analysis reveals in a quantitative manner that the enzyme phosphofructokinase
is the key-core of the metabolic system, behaving for all conditions as the
main source of the effective causal flows in yeast glycolysis.

The goal of the work is to implement molecular phylogenetic calculations
using the Grid paradigm by means of the MrBayes software using Directed Acyclic
Graphs (DAG) jobs. In this method, a set of jobs depends on the input or the
output of other jobs. Once the runs have been successfully done, all the
results can be collected by a specific Perl script inside the defined DAG job.
For testing this methodology, we calculate the evolution of papillomavirus with
121 sequences.

This document is an introduction to the use of the point-centered quarter
method. It briefly outlines its history, its methodology, and some of the
practical issues (and modifications) that inevitably arise with its use in the
field. Additionally this paper shows how data collected using point-centered
quarter method sampling may be used to determine importance values of different
species of trees and describes and derives several methods of estimating plant
density and corresponding confidence intervals. New to this revision is an
appendix of R functions to carry out these calculations.

We present a new method for inferring hidden Markov models from noisy time
sequences without the necessity of assuming a model architecture, thus allowing
for the detection of degenerate states. This is based on the statistical
prediction techniques developed by Crutchfield et al., and generates so called
causal state models, equivalent to hidden Markov models. This method is
applicable to any continuous data which clusters around discrete values and
exhibits multiple transitions between these values such as tethered particle
motion data or Fluorescence Resonance Energy Transfer (FRET) spectra. The
algorithms developed have been shown to perform well on simulated data,
demonstrating the ability to recover the model used to generate the data under
high noise, sparse data conditions and the ability to infer the existence of
degenerate states. They have also been applied to new experimental FRET data of
Holliday Junction dynamics, extracting the expected two state model and
providing values for the transition rates in good agreement with previous
results and with results obtained using existing maximum likelihood based
methods.

Background: The vast computational resources that became available during the
past decade enabled the development and simulation of increasingly complex
mathematical models of cancer growth. These models typically involve many free
parameters whose determination is a substantial obstacle to model development.
Direct measurement of biochemical parameters in vivo is often difficult and
sometimes impracticable, while fitting them under data-poor conditions may
result in biologically implausible values.
  Results: We discuss different methodological approaches to estimate
parameters in complex biological models. We make use of the high computational
power of the Blue Gene technology to perform an extensive study of the
parameter space in a model of avascular tumor growth. We explicitly show that
the landscape of the cost function used to optimize the model to the data has a
very rugged surface in parameter space. This cost function has many local
minima with unrealistic solutions, including the global minimum corresponding
to the best fit.
  Conclusions: The case studied in this paper shows one example in which model
parameters that optimally fit the data are not necessarily the best ones from a
biological point of view. To avoid force-fitting a model to a dataset, we
propose that the best model parameters should be found by choosing, among
suboptimal parameters, those that match criteria other than the ones used to
fit the model. We also conclude that the model, data and optimization approach
form a new complex system, and point to the need of a theory that addresses
this problem more generally.

In this Chapter, we ask questions (1) What is the right way to measure the
quality of information processing in a biological system? and (2) What can
real-life organisms do in order to improve their performance in
information-processing tasks? We then review the body of work that investigates
these questions experimentally, computationally, and theoretically in
biological domains as diverse as cell biology, population biology, and
computational neuroscience

Conan is a C++ library created for the accurate and efficient modelling,
inference and analysis of complex networks. It implements the generation and
modification of graphs according to several published models, as well as the
unexpensive computation of global and local network properties. Other features
include network inference and community detection. Furthermore, Conan provides
a Python interface to facilitate the use of the library and its integration in
currently existing applications.
  Conan is available at http://github.com/rhz/conan/.

We have shown elsewhere that the presence of mixed-culture growth of
microbial species in fermentation processes can be detected with high accuracy
by employing the wavelet transform. This is achieved because the crosses in the
different growth processes contributing to the total biomass signal appear as
singularities that are very well evidenced through their singularity cones in
the wavelet transform. However, we used very simple two-species cases. In this
work, we extend the wavelet method to a more complicated illustrative
fermentation case of three microbial species for which we employ several
wavelets of different number of vanishing moments in order to eliminate
possible numerical artifacts. Working in this way allows to filter in a more
precise way the numerical values of the H\"older exponents. Therefore, we were
able to determine the characteristic H\"older exponents for the corresponding
crossing singularities of the microbial growth processes and their stability
logarithmic scale ranges up to the first decimal in the value of the
characteristic exponents. Since calibrating the mixed microbial growth by means
of their H\"older exponents could have potential industrial applications, the
dependence of the H\"older exponents on the kinetic and physical parameters of
the growth models remains as a future experimental task

The microscopic green alga Ostreococcus tauri is rapidly emerging as a
promising model organism in the green lineage. In particular, recent results by
Corellou et al. [Plant Cell, 21, 3436 (2009)] and Thommen et al. [PLoS Comput.
Biol. 6, e1000990 (2010)] strongly suggest that its circadian clock is a
simplified version of Arabidopsis thaliana clock, and that it is architectured
so as to be robust to natural daylight fluctuations. In this work, we analyze
time series data from luminescent reporters for the two central clock genes
TOC1 and CCA1 and correlate them with microarray data previously analyzed. Our
mathematical analysis strongly supports both the existence of a simple two-gene
oscillator at the core of Ostreococcus tauri clock and the fact that its
dynamics is not affected by light in normal entrainment conditions, a signature
of its robustness.

This work emphasizes the assets of implementing the distributed computing for
the intensive use in computational science devoted to the search of new
medicines that could be applied in public healthy problems.

There are many instances in genetics in which we wish to determine whether
two candidate populations are distinguishable on the basis of their genetic
structure. Examples include populations which are geographically separated,
case--control studies and quality control (when participants in a study have
been genotyped at different laboratories). This latter application is of
particular importance in the era of large scale genome wide association
studies, when collections of individuals genotyped at different locations are
being merged to provide increased power. The traditional method for detecting
structure within a population is some form of exploratory technique such as
principal components analysis. Such methods, which do not utilise our prior
knowledge of the membership of the candidate populations. are termed
\emph{unsupervised}. Supervised methods, on the other hand are able to utilise
this prior knowledge when it is available.
  In this paper we demonstrate that in such cases modern supervised approaches
are a more appropriate tool for detecting genetic differences between
populations. We apply two such methods, (neural networks and support vector
machines) to the classification of three populations (two from Scotland and one
from Bulgaria). The sensitivity exhibited by both these methods is considerably
higher than that attained by principal components analysis and in fact
comfortably exceeds a recently conjectured theoretical limit on the sensitivity
of unsupervised methods. In particular, our methods can distinguish between the
two Scottish populations, where principal components analysis cannot. We
suggest, on the basis of our results that a supervised learning approach should
be the method of choice when classifying individuals into pre-defined
populations, particularly in quality control for large scale genome wide
association studies.

Nonlinear mixed effects models represent a powerful tool to simultaneously
analyze data from several individuals. In this study a compartmental model of
leucine kinetics is examined and extended with a stochastic differential
equation to model non-steady state concentrations of free leucine in the
plasma. Data obtained from tracer/tracee experiments for a group of healthy
control individuals and a group of individuals suffering from diabetes mellitus
type 2 are analyzed. We find that the interindividual variation of the model
parameters is much smaller for the nonlinear mixed effects models, compared to
traditional estimates obtained from each individual separately. Using the mixed
effects approach, the population parameters are estimated well also when only
half of the data are used for each individual. For a typical individual the
amount of free leucine is predicted to vary with a standard deviation of 8.9%
around a mean value during the experiment. Moreover, leucine degradation and
protein uptake of leucine is smaller, proteolysis larger, and the amount of
free leucine in the body is much larger for the diabetic individuals than the
control individuals. In conclusion nonlinear mixed effects models offers
improved estimates for model parameters in complex models based on
tracer/tracee data and may be a suitable tool to reduce data sampling in
clinical studies.

We report the key findings from numerical solutions of a model of transport
within an established perfusion bioreactor design. The model includes a
complete formulation of transport with fully coupled convection-diffusion and
scaffold cell attachment. It also includes the experimentally determined
internal (Poly-L-Lactic Acid (PLLA)) scaffold boundary, together with the
external vessel and flow-port boundaries. Our findings, obtained using parallel
lattice Boltzmann equation method, relate to (i) whole-device, steady-state
flow and species distribution and (ii) the properties of the scaffold. In
particular the results identify which elements of the problem may be addressed
by coarse grained methods such as the Darcy approximation and those which
require a more complete description. The work demonstrates that appropriate
numerical modelling will make a key contribution to the design and development
of large scale bioreactors.

We propose a novel two-stage Gene Set Gibbs Sampling (GSGS) framework, to
reverse engineer signaling pathways from gene sets inferred from molecular
profiling data. We hypothesize that signaling pathways are structurally an
ensemble of overlapping linear signal transduction events which we encode as
Information Flow Gene Sets (IFGS's). We infer pathways from gene sets
corresponding to these events subjected to a random permutation of genes within
each set. In Stage I, we use a source separation algorithm to derive unordered
and overlapping IFGS's from molecular profiling data, allowing cross talk among
IFGS's. In Stage II, we develop a Gibbs sampling like algorithm, Gene Set Gibbs
Sampler, to reconstruct signaling pathways from the latent IFGS's derived in
Stage I. The novelty of this framework lies in the seamless integration of the
two stages and the hypothesis of IFGS's as the basic building blocks for signal
pathways. In the proof-of-concept studies, our approach is shown to outperform
the existing Bayesian network approaches using both continuous and discrete
data generated from benchmark networks in the DREAM initiative. We perform a
comprehensive sensitivity analysis to assess the robustness of the approach.
Finally, we implement the GSGS framework to reconstruct signaling pathways in
breast cancer cells.

I derive formulas for the electrostatic potential of a charge in or near a
membrane modeled as one or more dielectric slabs lying between two
semi-infinite dielectrics. One can use these formulas in Monte Carlo codes to
compute the distribution of ions near cell membranes more accurately than by
using Poisson-Boltzmann theory or its linearized version. Here I use them to
discuss the electric field of a uniformly charged membrane, the image charges
of an ion, the distribution of salt ions near a charged membrane, the energy of
a zwitterion near a lipid slab, and the effect of including the phosphate head
groups as thin layers of high electric permittivity.

The quasi-steady state assumption (QSSA) forms the basis for rigorous
mathematical justification of the Michaelis-Menten formalism commonly used in
modeling a broad range of intracellular phenomena. A critical supposition of
QSSA-based analyses is that the underlying biochemical reaction is
enzymatically "closed," so that free enzyme is neither added to nor removed
from the reaction over the relevant time period. Yet there are multiple
circumstances in living cells under which this assumption may not hold, e.g.
during translation of genetic elements or metabolic regulatory events. Here we
consider a modified version of the most basic enzyme-catalyzed reaction which
incorporates enzyme input and removal. We extend the QSSA to this enzymatically
"open" system, computing inner approximations to its dynamics, and we compare
the behavior of the full open system, our approximations, and the closed system
under broad range of kinetic parameters. We also derive conditions under which
our new approximations are provably valid; numerical simulations demonstrate
that our approximations remain quite accurate even when these conditions are
not satisfied. Finally, we investigate the possibility of damped oscillatory
behavior in the enzymatically open reaction.

The overwhelming amount of available scholarly literature in the life
sciences poses significant challenges to scientists wishing to keep up with
important developments related to their research, but also provides a useful
resource for the discovery of recent information concerning genes, diseases,
compounds and the interactions between them. In this paper, we describe an
algorithm called Bio-LDA that uses extracted biological terminology to
automatically identify latent topics, and provides a variety of measures to
uncover putative relations among topics and bio-terms. Relationships identified
using those approaches are combined with existing data in life science datasets
to provide additional insight. Three case studies demonstrate the utility of
the Bio-LDA model, including association predication, association search and
connectivity map generation. This combined approach offers new opportunities
for knowledge discovery in many areas of biology including target
identification, lead hopping and drug repurposing.

Synthetic Biology is the new engineering-based approach to biology that
includes applications of designing complex biological devices. At present, it
is not yet clear what will emerge as the defining principles of Synthetic
Biology. One proposed approach is to build Synthetic Biology around the
classical engineering principles of standardization, modularity/decoupling and
abstraction/modeling to facilitate component-based design. In this article we
suggest and discuss an alternative paradigm, which we call High-throughput
Biologically Optimized Search Engineering (HT-BOSE). Stemming from directed
evolution, in HT-BOSE the focal point is a biological knowledge based rational
optimization of the search process in the space of device design possibilities.
The HT-BOSE approach may also be relevant in other contexts and we briefly
highlight how it could be applicable to the development of multi-drug cocktails
in a biomedical setting.

Periodic patterns play the important regulatory and structural roles in
genomic DNA sequences. Commonly, the underlying periodicities should be
understood in a broad statistical sense, since the corresponding periodic
patterns have been strongly distorted by the random point mutations and
insertions/deletions during molecular evolution. The latent periodicities in
DNA sequences can be efficiently displayed by Fourier transform. The criteria
of significance for observed periodicities are obtained via the comparison
versus the counterpart characteristics of the reference random sequences. We
show that the restrictions imposed on the significance criteria by the rigorous
spectral sum rules can be rationally described with De Finetti distribution.
This distribution provides the convenient intermediate asymptotic form between
Rayleigh distribution and exact combinatoric theory.

We examine how the shape of cells and the geometry of experiment affect the
reaction-diffusion kinetics at the binding between target and probe molecules
on molecular biochips. In particular, we compare the binding kinetics for the
probes immobilized on surface of the semispherical and flat circular cells, the
limit of thin slab of analyte solution over probe cell as well as hemispherical
gel pads and cells printed in gel slab over a substrate. It is shown that
hemispherical geometry provides significantly faster binding kinetics and
ensures more spatially homogeneous distribution of local (from a pixel) signals
over a cell in the transient regime. The advantage of using thin slabs with
small volume of analyte solution may be hampered by the much longer binding
kinetics needing the auxiliary mixing devices. Our analysis proves that the
shape of cells and the geometry of experiment should be included to the list of
essential factors at biochip designing.

Protein structural alignment is an important problem in computational
biology. In this paper, we present first successes on provably optimal pairwise
alignment of protein inter-residue distance matrices, using the popular Dali
scoring function. We introduce the structural alignment problem formally, which
enables us to express a variety of scoring functions used in previous work as
special cases in a unified framework. Further, we propose the first
mathematical model for computing optimal structural alignments based on dense
inter-residue distance matrices. We therefore reformulate the problem as a
special graph problem and give a tight integer linear programming model. We
then present algorithm engineering techniques to handle the huge integer linear
programs of real-life distance matrix alignment problems. Applying these
techniques, we can compute provably optimal Dali alignments for the very first
time.

Irregular bone remodeling is associated with a number of bone diseases such
as osteoporosis and multiple myeloma.
  Computational and mathematical modeling can aid in therapy and treatment as
well as understanding fundamental biology. Different approaches to modeling
give insight into different aspects of a phenomena so it is useful to have an
arsenal of various computational and mathematical models.
  Here we develop a mathematical representation of bone remodeling that can
effectively describe many aspects of the complicated geometries and spatial
behavior observed.
  There is a sharp interface between bone and marrow regions. Also the surface
of bone moves in and out, i.e. in the normal direction, due to remodeling.
Based on these observations we employ the use of a level-set function to
represent the spatial behavior of remodeling. We elaborate on a temporal model
for osteoclast and osteoblast population dynamics to determine the change in
bone mass which influences how the interface between bone and marrow changes.
  We exhibit simulations based on our computational model that show the motion
of the interface between bone and marrow as a consequence of bone remodeling.
The simulations show that it is possible to capture spatial behavior of bone
remodeling in complicated geometries as they occur \emph{in vitro} and \emph{in
vivo}.
  By employing the level set approach it is possible to develop computational
and mathematical representations of the spatial behavior of bone remodeling. By
including in this formalism further details, such as more complex cytokine
interactions and accurate parameter values, it is possible to obtain
simulations of phenomena related to bone remodeling with spatial behavior much
as \emph{in vitro} and \emph{in vivo}. This makes it possible to perform
\emph{in silica} experiments more closely resembling experimental observations.

Motivation: Capillary electrophoresis (CE) of nucleic acids is a workhorse
technology underlying high-throughput genome analysis and large-scale chemical
mapping for nucleic acid structural inference. Despite the wide availability of
CE-based instruments, there remain challenges in leveraging their full power
for quantitative analysis of RNA and DNA structure, thermodynamics, and
kinetics. In particular, the slow rate and poor automation of available
analysis tools have bottlenecked a new generation of studies involving hundreds
of CE profiles per experiment.
  Results: We propose a computational method called high-throughput robust
analysis for capillary electrophoresis (HiTRACE) to automate the key tasks in
large-scale nucleic acid CE analysis, including the profile alignment that has
heretofore been a rate-limiting step in the highest throughput experiments. We
illustrate the application of HiTRACE on thirteen data sets representing 4
different RNAs, three chemical modification strategies, and up to 480 single
mutant variants; the largest data sets each include 87,360 bands. By applying a
series of robust dynamic programming algorithms, HiTRACE outperforms prior
tools in terms of alignment and fitting quality, as assessed by measures
including the correlation between quantified band intensities between replicate
data sets. Furthermore, while the smallest of these data sets required 7 to 10
hours of manual intervention using prior approaches, HiTRACE quantitation of
even the largest data sets herein was achieved in 3 to 12 minutes. The HiTRACE
method therefore resolves a critical barrier to the efficient and accurate
analysis of nucleic acid structure in experiments involving tens of thousands
of electrophoretic bands.

Many cellular behaviors are regulated by gene regulation networks, kinetics
of which is one of the main subjects in the study of systems biology. Because
of the low number molecules in these reacting systems, stochastic effects are
significant. In recent years, stochasticity in modeling the kinetics of gene
regulation networks have been drawing the attention of many researchers. This
paper is a self contained review trying to provide an overview of stochastic
modeling. I will introduce the derivation of the main equations in modeling the
biochemical systems with intrinsic noise (chemical master equation, Fokker-Plan
equation, reaction rate equation, chemical Langevin equation), and will discuss
the relations between these formulations. The mathematical formulations for
systems with fluctuations in kinetic parameters are also discussed. Finally, I
will introduce the exact stochastic simulation algorithm and the approximate
explicit tau-leaping method for making numerical simulations.

A theory for direct quantitative analysis of an antigen is proposed. It is
based on a potential homogenous immunoreaction system. It establishes an
equation to describe the concentration change of the antigen and antibody
complex. A maximum point is found in the concentration profile of the complex
which can be used to calculate the concentration of the antigen. An
experimental scheme was designed for a commercial time-resolved
fluoroimmunoassay kit for HBsAg, which is based heterogeneous immunoreaction.
The results showed that the theory is practically applicable.

To investigate possible errors, length-weight parameters from FishBase.org
were used to graph length-weight curves for six different species: channel
catfish, black crappie, largemouth bass, rainbow trout, flathead catfish, and
lake trout along with the standard weight curves (Anderson and Neumann 1996,
Bister et al. 2000). Parameters noted as doubtful by FishBase were excluded.
For each species, variations in curves were noted, and the minimum and maximum
predicted weights for a 30 cm long fish were compared with each other and with
the standard weight for that length. For lake trout, additional comparisons
were made between the parameters and study details reported in FishBase.org for
6 of 8 length-weight relationships and those reported in the reference
(Carlander 1969) for those 6 relationships. In all species studied, minimum and
maximum curves produced with the length-weight parameters at FishBase.org are
notably different from each other, and in many cases predict weights that are
clearly absurd. For example, one set of parameters predicts a 30 cm rainbow
trout weighing 44 g. For 30 cm length, the range of weights (relative to the
standard weight) for each species are: channel catfish (31.4% to 193.1%), black
crappie (54.0% to 149.0%), largemouth bass (28.8% to 130.4%), rainbow trout
(14.9% to 113.4%), flathead catfish (29.3% to 250.7%), and lake trout (44.0% to
152.7%). Length-weight tables at FishBase.org are not generally reliable and
the on-line database contains dubious parameters. Assurance of quality probably
will require a systematic review with more careful and comprehensive methods
than those currently employed.

Cellular populations are typically heterogenous collections of cells at
different points in their respective cell cycles, each with a cell cycle time
that varies from individual to individual. As a result, true single-cell
behavior, particularly that which is cell-cycle--dependent, is often obscured
in population-level (averaged) measurements. We have developed a simple
deconvolution method that can be used to remove the effects of asynchronous
variability from population-level time-series data. In this paper, we summarize
some recent progress in the development and application of our approach, and
provide technical updates that result in increased biological fidelity. We also
explore several preliminary validation results and discuss several ongoing
applications that highlight the method's usefulness for estimating parameters
in differential equation models of single-cell gene regulation.

In a study of the heterogeneity in malaria infection rates among children
Smith et al.1 fitted several mathematical models to data from community studies
in Africa. They concluded that 20% of children receive 80% of infections, that
infections last about six months on average, that children who clear infections
are not immune to new infections, and that the sensitivity and specificity of
microscopy for the detection of malaria parasites are 95.8% and 88.4%,
respectively. These findings would have important implications for disease
control, but we show here that the statistical analysis is unsound and that the
data do not support their conclusions.

At present, the best hope for eliminating HIV transmission and bringing the
epidemic of HIV to an end lies in the use of anti-retroviral therapy for
prevention, a strategy referred to variously as Test and Treat (T&T), Treatment
as Prevention (TasP) or Treatment centred Prevention (TcP). One of the key
objections to the use of T&T to stop transmission concerns the role of the
acute phase in HIV transmission. The acute phase of infection lasts for one to
three months after HIV-seroconversion during which time the risk of
transmission may be ten to twenty times higher, per sexual encounter, than it
is during the chronic phase which lasts for the next ten years. Regular testing
for HIV is more likely to miss people who are in the acute phase than in the
chronic phase and it is essential to determine the extent to which this might
compromise the impact of T&T on HIV-transmission.
  Here we show that 1) provided the initial epidemic doubling time is about 1.0
to 1.5 years, as observed in South Africa, random testing with an average test
interval of one year will still bring the epidemic close to elimination even if
the acute phase lasts for 3 months during which time transmission is 26 times
higher than in the chronic phase; 2) testing people regularly at yearly
intervals is significantly more effective then testing them randomly; 3)
testing people regularly at six monthly intervals and starting them on ART
immediately, will almost certainly guarantee elimination.
  In general it seems unlikely that elevated transmission during the acute
phase is likely to change predictions of the impact of treatment on
transmission significantly. Other factors, in particular age structure, the
structure of sexual networks and variation in set-point viral load are likely
to be more important and should be given priority in further analyses.

We extend an hypergraph representation, introduced by Finkelstein and
Roytberg, to unify dynamic programming algorithms in the context of RNA folding
with pseudoknots. Classic applications of RNA dynamic programming energy
minimization, partition function, base-pair probabilities...) are reformulated
within this framework, giving rise to very simple algorithms. This
reformulation allows one to conceptually detach the conformation space/energy
model -- captured by the hypergraph model -- from the specific application,
assuming unambiguity of the decomposition. To ensure the latter property, we
propose a new combinatorial methodology based on generating functions. We
extend the set of generic applications by proposing an exact algorithm for
extracting generalized moments in weighted distribution, generalizing a prior
contribution by Miklos and al. Finally, we illustrate our full-fledged
programme on three exemplary conformation spaces (secondary structures,
Akutsu's simple type pseudoknots and kissing hairpins). This readily gives sets
of algorithms that are either novel or have complexity comparable to classic
implementations for minimization and Boltzmann ensemble applications of dynamic
programming.

Wood-decay fungi decompose their substrate by extracellular, degradative
enzymes and play an important role in natural ecosystems by recycling carbon
and minerals fixed in plants. Thereby, they cause significant damage to the
wood structure and limit the use of wood as building material. Besides their
role as biodeteriorators wood-decay fungi can be used for biotechnological
purposes, e.g. the white-rot fungus Physisporinus vitreus for improving the
uptake of preservatives and wood-modification substances of refractory wood.
Therefore, the visualization and the quantification of microscopic decay
patterns are important for the study of the impact of wood-decay fungi in
general, as well as for wood-decay fungi and microorganisms with possible
applications in biotechnology. In the present work, we developed a method for
the automated localization and quantification of microscopic cell wall elements
(CWE) of Norway spruce wood such as bordered pits, intrinsic defects, hyphae or
alterations induced by P. vitreus using high resolution X-ray computed
tomographic microscopy. In addition to classical destructive wood anatomical
methods such as light or laser scanning microscopy, our method allows for the
first time to compute the properties (e.g. area, orientation and
size-distribution) of CWE of the tracheids in a sample. This is essential for
modeling the influence of microscopic CWE to macroscopic properties such as
wood strength and permeability.

We consider a mathematical model comprising of four coupled ordinary
differential equations (ODEs) for studying the hepatitis C (HCV) viral
dynamics. The model embodies the efficacies of a combination therapy of
interferon and ribavirin. A condition for the stability of the uninfected and
the infected steady states is presented. A large number of sample points for
the model parameters (which were physiologically feasible) were generated using
Latin hypercube sampling. Analysis of our simulated values indicated
approximately 24% cases as having an uninfected steady state. Statistical tests
like the chi-square-test and the Spearman's test were also done on the sample
values. The results of these tests indicate a distinctly differently
distribution of certain parameter values and not in case of others, vis-a-vis,
the stability of the uninfected and the infected steady states.

The traditional power law model, W(L) = aL^b, is widely applied to describe
weight (W) vs. length (L) in fish. The model, W(L) = (L/L1)^b, is proposed as
an improvement. The Levenberg-Marquardt non-linear least squares technique is
used to determine the best-fit parameters L1 and b. This model has the
advantages that L1 has the same units (length) independent of the value of the
exponent and has an easily interpreted physical meaning as the typical length
of a fish with one unit of weight. This proposed model is compared with the
traditional model on length-weight data sets for black crappie, largemouth
bass, chain pickerel, yellow perch, and brown bullhead obtained from Stilwell
Reservoir, West Point, New York. The resulting best-fit parameters, parameter
standard errors, and covariances are compared between the two models. The
average relative weight for these species is determined, along with typical
meat yields for four species. For the five species, using the logarithmic
approach and a linear least-squares, standard errors in the coefficient, a,
range from 60.2% to 136.5% for the traditional model. Using a non-linear least
squares technique to determine best fit parameters, the standard errors for the
coefficient, a, range from 68.5% to 164.0% in the traditional model. In the
improved model, standard errors in the parameter L1 range from 0.94% to 15.0%.
The covariance between a and b in the traditional model has a magnitude between
0.999 and 1.000 in both linear and non-linear parameter estimation methods. In
the improved model, the covariances between L1 and b are smaller. The improved
model, W(L) = (L/L1)^b, is preferable for weight vs. length in fish, because
the estimated parameter uncertainties and covariances are smaller in magnitude.
Furthermore, the parameters both have consistent units and an easily
interpreted physical meaning.

Mathematical models of stem cell differentiation are commonly based upon the
concept of subsequent cell fate decisions, each controlled by a gene regulatory
network. These networks exhibit a multistable behavior and cause the system to
switch between qualitatively distinct stable steady states. However, the
network structure of such a switching module is often uncertain, and there is
lack of knowledge about the exact reaction kinetics. In this paper, we
therefore perform an elementary study of small networks consisting of three
interacting transcriptional regulators responsible for cell differentiation: We
investigate which network structures can reproduce a certain multistable
behavior, and how robustly this behavior is realized by each network. In order
to approach these questions, we use a modeling framework which only uses
qualitative information about the network, yet allows model discrimination as
well as to evaluate the robustness of the desired multistability properties. We
reveal structural network properties which are necessary and sufficient to
realize distinct steady state patterns required for cell differentiation. Our
results also show that structural and robustness properties of the networks are
related to each other.

Motivated by the biologically important and complex phenomena of A\beta\
peptide aggregation in Alzheimer's disease, we introduce a model and simulation
methodology for studying protein aggregation that includes extra-cellular
aggregation, aggregation on the cell-surface assisted by a membrane bound
protein, and in addition, supply, clearance, production and sequestration of
peptides and proteins. The model is used to produce equilibrium and
kinetic-aggregation phase diagrams for aggregation onset and of reduced stable
A\beta\ monomer concentrations due to aggregation. The methodology we
implemented permits modeling of a phenomenon involving orders of magnitude
differences in time scales and concentrations which can be retained in the
simulation. We demonstrate how to identify ranges of parameter values that give
monomer concentration depletion upon aggregation similar to that observed in
Alzheimer's disease. We show how very different behavior can be obtained as
reaction parameters and protein concentrations vary, and discuss the difficulty
reconciling results of experiments from two vastly different concentration
regimes. The latter is an important general issue in relating in-vitro and mice
based experiments to humans.

Carlander's Handbook of Freshwater Fishery Biology (1969) contains life
history data from many species of freshwater fish found in North America. It
has been cited over 1200 times and used to produce standard-weight curves for
some species. Recent work (Cole-Fletcher et al. 2011) suggests Carlander (1969)
contains numerous errors in listed weight-length equations. This paper assesses
the weight-length relationships listed in Carlander for muskellunge, northern
pike, and chain pickerel by comparing graphs of the weight vs. length equations
with other data listed and with standard weight curves published by independent
sources. A number of discrepancies are identified through this analysis and new
weight-length relationships are produced from listed data.

We analyzed the periodic patterns in E. coli promoters and compared the
distributions of the corresponding patterns in promoters and in the complete
genome to elucidate their function. Except the three-base periodicity,
coincident with that in the coding regions and growing stronger in the region
downstream from the transcriptions start (TS), all other salient periodicities
are peaked upstream of TS. We found that helical periodicities with the lengths
about B-helix pitch ~10.2-10.5 bp and A-helix pitch ~10.8-11.1 bp coexist in
the genomic sequences. We mapped the distributions of stretches with A-, B-,
and Z- like DNA periodicities onto E.coli genome. All three periodicities tend
to concentrate within non-coding regions when their intensity becomes stronger
and prevail in the promoter sequences. The comparison with available
experimental data indicates that promoters with the most pronounced
periodicities may be related to the supercoiling-sensitive genes.

For velocity-jump Markov processes with equivariant internal dynamics, we
remark that population distributions are invariant. This provides a
formalization of the fact that FCD (scale) and other symmetry invariant systems
perform identical spatial searches under input transformations.

We propose an Individual-Based Model of ant-trail formation. The ants are
modeled as self-propelled particles which deposit directed pheromones and
interact with them through alignment interaction. The directed pheromones
intend to model pieces of trails, while the alignment interaction translates
the tendency for an ant to follow a trail when it meets it. Thanks to adequate
quantitative descriptors of the trail patterns, the existence of a phase
transition as the ant-pheromone interaction frequency is increased can be
evidenced. Finally, we propose both kinetic and fluid descriptions of this
model and analyze the capabilities of the fluid model to develop trail
patterns. We observe that the development of patterns by fluid models require
extra trail amplification mechanisms that are not needed at the
Individual-Based Model level.

We examine two models for hepatitis C viral (HCV) dynamics, one for
monotherapy with interferon (IFN) and the other for combination therapy with
IFN and ribavirin. Optimal therapy for both the models is determined using the
steepest gradient method, by defining an objective functional which minimizes
the infected hepatocyte levels, virion population and the side-effects of the
drug(s). The optimal therapy for both the models shows an initial period of
high efficacy, followed by a gradual decline. The period of high efficacy
coincides with a significant decrease in the infected hepatocyte levels as well
as viral load, whereas the efficacy drops after liver regeneration through
restored hepatocyte levels. The period of high efficacy is not altered
significantly when the cost coefficients are varied, as long as the side
effects are relatively small. This suggests a higher dependence of the optimal
therapy on the model parameters in case of drugs with minimal side effects.
  We use the Latin hypercube sampling technique to randomly generate a large
number of patient scenarios (i.e, model parameter sets) and study the dynamics
of each set under the optimal therapy already determined. Results show an
increase in the percentage of responders (as indicated by drop in viral load
below detection levels) in case of combination therapy as compared to
monotherapy. Statistical tests performed to study the correlations between
sample parameters and the time required for the viral load to fall below
detection level, show a strong monotonic correlation with the death rate of
infected hepatocytes, identifying it to be an important factor in deciding
individual drug regimens.

Within the preprocessing pipeline of a Next Generation Sequencing sample, its
set of Single-Base Mismatches is one of the first outcomes, together with the
number of correctly aligned reads. The union of these two sets provides a 4x4
matrix (called Single Base Indicator, SBI in what follows) representing a
blueprint of the sample and its preprocessing ingredients such as the
sequencer, the alignment software, the pipeline parameters. In this note we
show that, under the same technological conditions, there is a strong relation
between the SBI and the biological nature of the sample. To reach this goal we
need to introduce a similarity measure between SBIs: we also show how two
measures commonly used in machine learning can be of help in this context.

In several countries in southern Africa, including South Africa, the
prevalence of HIV remains stubbornly high in spite of considerable efforts to
reduce transmission and to provide anti-retroviral therapy (ART). It is
important to know the extent to which the high prevalence of HIV reflects the
increasing number of people on ART in which case the prevalence of those not on
ART may be falling. Unfortunately, direct measures of the proportion of
HIV-positive people who are on ART are lacking in most countries and we need to
use dynamical models to estimate the impact of ART on the prevalence of HIV. In
this paper we show that the current level of ART provision in South Africa has
probably reduced the prevalence of HIV among those not on ART by 1.9 million,
averted 259 thousand new infections and 428 thousand deaths.

The magnitude of traction forces exerted by living animal cells on their
environment is a monotonically increasing and approximately sigmoidal function
of the stiffness of the external medium. This observation is rationalized using
active matter theory: adaptation to substrate rigidity results from an
interplay between passive elasticity and active contractility.

Genotyping errors are known to influence the power of both family-based and
case-control studies in the genetics of complex disease. Estimating genotyping
error rate in a given dataset can be complex, but when family information is
available error rates can be inferred from the patterns of Mendelian
inheritance between parents and offspring. I introduce a novel likelihood-based
method for calculating error rates from family data, given known allele
frequencies. I apply this to an example dataset, demonstrating a low genotyping
error rate in genotyping data from a personal genomics company.

We derive an exact Green's function of the diffusion equation for a pair of
spherical interacting particles in 2D subject to a back-reaction boundary
condition.

The paper presents an algorithm for syndromic surveillance of an epidemic
outbreak formulated in the context of stochastic nonlinear filtering. The
dynamics of the epidemic is modeled using a generalized compartmental
epidemiological model with inhomogeneous mixing. The syndromic (typically
non-medical) observations of the number of infected people (e.g. visits to
pharmacies, sale of certain products, absenteeism from work/study etc.) are
used for estimation. The state of the epidemic, including the number of
infected people and the unknown parameters of the model, are estimated via a
particle filter. The numerical results indicate that the proposed framework can
provide useful early prediction of the epidemic peak if the uncertainty in
prior knowledge of model parameters is not excessive.

Summary: CytoSaddleSum provides Cytoscape users with access to the
functionality of SaddleSum, a functional enrichment tool based on sum-of-weight
scores. It operates by querying SaddleSum locally (using the standalone
version) or remotely (through an HTTP request to a web server). The functional
enrichment results are shown as a term relationship network, where nodes
represent terms and edges show term relationships. Furthermore, query results
are written as Cytoscape attributes allowing easy saving, retrieval and
integration into network-based data analysis workflows.
  Availability: www.ncbi.nlm.nih.gov/CBBresearch/Yu/downloads The source code
is placed in Public Domain.

The toxins associated with infectious diseases are potential targets for
inhibitors which have the potential for prophylactic or therapeutic use. Many
antibodies have been generated for this purpose, and the objective of this
study was to develop a simple mathematical model that may be used to evaluate
the potential protective effect of antibodies. This model was used to evaluate
the contributions of antibody affinity and concentration to reducing
antibody-receptor complex formation and internalization. The model also enables
prediction of the antibody kinetic constants and concentration required to
provide a specified degree of protection. We hope that this model, once
validated experimentally, will be a useful tool for in vitro selection of
potentially protective antibodies for progression to in vivo evaluation.

Several authors have hypothesized that ecological systems are subject to
thermodynamic optimization, which, if proven correct, could represent a long
sought general principle of organization in ecology. Although there have been
recent advances, this still remains as an unresolved topic, and ecologists lack
a general method to test thermodynamic optimization hypotheses in specific
systems. Here we present a general, novel approach that allows generating a
null model for testing thermodynamic optimization on ecological systems. We
first describe the general methodology, which is based in the analysis of a
parametrized mathematical model of the system and the explicit consideration of
constraints. Next we present an application example to an animal population
using a general age-structured population model and physiological parameters
from the literature. We finalize discussing the relevance of this work in the
context of the current state of ecology, and implications for the further
development of a thermodynamic ecological theory.

We present a numerically efficient method to reconstruct a disordered network
of thin biopolymers, such as collagen gels, from three-dimensional (3D) image
stacks recorded with a confocal microscope. Our method is based on a template
matching algorithm that simultaneously performs a binarization and
skeletonization of the network. The size and intensity pattern of the template
is automatically adapted to the input data so that the method is scale
invariant and generic. Furthermore, the template matching threshold is
iteratively optimized to ensure that the final skeletonized network obeys a
universal property of voxelized random line networks, namely, solid-phase
voxels have most likely three solid-phase neighbors in a $3\times3$
neighborhood. This optimization criterion makes our method free of user-defined
parameters and the output exceptionally robust against imaging noise.

We propose a quantitative method to estimate the statistical properties of
sets of genes for which expression data are available and co-registered to a
reference atlas of the brain. It is based on graph-theoretic properties of
co-expression coefficients between pairs of genes. We apply this method to
mouse genes from the Allen Gene Expression Atlas. Co-expression patterns of a
list of several hundreds of genes related to addiction are analyzed, using ISH
data produced for the mouse brain at the Allen Institute. It appears that large
subsets of this set of genes are much more highly co-expressed than expected by
chance.

We consider a mathematical model that describes the release of
heparin-binding growth factors from an affinity-based delivery system. In the
delivery system, heparin binds to a peptide which has been covalently
cross-linked to a fibrin matrix. Growth factor in turn binds to the heparin,
and growth factor release is governed by both binding and diffusion mechanisms,
the purpose of the binding being to slow growth factor release. The governing
mathematical model, which in its original formulation consists of five partial
differential equations, is reduced to a system of just two equations. We
identify the governing non-dimensional parameters that can be varied to tune
the growth factor release rate. In particular, we identify a parameter regime
that ensures slow passive release (usually desirable) of at least a fraction of
the growth factor. It is found that slow release is assured if the matrix is
prepared with the concentration of cross-linked peptide greatly exceeding the
dissociation constant of heparin from the peptide, and with the concentration
of heparin greatly exceeding the dissociation constant of the growth factor
from heparin. Also, for the first time, in vitro experimental release data is
directly compared with theoretical release profiles generated by the model. We
propose that the two stage release behaviour frequently seen in experiments is
due to an initial rapid out-diffusion of free growth factor over a diffusion
time scale (typically days), followed by a much slower release of the bound
fraction over a time scale depending on both diffusion and binding parameters
(frequently months).

Randomising networks using a naive `accept-all' edge-swap algorithm is
generally biased. Building on recent results for nondirected graphs, we
construct an ergodic detailed balance Markov chain with non-trivial acceptance
probabilities for directed graphs, which converges to a strictly uniform
measure and is based on edge swaps that conserve all in- and out-degrees. The
acceptance probabilities can also be generalized to define Markov chains that
target any alternative desired measure on the space of directed graphs, in
order to generate graphs with more sophisticated topological features. This is
demonstrated by defining a process tailored to the production of directed
graphs with specified degree-degree correlation functions. The theory is
implemented numerically and tested on synthetic and biological network
examples.

Cre-lox and other systems are used as genetic tools to control site-specific
recombination (SSR) events in genomic DNA. If multiple recombination sites are
organized in a compact cluster within the same genome, a series of random
recombination events may generate substantial cell specific genomic diversity.
This diversity is used, for example, to distinguish neurons in the brain of the
same multicellular mosaic organism, within the brainbow approach to neuronal
connectome. In this paper we study an exactly solvable statistical model for
SSR operating on a cluster of recombination sites. We consider two types of
recombination events: inversions and excisions. Both of these events are
available in the Cre-lox system. We derive three properties of the sequences
generated by multiple recombination events. First, we describe the set of
sequences that can in principle be generated by multiple inversions operating
on the given initial sequence. We call this description the ergodicity theorem.
On the basis of this description we calculate the number of sequences that can
be generated from an initial sequence. This number of sequences is
experimentally testable. Second, we demonstrate that after a large number of
random inversions every sequence that can be generated is generated with equal
probability. Lastly, we derive the equations for the probability to find a
sequence as a function of time in the limit when excisions are much less
frequent than inversions, such as in shufflon sequences.

CSA is a web server for the comprehensive comparison of pairwise protein
structure alignments. Its exact alignment engine computes either optimal,
top-scoring alignments or heuristic alignments with quality guarantee for the
inter-residue distance based scorings of contact map overlap, PAUL, DALI and
MATRAS. These and additional, uploaded alignments are compared using a number
of quality measures and intuitive visualizations. CSA brings new insight into
the structural relationship of the protein pairs under investigation and is a
valuable tool for studying structural similarities. It is available at
http://csa.project.cwi.nl

It is well known that individuals who abuse drugs usually use more than one
substance. Toxic consequences of single and multiple drug use are well
documented in the Treatment Episodes Data Set that lists combinations that
result in hospital admissions. Using this list as a guide, we focused our
attention on combinations that result in the most hospital admissions and
searched the PubMed database to determine the number of publications dealing
with these toxic combinations. Of special interest were those publications that
looked for or used the term synergism in their titles or abstracts, a search
that produced an extensive list of published articles. However, a further
intersection of these with the term isobole revealed a surprisingly small
number of literature reports. Because the method of isoboles is the most common
quantitative method for distinguishing between drug synergism and simple
additivity, the small number of investigations that actually employed this
quantitation suggests that the term synergism is not properly documented in
describing the toxicity among these abused substances. The possible reasons for
this lack of quantitation may be related to a misunderstanding of the modeling
equations. The theory and modeling are discussed here.

Euclidean distance geometry is the study of Euclidean geometry based on the
concept of distance. This is useful in several applications where the input
data consists of an incomplete set of distances, and the output is a set of
points in Euclidean space that realizes the given distances. We survey some of
the theory of Euclidean distance geometry and some of the most important
applications: molecular conformation, localization of sensor networks and
statics.

Here are presenting the blank based time-alignment (BBTA) as a strong
analytical approach for treatment of non-linear shift in time occurring in
HPLC-MS data. Need of such tool in recent large dataset produced by analytical
chemistry and so-called omics studies is evident. Proposed approach is based on
measurement and comparison of blank and analyzed sample evident features. In
the first step of BBTA procedure, the number of compounds is reduced by
max-to-mean ratio thresholding, which extensively reduce the computational
time. Simple thresholding is followed by selection of time markers defined from
blank inflex points which are then used for the transformation function,
polynomial of second degree, in the example. BBTA approach was compared on real
HPLC-MS measurement with Correlation Optimized Warping (COW) method. It was
proved to have distinctively shorter computational time as well as lower level
of mathematical presumptions. The BBTA is computationally much easier, quicker
(more then 1000x) and accurate in comparison with warping. Moreover, markers
selection works efficiently without any peak detection. It is sufficient to
analyze only baseline contribution in the analyte measurement with sparse
knowledge of blank behavior. Finally, BBTA does not required usage of extra
internal standards and due to its simplicity it has a potential to be
widespread tool in HPLC-MS data treatment.

A/H1N1 epidemic data from Istanbul, Turkey during the period June
2009-February 2010 is analyzed with SEIR (Susceptible-Exposed-Infected-Removed)
model. The data consist of the daily adult hospitalization numbers and
fatalities recorded in various state hospitals serving an adult population of
about 1.5-2 million. June 2009-August 2009 period corresponds to the initial
stage of the epidemic where the hospitalization rate is nearly %100 and it is
excluded from further consideration. The analysis covers the September
2009-February 2010 period, the total number of hospitalizations and fatalities
being respectively 869 and 46. It is shown that the maximum correlation between
the number of fatalities and hospitalizations occur with a time shift of 9 days
and the proportionality constant is {\delta}=0.0537. The SEIR epidemic model is
applied to the data by back-shifting the number of fatalities. The
determination of the best fitting model is based on the L2 norms of errors
between the model and the data and the errors are around %10 and %2.6 for the
number of hospitalizations and fatalities, respectively. The parameters in the
model are I0, {\eta}, {\epsilon} and {\beta}, where I0 is the percentage of
people infected initially, {\eta} and {\epsilon} are related to the inverses of
the infection and incubation periods and {\beta}/{\eta} is the representative
of the basic reproduction number. These parameters are determined as
{\eta}=0.09 (1/{\eta} =11.11days), I0=10^-7.4, {\epsilon}=0.32 (1/{\epsilon}
=3.125 days), {\beta}=0.585, {\beta}/{\eta}=6.5.

The three-dimensional data-driven Allen Gene Expression Atlas of the adult
mouse brain consists of numerized in-situ hybridization data for thousands of
genes, co-registered to the Allen Reference Atlas. We propose quantitative
criteria to rank genes as markers of a brain region, based on the localization
of the gene expression and on its functional fitting to the shape of the
region. These criteria lead to natural generalizations to sets of genes. We
find sets of genes weighted with coefficients of both signs with almost perfect
localization in all major regions of the left hemisphere of the brain, except
the pallidum. Generalization of the fitting criterion with positivity
constraint provides a lesser improvement of the markers, but requires sparser
sets of genes.

Amphiphilic peptide conjugation affords a significant increase in sensitivity
with protein quantification by electrospray-ionization mass spectrometry. This
has been demonstrated here for human growth hormone in serum using
N-(3-iodopropyl)-N,N,N-dimethyloctylammonium iodide (IPDOA-iodide) as
derivatizing reagent. The signal enhancement achieved in comparison to the
method without derivatization enables extension of the applicable concentration
range down to the very low concentrations as encountered with clinical glucose
suppression tests for patients with acromegaly. The method has been validated
using a set of serum samples spiked with known amounts of recombinant 22 kDa
growth hormone in the range of 0.48 to 7.65 \mug/L. The coefficient of
variation (CV) calculated, based on the deviation of results from the expected
concentrations, was 3.5% and the limit of quantification (LoQ) was determined
as 0.4 \mug/L. The potential of the method as a tool in clinical practice has
been demonstrated with patient samples of about 1 \mug/L.

Background: Psychedelic drugs facilitate profound changes in consciousness
and have potential to provide insights into the nature of human mental
processes and their relation to brain physiology. Yet published scientific
literature reflects a very limited understanding of the effects of these drugs,
especially for newer synthetic compounds. The number of clinical trials and
range of drugs formally studied is dwarfed by the number of written
descriptions of the many drugs taken by people. Analysis of these descriptions
using machine-learning techniques can provide a framework for learning about
these drug use experiences. Methods: We collected 1000 reports of 10 drugs from
the drug information website Erowid.org and formed a term-document frequency
matrix. Using variable selection and a random-forest classifier, we identified
a subset of words that differentiated between drugs. Results: A random forest
using a subset of 110 predictor variables classified with accuracy comparable
to a random forest using the full set of 3934 predictors. Our estimated
accuracy was 51.1%, which compares favorably to the 10% expected from chance.
Reports of MDMA had the highest accuracy at 86.9%; those describing DPT had the
lowest at 20.1%. Hierarchical clustering suggested similarities between certain
drugs, such as DMT and Salvia divinorum. Conclusion: Machine-learning
techniques can reveal consistencies in descriptions of drug use experiences
that vary by drug class. This may be useful for developing hypotheses about the
pharmacology and toxicity of new and poorly characterized drugs.

Many stochastic systems in physics and biology are investigated by recording
the two-dimensional (2D) positions of a moving test particle in regular time
intervals. The resulting sample trajectories are then used to induce the
properties of the underlying stochastic process. Often, it can be assumed a
priori that the underlying discrete-time random walk model is independent from
absolute position (homogeneity), direction (isotropy) and time (stationarity),
as well as ergodic. In this article we first review some common statistical
methods for analyzing 2D trajectories, based on quantities with built-in
rotational invariance. We then discuss an alternative approach in which the
two-dimensional trajectories are reduced to one dimension by projection onto an
arbitrary axis and rotational averaging. Each step of the resulting 1D
trajectory is further factorized into sign and magnitude. The statistical
properties of the signs and magnitudes are mathematically related to those of
the step lengths and turning angles of the original 2D trajectories,
demonstrating that no essential information is lost by this data reduction. The
resulting binary sequence of signs lends itself for a pattern counting
analysis, revealing temporal properties of the random process that are not
easily deduced from conventional measures such as the velocity autocorrelation
function. In order to highlight this simplified 1D description, we apply it to
a 2D random walk with restricted turning angles (RTA model), defined by a
finite-variance distribution $p(L)$ of step length and a narrow turning angle
distribution $p(\phi)$, assuming that the lengths and directions of the steps
are independent.

Many stochastic time series can be modelled by discrete random walks in which
a step of random sign but constant length $\delta x$ is performed after each
time interval $\delta t$. In correlated discrete time random walks (CDTRWs),
the probability $q$ for two successive steps having the same sign is unequal
1/2. The resulting probability distribution $P(\Delta x,\Delta t)$ that a
displacement $\Delta x$ is observed after a lagtime $\Delta t$ is known
analytically for arbitrary persistence parameters $q$. In this short note we
show how a CDTRW with parameters $[\delta t, \delta x, q]$ can be mapped onto
another CDTRW with rescaled parameters $[\delta t/s, \delta x\cdot g(q,s),
q^{\prime}(q,s)]$, for arbitrary scaling parameters $s$, so that both walks
have the same displacement distributions $P(\Delta x,\Delta t)$ on long time
scales. The nonlinear scaling functions $g(q,s)$ and $q^{\prime}(q,s)$ and
derived explicitely. This scaling method can be used to model time series
measured at discrete sample intervals $\delta t$ but actually corresponding to
continuum processes with variations occuring on a much shorter time scale
$\delta t/s$.

We have developed a method combining microfluidics, time-lapsed
single-molecule microscopy and automated image analysis allowing for the
observation of an excess of 3000 complete cell cycles of exponentially growing
Escherichia coli cells per experiment. The method makes it possible to analyze
the rate of gene expression at the level of single proteins over the bacterial
cell cycle. We also demonstrate that it is possible to count the number of
non-specifically DNA binding LacI-Venus molecules using short excitation light
pulses. The transcription factors are localized on the nucleoids in the cell
and appear to be uniformly distributed on chromosomal DNA. An increase of the
expression of LacI is observed at the beginning of the cell cycle, possibly
because some gene copies are de-repressed as a result of partitioning
inequalities at cell division. Finally, observe a size-growth rate uncertainty
relation where cells living in rich media vary more in the length at birth than
in generation time and the opposite is true for cells living in poorer media.

The evolution of a continuous time Markov process with a finite number of
states is usually calculated by the Master equation - a linear differential
equations with a singular generator matrix. We derive a general method for
reducing the dimensionality of the Master equation by one by using the
probability normalization constraint, thus obtaining a affine differential
equation with a (non-singular) stable generator matrix. Additionally, the
reduced form yields a simple explicit expression for the stationary probability
distribution, which is usually derived implicitly. Finally, we discuss the
application of this method to stochastic differential equations.

We study kinetic models of reversible enzyme reactions and compare two
techniques for analytic approximate solutions of the model. Analytic
approximate solutions of non-linear reaction equations for reversible enzyme
reactions are calculated using the Homotopy Perturbation Method (HPM) and the
Simple Iteration Method (SIM). The results of the approximations are similar.
The Matlab programs are included in appendices.

Stochastic rearrangement of germline DNA by VDJ recombination is at the
origin of immune system diversity. This process is implemented via a series of
stochastic molecular events involving gene choices and random nucleotide
insertions between, and deletions from, genes. We use large sequence
repertoires of the variable CDR3 region of human CD4+ T-cell receptor beta
chains to infer the statistical properties of these basic biochemical events.
Since any given CDR3 sequence can be produced in multiple ways, the probability
distribution of hidden recombination events cannot be inferred directly from
the observed sequences; we therefore develop a maximum likelihood inference
method to achieve this end. To separate the properties of the molecular
rearrangement mechanism from the effects of selection, we focus on
non-productive CDR3 sequences in T-cell DNA. We infer the joint distribution of
the various generative events that occur when a new T-cell receptor gene is
created. We find a rich picture of correlation (and absence thereof), providing
insight into the molecular mechanisms involved. The generative event statistics
are consistent between individuals, suggesting a universal biochemical process.
Our distribution predicts the generation probability of any specific CDR3
sequence by the primitive recombination process, allowing us to quantify the
potential diversity of the T-cell repertoire and to understand why some
sequences are shared between individuals. We argue that the use of formal
statistical inference methods, of the kind presented in this paper, will be
essential for quantitative understanding of the generation and evolution of
diversity in the adaptive immune system.

Dynamical systems are used to model a variety of phenomena in which the
bifurcation structure is a fundamental characteristic. Here we propose a
statistical machine-learning approach to derive lowdimensional models that
automatically integrate information in noisy time-series data from partial
observations. The method is tested using artificial data generated from two
cell-cycle control system models that exhibit different bifurcations, and the
learned systems are shown to robustly inherit the bifurcation structure.

This short article presents a mathematical formula required for metric
corrections in image extraction and processing when using different length
scale factors in three-dimensional space which is normally encountered in
cryomicrotome image construction techniques.

A lot of criticism against the standard formulation of pharmacokinetics has
been raised by several authors. It seems that the natural reaction for that
criticism is to comment it from the point of view of the theory of conservation
laws. Simple example of balance equations for the intravenous administration of
drug has been given in 2011 and the corresponding equations for extravasal
administration are in the text. In principle, the equations of that kind allow
one to describe in the self consistent manner different processes of
administration, distribution, metabolism and elimination of drugs. Moreover, it
is possible to model different pharmacokinetic parameters of the
non-compartmental pharmacokinetics and therefore to comment criticism of
Rosigno. However, for practical purposes one needs approximate methods, in
particular, those based on separation of the time scales. In this text, such
method is described and its effectiveness is discussed. Basic equations are in
the next chapter. Final remarks are at the end of the text.

The standard genetic code is known to be much more efficient in minimizing
adverse effects of misreading errors and one-point mutations in comparison with
a random code having the same structure, i.e. the same number of codons coding
for each particular amino acid. We study the inverse problem, how the code
structure affects the optimal physico-chemical parameters of amino acids
ensuring the highest stability of the genetic code. It is shown that the choice
of two or more amino acids with given properties determines unambiguously all
the others. In this sense the code structure determines strictly the optimal
parameters of amino acids. In the code with the structure of the standard
genetic code the resulting values for hydrophobicity obtained in the scheme
leave one out and in the scheme with fixed maximum and minimum parameters
correlate significantly with the natural scale. This indicates the co-evolution
of the genetic code and physico-chemical properties of amino acids.

Embryonic stem cells (ESCs) and induced pluripotent stem cells (iPSCs)
derived from somatic cells (SCs) provide promising resources for regenerative
medicine and medical research, leading to a daily identification of new cell
lines. However, an efficient system to discriminate the cell lines is lacking.
Here, we developed a quantitative system to discriminate the three cell types,
iPSCs, ESCs and SCs. The system contains DNA-methylation biomarkers and
mathematical models, including an artificial neural network and support vector
machines. All biomarkers were unbiasedly selected by calculating an eigengene
score derived from analysis of genome-wide DNA methylations. With 30
biomarkers, or even with as few as 3 top biomarkers, this system can
discriminate SCs from ESCs and iPSCs with almost 100% accuracy, and with
approximately 100 biomarkers, the system can distinguish ESCs from iPSCs with
an accuracy of 95%. This robust system performs precisely with raw data without
normalization as well as with converted data in which the continuous
methylation levels are accounted. Strikingly, this system can even accurately
predict new samples generated from different microarray platforms and the
next-generation sequencing. The subtypes of cells, such as female and male
iPSCs and fetal and adult SCs, can also be discriminated with this system.
Thus, this quantitative system works as a novel general and accurate framework
for discriminating the three cell types, iPSCs, ESCs, and SCs and this strategy
supports the notion that DNA-methylation generally varies among the three cell
types.

In recent years, we are seeing the formulation and use of elaborate and
complex models in ecological studies. The questions related to the efficient,
systematic and error-proof exploration of parameter spaces are of great
importance to better understand, estimate confidences and make use of the
output from these models. In this work, we investigate some of the relevant
questions related to parameter space exploration, in particular using the
technique known as Latin Hypercube Sampling and focusing in quantitative output
analysis. We present the analysis of a structured population growth model and
contrast our findings with results from previously used techniques, known as
sensitivity and elasticity analyses. We also assess how are the questions
related to parameter space analysis being currently addressed in the ecological
literature.

Multiple sequence alignment (MSA) is a fundamental and ubiquitous technique
in bioinformatics used to infer related residues among biological sequences.
Thus alignment accuracy is crucial to a vast range of analyses, often in ways
difficult to assess in those analyses. To compare the performance of different
aligners and help detect systematic errors in alignments, a number of
benchmarking strategies have been pursued. Here we present an overview of the
main strategies--based on simulation, consistency, protein structure, and
phylogeny--and discuss their different advantages and associated risks. We
outline a set of desirable characteristics for effective benchmarking, and
evaluate each strategy in light of them. We conclude that there is currently no
universally applicable means of benchmarking MSA, and that developers and users
of alignment tools should base their choice of benchmark depending on the
context of application--with a keen awareness of the assumptions underlying
each benchmarking strategy.

The concentration of CD4 T-lymphocytes (CD4 count), in a person's plasma is
widely used to decide when to start HIV-positive people on anti-retroviral
therapy (ART) and to predict the impact of ART on the future course of HIV and
tuberculosis (TB). However, CD4 cell-counts vary widely within and among
populations and depend on many factors besides HIV-infection. The way in which
CD4 counts decline over the course of HIV infection is neither well understood
nor widely agreed. We review what is known about CD4 counts in relation to HIV
and TB and discuss areas in which more research is needed to build a consensus
on how to interpret and use CD4 counts in clinical practice and to develop a
better understanding of the dynamics and control of HIV and HIV-related TB.

Background: Animals from the same litter are often more alike compared with
animals from different litters. This litter-to-litter variation, or "litter
effects", can influence the results in addition to the experimental factors of
interest. Furthermore, an experimental treatment can be applied to whole
litters rather than to individual offspring. For example, in the valproic acid
(VPA) model of autism, VPA is administered to pregnant females thereby inducing
the disease phenotype in the offspring. With this type of experiment the sample
size is the number of litters and not the total number of offspring. If such
experiments are not appropriately designed and analysed, the results can be
severely biased as well as extremely underpowered.
  Results: A review of the VPA literature showed that only 9% (3/34) of studies
correctly determined that the experimental unit (n) was the litter and
therefore made valid statistical inferences. In addition, litter effects
accounted for up to 61% (p <0.001) of the variation in behavioural outcomes,
which was larger than the treatment effects. In addition, few studies reported
using randomisation (12%) or blinding (18%), and none indicated that a sample
size calculation or power analysis had been conducted.
  Conclusions: Litter effects are common, large, and ignoring them can make
replication of findings difficult and can contribute to the low rate of
translating preclinical in vivo studies into successful therapies. Only a
minority of studies reported using rigorous experimental methods, which is
consistent with much of the preclinical in vivo literature.

We investigate to what extent the interaction dynamics of a population of
wild house mouse (Mus musculus domesticus) in their environment can be
explained by a simple stochastic model. We use a Markov chain model to describe
the transitions of mice in a discrete space of nestboxes, and implement a
multi-agent simulation of the model. We find that some important features of
our behavioural dataset can be reproduced using this simplified stochastic
representation, and discuss the improvements that could be made to our model in
order to increase the accuracy of its predictions. Our findings have
implications for the understanding of the complexity underlying social
behaviour in the animal kingdom and the cognitive requirements of such
behaviour.

Leaf area LA, is a plant biometric index important to agroforestry and crop
production. Previous works have demonstrated the conservativeness of the
inverse of the product of the fresh leaf density and thickness, the so-called
Hughes constant, K. We use this fact to develop LAMM, an absolute method of LA
measurement, i.e. no regression fits or prior calibrations with planimeters.
Nor does it require drying the leaves. The concept involves the in situ
determination of K using geometrical shapes and their weights obtained from a
subset of fresh leaves of the set whose areas are desired. Subsequently the
LAs, at any desired stratification level, are derived by utilizing K and the
previously measured masses of the fresh leaves. The concept was first tested in
the simulated ideal case of complete planarity and uniform thickness by using
plastic film covered card-paper sheets. Next the species-specific
conservativeness of K over individual leaf zones and different leaf types from
leaves of plants from two species, Mandevilla splendens and Spathiphyllum
wallisii, was quantitatively validated. Using the global average K values, the
LA of these and additional plants, were obtained. LAMM was found to be a rapid,
simple, economic technique with accuracies, as measured for the geometrical
shapes, that were comparable to those obtained by the planimetric method that
utilizes digital image analysis, DIA. For the leaves themselves, there were no
statistically significant differences between the LAs measured by LAMM and by
the DIA and the linear correlation between the two methods was excellent.

In the text S.Piekarski, M.Rewekant,(arXiv:1208.3847)it has been mentioned
that some information on bioavailability and bioequivalence of drugs can be
obtained from simulations based on the conservation laws. Here we shortly
discuss that possibility starting from the fundamental pharmacokinetic
parameter called AUC (Area Under the Curve). The curve is is the profile shape
of plasma drug concentration in time intervals after drug administration into
organism. Our aim here is to give some information on the subject for the
reader with no experience in pharmacokinetics.

HIV increases the likelihood that a person will develop TB. Starting them on
anti-retroviral therapy (ART) reduces their risk of TB but not to the level in
HIV negative people. Since HIV-positive people who are on ART can expect to
live a normal life for several decades this raises the possibility that their
elevated risk of infection, lasting for a long time, could lead to an increase
in the population level incidence of TB. Here we investigate the conditions
under which this could happen and show that provided HIV-positive people start
ART when their CD4+ cell count is greater than 350/microL and that there is
high coverage, ART will not lead to a long-term increase in HIV. Only if people
start ART very late and there is low coverage of ART might starting people on
ART increase the population level incidence of TB.

Parameter estimation in ordinary differential equations, although applied and
refined in various fields of the quantitative sciences, is still confronted
with a variety of difficulties. One major challenge is finding the global
optimum of a log-likelihood function that has several local optima, e.g. in
oscillatory systems. In this publication, we introduce a formulation based on
continuation of the log-likelihood function that allows to restate the
parameter estimation problem as a boundary value problem. By construction, the
ordinary differential equations are solved and the parameters are estimated
both in one step. The formulation as a boundary value problem enables an
optimal transfer of information given by the measurement time courses to the
solution of the estimation problem, thus favoring convergence to the global
optimum. This is demonstrated explicitly for the fully as well as the partially
observed Lotka-Volterra system.

Light sheet microscopy promises to revolutionize developmental biology by
enabling live in toto imaging of entire embryos with minimal phototoxicity. We
present detailed instructions for building a compact and customizable Selective
Plane Illumination Microscopy (SPIM) system. The integrated OpenSPIM hardware
and software platform is shared with the scientific community through a public
website, thereby making light sheet microscopy accessible for widespread use
and optimization to various applications.

This note works out an advection-diffusion approximation to the density of a
population of E. coli bacteria undergoing chemotaxis in a one-dimensional
space. Simulations show the high quality of predictions under a
shallow-gradient regime.

Mathematical methods together with measurements of single-cell dynamics
provide unprecedented means to reconstruct intracellular processes that are
only partly or indirectly accessible experimentally. To obtain reliable
reconstructions the pooling of measurements from several cells of a clonal
population is mandatory. The population's considerable cell-to-cell variability
originating from diverse sources poses novel computational challenges for
process reconstruction. We introduce an exact Bayesian inference framework that
properly accounts for the population heterogeneity but also retains scalability
with respect to the number of pooled cells. The key ingredient is a stochastic
process that captures the heterogeneous kinetics of a population. The method
allows to infer inaccessible molecular states, kinetic parameters, compute
Bayes factors and to dissect intrinsic, extrinsic and technical contributions
to the variability in the data. We also show how additional single-cell
readouts such as morphological features can be included into the analysis. We
then reconstruct the expression dynamics of a gene under an inducible GAL1
promoter in yeast from time-lapse microscopy data. Based on Bayesian model
selection the data yields no evidence of a refractory period for this promoter.

Responsible for many complex human diseases including cancers, disrupted or
abnormal gene interactions can be identified through their expression changes
correlating with the progression of a disease. However, the examination of all
possible combinatorial interactions between gene features in a genome-wide
case-control study is computationally infeasible as the search space is
exponential in nature.
  In this paper, we propose a novel computational approach, QUIRE, to identify
discriminative complex interactions among informative gene features for cancer
diagnosis. QUIRE works in two stages, where it first identifies functionally
relevant feature groups for the disease and, then explores the search space
capturing the combinatorial relationships among the genes from the selected
informative groups. Using QUIRE, we explore the differential patterns and the
interactions among informative gene features in three different types of
cancers, Renal Cell Carcinoma(RCC), Ovarian Cancer(OVC) and Colorectal Cancer
(CRC). Our experimental results show that QUIRE identifies gene-gene
interactions that can better identify the different cancer stages of samples
and can predict CRC recurrence and death from CRC more successfully, as
compared to other state-of-the-art feature selection methods. A literature
survey shows that many of the interactions identified by QUIRE play important
roles in the development of cancer.

In this paper, we give an overview of the differential algebra approach to
identifiability, and then note a very simple observation about input-output
equivalence and identifiability, that describes the identifiability equivalence
between input-output equivalent models. We then give several simple
consequences of this observation that can be useful in showing identifiability,
including examining non-first order ODE models, nondimensionalization and
rescaling, model reducibility, and a modular approach to evaluating
identifiability. We also examine how input-output equivalence can allow us to
generate input output equations in the differential algebra approach through a
wider range of methods (e.g. substitution and differential or standard Groebner
basis approaches).

We briefly outline an algorithm for accurate quantification of specific
binding of gold particles to fixed biological tissue samples prepared for
immuno-transmission electron microscopy (TEM). The algorithm is based on
existing protocols for rational accounting of colloidal gold particles used in
secondary antibodies for immuno-gold labeling.

Although microarrays are routine analysis tools in biomedical research, they
still yield noisy output that often requires experimental confirmation. Many
studies have aimed at optimizing probe design and statistical analysis to
tackle this problem. However, less emphasis has been placed on controlling the
noise inherent to the experimental approach. To address this problem, we
investigate here a procedure that controls for such experimental variance and
combine it with an assessment of probe performance. Two custom arrays were used
to evaluate the procedure: one based on 25mer probes from an Affymetrix design
and the other based on 60mer probes from an Agilent design. To assess
experimental variance, all probes were replicated ten times. To assess probe
performance, the probes were calibrated using a dilution series of target
molecules and the signal response was fitted to an absorption model. We found
that significant variance of the signal could be controlled by averaging across
probes and removing probes that are nonresponsive. Thus, a more reliable signal
could be obtained using our procedure than conventional approaches. We suggest
that once an array is properly calibrated, absolute quantification of signals
becomes straight forward, alleviating the need for normalization and reference
hybridizations.

In the XYZ color space, the subset of the tri-stimuli corresponding to
spike-type (monochromatic) impingement of energy is the chromaticity cone, CC.
Using a family of concentric spheres, we describe a nonlinear transformation
over the CC and construct a bijection from the CC onto the flat plane. In the
process, we open up the CC and view it as a chart on the plane. Because the map
is a bijection, the color perception information is preserved (invariant)
through the transformation. We discuss stereographic projection of the
chromaticity chart and some examples.

Exploiting the information provided by the molecular noise of a biological
process has proven to be valuable in extracting knowledge about the underlying
kinetic parameters and sources of variability from single cell measurements.
However, quantifying this additional information a priori, to decide whether a
single cell experiment might be beneficial, is currently only possibly in very
simple systems where either the chemical master equation is computationally
tractable or a Gaussian approximation is appropriate. Here we show how the
information provided by distribution measurements can be approximated from the
first four moments of the underlying process. The derived formulas are
generally valid for any stochastic kinetic model including models that comprise
both intrinsic and extrinsic noise. This allows us to propose an optimal
experimental design framework for heterogeneous cell populations which we
employ to compare the utility of dual reporter and perturbation experiments for
separating extrinsic and intrinsic noise in a simple model of gene expression.
Subsequently, we compare the information content of different experiments which
have been performed in an engineered light-switch gene expression system in
yeast and show that well chosen gene induction patterns may allow one to
identify features of the system which remain hidden in unplanned experiments.

Pharmacological challenge imaging has mapped, but rarely quantified, the
sensitivity of a biological system to a given drug. We describe a novel method
called rapid quantitative pharmacodynamic imaging. This method combines
pharmacokinetic-pharmacodynamic modeling, repeated small doses of a challenge
drug over a short time scale, and functional imaging to rapidly provide
quantitative estimates of drug sensitivity including EC50 (the concentration of
drug that produces half the maximum possible effect). We first test the method
with simulated data, assuming a typical sigmoidal dose-response curve and
assuming imperfect imaging that includes artifactual baseline signal drift and
random error. With these few assumptions, rapid quantitative pharmacodynamic
imaging reliably estimates EC50 from the simulated data, except when noise
overwhelms the drug effect or when the effect occurs only at high doses. In
preliminary fMRI studies of primate brain using a dopamine agonist, the
observed noise level is modest compared with observed drug effects, and a
quantitative EC50 can be obtained from some regional time-signal curves. Taken
together, these results suggest that research and clinical applications for
rapid quantitative pharmacodynamic imaging are realistic.

The most established method of reconstructing neural circuits from animals
involves slicing tissue very thin, then taking mosaics of electron microscope
(EM) images. To trace neurons across different images and through different
sections, these images must be accurately aligned, both with the others in the
same section and to the sections above and below. Unfortunately, sectioning and
imaging are not ideal processes - some of the problems that make alignment
difficult include lens distortion, tissue shrinkage during imaging, tears and
folds in the sectioned tissue, and dust and other artifacts. In addition the
data sets are large (hundreds of thousands of images) and each image must be
aligned with many neighbors, so the process must be automated and reliable.
This paper discusses methods of dealing with these problems, with numeric
results describing the accuracy of the resulting alignments.

Our view of the microbial world and its impact on human health is changing
radically with the ability to sequence uncultured or unculturable microbes
sampled directly from their habitats, ability made possible by fast and cheap
next generation sequencing technologies. Such recent developments represents a
paradigmatic shift in the analysis of habitat biodiversity, be it the human,
soil or ocean microbiome. We review here some research examples and results
that indicate the importance of the microbiome in our lives and then discus
some of the challenges faced by metagenomic experiments and the subsequent
analysis of the generated data. We then analyze the economic and social impact
on genomic-medicine and research in both developing and developed countries. We
support the idea that there are significant benefits in building capacities for
developing high-level scientific research in metagenomics in developing
countries. Indeed, the notion that developing countries should wait for
developed countries to make advances in science and technology that they later
import at great cost has recently been challenged.

To facilitate the analysis of large-scale high-throughput capillary
electrophoresis data, we previously proposed a suite of efficient analysis
software named HiTRACE (High Throughput Robust Analysis of Capillary
Electrophoresis). HiTRACE has been used extensively for quantitating data from
RNA and DNA structure mapping experiments, including mutate-and-map contact
inference, chromatin footprinting, the EteRNA RNA design project and other
high-throughput applications. However, HiTRACE is based on a suite of
command-line MATLAB scripts that requires nontrivial efforts to learn, use, and
extend. Here we present HiTRACE-Web, an online version of HiTRACE that includes
standard features previously available in the command-line version as well as
additional features such as automated band annotation and flexible adjustment
of annotations, all via a user-friendly environment. By making use of
parallelization, the on-line workflow is also faster than software
implementations available to most users on their local computers. Free access:
http://hitrace.org

The problem of RNA secondary structure design (also called inverse folding)
is the following: given a target secondary structure, one aims to create a
sequence that folds into, or is compatible with, a given structure. In several
practical applications in biology, additional constraints must be taken into
account, such as the presence/absence of regulatory motifs, either at a
specific location or anywhere in the sequence. In this study, we investigate
the design of RNA sequences from their targeted secondary structure, given
these additional sequence constraints. To this purpose, we develop a general
framework based on concepts of language theory, namely context-free grammars
and finite automata. We efficiently combine a comprehensive set of constraints
into a unifying context-free grammar of moderate size. From there, we use
generic generic algorithms to perform a (weighted) random generation, or an
exhaustive enumeration, of candidate sequences. The resulting method, whose
complexity scales linearly with the length of the RNA, was implemented as a
standalone program. The resulting software was embedded into a publicly
available dedicated web server. The applicability demonstrated of the method on
a concrete case study dedicated to Exon Splicing Enhancers, in which our
approach was successfully used in the design of \emph{in vitro} experiments.

This paper presents a non-parametric classification technique for identifying
a candidate bi-allelic genetic marker set that best describes disease
susceptibility in gene-gene interaction studies. The developed technique
functions by creating a mapping between inferred haplotypes and case/control
status. The technique cycles through all possible marker combination models
generated from the available marker set where the best interaction model is
determined from prediction accuracy and two auxiliary criteria including
low-to-high order haplotype propagation capability and model parsimony. Since
variable-length haplotypes are created during the best model identification,
the developed technique is referred to as a variable-length haplotype
construction for gene-gene interaction (VarHAP) technique. VarHAP has been
benchmarked against a multifactor dimensionality reduction (MDR) program and a
haplotype interaction technique embedded in a FAMHAP program in various
two-locus interaction problems. The results reveal that VarHAP is suitable for
all interaction situations with the presence of weak and strong linkage
disequilibrium among genetic markers.

Analysis of the sequence-structure relationship in RNA molecules are
essential to evolutionary studies but also to concrete applications such as
error-correction methodologies in sequencing technologies. The prohibitive
sizes of the mutational and conformational landscapes combined with the volume
of data to proceed require efficient algorithms to compute sequence-structure
properties. More specifically, here we aim to calculate which mutations
increase the most the likelihood of a sequence to a given structure and RNA
family. In this paper, we introduce RNApyro, an efficient linear-time and space
inside-outside algorithm that computes exact mutational probabilities under
secondary structure and evolutionary constraints given as a multiple sequence
alignment with a consensus structure. We develop a scoring scheme combining
classical stacking base pair energies to novel isostericity scales, and apply
our techniques to correct point-wise errors in 5s and 16s rRNA sequences. Our
results suggest that RNApyro is a promising algorithm to complement existing
tools in the NGS error-correction pipeline.

Although cancer is known to be characterized by several unifying biological
hallmarks, systems biology has had limited success in identifying molecular
signatures present in in all types of cancer. The current availability of rich
data sets from many different cancer types provides an opportunity for thorough
computational data mining in search of such common patterns. Here we report the
identification of 18 "pan-cancer" molecular signatures resulting from analysis
of data sets containing values from mRNA expression, microRNA expression, DNA
methylation, and protein activity, from twelve different cancer types. The
membership of many of these signatures points to particular biological
mechanisms related to cancer progression, suggesting that they represent
important attributes of cancer in need of being elucidated for potential
applications in diagnostic, prognostic and therapeutic products applicable to
multiple cancer types.

We study collective behavior of Brodmann regions of human cerebral cortex
using functional Magnetic Resonance Imaging (fMRI) and Random Matrix Theory
(RMT). The raw fMRI data is mapped onto the cortex regions corresponding to the
Brodmann areas with the aid of the Talairach coordinates. Principal Component
Analysis (PCA) of the Pearson correlation matrix for 41 different Brodmann
regions is carried out to determine their collective activity in the idle state
and in the active state stimulated by tapping. The collective brain activity is
identified through the statistical analysis of the eigenvectors to the largest
eigenvalues of the Pearson correlation matrix. The leading eigenvectors have a
large participation ratio. This indicates that several Broadmann regions
collectively give rise to the brain activity associated with these
eigenvectors. We apply random matrix theory to interpret the underlying
multivariate data.

Photosynthetic starch reserves that accumulate in Arabidopsis leaves during
the day decrease approximately linearly with time at night to support
metabolism and growth. We find that the rate of decrease is adjusted to
accommodate variation in the time of onset of darkness and starch content, such
that reserves last almost precisely until dawn. Generation of these dynamics
therefore requires an arithmetic division computation between the starch
content and expected time to dawn. We introduce two novel chemical kinetic
models capable of implementing analog arithmetic division. Predictions from the
models are successfully tested in plants perturbed by a night-time light period
or by mutations in starch degradation pathways. Our experiments indicate which
components of the starch degradation apparatus may be important for appropriate
arithmetic division. Our results are potentially relevant for any biological
system dependent on a food reserve for survival over a predictable time period.

Boolean networks are special types of finite state time-discrete dynamical
systems. A Boolean network can be described by a function from an n-dimensional
vector space over the field of two elements to itself. A fundamental problem in
studying these dynamical systems is to link their long term behaviors to the
structures of the functions that define them. In this paper, a method for
deriving a Boolean network's dynamical information via its disjunctive normal
form is explained. For a given Boolean network, a matrix with entries 0 and 1
is associated with the polynomial function that represents the network, then
the information on the fixed points and the limit cycles is derived by
analyzing the matrix. The described method provides an algorithm for the
determination of the fixed points from the polynomial expression of a Boolean
network. The method can also be used to construct Boolean networks with
prescribed limit cycles and fixed points. Examples are provided to explain the
algorithm.

Identifiability is a necessary condition for successful parameter estimation
of dynamic system models. A major component of identifiability analysis is
determining the identifiable parameter combinations, the functional forms for
the dependencies between unidentifiable parameters. Identifiable combinations
can help in model reparameterization and also in determining which parameters
may be experimentally measured to recover model identifiability. Several
numerical approaches to determining identifiability of differential equation
models have been developed, however the question of determining identifiable
combinations remains incompletely addressed. In this paper, we present a new
approach which uses parameter subset selection methods based on the Fisher
Information Matrix, together with the profile likelihood, to effectively
estimate identifiable combinations. We demonstrate this approach on several
example models in pharmacokinetics, cellular biology, and physiology.

The development of potent drugs for the control of viraemia in people living
with HIV means that infected people may live a normal, healthy life and offers
the prospect of eliminating HIV transmission in the short term and HIV
infection in the long term. Other interventions, including the use of condoms,
pre-exposure prophylaxis, treatment of sexually transmitted infections and
behaviour change programmes, may also be effective in reducing HIV transmission
to varying degrees.
  Here we examine recommendations for when to start treatment with
anti-retroviral drugs, estimate the impact that treatment may have on HIV
transmission in the short and in the long term, and compare the impact and cost
of treatment with that of other methods of control. We focus on generalized HIV
epidemics in sub-Saharan Africa. We show that universal access to ART combined
with early treatment is the most effective and, in the long term, the most
cost-effective intervention. Elimination will require effective coverage of
about 80% or more but treatment is effective and cost effective even at low
levels of coverage.
  Other interventions may provide important support to a programme of early
treatment in particular groups. Condoms provide protection for both men and
women and should be readily available whenever they are needed. Medical male
circumcision will provide a degree of immediate protection for men and
microbicides will do the same for women. Behaviour change programmes in
themselves are unlikely to have a significant impact on overall transmission
but may play a critical role in supporting early treatment through helping to
avoid stigma and discrimination, ensuring the acceptability of testing and
early treatment as well as compliance.

Accurate measurements of kinetic rate constants for interacting biomolecules
is crucial for understanding the mechanisms underlying intracellular signalling
pathways. The magnitude of binding rates plays a very important molecular
regulatory role which can lead to very different cellular physiological
responses under different conditions. Here, we extend the k-space image
correlation spectroscopy (kICS) technique to study the kinetic binding rates of
systems wherein: (a) fluorescently labelled, free ligands in solution interact
with unlabelled, diffusing receptors in the plasma membrane and (b) systems
where labelled, diffusing receptors are allowed to bind/unbind and interconvert
between two different diffusing states on the plasma membrane. We develop the
necessary mathematical framework for the kICS analysis and demonstrate how to
extract the elevant kinetic binding parameters of the underlying molecular
system from fluorescence video-microscopy image time-series. Finally, by
examining real data for two model experimental systems, we demonstrate how kICS
can be a powerful tool to measure molecular transport coefficients and binding
kinetics.

A variety of methods have been proposed for structure similarity calculation,
which are called structure alignment or superposition. One major shortcoming in
current structure alignment algorithms is in their inherent design, which is
based on local structure similarity. In this work, we propose a method to
incorporate global information in obtaining optimal alignments and
superpositions. Our method, when applied to optimizing the TM-score and the GDT
score, produces significantly better results than current state-of-the-art
protein structure alignment tools. Specifically, if the highest TM-score found
by TMalign is lower than (0.6) and the highest TM-score found by one of the
tested methods is higher than (0.5), there is a probability of (42%) that
TMalign failed to find TM-scores higher than (0.5), while the same probability
is reduced to (2%) if our method is used. This could significantly improve the
accuracy of fold detection if the cutoff TM-score of (0.5) is used.
  In addition, existing structure alignment algorithms focus on structure
similarity alone and simply ignore other important similarities, such as
sequence similarity. Our approach has the capacity to incorporate multiple
similarities into the scoring function. Results show that sequence similarity
aids in finding high quality protein structure alignments that are more
consistent with eye-examined alignments in HOMSTRAD. Even when structure
similarity itself fails to find alignments with any consistency with
eye-examined alignments, our method remains capable of finding alignments
highly similar to, or even identical to, eye-examined alignments.

Metabolomics complements investigation of the genome, transcriptome, and
proteome of an organism. Today, the vast majority of metabolites remain
unknown, in particular for non-model organisms. Mass spectrometry is one of the
predominant techniques for analyzing small molecules such as metabolites. A
fundamental step for identifying a small molecule is to determine its molecular
formula.
  Here, we present and evaluate three algorithm engineering techniques that
speed up the molecular formula determination. For that, we modify an existing
algorithm for decomposing the monoisotopic mass of a molecule. These techniques
lead to a four-fold reduction of running times, and reduce memory consumption
by up to 94%. In comparison to the classical search tree algorithm, our
algorithm reaches a 1000-fold speedup.

Progressive methods offer efficient and reasonably good solutions to the
multiple sequence alignment problem. However, resulting alignments are biased
by guide-trees, especially for relatively distant sequences.
  We propose MSARC, a new graph-clustering based algorithm that aligns sequence
sets without guide-trees. Experiments on the BAliBASE dataset show that MSARC
achieves alignment quality similar to best progressive methods and
substantially higher than the quality of other non-progressive algorithms.
Furthermore, MSARC outperforms all other methods on sequence sets with the
similarity structure hardly represented by a phylogenetic tree. Furthermore,
MSARC outperforms all other methods on sequence sets whose evolutionary
distances are hardly representable by a phylogenetic tree. These datasets are
most exposed to the guide-tree bias of alignments.
  MSARC is available at http://bioputer.mimuw.edu.pl/msarc

Statistics in ranked lists is important in analyzing molecular biology
measurement data, such as ChIP-seq, which yields ranked lists of genomic
sequences. State of the art methods study fixed motifs in ranked lists. More
flexible models such as position weight matrix (PWM) motifs are not addressed
in this context. To assess the enrichment of a PWM motif in a ranked list we
use a PWM induced second ranking on the same set of elements. Possible orders
of one ranked list relative to the other are modeled by permutations. Due to
sample space complexity, it is difficult to characterize tail distributions in
the group of permutations. In this paper we develop tight upper bounds on tail
distributions of the size of the intersection of the top of two uniformly and
independently drawn permutations and demonstrate advantages of this approach
using our software implementation, mmHG-Finder, to study PWMs in several
datasets.

Tumorigenesis is an evolutionary process which involves a significant number
of genomic rearrangements typically coupled with changes in the gene copy
number profiles of numerous cells. Fluorescence in situ hybridization (FISH) is
a cytogenetic technique which allows counting copy numbers of genes in single
cells. The study of cancer progression using FISH data has received
considerably less attention compared to other types of cancer datasets.
  In this work we focus on inferring likely tumor progression pathways using
publicly available FISH data. We model the evolutionary process as a Markov
chain in the positive integer cone Z_+^g where g is the number of genes
examined with FISH. Compared to existing work which oversimplifies reality by
assuming independence of copy number changes, our model is able to capture
dependencies. We model the probability distribution of a dataset with
hierarchical log-linear models, a popular probabilistic model of count data.
Our choice provides an attractive trade-off between parsimony and good data
fit. We prove a theorem of independent interest which provides necessary and
sufficient conditions for reconstructing oncogenetic trees. Using this theorem
we are able to capitalize on the wealth of inter-tumor phylogenetic methods. We
show how to produce tumor phylogenetic trees which capture the dynamics of
cancer progression. We validate our proposed method on a breast tumor dataset.

In cell differentiation, a cell of a less specialized type becomes one of a
more specialized type, even though all cells have the same genome.
Transcription factors and epigenetic marks like histone modifications can play
a significant role in the differentiation process. In this paper, we present a
simple analysis of cell types and differentiation paths using phylogenetic
inference based on ChIP-Seq histone modification data. We propose new data
representation techniques and new distance measures for ChIP-Seq data and use
these together with standard phylogenetic inference methods to build
biologically meaningful trees that indicate how diverse types of cells are
related. We demonstrate our approach on H3K4me3 and H3K27me3 data for 37 and 13
types of cells respectively, using the dataset to explore various issues
surrounding replicate data, variability between cells of the same type, and
robustness. The promising results we obtain point the way to a new approach to
the study of cell differentiation.

Cell heterogeneity and the inherent complexity due to the interplay of
multiple molecular processes within the cell pose difficult challenges for
current single-cell biology. We introduce an approach that identifies a disease
phenotype from multiparameter single-cell measurements, which is based on the
concept of "supercell statistics", a single-cell-based averaging procedure
followed by a machine learning classification scheme. We are able to assess the
optimal tradeoff between the number of single cells averaged and the number of
measurements needed to capture phenotypic differences between healthy and
diseased patients, as well as between different diseases that are difficult to
diagnose otherwise. We apply our approach to two kinds of single-cell datasets,
addressing the diagnosis of a premature aging disorder using images of cell
nuclei, as well as the phenotypes of two non-infectious uveitides (the ocular
manifestations of Beh\c{c}et's disease and sarcoidosis) based on multicolor
flow cytometry. In the former case, one nuclear shape measurement taken over a
group of 30 cells is sufficient to classify samples as healthy or diseased, in
agreement with usual laboratory practice. In the latter, our method is able to
identify a minimal set of 5 markers that accurately predict Beh\c{c}et's
disease and sarcoidosis. This is the first time that a quantitative phenotypic
distinction between these two diseases has been achieved. To obtain this clear
phenotypic signature, about one hundred CD8+ T cells need to be measured.
Beyond these specific cases, the approach proposed here is applicable to
datasets generated by other kinds of state-of-the-art and forthcoming
single-cell technologies, such as multidimensional mass cytometry, single-cell
gene expression, and single-cell full genome sequencing techniques.

We utilized abundant transcriptomic data for the primary classes of brain
cancers to study the feasibility of separating all of these diseases
simultaneously based on molecular data alone. These signatures were based on a
new method reported herein that resulted in a brain cancer marker panel of 44
unique genes. Many of these genes have established relevance to the brain
cancers examined, with others having known roles in cancer biology. Analyses on
large-scale data from multiple sources must deal with significant challenges
associated with heterogeneity between different published studies, for it was
observed that the variation among individual studies often had a larger effect
on the transcriptome than did phenotype differences, as is typical. We found
that learning signatures across multiple datasets greatly enhanced
reproducibility and accuracy in predictive performance on truly independent
validation sets, even when keeping the size of the training set the same. This
was most likely due to the meta-signature encompassing more of the
heterogeneity across different sources and conditions, while amplifying signal
from the repeated global characteristics of the phenotype. When molecular
signatures of brain cancers were constructed from all currently available
microarray data, 90 percent phenotype prediction accuracy, or the accuracy of
identifying a particular brain cancer from the background of all phenotypes,
was found. Looking forward, we discuss our approach in the context of the
eventual development of organ-specific molecular signatures from peripheral
fluids such as the blood.

Light sheet fluorescence microscopy is able to image large specimen with high
resolution by imaging the sam- ples from multiple angles. Multi-view
deconvolution can significantly improve the resolution and contrast of the
images, but its application has been limited due to the large size of the
datasets. Here we present a Bayesian- based derivation of multi-view
deconvolution that drastically improves the convergence time and provide a fast
implementation utilizing graphics hardware.

These are the proceedings of the 13th Workshop on Algorithms in
Bioinformatics, WABI2013, which was held September 2-4 2013 in Sophia
Antipolis, France. All manuscripts were peer reviewed by the WABI2013 program
committee and external reviewers.

We apply a force-directed spring embedding graph layout approach to
electronic health records in order to visualise population-wide associations
between human disorders as presented in an individual biological organism. The
introduced visualisation is implemented on the basis of the Google maps
platform and can be found at http://disease-map.net . We argue that the
suggested method of visualisation can both validate already known specifics of
associations between disorders and identify novel never noticed association
patterns.

Numerous metrics of heart rate variability (HRV) have been described,
analyzed, and compared in the literature. However, they rarely cover the actual
metrics used in a class of HRV data acquisition devices - those designed
primarily to produce real-time metrics. This paper characterizes a class of
metrics that we term dynamic metrics. We also report the results of a pilot
study which compares one such dynamic metric, based on photoplethysmographic
data using a moving sampling window set to the length of an estimated breath
cycle (EBC), with established HRV metrics. The results show high correlation
coefficients between the dynamic EBC metrics and the established static SDNN
metric (standard deviation of Normal-to-Normal) based on electrocardiography.
These results demonstrate the usefulness of data acquisition devices designed
for real-time metrics.

We study the statistical properties of melanoma cell colonies grown in vitro
by analyzing the results of crystal violet assays at different concentrations
of initial plated cells and for different growth times. The distribution of
colony sizes is described well by a continuous time branching process. To
characterize the shape fluctuations of the colonies, we compute the
distribution of eccentricities. The experimental results are compared with
numerical results for models of random division of elastic cells, showing that
experimental results are best reproduced by restricting cell division to the
outer rim of the colony. Our results serve to illustrate the wealth of
information that can be extracted by a standard experimental method such as the
crystal violet assay.

RNA-seq has become a de facto standard for measuring gene expression.
Traditionally, RNA-seq experiments are mathematically averaged -- they sequence
the mRNA of individuals from different treatment groups, hoping to correlate
phenotype with differences in arithmetic read count averages at shared loci of
interest. Alternatively, the tissue from the same individuals may be pooled
prior to sequencing in what we refer to as a biologically averaged design. As
mathematical averaging sequences all individuals it controls for both
biological and technical variation; however, is the statistical resolution
gained always worth the additional cost? To compare biological and mathematical
averaging, we examined theoretical and empirical estimates of statistical
efficiency and relative cost efficiency. Though less efficient at a fixed
sample size, we found that biological averaging can be more cost efficient than
mathematical averaging. With this motivation, we developed a differential
expression classifier, ICRBC, that can detect alternatively expressed genes
between biologically averaged samples. In simulation studies, we found that
biological averaging and subsequent analysis with our classifier performed
comparably to existing methods, such as ASC, edgeR, and DESeq, especially when
individuals were pooled evenly and less than 20% of the regulome was expected
to be differentially regulated. In two technically distinct mouse datasets and
one plant dataset, we found that our method was over 87% concordant with edgeR
for the 100 most significant features. We therefore conclude biological
averaging may sufficiently control biological variation to a level that
differences in gene expression may be detectable. In such situations, ICRBC can
enable reliable exploratory analysis at a fraction of the cost, especially when
interest lies in the most differentially expressed loci.

Identifying reliable domain-domain interactions (DDIs) will increase our
ability to predict novel protein-protein interactions (PPIs), to unravel
interactions in protein complexes, and thus gain more information about the
function and behavior of genes. One of the challenges of identifying reliable
DDIs is domain promiscuity. Promiscuous domains are domains that can occur in
many domain architectures and are therefore found in many proteins. This
becomes a problem for a method where the score of a domain-pair is the ratio
between observed and expected frequencies because the PPI network is sparse. As
such, many protein-pairs will be non-interacting and domain-pairs with
promiscuous domains will be penalized. This domain promiscuity challenge to the
problem of inferring reliable DDIs from PPIs has been recognized, and a number
of work-arounds have been proposed. In this paper, we report an application of
Formal Concept Analysis (FCA) to this problem. We find that the relationship
between formal concepts provide a natural way for rare domains to elevate the
rank of promiscuous domains, and enrich highly ranked domain-pairs with
reliable DDIs. This piggy-backing of promiscuous domains onto rare domains is
possible due to the domain architecture of proteins which mixes promiscuous
with rare domains.

Weak-signal detection and single-particle selection from low-contrast
micrographs of frozen hydrated biomolecules by cryo-electron microscopy
(cryo-EM) presents a practical challenge. Cryo-EM image contrast degrades as
the size of biomolecules of structural interest decreases. When the image
contrast falls into a range where the location or presence of single particles
becomes ambiguous, a need arises for objective computational approaches to
detect weak signal and to select and verify particles from these low-contrast
micrographs. Here we propose an objective validation scheme for low-contrast
particle selection using a combination of two different target functions. In an
implementation of this dual-target function (DTF) validation, a first target
function of fast local correlation was used to select particles through
template matching, followed by signal validation through a second target
function of maximum likelihood. By a systematic study of simulated data, we
found that such an implementation of DTF validation is capable of selecting and
verifying particles from cryo-EM micrographs with a signal-to-noise ratio as
low as 0.002. Importantly, we demonstrated that DTF validation can robustly
evade over-fitting or reference bias from the particle-picking template,
allowing true signal to emerge from amidst heavy noise in an objective fashion.
The DTF approach allows efficient assembly of a large number of single-particle
cryo-EM images of smaller biomolecules or specimens containing
contrast-degrading agents like detergents in a semi-automatic manner.

Plant or soil water status are required in many scientific fields to
understand plant responses to drought. Because the transcriptomic response to
abiotic conditions, such as water deficit, reflects plant water status, genomic
tools could be used to develop a new type of molecular biomarker. Using the
sunflower (Helianthus annuus L.) as a model species to study the transcriptomic
response to water deficit both in greenhouse and field conditions, we
specifically identified three genes that showed an expression pattern highly
correlated to plant water status as estimated by the pre-dawn leaf water
potential, fraction of transpirable soil water, soil water content or fraction
of total soil water in controlled conditions. We developed a generalized linear
model to estimate these classical water status indicators from the expression
levels of the three selected genes under controlled conditions. This estimation
was independent of the four tested genotypes and the stage (pre- or
post-flowering) of the plant. We further validated this gene expression
biomarker under field conditions for four genotypes in three different trials,
over a large range of water status, and we were able to correct their
expression values for a large diurnal sampling period.

Extrinsic environmental factors influence the distribution and population
dynamics of many organisms, including insects that are of concern for human
health and agriculture. This is particularly true for vector-borne infectious
diseases, like malaria, which is a major source of morbidity and mortality in
humans. Understanding the mechanistic links between environment and population
processes for these diseases is key to predicting the consequences of climate
change on transmission and for developing effective interventions. An important
measure of the intensity of disease transmission is the reproductive number
$R_0$. However, understanding the mechanisms linking $R_0$ and temperature, an
environmental factor driving disease risk, can be challenging because the data
available for parameterization are often poor. To address this we show how a
Bayesian approach can help identify critical uncertainties in components of
$R_0$ and how this uncertainty is propagated into the estimate of $R_0$. Most
notably, we find that different parameters dominate the uncertainty at
different temperature regimes: bite rate from 15-25$^\circ$ C; fecundity across
all temperatures, but especially $\sim$25-32$^\circ$ C; mortality from
20-30$^\circ$ C; parasite development rate at $\sim$15-16$^\circ$C and again at
$\sim$33-35$^\circ$C. Focusing empirical studies on these parameters and
corresponding temperature ranges would be the most efficient way to improve
estimates of $R_0$. While we focus on malaria, our methods apply to improving
process-based models more generally, including epidemiological, physiological
niche, and species distribution models.

We developed a novel method based on the Fourier analysis of protein
molecular surfaces to speed up the analysis of the vast structural data
generated in the post-genomic era. This method computes the power spectrum of
surfaces of the molecular electrostatic potential, whose three-dimensional
coordinates have been either experimentally or theoretically determined. Thus
we achieve a reduction of the initial three-dimensional information on the
molecular surface to the one-dimensional information on pairs of points at a
fixed scale apart. Consequently, the similarity search in our method is
computationally less demanding and significantly faster than shape comparison
methods. As proof of principle, we applied our method to a training set of
viral proteins that are involved in major diseases such as Hepatitis C, Dengue
fever, Yellow fever, Bovine viral diarrhea and West Nile fever. The training
set contains proteins of four different protein families, as well as a
mammalian representative enzyme. We found that the power spectrum successfully
assigns a unique signature to each protein included in our training set, thus
providing a direct probe of functional similarity among proteins. The results
agree with established biological data from conventional structural
biochemistry analyses.

Cell functional diversity is a significant determinant on how biological
processes unfold. Most accounts of diversity involve a search for sequence or
expression differences. Perhaps there are more subtle mechanisms at work. Using
the metaphor of information processing and decision-making might provide a
clearer view of these subtleties. Understanding adaptive and transformative
processes (such as cellular reprogramming) as a series of simple decisions
allows us to use a technique called cellular signal detection theory (cellular
SDT) to detect potential bias in mechanisms that favor one outcome over
another. We can apply method of detecting cellular reprogramming bias to
cellular reprogramming and other complex molecular processes. To demonstrate
scope of this method, we will critically examine differences between cell
phenotypes reprogrammed to muscle fiber and neuron phenotypes. In cases where
the signature of phenotypic bias is cryptic, signatures of genomic bias
(pre-existing and induced) may provide an alternative. The examination of these
alternates will be explored using data from a series of fibroblast cell lines
before cellular reprogramming (pre-existing) and differences between fractions
of cellular RNA for individual genes after drug treatment (induced). In
conclusion, the usefulness and limitations of this method and associated
analogies will be discussed.

The measurement of information flows within moving animal groups has recently
been a topic of considerable interest, and it has become clear that the
individual(s) that drive collective movement may change over time, and that
such individuals may not necessarily always lead from the front. However,
methods to quantify the influence of specific individuals on the behaviour of
other group members and the direction of information flow in moving group, are
lacking on the level of empirical studies and theoretical models. Using high
spatio-temporal resolution GPS trajectories of foraging meerkats, Suricata
suricatta, we provide an information-theoretic framework to identify dynamical
coupling between animals independent of their relative spatial positions. Based
on this identification, we then compare designations of individuals as either
drivers or responders against designations provided by the relative spatial
position. We find that not only does coupling occur both from the frontal to
the trailing individuals and vice versa, but also that the coupling direction
is a non-linear function of the relative position. This provides evidence for
(i) intermittent fluctuation of the coupling strength and (ii) alternation in
the coupling direction within foraging meerkat pairs. The framework we
introduce allows for a detailed description of the dynamical patterns of mutual
influence between all pairs of individuals within moving animal groups. We
argue that applying an information-theoretic perspective to the study of
coordinated phenomena in animal groups will eventually help to understand cause
and effect in collective behaviour.

Current efforts in the biomedical sciences and related interdisciplinary
fields are focused on gaining a molecular understanding of health and disease,
which is a problem of daunting complexity that spans many orders of magnitude
in characteristic length scales, from small molecules that regulate cell
function to cell ensembles that form tissues and organs working together as an
organism. In order to uncover the molecular nature of the emergent properties
of a cell, it is essential to measure multiple cell components simultaneously
in the same cell. In turn, cell heterogeneity requires multiple cells to be
measured in order to understand health and disease in the organism. This review
summarizes current efforts towards a data-driven framework that leverages
single-cell technologies to build robust signatures of healthy and diseased
phenotypes. While some approaches focus on multicolor flow cytometry data and
other methods are designed to analyze high-content image-based screens, we
emphasize the so-called Supercell/SVM paradigm (recently developed by the
authors of this review and collaborators) as a unified framework that captures
mesoscopic-scale emergence to build reliable phenotypes. Beyond their specific
contributions to basic and translational biomedical research, these efforts
illustrate, from a larger perspective, the powerful synergy that might be
achieved from bringing together methods and ideas from statistical physics,
data mining, and mathematics to solve the most pressing problems currently
facing the life sciences.

South Africa has more people infected with HIV but, by providing access to
anti-retroviral therapy (ART), has kept more people alive than any other
country. The effectiveness, availability and affordability of potent
anti-retroviral therapy (ART) make it possible to contemplate ending the
epidemic of HIV/AIDS. We consider what would have happened without ART, the
impact of the current roll-out of ART, what might be possible if early
treatment becomes available to all, and what could have happened if ART had
been provided much earlier in the epidemic. In 2013 the provision of ART has
reduced the prevalence of HIV from an estimated 15% to 9% among adults not on
ART, the annual incidence from 2% to 0.9%, and the AIDS related deaths from
0.9% to 0.3% p.a. saving 1.5 million lives and USD727M. Regular testing and
universal access to ART could reduce the prevalence among adults not on ART in
2023 to 0.06%, annual incidence to 0.05%, and eliminate AIDS deaths. Cumulative
costs between 2013 ands 2023 would increase by USD692M only 4% of the total
cost of USD17Bn. If a universal testing and early treatment had started in 1998
the prevalence of HIV among adults not on ART in 2013 would have fallen to
0.03%, annual incidence to 0.03%, and saved 2.5 million lives. The cost up to
2013 would have increased by USD18Bn but this would have been cost effective at
US$7,200 per life saved. Future surveys of HIV among women attending ante-natal
clinics should include testing women for the presence of anti-retroviral drugs,
measuring their viral loads, and using appropriate assays for estimating HIV
incidence. These data would make it possible to develop better and more
reliable estimates of the current state of the epidemic, the success of the
current ART programme, levels of viral load suppression for those on ART and
the incidence of infection.

A number of biological systems can be modeled by Markov chains. Recently,
there has been an increasing concern about when biological systems modeled by
Markov chains will perform a dynamic phenomenon called overshoot. In this
article, we found that the steady-state behavior of the system will have a
great effect on the occurrence of overshoot. We confirmed that overshoot in
general cannot occur in systems which will finally approach an equilibrium
steady state. We further classified overshoot into two types, named as simple
overshoot and oscillating overshoot. We showed that except for extreme cases,
oscillating overshoot will occur if the system is far from equilibrium. All
these results clearly show that overshoot is a nonequilibrium dynamic
phenomenon with energy consumption. In addition, the main result in this
article is validated with real experimental data.

We address the problem of using nonlinear models to design experiments to
characterize the dynamics of cellular processes by using the approach of the
Maximally Informative Next Experiment (MINE), which was introduced in [W. Dong,
et al. Systems biology of the clock in neurospora crassa. {\em PLoS ONE}, page
e3105, 2008] and independently in [M. M. Donahue, et al. Experiment design
through dynamical characterization of non-linear systems biology models
utilising sparse grids. {\em IET System Biology}, 4:249--262, 2010]. In this
approach, existing data is used to define a probability distribution on the
parameters; the next measurement point is the one that yields the largest model
output variance with this distribution. Building upon this approach, we
introduce the Expected Dynamics Estimator (EDE), which is the expected value
using this distribution of the output as a function of time. We prove the
consistency of this estimator (uniform convergence to true dynamics) even when
the chosen experiments cluster in a finite set of points. We extend this proof
of consistency to various practical assumptions on noisy data and moderate
levels of model mismatch. Through the derivation and proof, we develop a
relaxed version of MINE that is more computationally tractable and robust than
the original formulation. The results are illustrated with numerical examples
on two nonlinear ordinary differential equation models of biomolecular and
cellular processes.

The discovery of peptides having high biological activity is very challenging
mainly because there is an enormous diversity of compounds and only a minority
have the desired properties. To lower cost and reduce the time to obtain
promising compounds, machine learning approaches can greatly assist in the
process and even replace expensive laboratory experiments by learning a
predictor with existing data. Unfortunately, selecting ligands having the
greatest predicted bioactivity requires a prohibitive amount of computational
time. For this combinatorial problem, heuristics and stochastic optimization
methods are not guaranteed to find adequate compounds.
  We propose an efficient algorithm based on De Bruijn graphs, guaranteed to
find the peptides of maximal predicted bioactivity. We demonstrate how this
algorithm can be part of an iterative combinatorial chemistry procedure to
speed up the discovery and the validation of peptide leads. Moreover, the
proposed approach does not require the use of known ligands for the target
protein since it can leverage recent multi-target machine learning predictors
where ligands for similar targets can serve as initial training data. Finally,
we validated the proposed approach in vitro with the discovery of new cationic
anti-microbial peptides.
  Source code is freely available at
http://graal.ift.ulaval.ca/peptide-design/.

Optical mapping by direct visualization of individual DNA molecules,
stretched in nanochannels with sequence-specific fluorescent labeling,
represents a promising tool for disease diagnostics and genomics. An important
challenge for this technique is thermal motion of the DNA as it undergoes
imaging; this blurs fluorescent patterns along the DNA and results in
information loss. Correcting for this effect (a process referred to as
kymograph alignment) is a common preprocessing step in nanochannel-based
optical mapping workflows, and we present here a highly efficient algorithm to
accomplish this via pattern recognition. We compare our method with the one
previous approach, and we find that our method is orders of magnitude faster
while producing data of similar quality. We demonstrate proof of principle of
our approach on experimental data consisting of melt mapped bacteriophage DNA.

Portable low-cost sensors and sensing systems for the identification and
quantitative measurement of bacteria in field water are critical in preventing
drinking water from being contaminated by bacteria. In this article, we
reported the design, fabrication and testing of a low-cost, miniaturized and
sensitive bacteria sensor based on electrical impedance spectroscopy method
using a smartphone as the platform. Our design of microfluidics enabled the
pre-concentration of the bacteria which lowered the detection limit to 10
bacterial cells per milliliter. We envision that our demonstrated
smartphone-based sensing system will realize highly-sensitive and rapid
in-field quantification of multiple species of bacteria and pathogens.

The ultimate target of proteomics identification is to identify and quantify
the protein in the organism. Mass spectrometry (MS) based on label-free protein
quantitation has mainly focused on analysis of peptide spectral counts and ion
peak heights. Using several observed peptides (proteotypic) can identify the
origin protein. However, each peptide's possibility to be detected was severely
influenced by the peptide physicochemical properties, which confounded the
results of MS accounting. Using about a million peptide identification
generated by four different kinds of proteomic platforms, we successfully
identified >16,000 proteotypic peptides. We used machine learning
classification to derive peptide detection probabilities that are used to
predict the number of trypic peptides to be observed, which can serve to
estimate the absolutely abundance of protein with highly accuracy. We used the
data of peptides (provides by CAS lab) to derive the best model from different
kinds of methods. We first employed SVM and Random Forest classifier to
identify the proteotypic and unobserved peptides, and then searched the best
parameter for better prediction results. Considering the excellent performance
of our model, we can calculate the absolutely estimation of protein abundance.

A nucleotide sequence 35 base pairs long can take
1,180,591,620,717,411,303,424 possible values. An example of systems biology
datasets, protein binding microarrays, contain activity data from about 40000
such sequences. The discrepancy between the number of possible configurations
and the available activities is enormous. Thus, albeit that systems biology
datasets are large in absolute terms, they oftentimes require methods developed
for rare events due to the combinatorial increase in the number of possible
configurations of biological systems. A plethora of techniques for handling
large datasets, such as Empirical Bayes, or rare events, such as importance
sampling, have been developed in the literature, but these cannot always be
simultaneously utilized. Here we introduce a principled approach to Empirical
Bayes based on importance sampling, information theory, and theoretical physics
in the general context of sequence phenotype model induction. We present the
analytical calculations that underlie our approach. We demonstrate the
computational efficiency of the approach on concrete examples, and demonstrate
its efficacy by applying the theory to publicly available protein binding
microarray transcription factor datasets and to data on synthetic
cAMP-regulated enhancer sequences. As further demonstrations, we find
transcription factor binding motifs, predict the activity of new sequences and
extract the locations of transcription factor binding sites. In summary, we
present a novel method that is efficient (requiring minimal computational time
and reasonable amounts of memory), has high predictive power that is comparable
with that of models with hundreds of parameters, and has a limited number of
optimized parameters, proportional to the sequence length.

The goal of animal movement analysis is to understand how organisms explore
and exploit the complex and varying environment. Animals usually exhibit varied
and complicated movements, from apparently deterministic behaviors to highly
random ones. This is critical for assessing movement efficiency and strategies
that are used to quantify and analyze movement trajectories. Here we introduce
a tortuosity entropy (TorEn) based on comparison of parameters, e.g. heading,
bearing, speed, of consecutive points in movement trajectory, which is a simple
measure for quantifying the behavioral change in animal movement data in a fine
scale. In our approach, the differences between pairwise successive track
points are transformed inot symbolic sequences, then we map these symbols into
a group of pattern vectors and calculate the information entropy of pattern
vector. Tortuosity entropy can be easily applied to arbitrary real-world
data-deterministic or stochastic, stationary or non-stationary. We test the
algorithm on both simulated trajectories and real trajectories and show that
both mixed segments in synthetic data and different phases in real movement
data are identified accurately. The results show that the algorithm is
applicable to various situations, indicating that our approach is a promising
tool to reveal the behavioral pattern in movement data.

Computational discovery of microRNAs (miRNA) is based on pre-determined sets
of features from miRNA precursors (pre-miRNA). These feature sets used by
current tools for pre-miRNA recognition differ in construction and dimension.
Some feature sets are composed of sequence-structure patterns commonly found in
pre-miRNAs, while others are a combination of more sophisticated RNA features.
Current tools achieve similar predictive performance even though the feature
sets used - and their computational cost - differ widely. In this work, we
analyze the discriminant power of seven feature sets, which are used in six
pre-miRNA prediction tools. The analysis is based on the classification
performance achieved with these feature sets for the training algorithms used
in these tools. We also evaluate feature discrimination through the F-score and
feature importance in the induction of random forests. More diverse feature
sets produce classifiers with significantly higher classification performance
compared to feature sets composed only of sequence-structure patterns. However,
small or non-significant differences were found among the estimated
classification performances of classifiers induced using sets with
diversification of features, despite the wide differences in their dimension.
Based on these results, we applied a feature selection method to reduce the
computational cost of computing the feature set, while maintaining discriminant
power. We obtained a lower-dimensional feature set, which achieved a
sensitivity of 90% and a specificity of 95%. Our feature set achieves a
sensitivity and specificity within 0.1% of the maximal values obtained with any
feature set while it is 34x faster to compute. Even compared to another feature
set, which is the computationally least expensive feature set of those from the
literature which perform within 0.1% of the maximal values, it is 34x faster to
compute.

Antibody-functionalized silicon nanowire field-effect transistors have been
shown to exhibit excellent analyte detection sensitivity enabling sensing of
analyte concentrations at levels not readily accessible by other methods. One
example where accurate measurement of small concentrations is necessary is
detection of serum biomarkers, such as the recently discovered tumor necrosis
factor receptor superfamily member TROY (TNFRSF19), which may serve as a
biomarker for melanoma. TROY is normally only present in brain but it is
aberrantly expressed in primary and metastatic melanoma cells and shed into the
surrounding environment. In this study, we show the detection of different
concentrations of TROY in buffer solution using top-down fabricated silicon
nanowires. We demonstrate the selectivity of our sensors by comparing the
signal with that obtained from bovine serum albumin in buffer solution. Both
the signal size and the reaction kinetics serve to distinguish the two signals.
Using a fast-mixing two-compartment reaction model, we are able to extract the
association and dissociation rate constants for the reaction of TROY with the
antibody immobilized on the sensor surface.

Estimating the required dose in radiotherapy is of crucial importance since
the administrated dose should be sufficient to eradicate the tumor and at the
same time should inflict minimal damage on normal cells. The probability that a
given dose and schedule of ionizing radiation eradicates all the tumor cells in
a given tissue is called the tumor control probability (TCP), and is often used
to compare various treatment strategies used in radiation therapy. In this
paper, we aim to investigate the effects of including cell-cycle phase on the
TCP by analyzing a stochastic model of a tumor comprised of actively dividing
cells and quiescent cells with different radiation sensitivities. We derive an
exact phase-diagram for the steady-state TCP of the model and show that at
high, clinically-relevant doses of radiation, the distinction between active
and quiescent tumor cells (i.e. accounting for cell-cycle effects) becomes of
negligible importance in terms of its effect on the TCP curve. However, for
very low doses of radiation, these proportions become significant determinants
of the TCP. Moreover, we use a novel numerical approach based on the method of
characteristics for partial differential equations, validated by the Gillespie
algorithm, to compute the TCP as a function of time. We observe that our
results differ from the results in the literature using similar existing
models, even though similar parameters values are used, and the reasons for
this are discussed.

Algorithms that detect covariance between pairs of columns in multiple
sequence alignments are commonly employed to predict functionally important
residues and structural contacts. However, the assumption that co-variance only
occurs between individual residues in the protein is more driven by
computational convenience rather than fundamental protein architecture. Here we
develop a novel algorithm that defines a covariance score across two groups of
columns where each group represents a stretch of contiguous columns in the
alignment. We define a test set that consists of secondary structure elements
({\alpha}-helixes and {\beta}-strands) across more than 1,100 PFAM families.
Using these alignments to predict segments that are physically close in
structure, we show that our method substantially out-performs approaches that
aggregate the results of algorithms that operate on individual column pairs.
Our approach demonstrates that considering units of proteins beyond pairs of
columns can improve the power and utility of covariance algorithms.

Silicon nanochannel field effect transistor (FET) biosensors are one of the
most promising technologies in the development of highly sensitive and
label-free analyte detection for cancer diagnostics. With their exceptional
electrical properties and small dimensions, silicon nanochannels are ideally
suited for extraordinarily high sensitivity. In fact, the high
surface-to-volume ratios of these systems make single molecule detection
possible. Further, FET biosensors offer the benefits of high speed, low cost,
and high yield manufacturing, without sacrificing the sensitivity typical for
traditional optical methods in diagnostics. Top down manufacturing methods
leverage advantages in Complementary Metal Oxide Semiconductor (CMOS)
technologies, making richly multiplexed sensor arrays a reality. Here, we
discuss the fabrication and use of silicon nanochannel FET devices as
biosensors for breast cancer diagnosis and monitoring.

Complex phenotypic differences among different acute leukemias cannot be
fully captured by analyzing the expression levels of one single molecule, such
as a miR, at a time, but requires systematic analysis of large sets of miRs.
While a popular approach for analysis of such datasets is principal component
analysis (PCA), this method is not designed to optimally discriminate different
phenotypes. Moreover, PCA and other low-dimensional representation methods
yield linear or non-linear combinations of all measured miRs. Global human miR
expression was measured in AML, B-ALL, and T-ALL cell lines and patient RNA
samples. By systematically applying support vector machines to all measured
miRs taken in dyad and triad groups, we built miR networks using cell line data
and validated our findings with primary patient samples. All the coordinately
transcribed members of the miR-23a cluster (which includes also miR-24 and
miR-27a), known to function as tumor suppressors of acute leukemias, appeared
in the AML, B-ALL and T-ALL centric networks. Subsequent qRT-PCR analysis
showed that the most connected miR in the B-ALL-centric network, miR-708, is
highly and specifically expressed in B-ALLs, suggesting that miR-708 might
serve as a biomarker for B-ALL. This approach is systematic, quantitative,
scalable, and unbiased. Rather than a single signature, our approach yields a
network of signatures reflecting the redundant nature of biological signaling
pathways. The network representation allows for visual analysis of all
signatures by an expert and for future integration of additional information.
Furthermore, each signature involves only small sets of miRs, such as dyads and
triads, which are well suited for in depth validation through laboratory
experiments such as loss- and gain-of-function assays designed to drive changes
in leukemia cell survival, proliferation and differentiation.

Glioblastoma multiforme, the most frequent type of primary brain tumor, is a
rapidly evolving and spatially heterogeneous high-grade astrocytoma that
presents areas of necrosis, hypercellularity and microvascular hyperplasia. The
aberrant vasculature leads to hypoxic areas and results in an increase of the
oxidative stress selecting for more invasive tumor cell phenotypes. In our
study we assay in silico different therapeutic approaches which combine
antithrombotics, antioxidants and standard radiotherapy. To do so, we have
developed a biocomputational model of glioblastoma multiforme that incorporates
the spatio-temporal interplay among two glioma cell phenotypes corresponding to
oxygenated and hypoxic cells, a necrotic core and the local vasculature whose
response evolves with tumor progression. Our numerical simulations predict that
suitable combinations of antithrombotics and antioxidants may diminish, in a
synergetic way, oxidative stress and the subsequent hypoxic response. This
novel therapeutical strategy, with potentially low or no toxicity, might reduce
tumor invasion and further sensitize glioblastoma multiforme to conventional
radiotherapy or other cytotoxic agents, hopefully increasing median patient
overall survival time.

Extended systems governed by partial differential equations can, under
suitable conditions, be approximated by means of sets of ordinary differential
equations for global quantities capturing the essential features of the systems
dynamics. Here we obtain a small number of effective equations describing the
dynamics of single-front and localized solutions of Fisher-Kolmogorov type
equations. These solutions are parametrized by means of a minimal set of
time-dependent quantities for which ordinary differential equations ruling
their dynamics are found. A comparison of the finite dimensional equations and
the dynamics of the full partial differential equation is made showing a very
good quantitative agreement with the dynamics of the partial differential
equation. We also discuss some implications of our findings for the
understanding of the growth progression of certain types of primary brain
tumors and discuss possible extensions of our results to related equations
arising in different modelling scenarios.

Low grade gliomas (LGGs) are a group of primary brain tumors usually
encountered in young patient populations. These tumors represent a difficult
challenge because many patients survive a decade or more and may be at a higher
risk for treatment-related complications. Specifically, radiation therapy is
known to have a relevant effect on survival but in many cases it can be
deferred to avoid side effects while maintaining its beneficial effect.
However, a subset of low-grade gliomas manifests more aggressive clinical
behavior and requires earlier intervention. Moreover, the effectiveness of
radiotherapy depends on the tumor characteristics. Recently Pallud et al.,
[Neuro-oncology, 14(4):1-10, 2012], studied patients with LGGs treated with
radiation therapy as a first line therapy. and found the counterintuitive
result that tumors with a fast response to the therapy had a worse prognosis
than those responding late. In this paper we construct a mathematical model
describing the basic facts of glioma progression and response to radiotherapy.
The model provides also an explanation to the observations of Pallud et al.
Using the model we propose radiation fractionation schemes that might be
therapeutically useful by helping to evaluate the tumor malignancy while at the
same time reducing the toxicity associated to the treatment.

The Multistage Differential Transform Method (MDTM) is employed to solve the
model for HIV infection of CD4+T cells. Comparing the numerical results to
those obtained by the classical fourth order Runge-Kutta method showed the
preciseness and efficacy of the multistep differential transform method. The
study shows that the method is a powerful and promising tool for solving
coupled systems of differential equations.

Markovian population models are suitable abstractions to describe well-mixed
interacting particle systems in situation where stochastic fluctuations are
significant due to the involvement of low copy particles. In molecular biology,
measurements on the single-cell level attest to this stochasticity and one is
tempted to interpret such measurements across an isogenic cell population as
different sample paths of one and the same Markov model. Over recent years
evidence built up against this interpretation due to the presence of
cell-to-cell variability stemming from factors other than intrinsic
fluctuations. To account for this extrinsic variability, Markovian models in
random environments need to be considered and a key emerging question is how to
perform inference for such models. We model extrinsic variability by a random
parametrization of all propensity functions. To detect which of those
propensities have significant variability, we lay out a sparse learning
procedure captured by a hierarchical Bayesian model whose evidence function is
iteratively maximized using a variational Bayesian expectation-maximization
algorithm.

This pilot study explored physiological responses to playing and listening to
the Native American flute. Autonomic, electroencephalographic (EEG), and heart
rate variability (HRV) metrics were recorded while participants (N = 15) played
flutes and listened to several styles of music. Flute playing was accompanied
by an 84% increase in HRV (p < .001). EEG theta (4-8 Hz) activity increased
while playing flutes (p = .007) and alpha (8-12 Hz) increased while playing
lower-pitched flutes (p = .009). Increase in alpha from baseline to the flute
playing conditions strongly correlated with experience playing Native American
flutes (r = +.700). Wide-band beta (12-25 Hz) decreased from the silence
conditions when listening to solo Native American flute music (p = .013). The
findings of increased HRV, increasing slow-wave rhythms, and decreased beta
support the hypothesis that Native American flutes, particularly those with
lower pitches, may have a role in music therapy contexts. We conclude that the
Native American flute may merit a more prominent role in music therapy and that
a study of the effects of flute playing on clinical conditions, such as
post-traumatic stress disorder (PTSD), asthma, chronic obstructive pulmonary
disease (COPD), hypertension, anxiety, and major depressive disorder, is
warranted.

This paper aims at predicting the next maxima values of the state variables
of the seasonal SEIR epidemic model and their in-between time intervals.
Lorenz's method of analogues is applied on the attractor formed by the maxima
of the corresponding state variables. It is found that both quantities are
characterized by a high degree of predictability in the case of the chaotic
regime of the parameter space.

1. Complex systems of moving and interacting objects are ubiquitous in the
natural and social sciences. Predicting their behavior often requires models
that mimic these systems with sufficient accuracy, while accounting for their
inherent stochasticity. Though tools exist to determine which of a set of
candidate models is best relative to the others, there is currently no generic
goodness-of-fit framework for testing how close the best model is to the real
complex stochastic system.
  2. We propose such a framework, using a novel application of the Earth
mover's distance, also known as the Wasserstein metric. It is applicable to any
stochastic process where the probability of the model's state at time $t$ is a
function of the state at previous times. It generalizes the concept of a
residual, often used to analyze 1D summary statistics, to situations where the
complexity of the underlying model's probability distribution makes standard
residual analysis too imprecise for practical use.
  3. We give a scheme for testing the hypothesis that a model is an accurate
description of a data set. We demonstrate the tractability and usefulness of
our approach by application to animal movement models in complex, heterogeneous
environments. We detail methods for visualizing results and extracting a
variety of information on a given model's quality, such as whether there is any
inherent bias in the model, or in which situations it is most accurate. We
demonstrate our techniques by application to data on multi-species flocks of
insectivore birds in the Amazon rainforest.
  4. This work provides a usable toolkit to assess the quality of generic
movement models of complex systems, in an absolute rather than a relative
sense.

A hypercomplex representation of DNA is proposed to facilitate comparing DNA
sequences with fuzzy composition. With the hypercomplex number representation,
the conventional sequence analysis method, such as, dot matrix analysis,
dynamic programming, and cross-correlation method have been extended and
improved to align DNA sequences with fuzzy composition. The hypercomplex dot
matrix analysis can provide more control over the degree of alignment desired.
A new scoring system has been proposed to accommodate the hypercomplex number
representation of DNA and integrated with dynamic programming alignment method.
By using hypercomplex cross-correlation, the match and mismatch alignment
information between two aligned DNA sequences are separately stored in the
resultant real part and imaginary parts respectively. The mismatch alignment
information is very useful to refine consensus sequence based motif scanning.

Background: The identification of transcription factor binding sites (TFBSs)
and cis-regulatory modules (CRMs) is a crucial step in studying gene
expression, but the computational method attempting to distinguish CRMs from
NCNRs still remains a challenging problem due to the limited knowledge of
specific interactions involved. Methods: The statistical properties of
cis-regulatory modules (CRMs) are explored by estimating the similar-word set
distribution with overrepresentation (Z-score). It is observed that CRMs tend
to have a thin-tail Z-score distribution. A new statistical thin-tail test with
two thinness coefficients is proposed to distinguish CRMs from non-coding
non-regulatory regions (NCNRs). Results: As compared with the existing
fluffy-tail test, the first thinness coefficient is designed to reduce
computational time, making the novel thin-tail test very suitable for long
sequences and large database analysis in the post-genome time and the second
one to improve the separation accuracy between CRMs and NCNRs. These two
thinness coefficients may serve as valuable filtering indexes to predict CRMs
experimentally. Conclusions: The novel thin-tail test provides an efficient and
effective means for distinguishing CRMs from NCNRs based on the specific
statistical properties of CRMs and can guide future experiments aimed at
finding new CRMs in the post-genome time.

Familiarity with a simulation platform can seduce modellers into accepting
untested assumptions for convenience of implementation. These assumptions may
have consequences greater than commonly suspected, and it is important that
modellers remain mindful of assumptions and remain diligent with sensitivity
testing. Familiarity with a technique can lead to complacency, and alternative
approaches and software can reveal untested assumptions. Visual modelling
environments based on system dynamics may help to make critical assumptions
more evident by offering an accessible visual overview and empowering a focus
on representational rather than computational efficiency. This capacity is
illustrated using a cohort-based forest growth model developed for mixed
species forest. The alternative model implementation revealed that untested
assumptions in the original model could have substantial influence on simulated
outcomes. An important implication is that modellers should remain conscious of
all assumptions, consider alternative implementations that reveal assumptions
more clearly, and conduct sensitivity tests to inform decisions.

After more than a century of research the typical growth pattern of a tree
was thought to be fairly well understood. Following germination height growth
accelerates for some time, then increment peaks and the added height each year
becomes less and less. The cross sectional area (basal area) of the tree
follows a similar pattern, but the maximum basal area increment occurs at some
time after the maximum height increment. An increase in basal area in a tall
tree will add more volume to the stem than the same increase in a short tree,
so the increment in stem volume (or mass) peaks very late. Stephenson et al.
challenge this paradigm, and suggest that mass increment increases
continuously. Their analysis methods however are a textbook example of the
ecological fallacy, and their conclusions therefore unsupported.

We have, using spin-echo nuclear magnetic resonance spectroscopy, measured
the relaxation times and diffusion coefficient of water protons in primary
mammary adenocarcinomas of mice. In our biological model, three morphological
stages were defined: (a) mammary gland tissue from pregnant mice, (b)
preneoplastic nodules, and (c) neoplastic tissue. It was found that neoplastic
tissues could be distinguished from normal and prenoeplastic tissue. Spin-spin
and spin-lattice relaxation times and the diffusion coefficient of water
protons are increased in the neoplastic tissue relative to mammary gland tissue
from pregnant mice and preneoplastic nodule tissue. These results suggested
that one can use a pulsed NMR method to detect and even predict breast cancer.

Biologically inspired pressure actuated cellular structures can alter their
shape through pressure variations. Previous work introduced a computational
framework for pressure actuated cellular structures which was limited to two
cell rows and central cell corner hinges. This article rigorously extends these
results by taking into account an arbitrary number of cell rows, a more
complicated cell kinematics that includes hinge eccentricities and varying side
lengths as well as rotational and axial cell side springs. The nonlinear
effects of arbitrary cell deformations are fully considered. Furthermore, the
optimization is considerably improved by using a second-order approach. The
presented framework enables the design of compliant pressure actuated cellular
structures that can change their form from one shape to another within a set of
one-dimensional C1 continuous functions.

Motivation: Although principal component analysis (PCA) is widely used for
the dimensional reduction of biomedical data, interpretation of PCA results
remains daunting. Most existing methods attempt to explain each principal
component (PC) in terms of a small number of variables by generating
approximate PCs with few non-zero loadings. Although useful when just a few
variables dominate the population PCs, these methods are often inadequate for
characterizing the PCs of high-dimensional genomic data. For genomic data,
reproducible and biologically meaningful PC interpretation requires methods
based on the combined signal of functionally related sets of genes. While gene
set testing methods have been widely used in supervised settings to quantify
the association of groups of genes with clinical outcomes, these methods have
seen only limited application for testing the enrichment of gene sets relative
to sample PCs. Results: We describe a novel approach, principal component gene
set enrichment (PCGSE), for computing the statistical association between gene
sets and the PCs of genomic data. The PCGSE method performs a two-stage
competitive gene set test using the correlation between each gene and each PC
as the gene-level test statistic with flexible choice of both the gene set test
statistic and the method used to compute the null distribution of the gene set
statistic. Using simulated data with simulated gene sets and real gene
expression data with curated gene sets, we demonstrate that biologically
meaningful and computationally efficient results can be obtained from a simple
parametric version of the PCGSE method that performs a correlation-adjusted
two-sample t-test between the gene-level test statistics for gene set members
and genes not in the set. Availability:
http://cran.r-project.org/web/packages/PCGSE/index.html Contact:
rob.frost@dartmouth.edu or jason.h.moore@dartmouth.edu

The prevalence of HIV in West Africa is lower than elsewhere in Africa but
Gabon has one of the highest rates of HIV in that region. Gabon has a small
population and a high per capita gross domestic product making it an ideal
place to carry out a programme of early treatment for HIV. The effectiveness,
availability and affordability of triple combination therapy make it possible
to contemplate ending AIDS deaths and HIV transmission in the short term and
HIV prevalence in the long term. Here we consider what would have happened in
Gabon without the development of potent anti-retroviral therapy (ART), the
impact that the current roll-out of ART has had on HIV, and what might be
possible if early treatment with ART becomes available to all. We fit a dynamic
transmission model to trends in the adult prevalence of HIV and infer trends in
incidence, mortality and the impact of ART. The availability of ART has reduced
the prevalence of HIV among adults not on ART from 4.2% to 2.9%, annual
incidence from 0.43% to 0.27%, and the proportion of adults dying from AIDS
illnesses each year from 0.36% to 0.13% saving the lives of 2.3 thousand people
in 2013 alone. The provision of ART has been highly cost effective saving the
country at least $18 million up to 2013.

In a recent article Hontelez and colleagues investigate the prospects for
elimination of HIV in South Africa through expanded access to antiretroviral
therapy (ART) using STDSIM, a micro-simulation model. One of the first
published models to suggest that expanded access to ART could lead to the
elimination of HIV, referred to by the authors as the Granich Model, was
developed and implemented by the present author. The notion that expanded
access to ART could lead to the end of the AIDS epidemic gave rise to
considerable interest and debate and remains contentious. In considering this
notion Hontelez et al. start by stripping down STDSIM to a simple model that is
equivalent to the model developed by the present author3 but is a stochastic
event driven model. Hontelez and colleagues then reintroduce levels of
complexity to explore ways in which the model structure affects the results. In
contrast to our earlier conclusions Hontelez and colleagues conclude that
universal voluntary counselling and testing with immediate ART at 90% coverage
should result in the elimination of HIV but would take three times longer than
predicted by the model developed by the present author. Hontelez et al. suggest
that the current scale-up of ART at CD4 cell counts less than 350 cells/microL
will lead to elimination of HIV in 30 years. I disagree with both claims and
believe that their more complex models rely on unwarranted and unsubstantiated
assumptions.

The past decade has witnessed a dramatic increase in the size and scope of
biological and behavioral experiments. These experiments are providing an
unprecedented level of detail and depth of data. However, this increase in data
presents substantial statistical and graphical hurdles to overcome, namely how
to distinguish signal from noise and how to visualize multidimensional results.
Here we present a series of tools designed to support a research project from
inception to publication. We provide implementation of dimension reduction
techniques and visualizations that function well with the types of data often
seen in animal behavior studies. This package is designed to be used with
experimental data but can also be used for experimental design and sample
justification. The goal for this project is to create a package that will
evolve over time, thereby remaining relevant and reflective of current methods
and techniques.

Cryopreservation is beset with the challenge of protocol alignment across a
wide range of cell types and process variables. By taking a cross-sectional
assessment of previously published cryopreservation data (sample means and
standard errors) as preliminary meta-data, a decision tree learning analysis
(DTLA) was performed to develop an understanding of target survival and
optimized pruning methods based on different approaches. Briefly, a clear
direction on the decision process for selection of methods was developed with
key choices being the cooling rate, plunge temperature on the one hand and
biomaterial choice, use of composites (sugars and proteins), loading procedure
and cell location in 3D scaffold on the other. Secondly, using machine learning
and generalized approaches via the Na\"ive Bayes Classification (NBC) approach,
these metadata were used to develop posterior probabilities for combinatorial
approaches that were implicitly recorded in the metadata. These latter results
showed that newer protocol choices developed using probability elicitation
techniques can unearth improved protocols consistent with multiple
unidimensional optimized physical protocols. In conclusion, this article
proposes the use of DTLA models and subsequently NBC for the improvement of
modern cryopreservation techniques through an integrative approach.
  Keywords: 3D cryopreservation, decision-tree learning (DTL), sugars, mouse
embryonic stem cells, meta-data, Na\"ive Bayes Classifier (NBC)

Four chapters of the synthesis represent four major areas of my research
interests: 1) data analysis in molecular biology, 2) mathematical modeling of
biological networks, 3) genome evolution, and 4) cancer systems biology. The
first chapter is devoted to my work in developing non-linear methods of
dimension reduction (methods of elastic maps and principal trees) which extends
the classical method of principal components. Also I present application of
matrix factorization techniques to analysis of cancer data. The second chapter
is devoted to the complexity of mathematical models in molecular biology. I
describe the basic ideas of asymptotology of chemical reaction networks aiming
at dissecting and simplifying complex chemical kinetics models. Two
applications of this approach are presented: to modeling NFkB and apoptosis
pathways, and to modeling mechanisms of miRNA action on protein translation.
The third chapter briefly describes my investigations of the genome structure
in different organisms (from microbes to human cancer genomes). Unsupervised
data analysis approaches are used to investigate the patterns in genomic
sequences shaped by genome evolution and influenced by the basic properties of
the environment. The fourth chapter summarizes my experience in studying cancer
by computational methods (through combining integrative data analysis and
mathematical modeling approaches). In particular, I describe the on-going
research projects such as mathematical modeling of cell fate decisions and
synthetic lethal interactions in DNA repair network. The synthesis is concluded
by listing major challenges in computational systems biology, connected to the
topics of this text, i.e. dealing with complexity of biological systems.

An accurate method for enumerating pathogen indicators, such as Escherichia
coli (E. coli), and Salmonella spp. is important for assessing the safety of
compost samples. This study aimed to determine the occurrence of pathogen
indicators in compost samples by using a molecular approach known as Polymerase
Chain Reaction (PCR). The DNA sample was extracted from sewage sludge compost.
The specificity of the probes and primers at the species level were verified by
performing NCBI-BLAST2 (Basic Local Alignment Search Tool). Primers that target
the gadAB gene for E.coli and invA gene for Salmonella spp. were selected which
produce fragment lengths around 670bp and 285bp respectively. The primers were
tested against bacterial cultures of both species and produced a strong signal
band of the expected fragment length. It provided results within 6 hours which
is relatively rapid compared to conventional culturing techniques. The other
advantages of PCR are shown to be its high sensitivity, and high specificity.

Accurate enumeration of Salmonella spp. is important for assessing whether
this pathogen has survived composting. Recent literature has reported that
enumeration of Salmonella spp. using standard microbiological methods has a
numbers of disadvantages, particularly the time taken to obtain a result. This
research is an attempt to develop a rapid, low-cost detection method that is
quantitative, highly sensitive and target specific. This paper reports the
development of a DNA fragment that can be used to quantify Salmonella spp. by
competitive polymerase chain reaction (cPCR) targeted at the invA gene of
Salmonella (PCR primers that target the invA gene are reported to have very
high specificity for Salmonella strains). It is shown that cPCR, which could be
completed in 5 hours, could quantify the number of copies of the Salmonella
invA gene in a sample solution.

Composting is defined as the biological decomposition and stabilization of
organic substrates under aerobic conditions to allow the development of
thermophilic temperatures. This thermophilic temperature is a result of
biologically produced heat. Composting produces the final product which is
sufficiently stable for storage and application to land without adverse
environmental effects. There are many factors which affect the decomposition of
organic matter in the composting process. Since the composting process is very
intricate, it is not easy to estimate the effect of a single factor on the rate
of organic matter decomposition. This paper looked at the main factors
affecting the composting process. Problems regarding the controlling,
inactivation and regrowth of pathogen in compost material are also discussed.

The survival of Salmonella spp. as pathogen indicator in composting was
studied. The inoculums technique was used to gives the known amounts of
Salmonella spp. involved in composting. The inoculums of Salmonella spp.
solution was added directly into the compost material. The direct inoculum was
compared with inoculums in vial technique. The Salmonella spp. solution placed
into a vial and inserted into the middle of compost material before starting
the composting process. The conventional method that is used for the
enumeration of Salmonella spp. is serial dilution followed by standard membrane
filtration as recommended in the compost quality standard method PAS 100 and
the British Standard (BS EN ISO 6579:2002). This study was designed to
investigate the relationship of temperature and contact material that may also
involve in pathogen activation specifically to Salmonella spp. The exposure to
an average temperature during composting of about 55-60{\deg}C was kept for at
least 3 days as it was reported sufficiently kills the vast majority of enteric
pathogen (Deportes et al., 1995). The amount of Salmonella spp. and temperature
for both samples was set as indicator to determine the survival of Salmonella
spp. in direct and non-direct inoculums. This study gives the figures of
die-off rate for Salmonella spp. during composting. The differentiation between
direct contact (Sample A) and non-contact of Salmonella spp. with compost
material (Sample B) during composting was also revealed. The results from
laboratory scale of composting study has been showed that after 8 days (which
included at least at 66{\deg}C) the numbers of Salmonella spp. in Sample A were
below the limits in UK compost standard (known as PAS 100)(BSI, 2005) which
required the compost to be free of Salmonella spp. Meanwhile, Sample B still
gives high amount of Salmonella spp. in even after composting for 20 days.

The mathematical models used to capture features of complex, biological
systems are typically non-linear, meaning that there are no generally valid
simple relationships between their outputs and the data that might be used to
validate them. This invalidates the assumptions behind standard statistical
methods such as linear regression, and often the methods used to parameterise
biological models from data are ad hoc. In this perspective, I will argue for
an approach to model fitting in mathematical biology that incorporates modern
statistical methodology without losing the insights gained through non-linear
dynamic models, and will call such an approach principled model fitting.
Principled model fitting therefore involves defining likelihoods of observing
real data on the basis of models that capture key biological mechanisms.

Recent studies show that the microbial communities inhabiting the human
intestine can have profound impact on our well-being and health. However, we
have limited understanding of the mechanisms that control this complex
ecosystem. Based on a deep phylogenetic analysis of the intestinal microbiota
in a thousand western adults we identified groups of bacteria that tend to be
either nearly absent, or abundant in most individuals. The abundances of these
bimodally distributed bacteria vary independently, and their contrasting
alternative states are associated with host factors such as ageing and
overweight. We propose that such bimodal groups represent independent tipping
elements of the intestinal microbiota. These reflect the overall state of the
intestinal ecosystem whose critical transitions can have profound health
implications and diagnostic potential.

The binding of transcription factors (TFs) is essential for gene expression.
One important characteristic is the actual occupancy of a putative binding site
in the genome. In this study, we propose an analytical model to predict genomic
occupancy that incorporates the preferred target sequence of a TF in the form
of a position weight matrix (PWM), DNA accessibility data (in case of
eukaryotes), the number of TF molecules expected to be bound specifically to
the DNA and a parameter that modulates the specificity of the TF. Given actual
occupancy data in form of ChIP-seq profiles, we backwards inferred copy number
and specificity for five Drosophila TFs during early embryonic development:
Bicoid, Caudal, Giant, Hunchback and Kruppel. Our results suggest that these
TFs display thousands of molecules that are specifically bound to the DNA and
that, while Bicoid and Caudal display a higher specificity, the other three
transcription factors (Giant, Hunchback and Kruppel) display lower specificity
in their binding (despite having PWMs with higher information content). This
study gives further weight to earlier investigations into TF copy numbers that
suggest a significant proportion of molecules are not bound specifically to the
DNA.

Preterm infants with very low birth weight suffer from a high risk of
intra-ventricular hemorrhage(IVH) and other serious diseases. To improve the
clinical risk assessment of preterm infants and develop potential clinically
makers for the adverse outcome, the first part of the paper develops the
frequency spectral analysis on the non-invasively measured heart rate
variability, blood pressure variability and cerebral near-infrared spectroscopy
measures. Moderate and high correlations with the clinical risk index for
babies were identified from various spectral measures of arterial baroreflex
and cerebral autoregulation functions. It was also observed that the
cross-spectral transfer function analysis of cerebral NIRS and arterial blood
pressure was able to provide a number of parameters that were potentially
useful for distinguishing between preterm infants with or without IVH.
Furthermore, the detrended fluctuation analysis that quantifies the fractal
correlation properties of physiological signals has been examined, to determine
whether it could derive markers for the identification of preterm infants with
IVH. Cardiac output(CO) and total peripheral resistance(TPR) are two important
parameters of the cardiovascular system. Measurement of these two parameters
can provide valuable information for the assessment and management of patients
needing intensive care, including preterm infants in the neonatal intensive
care unit. To further assess the changes in CO and TPR in the preterm infants,
the multivariate regression model based on the useful features from ABP method
was used to improve the accuracy and robustness of the estimation. The
combination of signal analysis and multivariate regression model in estimation
of CO has produced some outcomes, and in the future, more effort should be
involved in this kind of research to improve the prediction of serious diseases
in preterm infants.

A remarkable property of nastic, shape changing plants is their complete
fusion between actuators and structure. This is achieved by combining a large
number of cells whose geometry, internal pressures and material properties are
optimized for a given set of target shapes and stiffness requirements. An
advantage of such a fusion is that cell walls are prestressed by cell pressures
which increases, decreases the overall structural stiffness, weight. Inspired
by the nastic movement of plants, Pagitz et al. 2012 Bioinspir. Biomim. 7
published a novel concept for pressure actuated cellular structures. This
article extends previous work by introducing a modular approach to adaptive
structures. An algorithm that breaks down any continuous target shapes into a
small number of standardized modules is presented. Furthermore it is shown how
cytoskeletons within each cell enhance the properties of adaptive modules. An
adaptive passenger seat and an aircrafts leading, trailing edge is used to
demonstrate the potential of a modular approach.

Several methods are available for the detection of covarying positions from a
multiple sequence alignment (MSA). If the MSA contains a large number of
sequences, information about the proximities between residues derived from
covariation maps can be sufficient to predict a protein fold. If the structure
is already known, information on the covarying positions can be valuable to
understand the protein mechanism.
  In this study we have sought to determine whether a multivariate extension of
traditional mutual information (MI) can be an additional tool to study
covariation. The performance of two multidimensional MI (mdMI) methods,
designed to remove the effect of ternary/quaternary interdependencies, was
tested with a set of 9 MSAs each containing <400 sequences, and was shown to be
comparable to that of methods based on maximum entropy/pseudolikelyhood
statistical models of protein sequences. However, while all the methods tested
detected a similar number of covarying pairs among the residues separated by <
8 {\AA} in the reference X-ray structures, there was on average less than 65%
overlap between the top scoring pairs detected by methods that are based on
different principles.
  We have also attempted to identify whether the difference in performance
among methods is due to different efficiency in removing covariation
originating from chains of structural contacts. We found that the reason why
methods that derive partial correlation between the columns of a MSA provide a
better recognition of close contacts is not because they remove chaining
effects, but because they filter out the correlation between distant residues
that originates from general fitness constraints. In contrast we found that
true chaining effects are expression of real physical perturbations that
propagate inside proteins, and therefore are not removed by the derivation of
partial correlation between variables.

Clonal structure of the human peripheral T-cell repertoire is shaped by a
number of homeostatic mechanisms, including antigen presentation, cytokine and
cell regulation. Its accurate tuning leads to a remarkable ability to combat
pathogens in all their variety, while systemic failures may lead to severe
consequences like autoimmune diseases. Here we develop and make use of a
non-parametric statistical approach to assess T cell clonal size distributions
from recent next generation sequencing data. For 41 healthy individuals and a
patient with ankylosing spondylitis, who undergone treatment, we invariably
find power law scaling over several decades and for the first time calculate
quantitatively meaningful values of decay exponent. It has proved to be much
the same among healthy donors, significantly different for an autoimmune
patient before the therapy, and converging towards a typical value afterwards.
We discuss implications of the findings for theoretical understanding and
mathematical modeling of adaptive immunity.

Background: Recent research in animal behaviour has contributed to determine
how alignment, turning responses, and changes of speed mediate flocking and
schooling interactions in different animal species. Here, we address
specifically the problem of what interaction responses support different
nearest neighbour configurations in terms of mutual position and distance.
Results: We find that the different interaction rules observed in different
animal species may be a simple consequence of the relative positions that
individuals assume when they move together, and of the noise inherent with the
movement of animals, or associated with tracking inaccuracy. Conclusions: The
anisotropic positioning of individuals with respect to their neighbours, in
combination with noise, can explain several aspects of the movement responses
observed in real animal groups, and should be considered explicitly in future
models of flocking and schooling. By making a distinction between interaction
responses involved in maintaining a preferred flock configuration, and
interaction responses directed at changing it, we provide a frame to
discriminate movement interactions that signal directional conflict from those
underlying consensual group motion.

In this paper we study a reduced continuous model describing the local
evolution of high grade gliomas - a lethal type of primary brain tumor -
through the interplay of different cellular phenotypes. We show how hypoxic
events, even sporadic and/or limited in space may have a crucial role on the
acceleration of the growth speed of high grade gliomas. Our modeling approach
is based on two cellular phenotypes one of them being more migratory and the
second one more proliferative with transitions between them being driven by the
local oxygen values, assumed in this simple model to be uniform. Surprisingly
even acute hypoxia events (i.e. very localized in time) leading to the
appearance of migratory populations have the potential of accelerating the
invasion speed of the proliferative phenotype up to speeds close to those of
the migratory phenotype. The high invasion speed of the tumor persists for
times much longer than the lifetime of the hypoxic event and the phenomenon is
observed both when the migratory cells form a persistent wave of cells located
on the invasion front and when they form a evanecent wave dissapearing after a
short time by decay into the more proliferative phenotype.
  Our findings are obtained through numerical simulations of the model
equations. We also provide a deeper mathematical analysis of some aspects of
the problem such as the conditions for the existence of persistent waves of
cells with a more migratory phenotype.

One of the promising frontiers of bioengineering is the controlled release of
a therapeuticdrug from a vehicle across the skin (transdermal drug delivery).
In order to study the complete process, a two-phase mathematical model
describing the dynamics of a substance between two coupled media of different
properties and dimensions is presented. A system of partial differential
equations describes the diffusion and the binding/unbinding processes in both
layers. Additional flux continuity at the interface and clearance conditions
into systemic circulation are imposed. An eigenvalue problem with discontinuous
coefficients is solved and an analytical solution is given in the form of an
infinite series expansion. The model points out the role of the diffusion and
reaction parameters, which control the complex transfer mechanism and the drug
kinetics across the two layers. Drug masses are given and their dependence on
the physical parameters is discussed.

In the autoregressive process of first order AR(1), a homogeneous correlated
time series $u_t$ is recursively constructed as $u_t = q\; u_{t-1} + \sigma
\;\epsilon_t$, using random Gaussian deviates $\epsilon_t$ and fixed values for
the correlation coefficient $q$ and for the noise amplitude $\sigma$. To model
temporally heterogeneous time series, the coefficients $q_t$ and $\sigma_t$ can
be regarded as time-dependend variables by themselves, leading to the
time-varying autoregressive processes TVAR(1). We assume here that the time
series $u_t$ is known and attempt to infer the temporal evolution of the
'superstatistical' parameters $q_t$ and $\sigma_t$. We present a sequential
Bayesian method of inference, which is conceptually related to the Hidden
Markov model, but takes into account the direct statistical dependence of
successively measured variables $u_t$. The method requires almost no prior
knowledge about the temporal dynamics of $q_t$ and $\sigma_t$ and can handle
gradual and abrupt changes of these superparameters simultaneously. We compare
our method with a Maximum Likelihood estimate based on a sliding window and
show that it is superior for a wide range of window sizes.

Measurement of serum growth hormone by mass spectrometry is demonstrated to
be unaffected by interferences with growth hormone binding protein as
frequently encountered with antibody-based routine test methods and provides an
alternative approach, therefore, to acquisition of accurate results.

A novel portable fluorometer combining the attributes of a smartphone with an
easy fit, simple and compact sample chamber fabricated using 3D printing has
been developed for pH measurements of environmental water in the field. The
results were then compared directly with those obtained using conventional
electrode based measurements.

Size-structured population models provide a popular means to mathematically
describe phenomena such as bacterial aggregation, schooling fish, and
planetesimal evolution. For parameter estimation, generalized sensitivity
functions (GSFs) provide a tool that quantifies the impact of data from
specific regions of the experimental domain. These functions help identify the
most relevant data subdomains, which enhances the optimization of experimental
design. To our knowledge, GSFs have not been used in the partial differential
equation (PDE) realm, so we provide a novel PDE extension of the discrete and
continuous ordinary differential equation (ODE) concepts of Thomaseth and
Cobelli and Banks et al. respectively. We analyze the GSFs in the context of
size-structured population models, and specifically analyze the Smoluchowski
coagulation equation to determine the most relevant time and volume domains for
three, distinct aggregation kernels. Finally, we provide evidence that
parameter estimation for the Smoluchowski coagulation equation does not require
post-gelation data.

Imposing a minimum principle in the framework of the so called crystal basis
model of the genetic code, we determine the structure of the minimum set of 22
anticodons which allows the translational-transcription for animal
mitochondrial code. The results are in very good agreement with the observed
anticodons.
  Then, we analyze the evolution of the genetic code, with 20 amino acids
encoded from the beginning, from the viewpoint of codon-anticodon interaction.
Following the same spirit as above, we determine the structure of the
anticodons in the Ancient, Archetypal and Early Genetic codes. Most of our
results agree with the generally accepted scheme.

Localizing the sources of electrical activity in the brain from
Electroencephalographic (EEG) data is an important tool for non-invasive study
of brain dynamics. Generally, the source localization process involves a
high-dimensional inverse problem that has an infinite number of solutions and
thus requires additional constraints to be considered to have a unique
solution. In the context of EEG source localization, we propose a novel
approach that is based on dividing the cerebral cortex of the brain into a
finite number of Functional Zones which correspond to unitary functional areas
in the brain. In this paper we investigate the use of Brodmanns areas as the
Functional Zones. This approach allows us to apply a sparsity constraint to
find a unique solution for the inverse EEG problem. Compared to previously
published algorithms which use different sparsity constraints to solve this
problem, the proposed method is potentially more consistent with the known
sparsity profile of the human brain activity and thus may be able to ensure
better localization. Numerical experiments are conducted on a realistic head
model obtained from segmentation of MRI images of the head and includes four
major compartments namely scalp, skull, cerebrospinal fluid (CSF) and brain
with relative conductivity values. Three different electrode setups are tested
in the numerical experiments.

Thermodynamic aspects of chemical reactions have a long history in the
Physical Chemistry literature. In particular, biochemical cycles - the
building-blocks of biochemical systems - require a source of energy to
function. However, although fundamental, the role of chemical potential and
Gibb's free energy in the analysis of biochemical systems is often overlooked
leading to models which are physically impossible. The bond graph approach was
developed for modelling engineering systems where energy generation, storage
and transmission are fundamental. The method focuses on how power flows between
components and how energy is stored, transmitted or dissipated within
components. Based on early ideas of network thermodynamics, we have applied
this approach to biochemical systems to generate models which automatically
obey the laws of thermodynamics. We illustrate the method with examples of
biochemical cycles. We have found that thermodynamically compliant models of
simple biochemical cycles can easily be developed using this approach. In
particular, both stoichiometric information and simulation models can be
developed directly from the bond graph. Furthermore, model reduction and
approximation while retaining structural and thermodynamic properties is
facilitated. Because the bond graph approach is also modular and scaleable, we
believe that it provides a secure foundation for building thermodynamically
compliant models of large biochemical networks.

The nonlinearity of dynamics in systems biology makes it hard to infer them
from experimental data. Simple linear models are computationally efficient, but
cannot incorporate these important nonlinearities. An adaptive method based on
the S-system formalism, which is a sensible representation of nonlinear
mass-action kinetics typically found in cellular dynamics, maintains the
efficiency of linear regression. We combine this approach with adaptive model
selection to obtain efficient and parsimonious representations of cellular
dynamics. The approach is tested by inferring the dynamics of yeast glycolysis
from simulated data. With little computing time, it produces dynamical models
with high predictive power and with structural complexity adapted to the
difficulty of the inference problem.

1. Understanding how to find targets with very limited information is a topic
of interest in many disciplines. In ecology, such research has often focused on
the development of two movement models: i) the L\'evy walk and; ii) the
composite correlated random walk and its associated area-restricted search
behaviour. Although the processes underlying these models differ, they can
produce similar movement patterns. Due to this similarity and because of their
disparate formulation, current methods cannot reliably differentiate between
these two models.
  2. Here, we present a method that differentiates between the two models. It
consists of likelihood functions, including one for a hidden Markov model, and
associated statistical measures that assess the relative support for and
absolute fit of each model.
  3. Using a simulation study, we show that our method can differentiate
between the two search models over a range of parameter values. Using the
movement data of two polar bears (\textit{Ursus maritimus}), we show that the
method can be applied to complex, real-world movement paths.
  4. By providing the means to differentiate between the two most prominent
search models in the literature, and a framework that could be extended to
include other models, we facilitate further research into the strategies
animals use to find resources.

Persistent homology computes topological invariants from point cloud data.
Recent work has focused on developing statistical methods for data analysis in
this framework. We show that, in certain models, parametric inference can be
performed using statistics defined on the computed invariants. We develop this
idea with a model from population genetics, the coalescent with recombination.
We apply our model to an influenza dataset, identifying two scales of
topological structure which have a distinct biological interpretation.

We identify fundamental issues with discretization when estimating
information-theoretic quantities in the analysis of data. These difficulties
are theoretical in nature and arise with discrete datasets carrying significant
implications for the corresponding claims and results. Here we describe the
origins of the methodological problems, and provide a clear illustration of
their impact with the example of biological network reconstruction. We propose
an algorithm (shared information metric) that corrects for the biases and the
resulting improved performance of the algorithm demonstrates the need to take
due consideration of this issue in different contexts.

Metagenomics is an approach for characterizing environmental microbial
communities in situ, it allows their functional and taxonomic characterization
and to recover sequences from uncultured taxa. For communities of up to medium
diversity, e.g. excluding environments such as soil, this is often achieved by
a combination of sequence assembly and binning, where sequences are grouped
into 'bins' representing taxa of the underlying microbial community from which
they originate. Assignment to low-ranking taxonomic bins is an important
challenge for binning methods as is scalability to Gb-sized datasets generated
with deep sequencing techniques. One of the best available methods for the
recovery of species bins from an individual metagenome sample is the
expert-trained PhyloPythiaS package, where a human expert decides on the taxa
to incorporate in a composition-based taxonomic metagenome classifier and
identifies the 'training' sequences using marker genes directly from the
sample. Due to the manual effort involved, this approach does not scale to
multiple metagenome samples and requires substantial expertise, which
researchers who are new to the area may not have. With these challenges in
mind, we have developed PhyloPythiaS+, a successor to our previously described
method PhyloPythia(S). The newly developed + component performs the work
previously done by the human expert. PhyloPythiaS+ also includes a new k-mer
counting algorithm, which accelerated k-mer counting 100-fold and reduced the
overall execution time of the software by a factor of three. Our software
allows to analyze Gb-sized metagenomes with inexpensive hardware, and to
recover species or genera-level bins with low error rates in a fully automated
fashion.

Growth hormone (GH) constitutes a set of closely related protein isoforms. In
clinical practice, the disagreement of test results between commercially
available ligand-binding assays is still an ongoing issue, and incomplete
knowledge about the particular function of the different forms leaves an
uncertainty of what should be the appropriate measurand. Mass spectrometry is
promising to be a way forward. Not only is it capable of providing SI-traceable
reference values for the calibration of current GH-tests, but it also offers an
independent approach to highly reliable mass-selective quantification of
individual GH-isoforms. This capability may add to reliability in doping
control too. The article points out why and how.

Radiotherapy is a commonly used treatment for cancer and is usually given in
varying doses. At low radiation doses relatively few cells die as a direct
response to radiation but secondary radiation effects such as DNA mutation or
bystander effects affect many cells. Consequently it is at low radiation levels
where an understanding of bystander effects is essential in designing novel
therapies with superior clinical outcomes. In this article, we use a hybrid
multiscale mathematical model to study the direct effects of radiation as well
as radiation-induced bystander effects on both tumour cells and normal cells.
We show that bystander responses may play a major role in mediating radiation
damage to cells at low-doses of radiotherapy, doing more damage than that due
to direct radiation. The survival curves derived from our computational
simulations showed an area of hyper-radiosensitivity at low-doses that are not
obtained using a traditional radiobiological model.

Grade II gliomas are slowly growing primary brain tumors that affect mostly
young patients and become fatal after a few years. Current clinical handling
includes surgery as first line treatment. Cytotoxic therapies (radiotherapy RT
or chemotherapy QT) are used initially only for patients having a bad
prognosis. Therapies are administered following the 'maximum dose in minimum
time' principle, what is the same schedule used for high grade brain tumors.
Using mathematical models describing the growth of these tumors in response to
radiotherapy, we find that a extreme protraction therapeutical strategy, i.e.
enlarging substantially the time interval between RT fractions, may lead to a
better tumor control. Explicit formulas are found providing the optimal spacing
between doses in a very good agreement with the simulations of the full
three-dimensional mathematical model approximating the tumor spatio-temporal
dynamics. This idea, although breaking the well-stablished paradigm, has
biological meaning since in these slowly growing tumors it may be more
favourable to treat the tumor as the different tumor subpopulations move to
more sensitive phases of the cell cycle.

Using very precise (up to 0.05%) measurements of the growth parameters for
bacteria E. coli grown on minimal media, we aimed to determine the lowest
deuterium concentration at which the adverse effects that are prominent at
higher enrichments start to become noticeable. Such a threshold was found at
0.5% D, a surprisingly high value, while the ultralow deuterium concentrations
(up to 0.25% D) showed signs of the opposite trend. Bacterial adaptation for
400 generations in isotopically different environment confirmed preference for
ultralow (up to 0.25% D) enrichment. This effect appears to be similar to those
described in sporadic but multiple earlier reports. Possible explanations
include hormesis and isotopic resonance phenomena, with the latter explanation
being favored.

We illustrate shape mode analysis as a simple, yet powerful technique to
concisely describe complex biological shapes and their dynamics. We
characterize undulatory bending waves of beating flagella and reconstruct a
limit cycle of flagellar oscillations, paying particular attention to the
periodicity of angular data. As a second example, we analyze non-convex
boundary outlines of gliding flatworms, which allows us to expose stereotypic
body postures that can be related to two different locomotion mechanisms.
Further, shape mode analysis based on principal component analysis allows to
discriminate different flatworm species, despite large motion-associated shape
variability. Thus, complex shape dynamics is characterized by a small number of
shape scores that change in time. We present this method using descriptive
examples, explaining abstract mathematics in a graphic way.

Motivation: Assigning statistical significance accurately has become
increasingly important as meta data of many types, often assembled in
hierarchies, are constructed and combined for further biological analyses.
Statistical inaccuracy of meta data at any level may propagate to downstream
analyses, undermining the validity of scientific conclusions thus drawn. From
the perspective of mass spectrometry based proteomics, even though accurate
statistics for peptide identification can now be achieved, accurate protein
level statistics remain challenging.
  Results: We have constructed a protein ID method that combines peptide
evidences of a candidate protein based on a rigorous formula derived earlier;
in this formula the database $P$-value of every peptide is weighted, prior to
the final combination, according to the number of proteins it maps to. We have
also shown that this protein ID method provides accurate protein level
$E$-value, eliminating the need of using empirical post-processing methods for
type-I error control. Using a known protein mixture, we find that this protein
ID method, when combined with the Soric formula, yields accurate values for the
proportion of false discoveries. In terms of retrieval efficacy, the results
from our method are comparable with other methods tested.
  Availability: The source code, implemented in C++ on a linux system, is
available for download at
ftp://ftp.ncbi.nlm.nih.gov/pub/qmbp/qmbp_ms/RAId/RAId_Linux_64Bit

Locomotion and gross morphology have been important phenotypes for C. elegans
genetics since the inception of the field and remain relevant. In parallel with
developments in genome sequencing and editing, phenotyping methods have become
more automated and quantitative, making it possible to detect subtle
differences between mutants and wild-type animals. In this chapter, we describe
how to calibrate a single worm tracker consisting of a USB microscope mounted
on a motorized stage and how to record and analyze movies of worms crawling on
food. The resulting quantitative phenotypic fingerprint can sensitively
identify differences between mutant and wild type worms.

We mapped current and future temperature suitability for malaria transmission
in Africa using a published model that incorporates nonlinear physiological
responses to temperature of the mosquito vector Anopheles gambiae and the
malaria parasite Plasmodium falciparum. We found that a larger area of Africa
currently experiences the ideal temperature for transmission than previously
supposed. Under future climate projections, we predicted a modest increase in
the overall area suitable for malaria transmission, but a net decrease in the
most suitable area. Combined with population density projections, our maps
suggest that areas with temperatures suitable for year-round, highest risk
transmission will shift from coastal West Africa to the Albertine Rift between
Democratic Republic of Congo and Uganda, while areas with seasonal transmission
suitability will shift toward sub-Saharan coastal areas. Mapping temperature
suitability places important bounds on malaria transmissibility and, along with
local level demographic, socioeconomic, and ecological factors, can indicate
where resources may be best spent on malaria control.

UNAIDS has embraced an ambitious global target for the implementation of
treatment for people living with HIV. This 90-90-90 target would mean that, by
2020, 90% of all those living with HIV should know their status, 90% of these
would be on treatment and 90% of these would have fully suppressed plasma viral
loads. To reach this target in the next five years presents a major logistical
challenge. However, the prevalence of HIV varies greatly by risk groups, age,
gender, geography and social conditions. For reasons of effectiveness and
impact the focus must first be on those who are most likely to be infected with
HIV and therefore most likely to infect others. In Kenya the prevalence of HIV
in adults varies by two orders of magnitude among the counties. The effective
implementation of 90-90-90 will depend on first providing ART where the
prevalence of infection is greatest, then to those that are most easily reached
in large numbers and finally to the whole population. Here we use routine data
from ante-natal clinics and national survey data to assess the variation of the
prevalence of HIV among counties in Kenya; we suggest reasons for this
variation, and estimate the effectiveness of targeting the role out of ART. The
highest prevalence occurs in some of the counties bordering Lake Victoria and
these are most in need of ART. These districts in Nyanza Province, account for
31% of all cases in Kenya but make up 10% of the population and cover 1.8% of
the land-area. The highest concentrations of HIV cases are in Nairobi and
Mombasa which account for a further 18% of all cases in Kenya but make up 12%
of the population and cover 0.1% of the land-area. Providing ART in these two
cities will be relatively straightforward given their small geographical area.

There are various cases of animal movement where behaviour broadly switches
between two modes of operation, corresponding to a long distance movement state
and a resting or local movement state. Here a mathematical description of this
process is formulated, adapted from Friedrich et. al. (2006). The approach
allows the specification any running or waiting time distribution along with
any angular and speed distributions. The resulting system of partial
integro-differential equations are tumultuous and therefore it is necessary to
both simplify and derive summary statistics. An expression for the mean squared
displacement is derived which shows good agreement with experimental data from
the bacterium Escherichia coli and the gull Larus fuscus. Finally a large time
diffusive approximation is considered via a Cattaneo approximation (Hillen,
2004). This leads to the novel result that the effective diffusion constant is
dependent on the mean and variance of the running time distribution but only on
the mean of the waiting time distribution.

Multiple-scale and broad-scale assessments often require rescaling the
original data to a consistent grain size for analysis. Rescaling categorical
raster data by spatial aggregation is common in large area ecological
assessments. However, distortion and loss of information are associated with
aggregation. Using a majority rule generally results in dominant classes
becoming more pronounced and rare classes becoming less pronounced. Using
nearest neighbor techniques generally maintains the global proportion of each
category in the original map but can lead to disaggregation. In this paper we
implement the spatial scan statistic for spatial aggregation of categorical
raster maps and describe the behavior of the technique at the local level
(aggregation unit) and global level (map). We also contrast the spatial scan
statistic technique with the majority rule and nearest neighbor approaches. In
general, the scan statistic technique behaved inverse the majority rule
approach in that rare classes rather than abundant classes were preserved. We
suggest the scan statistic techniques should be used for spatial aggregation of
categorical maps when preserving heterogeneity and information from rare
classes are important goals of the study or assessment.

Motivation: The study of diverse enzyme superfamilies can provide important
insight into the relationships between protein sequence, structure and
function. It is often challenging, however, to discover these relationships
across a large and diverse superfamily. Contemporary similarity network
visualization techniques allow researchers to aggregate sequence similarity
information into a single global view. Network visualization provides a
qualitative estimate of functional diversity within a superfamily, but is
unable to quantitate explicit boundaries, when present, between neighboring
families in sequence space. This limits the potential of existing
sequence-based algorithms to generate functional predictions from superfamily
datasets.
  Results: By building on current network analysis tools, we have developed a
new algorithm for elucidating pairs of homologous families within a sequence
dataset. Our algorithm is able to filter through a dense similarity network in
order to estimate both the boundaries of individual families and also how the
families neighbor one another. Globally, these neighboring families define a
topology across the entire superfamily. The topology is simple to interpret by
visualizing the network output generated by our filtration protocol. We have
compared the network topology within the kinase superfamily against available
phylogenetic data. Our results suggest that neighbors within the filtered
kinase network are more likely to share structural and functional properties
than more distant network clusters.

We present a calculation technique for modeling inhomogeneous DNA replication
kinetics, where replication factors such as initiation rates or fork speeds can
change with both position and time. We can use our model to simulate data sets
obtained by molecular combing, a widely used experimental technique for probing
replication. We can also infer information about the replication program by
fitting our model to experimental data sets and also test the efficacy of
planned experiments by fitting our model to simulated data sets. We consider
asynchronous data sets and illustrate how a lack of synchrony affects
replication profiles. In addition to combing data, our technique is
well-adapted to microarray-based studies of replication.

Metabonomics, the measure of the fingerprint of biochemical perturbations
caused by disease, drugs or toxins, recently has become a major focus of
research in various areas especially indications of drug toxicity. Two types of
technology (known by the initials NMR and MS) are employed and both produce
massive data in form of spectra. Sophisticated statistical models, known as
pattern recognition techniques, are commonly applied for summarizing and
analyzing these multidimensional data. However, strong signals from compounds
that are administered during toxicological trials interfere with these models.
So called 'spectral replacement' is a method to eliminate these signals by
replacing them with the signals in their corresponding regions in control
spectrum. The replaced regions are subsequently scaled. However, this scaling
is not accurately measured and often results in overestimation of integrated
intensity of the replaced signals. Here, a novel protocol is proposed which
provides an accurate estimation of the replaced regions.

Solving the chemical master equation exactly is typically not possible, so
instead we must rely on simulation based methods. Unfortunately, drawing exact
realisations, results in simulating every reaction that occurs. This will
preclude the use of exact simulators for models of any realistic size and so
approximate algorithms become important. In this paper we describe a general
framework for assessing the accuracy of the linear noise and two moment
approximations. By constructing an efficient space filling design over the
parameter region of interest, we present a number of useful diagnostic tools
that aids modellers in assessing whether the approximation is suitable. In
particular, we leverage the normality assumption of the linear noise and moment
closure approximations.

The epidemic of HIV in Malawi started early and at its peak 15% of all adults
were infected with HIV. Malawi is a low-income country and the cost of putting
all HIV-positive people in Malawi onto ART, expressed as a percentage of the
gross domestic product, is the highest in the world. Nevertheless, Malawi has
made great progress and the greatly reduced cost of potent anti-retroviral
therapy (ART) makes it possible to contemplate ending the epidemic of HIV/AIDS.
Here we consider what would have happened without ART, the No ART
counterfactual, the impact if the current level of roll-out of ART is
maintained, the Current Programme, and the likely impact if treatment is made
available to everyone who is eligible under the 2013 guidelines of the World
Health Organization reaching full coverage by 2020, the Expanded Programme.
  The Current Programme has substantially reduced the epidemic of HIV and the
number of people dying of AIDS. The Expanded Programme has the potential to
avert more infections, save more lives and end the epidemic. The annual cost of
managing HIV will increase from about US$132 million in 2014 to about US$155
million in 2020 but will fall after that. If the Expanded Programme is
implemented several key areas must be addressed. Testing services will need to
be expanded and supported by mass testing campaigns, so as to diagnose people
with HIV and enrol them in treatment and care as early as possible. A regular
and uninterrupted supply of drugs will have to be assured. The quantity and
quality of existing health staff will need to be strengthened. Community health
workers will need to be mobilized and trained to encourage people to be tested
and accept treatment, to monitor progress and to support people on treatment;
this in turn will help to reduce stigma and discrimination, loss to follow up
of people diagnosed with HIV, and improve adherence for those on treatment.

A unified mathematical language for medicine and science will be presented.
Using this language, models for DNA replication, protein synthesis, chemical
reactions, neurons and a cardiac cycle of a heart have been built. Models for
Turing machines, cellular automaton, fractals and physical systems are also
represented with the use of this language. Interestingly, the language comes
with a way to represent probability theory concepts and also programming
statements. With this language, questions and processes in medicine can be
represented as systems of equations; and solutions to these equations are
viewed as treatments or previously unknown processes. This language can serve
as the framework for the creation of a large interactive open-access scientific
database that allows extensive mathematical medicine computations. It can also
serve as a basis for exploring ideas related to what could be called
metascience.

The effects of different levels of maltose on feed pellet water stability and
nutrient leaching were studied. Five treatments, including control with three
replicates with setup (0.0, 20, 25, 30 and 35%). Pellet leaching rates were
used to indicate pellet water stability. The results show that the presence of
maltose in the diets significantly improved pellet water stability (p<0.05),
but the leaching rates of the feed (35% maltose) observed higher than other
feeds. Increased maltose resulted in the corresponding decrease in pellet
stability. The protein leaching rate of control feed and feed (20% maltose) was
significantly (p < 0.05) lower than the rates of other diets The lipid leaching
rate of control feed was lower than the rates of other diets, while the feed
(35% maltose) was more leaching rate. It improved feeds water stability is one
important reason why maltose enhances fish growth.

We present the results of an experiment with light microscopy performed to
capture the trajectories of live Nitzschia sp. diatoms. The time series
corresponding to the motility of this kind of cells along ninety-five
circular-like trajectories have been obtained and analyzed with the scaling
statistical method of detrended fluctuation analysis optimized via a wavelet
transform. In this way, we determined the Hurst parameters, in two orthogonal
directions, which characterize the nature of the motion of live diatoms in
light microscopy experiments. We have found mean values of these directional
Hurst parameters between 0.70 and 0.63 with overall standard errors below 0.15.
These numerical values give evidence that the motion of Nitzschia sp. diatoms
is of persistent type and suggest an active cell motility with a kind of memory
associated with long-range correlations on the path of their trajectories. For
the collected statistics, we also find that the values of the Hurst exponents
depend on the number of abrupt turns that occur in the diatom trajectory and on
the type of wavelet, although their mean values do not change much

Natural and man-made transport webs are frequently dominated by dense sets of
nested cycles. The architecture of these networks, as defined by the topology
and edge weights, determines how efficiently the networks perform their
function. Yet, the set of tools that can characterize such a weighted
cycle-rich architecture in a physically relevant, mathematically compact way is
sparse. In order to fill this void, we have developed a new algorithm that
rests on an abstraction of the physical `tiling' in the case of a two
dimensional network to an effective tiling of an abstract surface in space that
the network may be thought to sit in. Generically these abstract surfaces are
richer than the flat plane and as a result there are now two families of
fundamental units that may aggregate upon cutting weakest links -- the
plaquettes of the tiling and the longer `topological' cycles associated with
the abstract surface itself. Upon sequential removal of the weakest links, as
determined by the edge weight, neighboring plaquettes merge and a tree
characterizing this merging process results. The properties of this
characteristic tree can provide the physical and topological data required to
describe the architecture of the network and to build physical models. The new
algorithm can be used for automated phenotypic characterization of any weighted
network whose structure is dominated by cycles, such as mammalian vasculature
in the organs, the root networks of clonal colonies like quaking aspen, or the
force networks in jammed granular matter.

Finding the underlying relationships among multiple imaging modalities in a
coherent fashion is one of challenging problems in the multimodal analysis. In
this study, we propose a novel multimodal network approach based on multidi-
mensional persistent homology. In this extension of the previous threshold-free
method of persistent homology, we visualize and discriminate the topological
change of integrated brain networks by varying not only threshold but also
mixing ratios between two different imaging modalities. Moreover, we also pro-
pose an integration method for multimodal networks, called one-dimensional
projection, with a specific mixing ratio between modalities. We applied the
proposed methods to PET and MRI data from 21 autism spectrum disorder (ASD)
children and 10 pediatric control subjects. From the results, we found that the
brain networks of ASD children and controls differ significantly, with ASD
showing asymmetrical changes of connected structures between PET and MRI. The
integrated MRI and PET networks showed that ASD children had weaker connections
than controls within the visual cortex, between dorsal and ventral parts of the
temporal pole, between frontal and parietal regions, and between the left
perisylvian and other brain regions. These results provide a multidimensional
homological understanding of disease-related PET and MRI networks that
discloses the network association with ASD.

Besides humans, several marine mammal species exhibit prerequisites to evolve
language: high cognitive abilities, flexibility in vocal production and
advanced social interactions. Here, we describe and analyse the vocal
repertoire of long-finned pilot whales (Globicephalus melas) recorded in
northern Norway. Observer based analysis reveals a complex vocal repertoire
with 140 different call types, call sequences, call repetitions and
group-specific differences in the usage of call types. Developing and applying
a new automated analysis method, the bag-of-calls approach, we find that groups
of pilot whales can be distinguished purely by statistical properties of their
vocalisations. Comparing inter-and intra-group differences of ensembles of
calls allows to identify and quantify group-specificity. Consequently, the
bag-of-calls approach is a valid method to specify difference and concordance
in acoustic communication in the absence of exact knowledge about signalers,
which is common observing marine mammals under natural conditions.

In this paper we develop a simple two compartment model which extends the
Farhi equation to the case when the inhaled concentration of a volatile organic
compound (VOC) is not zero. The model connects the exhaled breath concentration
of systemic VOCs with physiological parameters such as endogenous production
rates and metabolic rates. Its validity is tested with data obtained for
isoprene and inhaled deuterated isoprene-D5.

Chemical reactions inside cells are generally considered to happen within
fixed-size compartments. Needless to say, cells and their compartments are
highly dynamic. Thus, such stringent assumptions may not reflect biochemical
reality, and can highly bias conclusions from simulation studies. In this work,
we present an intuitive algorithm for particle-based diffusion in and on moving
boundaries, for both point particles and spherical particles. We first
benchmark in appropriate scenarios our proposed stochastic method against
solutions of partial differential equations, and further demonstrate that
moving boundaries can give rise to super diffusive motion as well as
time-inhomogeneous reaction rates. Finally, we conduct a numerical experiment
representing photobleaching of diffusing fluorescent proteins in dividing
Saccharomyces cerevisiae cells to demonstrate that moving boundaries might
cause important effects neglected in previously published studies.

Rhizoctonia solani anastomosis group AG2-2 IIIB is a severe sugar beet and
maize pathogen. It causes crown and root rot disease which leads to yield
losses world-wide. The soil-borne pathogen is difficult to detect and quantify
by conventional methods. We developed a real-time PCR (qPCR) assay for the
quantification of genomic DNA of Rhizoctonia solani AG2-2 IIIB based on the ITS
region of rDNA genes. The limit of quantification of the assay is 1.8 pg
genomic DNA. The amplification efficiency was 96.4. The assay will be helpful
in the diagnoses of Rhizoctonia solani infection of sugar beet and maize roots
and in the quantification of R. solani AG2-2 IIIB inoculum in plant debris and
soil.

Background: Gene expression studies on non-model organisms require open-end
strategies for transcription profiling. Gel-based analysis of cDNA fragments
allows to detect alterations in gene expression for genes which have neither
been sequenced yet nor are available in cDNA libraries. Commonly used protocols
are cDNA Differential Display (DDRT-PCR) and cDNA-AFLP. Both methods have been
used merely as qualitative gene discovery tools so far. Results: We developed
procedures for the conversion of DDRT-PCR data into quantitative transcription
profiles. Amplified cDNA fragments are separated on a DNA sequencer. Data
processing consists of four steps: (i) cDNA bands in lanes corresponding to
samples treated with the same primer combination are matched in order to
identify fragments originating from the same transcript, (ii) intensity of
bands is determined by densitometry, (iii) densitometric values are normalized,
and (iv) intensity ratio is calculated for each pair of corresponding bands.
Transcription profiles are represented by sets of intensity ratios (control vs.
treatment) for cDNA fragments defined by primer combination and DNA mobility.
We demonstrated the procedure by analyzing DDRT-PCR data on the effect of
secondary metabolites of oilseed rape Brassica napus on the transcriptome of
the pathogenic fungus Leptosphaeria maculans. Conclusion: We developed a data
processing procedure for quantitative analysis of amplified cDNA fragments. The
system utilizes common software and provides an open-end alternative to
microarray analysis. The processing is expected to work equally well with
DDRT-PCR and cDNA-AFLP data and be useful in research on organisms for which
microarray analysis is not available or economical.

Phenotypes are the observable characteristics of an organism arising from its
response to the environment. Phenotypes associated with engineered and natural
genetic variation are widely recorded using phenotype ontologies in model
organisms, as are signs and symptoms of human Mendelian diseases in databases
such as OMIM and Orphanet. Exploiting these resources, several computational
methods have been developed for integration and analysis of phenotype data to
identify the genetic etiology of diseases or suggest plausible interventions. A
similar resource would be highly useful not only for rare and Mendelian
diseases, but also for common, complex and infectious diseases. We apply a
semantic text- mining approach to identify the phenotypes (signs and symptoms)
associated with over 8,000 diseases. We demonstrate that our method generates
phenotypes that correctly identify known disease-associated genes in mice and
humans with high accuracy. Using a phenotypic similarity measure, we generate a
human disease network in which diseases that share signs and symptoms cluster
together, and we use this network to identify phenotypic disease modules.

Models of diffusion MRI within a voxel are useful for making inferences about
the properties of the tissue and inferring fiber orientation distribution used
by tractography algorithms. A useful model must fit the data accurately.
However, evaluations of model-accuracy of some of the models that are commonly
used in analyzing human white matter have not been published before. Here, we
evaluate model-accuracy of the two main classes of diffusion MRI models. The
diffusion tensor model (DTM) summarizes diffusion as a 3-dimensional Gaussian
distribution. Sparse fascicle models (SFM) summarize the signal as a linear sum
of signals originating from a collection of fascicles oriented in different
directions. We use cross-validation to assess model-accuracy at different
gradient amplitudes (b-values) throughout the white matter. Specifically, we
fit each model to all the white matter voxels in one data set and then use the
model to predict a second, independent data set. This is the first evaluation
of model-accuracy of these models. In most of the white matter the DTM predicts
the data more accurately than test-retest reliability; SFM model-accuracy is
higher than test-retest reliability and also higher than the DTM, particularly
for measurements with (a) a b-value above 1000 in locations containing fiber
crossings, and (b) in the regions of the brain surrounding the optic
radiations. The SFM also has better parameter-validity: it more accurately
estimates the fiber orientation distribution function (fODF) in each voxel,
which is useful for fiber tracking.

The production processes of proteins in prokaryotic cells are investigated.
Most of the mathematical models in the literature study the production of {\em
one} fixed type of proteins. When several classes of proteins are considered,
an important additional aspect has to be taken into account, the limited common
resources of the cell (polymerases and ribosomes) used by the production
process. Understanding the impact of this limitation is a key issue in this
domain. In this paper we focus on the allocation of ribosomes in the case of
the production of multiple proteins. The cytoplasm of the cell being a
disorganized medium subject to thermal noise, the protein production process
has an important stochastic component. For this reason, a Markovian model of
this process is introduced. Asymptotic results of the equilibrium are obtained
under a scaling procedure and a realistic biological assumption of saturation
of the ribosomes available in the cell. It is shown in particular that, in the
limit, the number of non-allocated ribosomes at equilibrium converges in
distribution to a Poisson distribution whose parameter satisfies a fixed point
equation. It is also shown that the production process of different types of
proteins can be seen as independent production processes but with modified
parameters.

It has been a common practice to place electrodes based on external
landmarks, rather than locating the appropriate organ first by imaging
technuiqes such as CT scan, ultrasound, etc. Therefore, aside from abiding the
cutaneous EGG (electrogastrography) electrodes placement rule, identification
of its waveform should be performed to ease the validation of one's EGG
recording method. This research focused on the assembly of EGG instrument, its
performance testing, and its usage on local white rabbit (O. cuniculus). A
total of 72 recordings obtained and processed. Data procession implies EGG
parameterization based on segmentation and time domain analysis. Therefore this
research gives an insight of an EGG recording method that could be applied on
another preclinical, veterinary, and even for clinical examination.

Many microbes associate with higher eukaryotes and impact their vitality. In
order to engineer microbiomes for host benefit, we must understand the rules of
community assembly and maintenence, which in large part, demands an
understanding of the direct interactions between community members. Toward this
end, we've developed a Poisson-multivariate normal hierarchical model to learn
direct interactions from the count-based output of standard metagenomics
sequencing experiments. Our model controls for confounding predictors at the
Poisson layer, and captures direct taxon-taxon interactions at the multivariate
normal layer using an $\ell_1$ penalized precision matrix. We show in a
synthetic experiment that our method handily outperforms state-of-the-art
methods such as SparCC and the graphical lasso (glasso). In a real, in planta
perturbation experiment of a nine member bacterial community, we show our
model, but not SparCC or glasso, correctly resolves a direct interaction
structure among three community members that associate with Arabidopsis
thaliana roots. We conclude that our method provides a structured, accurate,
and distributionally reasonable way of modeling correlated count based random
variables and capturing direct interactions among them.

EGG recordings performed on 13 local white rabbits (O. cuniculus) which
divided into 3 groups; acetosal 35 mg/kgBM receiver, reserpine 37.5 mg/kgBM
receiver, and control group. A total of 72 EGG recordings obtained from them,
which divided furthermore into 9 datasets based on prepandrial state,
postpandrial state, and post 1 hour drug administration state. EGG parameters
such as the number of cycle per minute ($cpm$), average voltage of action
potential segment ($\bar{V_a}$), root mean square voltage of action potential
segment ($A_{rms}$), root mean square voltage of all EGG segment ($V_{rms}$),
average period of action potential segment ($\bar{T_a}$), average period of
resting plateau ($\bar{T_i}$), average period difference among action potential
segment and resting plateau ($\bar{T_a - T_i}$), and dominant frequency ($f_d$)
are obtained. Insignificant difference of $f_d$ ($P$ = 0.9112993) and cpm ($P$
= 0.9382463) among 9 EGG datasets were found. These findings contrasted the
common practice of EGG assessment, which $f_d$ and $cpm$ are the main
parameters for diagnosis base. In other hand, significant difference between 9
EGG datasets found for $\bar{V_a}$, $A_{rms}$, and $V_{rms}$ parameter with $P$
= 0.0007346, 0.0039191, and 0.0000559 respectively. In conclusion, EGG
parameterization should not be limited to $f_d$ and $cpm$ only.

Pulse-type weakly electric fishes communicate through electrical discharges
with a stereotyped waveform, varying solely the interval between pulses
according to the information being transmitted. This simple codification
mechanism is similar to the one found in various known neuronal circuits, which
renders these animals as good models for the study of natural communication
systems, allowing experiments involving behavioral and neuroethological
aspects. Performing analysis of data collected from more than one freely
swimming fish is a challenge since the detected electric organ discharge (EOD)
patterns are dependent on each animal's position and orientation relative to
the electrodes. However, since each fish emits a characteristic EOD waveform,
computational tools can be employed to match each EOD to the respective fish.
In this paper we describe a computational method able to recognize fish EODs
from dyads using normalized feature vectors obtained by applying Fourier and
dual-tree complex wavelet packet transforms. We employ support vector machines
as classifiers, and a continuity constraint algorithm allows us to solve issues
caused by overlapping EODs and signal saturation. Extensive validation
procedures with Gymnotus sp. showed that EODs can be assigned correctly to each
fish with only two errors per million discharges.

Spaced seeds have been recently shown to not only detect more alignments, but
also to give a more accurate measure of phylogenetic distances (Boden et al.,
2013, Horwege et al., 2014, Leimeister et al., 2014), and to provide a lower
misclassification rate when used with Support Vector Machines (SVMs) (On-odera
and Shibuya, 2013), We confirm by independent experiments these two results,
and propose in this article to use a coverage criterion (Benson and Mak, 2008,
Martin, 2013, Martin and No{\'e}, 2014), to measure the seed efficiency in both
cases in order to design better seed patterns. We show first how this coverage
criterion can be directly measured by a full automaton-based approach. We then
illustrate how this criterion performs when compared with two other criteria
frequently used, namely the single-hit and multiple-hit criteria, through
correlation coefficients with the correct classification/the true distance. At
the end, for alignment-free distances, we propose an extension by adopting the
coverage criterion, show how it performs, and indicate how it can be
efficiently computed.

The purpose of this study was to enhance the existing time dependent flux
model for the transdermal iontophoretic transport of drugs. This study
evaluated the flux data as influenced by time and current density. In vitro
iontophoresis performed on the piglet (Sus scrofa) necropsy-taken medial scapha
pinneal skin that mounted in the U shaped sink chamber. Iontophoresis of
atenolol with a constant dose of 1000 ppm was implemented for 3 hours with
acceptor phase sampling every 30 minutes. Data were analised based on
exponential fitting of each current density value to produce a current density
dependent flux model. This model then combined with the time differential model
of flux to produce a flux model that takes account of both current density and
time.

Understanding historical trends in the epidemic of HIV is important for
assessing current and projecting future trends in prevalence, incidence and
mortality and for evaluating the impact and cost-effectiveness of control
measures. In generalized epidemics the available data are of variable quality
among countries and limited mainly to trends in the prevalence of HIV among
women attending ante-natal clinics. In concentrated epidemics one needs, at the
very least, time trends in the prevalence of HIV among different risk groups,
including intravenous drug users, men-who-have-sex-with-men, and commercial sex
workers as well as the size of each group and the degree of overlap between
them. Here we focus on the comparatively straight forward problems presented by
generalized epidemics. We fit data from Kenya to a susceptible-infected model
and then successively add structure to the model, drawing on our knowledge of
the natural history of HIV, to explore the effect that different structural
aspects of the model have on the fits and the projections.
  Both heterogeneity in risk and changes in behaviour over time are important
but easily confounded. Using a Weibull rather than exponential survival
function for people infected with HIV, in the absence of treatment, makes a
significant difference to the estimated trends in incidence and mortality and
to the projected trends. Allowing for population growth has a small effect on
the fits and the projections but is easy to include. Including details of the
demography adds substantially to the complexity of the model, increases the run
time by several orders of magnitude, but changes the fits and projections only
slightly and to an extent that is less than the uncertainty inherent in the
data. We make specific recommendations for the kind of model that would be
suitable for understanding and managing HIV epidemics in east and southern
Africa.

Systems that evolve over time and follow mathematical laws as they do so, are
called dynamical systems. Lymphocyte recovery and clinical outcomes in 41
allograft recipients conditioned using anti-thymocyte globulin (ATG) and 4.5
Gray total-body-irradiation were studied to determine if immune reconstitution
could be described as a dynamical system. Survival, relapse, and graft vs. host
disease (GVHD) were not significantly different in two cohorts of patients
receiving different doses of ATG. However, donor-derived CD3+ (ddCD3) cell
reconstitution was superior in the lower ATG dose cohort, and there were fewer
instances of donor lymphocyte infusion (DLI). Lymphoid recovery was plotted in
each individual over time and demonstrated one of three sigmoid growth
patterns; Pattern A (n=15), had rapid growth with high lymphocyte counts,
pattern B (n=14), slower growth with intermediate recovery and pattern C, poor
lymphocyte reconstitution (n=10). There was a significant association between
lymphocyte recovery patterns and both the rate of change of ddCD3 at day 30
post-SCT and the clinical outcomes. GVHD was observed more frequently with
pattern A; relapse and DLI more so with pattern C, with a consequent survival
advantage in patients with patterns A and B. We conclude that evaluating immune
reconstitution following SCT as a dynamical system may differentiate patients
at risk of adverse outcomes and allow early intervention to modulate that risk.

This paper presents a novel model for wine fermentation including a death
phase for yeast and the influence of oxygen on the process. A model for the
inclusion of the yeast dying phase is derived and compared to a model taken
from the literature. The modeling ability of the several models is analyzed by
comparing their simulation results.

Geometry of the metabolic trajectories is characteristic of the biological
response (Keun, Ebbels et al. 2004). Yet, due to unavoidable inter-individual
variations, the exact trajectories characterising the biological responses
differ. We examined whether the differences seen between metabolic trajectories
of a specific treatment, correspond to the variations seen in the other
biological manifestations of the same treatment. Differences in trajectories
were measured via alignment procedures which introduced and implemented in this
study. Our study revealed strong correlation between the scales of the aligned
trajectories of metabolic responses and the severity of the hepatocelluar
lesions induced after administration of hydrazine. Thus the results confirm
that aligned trajectories are characteristic of a specific treatment. They then
can be used for comparison with other treatment specific or unknown metabolic
trajectories and can have many metabonomic applications such as preclinical
toxicological screening

We use a large single particle tracking data set to analyze the short time
and small spatial scale motion of quantum dots labeling proteins in cell
membranes. Our analysis focuses on the jumps which are the changes in the
position of the quantum dots between frames in a movie of their motion.
Previously we have shown that the directions of the jumps are uniformly
distributed and the jump lengths can be characterized by a double power law
distribution.
  Here we show that the jumps over a small number of time steps can be
described by scalings of a {\em single} double power law distribution. This
provides additional strong evidence that the double power law provides an
accurate description of the fine scale motion. This more extensive analysis
provides strong evidence that the double power law is a novel stable
distribution for the motion. This analysis provides strong evidence that an
earlier result that the motion can be modeled as diffusion in a space of
fractional dimension roughly 3/2 is correct. The form of the power law
distribution quantifies the excess of short jumps in the data and provides an
accurate characterization of the fine scale diffusion and, in fact, this
distribution gives an accurate description of the jump lengths up to a few
hundred nanometers. Our results complement of the usual mean squared
displacement analysis used to study diffusion at larger scales where the
proteins are more likely to strongly interact with larger membrane structures.

A network is a set of nodes that are linked together by a set of edges.
Networks can represent any set of objects that have relations among themselves.
Communities are sets of nodes that are related in an important way, probably
sharing common properties and/or playing similar roles within a network. When
network analysis is applied to study the livestock movement patterns, the
epidemiological units of interest (farm premises, counties, states, countries,
etc.) are represented as nodes, and animal movements between the nodes are
represented as the edges of a network. Unraveling a network structure, and
hence the trade preferences and pathways, could be very useful to a researcher
or a decision-maker. We implemented a community detection algorithm to find
livestock communities that is consistent with the definition of a livestock
production zone, assuming that a community is a group of farm premises in which
an animal is more likely to stay during its life time than expected by chance.
We applied this algorithm to the network of within animal movements made inside
the State of Mato Grosso, for the year of 2007. This database holds information
about 87,899 premises and 521,431 movements throughout the year, totalizing
15,844,779 animals moved. The community detection algorithm achieved a network
partition that shows a clear geographical and commercial pattern, two crucial
features to preventive veterinary medicine applications, and also has a
meaningful interpretation in trade networks where links emerge from the choice
of trader nodes.

Objectives: In the United States, 25% of people with type 2 diabetes are
undiagnosed. Conventional screening models use limited demographic information
to assess risk. We evaluated whether electronic health record (EHR) phenotyping
could improve diabetes screening, even when records are incomplete and data are
not recorded systematically across patients and practice locations. Methods: In
this cross-sectional, retrospective study, data from 9,948 US patients between
2009 and 2012 were used to develop a pre-screening tool to predict current type
2 diabetes, using multivariate logistic regression. We compared (1) a full EHR
model containing prescribed medications, diagnoses, and traditional predictive
information, (2) a restricted EHR model where medication information was
removed, and (3) a conventional model containing only traditional predictive
information (BMI, age, gender, hypertensive and smoking status). We
additionally used a random-forests classification model to judge whether
including additional EHR information could increase the ability to detect
patients with Type 2 diabetes on new patient samples. Results: Using a
patient's full or restricted EHR to detect diabetes was superior to using basic
covariates alone (p<0.001). The random forests model replicated on out-of-bag
data. Migraines and cardiac dysrhythmias were negatively associated with type 2
diabetes, while acute bronchitis and herpes zoster were positively associated,
among other factors. Conclusions: EHR phenotyping resulted in markedly superior
detection of type 2 diabetes in a general US population, could increase the
efficiency and accuracy of disease screening, and are capable of picking up
signals in real-world records.

Spatial reaction-diffusion models have been employed to describe many
emergent phenomena in biological systems. The modelling technique most commonly
adopted in the literature implements systems of partial differential equations
(PDEs), which assumes there are sufficient densities of particles that a
continuum approximation is valid. However, due to recent advances in
computational power, the simulation, and therefore postulation, of
computationally intensive individual-based models has become a popular way to
investigate the effects of noise in reaction-diffusion systems in which regions
of low copy numbers exist.
  The stochastic models with which we shall be concerned in this manuscript are
referred to as `compartment-based'. These models are characterised by a
discretisation of the computational domain into a grid/lattice of
`compartments'. Within each compartment particles are assumed to be well-mixed
and are permitted to react with other particles within their compartment or to
transfer between neighbouring compartments.
  We develop two hybrid algorithms in which a PDE is coupled to a
compartment-based model. Rather than attempting to balance average fluxes, our
algorithms answer a more fundamental question: `how are individual particles
transported between the vastly different model descriptions?' First, we present
an algorithm derived by carefully re-defining the continuous PDE concentration
as a probability distribution. Whilst this first algorithm shows strong
convergence to analytic solutions of test problems, it can be cumbersome to
simulate. Our second algorithm is a simplified and more efficient
implementation of the first, it is derived in the continuum limit over the PDE
region alone. We test our hybrid methods for functionality and accuracy in a
variety of different scenarios by comparing the averaged simulations to
analytic solutions of PDEs for mean concentrations.

Recent studies have revealed that for the majority of species the length
distributions of duplicated sequences in natural DNA follow a power-law tail.
We study duplication-mutation models for processes in natural DNA sequences and
the length distributions of exact matches computed from both synthetic and
natural sequences. Here we present a hierarchy of equations for various number
of exact matches for these models. The reduction of these equations to one
equation for pairs of exact repeats is found. Quantitative correspondence of
solutions of the equation to simulations is demonstrated.

Models accounting for imperfect detection are important. Single-visit methods
have been proposed as an alternative to multiple-visits methods to relax the
assumption of closed population. Knape and Korner-Nievergelt (2015) showed that
under certain models of probability of detection single-visit methods are
statistically non-identifiable leading to biased population estimates. There is
a close relationship between estimation of the resource selection probability
function (RSPF) using weighted distributions and single-visit methods for
occupancy and abundance estimation. We explain the precise mathematical
conditions needed for RSPF estimation as stated in Lele and Keim (2006). The
identical conditions, that remained unstated in our papers on single-visit
methodology, are needed for single-visit methodology to work. We show that the
class of admissible models is quite broad and does not excessively restrict the
application of the RSPF or the single-visit methodology. To complement the work
by Knape and Korner-Nievergelt, we study the performance of multiple-visit
methods under the scaled logistic detection function and a much wider set of
situations. In general, under the scaled logistic detection function
multiple-visits methods also lead to biased estimates. As a solution to this
problem, we extend the single-visit methodology to a class of models that
allows use of scaled probability function. We propose a Multinomial extension
of single visit methodology that can be used to check whether the detection
function satisfies the RSPF condition or not. Furthermore, we show that if the
scaling factor depends on covariates, then it can also be estimated.

Auto-logistic and related auto-models, implemented approximately as
autocovariate regression, provide simple and direct modelling of spatial
dependence. The autologistic model has been widely applied in ecology since
Augustin, Mugglestone and Buckland (J. Appl. Ecol., 1996, 33, 339) analysed red
deer census data using a hybrid estimation approach, combining maximum
pseudo-likelihood estimation with Gibbs sampling of missing data. However
Dormann (Ecol. Model., 2007, 207, 234) questioned the validity of auto-logistic
regression, giving examples of apparent underestimation of covariate parameters
in analysis of simulated "snouter" data. Dormann et al. (Ecography, 2007, 30,
609) extended this analysis to auto-Poisson and auto-normal models, reporting
similar anomalies. All the above studies employ neighbourhood weighting schemes
inconsistent with conditions (Besag, J. R. Stat. Soc., Ser. B, 1974, 36, 192)
required for auto-model validity; furthermore the auto-Poisson analysis fails
to exclude cooperative interactions. We show that all "snouter" anomalies are
resolved by correct auto-model implementation. Re-analysis of the red deer data
shows that invalid neighbourhood weightings generate only small estimation
errors for the full dataset, but larger errors occur on geographic subsamples.
A substantial fraction of papers applying auto-logistic regression to
ecological data use these invalid weightings, which are default options in the
widely used "spdep" spatial dependence package for R. Auto-logistic analyses
using invalid neighbourhood weightings will be erroneous to an extent that can
vary widely. These analyses can easily be corrected by using valid
neighbourhood weightings available in "spdep". The hybrid estimation approach
for missing data is readily adapted for valid neighbourhood weighting schemes
and is implemented here in R for application to sparse presence-absence data.

The delay difference model was implemented to fit 21 years of brown tiger
prawn (Penaeus esculentus) catch in Moreton Bay by maximum likelihood to assess
the status of this stock. Monte Carlo simulations testing of the stock
assessment software coded in C++ showed that the model could estimate
simultaneously natural mortality in addition to catchability, recruitment and
initial biomasses. Applied to logbooks data collected from 1990 to 2010, this
implementation of the delay difference provided for the first time an estimate
of natural mortality for brown tiger prawn in Moreton Bay, equal to $0.031 \pm
0.002$ week$^{-1}$. This estimate is approximately 30\% lower than the value of
natural mortality (0.045 week$^{-1}$) used in previous stock assessments of
this species.

Individuals traversing challenging obstacles are faced with a decision: they
can adopt traversal strategies that minimally disrupt their normal locomotion
patterns or they can adopt strategies that substantially alter their gait,
conferring new advantages and disadvantages. We flew pigeons (Columba livia)
through an array of vertical obstacles in a flight arena, presenting them with
this choice. The pigeons selected either a strategy involving only a slight
pause in the normal wingbeat cycle, or a wings folded posture granting reduced
efficiency but greater stability should a misjudgment lead to collision. The
more stable but less efficient flight strategy was not employed to traverse
easy obstacles with wide gaps for passage, but came to dominate the postures
used as obstacle challenge increased with narrower gaps and there was a greater
chance of a collision. These results indicate that birds weigh potential
obstacle negotiation strategies and estimate task difficulty during locomotor
pattern selection.

Carey's Equality pertaining to stationary models is well known. In this
paper, we have stated and proved a fundamental theorem related to the formation
of this Equality. This theorem will provide an in-depth understanding of the
role of each captive subject, and their corresponding follow-up duration in a
stationary population. We have demonstrated a numerical example of a captive
cohort and the survival pattern of medfly populations. These results can be
adopted to understand age-structure and aging process in stationary and
non-stationary population population models. Key words: Captive cohort, life
expectancy, symmetric patterns.

Respiration measurements of whole tree plants have been reported that give
evidence that the relative per volume/mass unit respiration decreases with
increase of tree body size. In this study, based on the available data
published a question was explored if the relative per area unit respiration in
trees can be a constant, independent of the surface area size. There is a
definite gap in the published data when the allometric studies of tree body
structure do not intercept with studies on trees respiration. Thus the question
was studied with the help of indirect comparison between various data. The
comparison showed that the scaling exponents, volume vs. surface area and
respiration vs. stem volume, are slightly larger than they should be for the
hypothesis of the relative respiration constancy to hold. The data studied give
evidence that the relative per area unit respiration slightly increases with
the increase in tree surface area. Possible explanations of the relationship
include a different distribution of metabolically active parts of stem and
higher nitrogen content in larger trees. Also, the published datasets might
include large fast growing trees, which imply that larger trees grow faster and
hence have higher per unit surface area growth respiration. A crucial
experiment is required in which the respiration measurements were performed for
the same data as the measurements of scaling between stem volume and surface
area.

We describe the current state and future plans for a set of tools for
scientific data management (SDM) designed to support scientific transparency
and reproducible research. SDM has been in active use at our MRI Center for
more than two years. We designed the system to be used from the beginning of a
research project, which contrasts with conventional end-state databases that
accept data as a project concludes. A number of benefits accrue from using
scientific data management tools early and throughout the project, including
data integrity as well as reuse of the data and of computational methods.

Colony Collapse Disorder has become a global problem for beekeepers and for
the crops which depend on bee polination. Multiple factors are known to
increase the risk of colony colapse, and the ectoparasitic mite Varroa
destructor that parasitizes honey bees is among the main threats to colony
health. Although this mite is unlikely to, by itself, cause the collapse of
hives, it plays an important role as it is a vector for many viral diseases.
Such diseases are among the likely causes for Colony Collapse Disorder.
  The effects of V. destructor infestation are disparate in different parts of
the world. Greater morbidity - in the form of colony losses - has been reported
in colonies of European honey bees (EHB) in Europe, Asia and North America.
However, this mite has been present in Brasil for many years and yet there are
no reports of Africanized honey bee (AHB) colonies losses.
  Studies carried out in Mexico showed that some resistance behaviors to the
mite - especially grooming and hygienic behavior - appear to be different in
each subspecies. Could those difference in behaviors explain why the AHB are
less susceptible to Colony Collapse Disorder?
  In order to answer this question, we propose a mathematical model of the
coexistence dynamics of these two species, the bee and the mite, to analyze the
role of resistance behaviors in the overall health of the colony, and, as a
consequence, its ability to face epidemiological challenges.

The escape trajectories animals take following a predatory attack appear to
show high degrees of apparent 'randomness' - a property that has been described
as 'protean behaviour'. Here we present a method of quantifying the escape
trajectories of individual animals using a path complexity approach. When fish
(Pseudomugil signifer) were attacked either on their own or in groups, we find
that an individual's path rapidly increases in entropy (our measure of
complexity) following the attack. For individuals on their own, this entropy
remains elevated (indicating a more random path) for a sustained period (10
seconds) after the attack, whilst it falls more quickly for individuals in
groups. The entropy of the path is context dependent. When attacks towards
single fish come from greater distances, a fish's path shows less complexity
compared to attacks that come from short range. This context dependency effect
did not exist, however, when individuals were in groups. Nor did the path
complexity of individuals in groups depend on a fish's local density of
neighbours. We separate out the components of speed and direction changes to
determine which of these components contributes to the overall increase in path
complexity following an attack. We found that both speed and direction measures
contribute similarly to an individual's path's complexity in absolute terms.
Our work highlights the adaptive behavioural tactics that animals use to avoid
predators and also provides a novel method for quantifying the escape
trajectories of animals.

A total of 50 patients were enrolled in the study, and MRI brain with MR
spectroscopy was done. Tuberculosis was the most common neurologic disease
found in the HIV positive group, consisting of 9 patients. Seven of these
patients had tuberculous meningitis amongst which a further 2 had vasculitic
infarcts.PML was seen in 6 patients. NAA to Cr ratio was found to be reduced in
all the patients, and in fact the value was further reduced compared to the HIV
positive group as a whole. Raised choline and myoInositol peaks were also found
in all the patients. MR Spectroscopy showed lipid lactate peaks confirming the
diagnosis. 2 patients had HIV encephalopathy on the imaging study. Their
spectra also revealed lowered NAA peaks along with raised choline peaks. 2
patients with cryptococcosis showed characteristic imaging finding of enlarged
Virchow Robin (perivascular) spaces. They revealed elevated choline peaks in
addition to reduced NAA. The values of NAA to Cr ratio were determined after
duly processing the spectroscopic data from both cases and controls. Each group
(Cases and controls) were divided on the basis of age into two age groups:
Lesser than or equal to 40 years, and greater than 40 years. In all three
groups the values of the mean NAA to Cr ratio ratio was significantly (p-value
less than 0.05) reduced in comparison to controls. An ancillary finding was the
reduction of NAA to Cr ratio further in cases of PML. Combined use of both the
conventional and advanced MRI sequences is advisable as spectroscopy helps in
confirming the diagnosis of opportunistic infection of the CNS in HIV positive
patients. NAA to Cr ratio ratio is reduced in HIV positive patients and is a
marker for HIV infection of the brain even in the absence of imaging findings
of HIV encephalopathy or when the patient is symptomatic due to neurological
disease of other etiologies.

An analysis of breast cancer incidences in women and the relationship between
ethnicity and survival rate has been an ongoing study with recorded incidences
of missing values in the secondary data. In this paper, we study and report the
results of breast cancer survival rate by ethnicity, age and income groups from
the dataset collected for 53593 patients in South East England between the
years 1998 and 2003. In addition to this, we also predict the missing values
for the ethnic groups in the dataset. The principle findings in our study
suggest that: 1) women of white ethnicity in South East England have a highest
percentage of survival rate when compared to the black ethnicity, 2) High
income groups have higher survival rates to that of lower income groups and 3)
Age groups between 80-95 have lower percentage of survival rate.

We present a variant of the well sounded Expectation-Maximization Clustering
algorithm that is constrained to generate partitions of the input space into
high and low values. The motivation of splitting input variables into high and
low values is to favour the semantic interpretation of the final clustering.
The Expectation-Maximization binary Clustering is specially useful when a
bimodal conditional distribution of the variables is expected or at least when
a binary discretization of the input space is deemed meaningful. Furthermore,
the algorithm deals with the reliability of the input data such that the larger
their uncertainty the less their role in the final clustering. We show here its
suitability for behavioural annotation of movement trajectories. However, it
can be considered as a general purpose algorithm for the clustering or
segmentation of multivariate data or temporal series.

In metastatic castration-resistant prostate cancer (mCRPC) clinical trials,
the assessment of treatment efficacy essentially relies on the time-to-death
and the kinetics of prostate-specific antigen (PSA). Joint modelling has been
increasingly used to characterize the relationship between a time-to-event and
a biomarker kinetics but numerical difficulties often limit this approach to
linear models. Here we evaluated by simulation the capability of a new feature
of the Stochastic Approximation Expectation-Maximization algorithm in Monolix
to estimate the parameters of a joint model where PSA kinetics was defined by a
mechanistic nonlinear mixed-effect model. The design of the study and the
parameter values were inspired from one arm of a clinical trial. Increasingly
high levels of association between PSA and survival were considered and results
were compared with those found using two simplified alternatives to joint
model, a two-stage and a joint sequential model. We found that joint model
allowed for a precise estimation of all longitudinal and survival parameters.
In particular the effect of PSA kinetics on survival could be precisely
estimated, regardless of the strength of the association. In contrast, both
simplified approaches led to bias on longitudinal parameters and two-stage
model systematically underestimated the effect of PSA kinetics on survival. In
summary we showed that joint model can be used to characterize the relationship
between a nonlinear kinetics and survival. This opens the way for the use of
more complex and physiological models to improve treatment evaluation and
prediction in oncology.

Clinicians need to predict patient outcomes with high accuracy as early as
possible after disease inception. In this manuscript, we show that
patient-to-patient variability sets a fundamental limit on outcome prediction
accuracy for a general class of mathematical models for the immune response to
infection. However, accuracy can be increased at the expense of delayed
prognosis. We investigate several systems of ordinary differential equations
(ODEs) that model the host immune response to a pathogen load. Advantages of
systems of ODEs for investigating the immune response to infection include the
ability to collect data on large numbers of `virtual patients', each with a
given set of model parameters, and obtain many time points during the course of
the infection. We implement patient-to-patient variability $v$ in the ODE
models by randomly selecting the model parameters from Gaussian distributions
with variance $v$ that are centered on physiological values. We use logistic
regression with one-versus-all classification to predict the discrete
steady-state outcomes of the system. We find that the prediction algorithm
achieves near $100\%$ accuracy for $v=0$, and the accuracy decreases with
increasing $v$ for all ODE models studied. The fact that multiple steady-state
outcomes can be obtained for a given initial condition, i.e. the basins of
attraction overlap in the space of initial conditions, limits the prediction
accuracy for $v>0$. Increasing the elapsed time of the variables used to train
and test the classifier, increases the prediction accuracy, while adding
explicit external noise to the ODE models decreases the prediction accuracy.
Our results quantify the competition between early prognosis and high
prediction accuracy that is frequently encountered by clinicians.

Photoactivatable ribonucleoside-enhanced cross-linking and
immunoprecipitation (PAR-CLIP) is an experimental method based on
next-generation sequencing for identifying the RNA interaction sites of a given
protein. The method deliberately inserts T-to-C substitutions at the
RNA-protein interaction sites, which provides a second layer of evidence
compared to other CLIP methods. However, the experiment includes several
sources of noise which cause both low-frequency errors and spurious
high-frequency alterations. Therefore, rigorous statistical analysis is
required in order to separate true T-to-C base changes, following
cross-linking, from noise. So far, most of the existing PAR-CLIP data analysis
methods focus on discarding the low-frequency errors and rely on high-frequency
substitutions to report binding sites, not taking into account the possibility
of high-frequency false positive substitutions. Here, we introduce BMix, a new
probabilistic method which explicitly accounts for the sources of noise in PAR-
CLIP data and distinguishes cross-link induced T-to-C substitutions from low
and high-frequency erroneous alterations. We demonstrate the superior speed and
accuracy of our method compared to existing approaches on both simulated and
real, publicly available human datasets. The model is implemented in the Matlab
toolbox BMix, freely available at www.cbg.bsse.ethz.ch/software/BMix.

Diverse classes of proteins function through large-scale conformational
changes; sophisticated enhanced sampling methods have been proposed to generate
these macromolecular transition paths. As such paths are curves in a
high-dimensional space, they have been difficult to compare quantitatively, a
prerequisite to, for instance, assess the quality of different sampling
algorithms. The Path Similarity Analysis (PSA) approach alleviates these
difficulties by utilizing the full information in 3N-dimensional trajectories
in configuration space. PSA employs the Hausdorff or Fr\'echet path
metrics---adopted from computational geometry---enabling us to quantify path
(dis)similarity, while the new concept of a Hausdorff-pair map permits the
extraction of atomic-scale determinants responsible for path differences.
Combined with clustering techniques, PSA facilitates the comparison of many
paths, including collections of transition ensembles. We use the closed-to-open
transition of the enzyme adenylate kinase (AdK)---a commonly used testbed for
the assessment enhanced sampling algorithms---to examine multiple microsecond
equilibrium molecular dynamics (MD) transitions of AdK in its substrate-free
form alongside transition ensembles from the MD-based dynamic importance
sampling (DIMS-MD) and targeted MD (TMD) methods, and a geometrical targeting
algorithm (FRODA). A Hausdorff pairs analysis of these ensembles revealed, for
instance, that differences in DIMS-MD and FRODA paths were mediated by a set of
conserved salt bridges whose charge-charge interactions are fully modeled in
DIMS-MD but not in FRODA. We also demonstrate how existing trajectory analysis
methods relying on pre-defined collective variables, such as native contacts or
geometric quantities, can be used synergistically with PSA, as well as the
application of PSA to more complex systems such as membrane transporter
proteins.

In this paper we consider a model based on branching process theory for the
proliferation and the dissemination network of T cells in the adaptive immune
response. A multi-type Galton Watson branching process is assumed as the basic
proliferation mechanism, associated to the migration of T cells of the
different generations from the draining lymph node to the spleen and other
lymphoid organs. Time recursion equations for the mean values and the
covariance matrices of the the cell population counts are derived in all the
compartments of the network model. Moreover, a normal approximation of the
log-likelihood function of the cell relative frequencies is derived, which
allows one to obtain estimates of both the probability parameters of the
branching process and the migration rates in the various compartments of the
network.

Unicellular organisms are open metabolic systems that need to process
information about their external environment in order to survive. In most types
of tissues and organisms, cells use calcium signaling to carry information from
the extracellular side of the plasma membrane to the different metabolic
targets of their internal medium. This information might be encoded in the
amplitude, frequency, duration, waveform or timing of the calcium oscillations.
Thus, specific information coming from extracellular stimuli can be encoded in
the calcium signal and decoded again later in different locations within the
cell. Despite its cellular importance, little is known about the quantitative
informative properties of the calcium concentration dynamics inside the cell.
In order to understand some of these informational properties, we have studied
experimental Ca2+ series of Xenopus laevis oocytes under different external pH
stimulus. The data has been analyzed by means of information-theoretic
approaches such as Conditional Entropy, Information Retention, and other
non-linear dynamics tools such as the power spectra, the Largest Lyapunov
exponent and the bridge detrended Scaled Window Variance analysis. We have
quantified the biomolecular information flows of the experimental data in bits,
and essential aspects of the information contained in the experimental calcium
fluxes have been exhaustively analyzed. Our main result shows that inside all
the studied intracellular Ca2+ flows a highly organized informational structure
emerge, which exhibit deterministic chaotic behavior, long term memory and
complex oscillations of the uncertainty reduction based on past values. The
understanding of the informational properties of calcium signals is one of the
key elements to elucidate the physiological functional coupling of the cell and
the integrative dynamics of cellular life.

NeXML is a powerful and extensible exchange standard recently proposed to
better meet the expanding needs for phylogenetic data and metadata sharing.
Here we present the RNeXML package, which provides users of the R programming
language with easy-to-use tools for reading and writing NeXML documents,
including rich metadata, in a way that interfaces seamlessly with the extensive
library of phylogenetic tools already available in the R ecosystem.

The success of metabolomics studies depends upon the "fitness" of each
biological sample used for analysis: it is critical that metabolite levels
reported for a biological sample represent an accurate snapshot of the studied
organism's metabolite profile at time of sample collection. Numerous factors
may compromise metabolite sample fitness, including chemical and biological
factors which intervene during sample collection, handling, storage, and
preparation for analysis. We propose a probabilistic model for the quantitative
assessment of metabolite sample fitness. Collection and processing of nuclear
magnetic resonance (NMR) and ultra-performance liquid chromatography (UPLC-MS)
metabolomics data is discussed. Feature selection methods utilized for
multivariate data analysis are briefly reviewed, including feature clustering
and computation of latent vectors using spectral methods. We propose that the
time-course of metabolite changes in samples stored at different temperatures
may be utilized to identify changing-metabolite-to-stable-metabolite ratios as
markers of sample fitness. Tolerance intervals may be computed to characterize
these ratios among fresh samples. In order to discover additional structure in
the data relevant to sample fitness, we propose using data labeled according to
these ratios to train a Dirichlet process mixture model (DPMM) for assessing
sample fitness. DPMMs are highly intuitive since they model the metabolite
levels in a sample as arising from a combination of processes including, e.g.,
normal biological processes and degradation- or contamination-inducing
processes. The outputs of a DPMM are probabilities that a sample is associated
with a given process, and these probabilities may be incorporated into a final
classifier for sample fitness.

Engineering genetic networks to be both predictable and robust is a key
challenge in synthetic biology. Synthetic circuits must reliably function in
dynamic, stochastic and heterogeneous environments, and simple circuits can be
studied to refine complex gene-regulation models. Although robust behaviours
such as genetic oscillators have been designed and implemented in prokaryotic
and eukaryotic organisms, a priori genetic engineering of even simple networks
remains difficult, and many aspects of cell and molecular biology critical to
engineering robust networks are still inadequately characterized. Particularly,
periodic processes such as gene doubling and cell division are rarely
considered in gene regulatory models, which may become more important as
synthetic biologists utilize new tools for chromosome integration. We studied a
chromosome-integrated, negative-feedback circuit based upon the bacteriophage
{\lambda} transcriptional repressor Cro and observed strong, feedback-dependent
oscillations in single-cell time traces. This finding was surprising due to a
lack of cooperativity, long delays or fast protein degradation. We further show
that oscillations are synchronized to the cell cycle by gene duplication, with
phase shifts predictably correlating with estimated gene doubling times.
Furthermore, we characterized the influence of negative feedback on the
magnitude and dynamics of noise in gene expression. Our results show that
cell-cycle effects must be accounted for in accurate, predictive models for
even simple gene circuits. Cell-cycle-periodic expression of {\lambda} Cro also
suggests an explanation for cell-size dependence in lysis probability and an
evolutionary basis for site-specific {\lambda} integration.

Tomasetti and Vogelstein recently proposed that the majority of variation in
cancer risk among tissues is due to "bad luck," that is, random mutations
arising during DNA replication in normal noncancerous stem cells. They
generalize this finding to cancer overall, claiming that "the stochastic
effects of DNA replication appear to be the major contributor to cancer in
humans." We show that this conclusion results from a logical fallacy based on
ignoring the influence of population heterogeneity in correlations exhibited at
the level of the whole population. Because environmental and genetic factors
cannot explain the huge differences in cancer rates between different organs,
it is wrong to conclude that these factors play a minor role in cancer rates.
In contrast, we show that one can indeed measure huge differences in cancer
rates between different organs and, at the same time, observe a strong effect
of environmental and genetic factors in cancer rates.

Diffusive transport is a universal phenomenon, throughout both biological and
physical sciences, and models of diffusion are routinely used to interrogate
diffusion-driven processes. However, most models neglect to take into account
the role of volume exclusion, which can significantly alter diffusive
transport, particularly within biological systems where the diffusing particles
might occupy a significant fraction of the available space. In this work we use
a random walk approach to provide a means to reconcile models that incorporate
crowding effects on different spatial scales. Our work demonstrates that
coarse-grained models incorporating simplified descriptions of excluded volume
can be used in many circumstances, but that care must be taken in pushing the
coarse-graining process too far.

Automated analysis of imaged phenotypes enables fast and reproducible
quantification of biologically relevant features. Despite recent developments,
recordings of complex, networked structures, such as: leaf venation patterns,
cytoskeletal structures, or traffic networks, remain challenging to analyze.
Here we illustrate the applicability of img2net to automatedly analyze such
structures by reconstructing the underlying network, computing relevant network
properties, and statistically comparing networks of different types or under
different conditions. The software can be readily used for analyzing image data
of arbitrary 2D and 3D network-like structures. img2net is open-source software
under the GPL and can be downloaded from
http://mathbiol.mpimp-golm.mpg.de/img2net/, where supplementary information and
data sets for testing are provided.

The leaves of angiosperms contain highly complex venation networks consisting
of recursively nested, hierarchically organized loops. We describe a new
phenotypic trait of reticulate vascular networks based on the topology of the
nested loops. This phenotypic trait encodes information orthogonal to widely
used geometric phenotypic traits, and thus constitutes a new dimension in the
leaf venation phenotypic space. We apply our metric to a database of 186 leaves
and leaflets representing 137 species, predominantly from the Burseraceae
family, revealing diverse topological network traits even within this single
family. We show that topological information significantly improves
identification of leaves from fragments by calculating a "leaf venation
fingerprint" from topology and geometry. Further, we present a phenomenological
model suggesting that the topological traits can be explained by noise effects
unique to specimen during development of each leaf which leave their imprint on
the final network. This work opens the path to new quantitative identification
techniques for leaves which go beyond simple geometric traits such as vein
density and is directly applicable to other planar or sub-planar networks such
as blood vessels in the brain.

The origin of allometric scaling patterns that are multiples of 1/4 has long
fascinated biologists. While not universal, scaling relationships with
exponents that are close to multiples of 1/4 are common and have been described
in all major clades. Foremost among these relationships is the 3/4 scaling of
metabolism with mass which underpins the 1/4 power dependence of biological
rates and times. Several models have been advanced to explain the underlying
mechanistic drivers of such patterns, but questions regarding a disconnect
between model structures and empirical data have limited their widespread
acceptance. Notable among these is a fractal branching model which predicts
power law scaling of both metabolism and physical dimensions. While a power law
is a useful first approximation to many datasets, non-linearity in some large
data compilations suggest the possibility of more complex or alternative
mechanisms. Here, we first show that quarter power scaling can be derived using
only the preservation of volume flow rate and velocity as model constraints.
Applying our model to the specific case of land plants, we show that
incorporating biomechanical principles and allowing different parts of plant
branching networks to be optimized to serve different functions predicts
non-linearity in allometric relationships, and helps explain why interspecific
scaling exponents covary along a fractal continuum. We also demonstrate that
while branching may be a stochastic process, due to the conservation of volume,
data may still be consistent with the expectations for a fractal network when
one examines subtrees within a tree. Data from numerous sources at the level of
plant shoots, stems, petioles, and leaves show strong agreement with our model
predictions. This novel theoretical framework provides an easily testable
alternative to current general models of plant metabolic allometry.

Multiview light sheet fluorescence microscopy (LSFM) allows to image
developing organisms in 3D at unprecedented temporal resolution over long
periods of time. The resulting massive amounts of raw image data requires
extensive processing interactively via dedicated graphical user interface (GUI)
applications. The consecutive processing steps can be easily automated and the
individual time points can be processed independently, which lends itself to
trivial parallelization on a high performance cluster (HPC). Here we introduce
an automated workflow for processing large multiview, multi-channel,
multi-illumination time-lapse LSFM data on a single workstation or in parallel
on a HPC. The pipeline relies on snakemake to resolve dependencies among
consecutive processing steps and can be easily adapted to any cluster
environment for processing LSFM data in a fraction of the time required to
collect it.

In medical sciences, a biomarker is "a characteristic that is objectively
measured and evaluated as an indicator of normal biological processes,
pathogenic processes, or pharmacologic responses to a therapeutic
intervention". Molecular experiments are providing rapid and systematic
approaches to search for biomarkers, but because single-molecule biomarkers
have shown a disappointing lack of robustness for clinical diagnosis,
researchers have begun searching for distinctive sets of molecules, called
"biosignatures". However, the most popular statistics are not appropriate for
their identification, and the number of possible biosignatures to be tested is
frequently intractable. In the present work, we developed a "multivariate
filter" using genetic algorithms (GA) as a feature (gene) selector to optimize
a measure of intra-group cohesion and inter-group dispersion. This method was
implemented using Python and R (pyBioSig, available at
https://github.com/fredgca/pybiosig under LGPL) and can be manipulated via
graphical interface or Python scripts. Using it, we were able to identify
putative biosignatures composed by just a few genes and capable of recovering
multiple groups simultaneously in a hierarchical clustering, even ones that
were not recovered using the whole transcriptome, within a feasible length of
time using a personal computer. Our results allowed us to conclude that using
GA to optimize our new intra-group cohesion and inter-group dispersion measure
is a clear, effective, and computationally feasible strategy for the
identification of putative "omical" biosignatures that could support
discrimination among multiple groups simultaneously.

Social foraging shows unexpected features such as the existence of a group
size threshold to accomplish a successful hunt. Above this threshold,
additional individuals do not increase the probability of capturing the prey.
Recent direct observations of wolves in Yellowstone Park show that the group
size threshold when hunting its most formidable prey, bison, is nearly three
times greater than when hunting elk, a prey that is considerably less
challenging to capture than bison. These observations provide empirical support
to a computational particle model of group hunting which was previously shown
to be effective in explaining why hunting success peaks at apparently small
pack sizes when hunting elk. The model is based on considering two critical
distances between wolves and prey: the minimal safe distance at which wolves
stand from the prey, and the avoidance distance at which wolves move away from
each other when they approach the prey. The minimal safe distance is longer
when the prey is more dangerous to hunt. We show that the model explains
effectively that the group size threshold is greater when the minimal safe
distance is longer. Although both distances are longer when the prey is more
dangerous, they contribute oppositely to the value of the group size threshold:
the group size threshold is smaller when the avoidance distance is longer. This
unexpected mechanism gives rise to a global increase of the group size
threshold when considering bison with respect to elk, but other prey more
dangerous than elk can lead to specific critical distances that can give rise
to the same group size threshold. Our results show that the computational model
can guide further research on group size effects, suggesting that more
experimental observations should be obtained for other kind of prey as e.g.
moose.

Land cover has been evaluated and classified on the basis of general features
using reflectance or digital levels of photographic or satellite data. One of
the most common methodologies based on CORINE land cover (Coordination of
Information on the Environment) data, which classifies natural cover according
to a small number of categories. This method produces generalizations about the
inventoried areas, resulting in the loss of important floristic and structural
information about vegetation types present (such as palm groves, tall dense
mangroves, and dense forests). This classification forfeits relevant
information on sites with high heterogeneity and diversity. Especially in the
tropics, simplification of coverage types reaches its maximum level with the
use of deforestation analysis, particularly when it is reduced to the two
classes of forests and nonforests. As this paper demonstrates, these results
have considerable consequences for political efforts to conserve the
biodiversity of megadiverse countries. We designed a new methodological
approach that incorporates biological distinctiveness combined with
phytosociological classification of vegetation and its relation to physical
features. This approach is based on parameters obtained through canonical
correspondence analysis on a fuzzy logic model, which are used to construct
multiple coverage maps. This tool is useful for monitoring and analyzing
vegetation dynamics, since it maintains the typological integrity of a
cartographic series. The methodology creates cartographic series congruent in
time and scale, can be applied to multiple and varied satellite inputs, and
always evaluates the same model parameters. We tested this new method in the
southwestern Colombian Caribbean region and compared our results with those
from what we believe are outdated tools used in other analyses of deforestation
around the world.

A nondestructive method for estimating the amount of carbon stored by
individuals, communities, vegetation types, and coverages, as well as their
volume and aboveground biomass, is presented. This methodology is based on
information on carbon stocks obtained through three-dimensional analysis of
tree architecture and artificial neural networks. This technique accurately
incorporates the diversity of plant forms measured in plots, transects, and
relev\'es. Stored carbon in any vegetation type is usually calculated as half
the biomass of sampled individuals, estimated with allometric formulas. The
most complete of these formulas incorporate diameter, height, and specific
gravity of wood but do not consider the variation in carbon stored in different
organs or different species, nor do they include information on the wide array
of architectures present in different plant communities. To develop these
allometric models, many individuals of different species must be sacrificed to
identify and validate samples and to minimize error. It is common to find
cutting-edge studies that encourage logging to improve estimates of carbon. In
our approach we replace this destructive methodology with a new technique for
quantifying global aboveground carbon. We demonstrate that carbon content in
forest aboveground biomass in the pantropics could rise to 723.97 Pg C. This
study shows that a reevaluation of climatic and ecological models is needed to
move toward a better understanding of the adverse effects of climate change,
deforestation, and degradation of tropical vegetation.

The asymmetric simple exclusion process (ASEP) is an important model from
statistical physics describing particles that hop randomly from one site to the
next along an ordered lattice of sites, but only if the next site is empty.
ASEP has been used to model and analyze numerous multiagent systems with local
interactions including the flow of ribosomes along the mRNA strand.
  In ASEP with periodic boundary conditions a particle that hops from the last
site returns to the first one. The mean field approximation of this model is
referred to as the ribosome flow model on a ring (RFMR). The RFMR may be used
to model both synthetic and endogenous gene expression regimes.
  We analyze the RFMR using the theory of monotone dynamical systems. We show
that it admits a continuum of equilibrium points and that every trajectory
converges to an equilibrium point. Furthermore, we show that it entrains to
periodic transition rates between the sites. We describe the implications of
the analysis results to understanding and engineering cyclic mRNA translation
in-vitro and in-vivo.

In many chemical and biological applications, systems of differential
equations containing unknown parameters are used to explain empirical
observations and experimental data. The DEs are typically nonlinear and
difficult to analyze, requiring numerical methods to approximate the solutions.
Compounding this difficulty are the unknown parameters in the DE system, which
must be given specific numerical values in order for simulations to be run.
  Estrogen receptor protein dimerization is used as an example to demonstrate
model construction, reduction, simulation, and parameter estimation.
Mathematical, computational, and statistical methods are applied to empirical
data to deduce kinetic parameter estimates and guide decisions regarding future
experiments and modeling. The process demonstrated serves as a pedagogical
example of quantitative methods being used to extract parameter values from
biochemical data models.

The promise of extracting connectomes and performing useful analysis on large
electron microscopy (EM) datasets has been an elusive dream for many years.
Tracing in even the smallest portions of neuropil requires copious human
annotation, the rate-limiting step for generating a connectome. While a
combination of improved imaging and automatic segmentation will lead to the
analysis of increasingly large volumes, machines still fail to reach the
quality of human tracers. Unfortunately, small errors in image segmentation can
lead to catastrophic distortions of the connectome.
  In this paper, to analyze very large datasets, we explore different
mechanisms that are less sensitive to errors in automation. Namely, we advocate
and deploy extensive synapse detection on the entire antennal lobe (AL)
neuropil in the brain of the fruit fly Drosophila, a region much larger than
any densely annotated to date. The resulting synapse point cloud produced is
invaluable for determining compartment boundaries in the AL and choosing
specific regions for subsequent analysis. We introduce our methodology in this
paper for region selection and show both manual and automatic synapse
annotation results. Finally, we note the correspondence between image datasets
obtained using the synaptic marker, antibody nc82, and our datasets enabling
registration between light and EM image modalities.

This article details the results of analyses we conducted on the discourse of
schizophrenic patients, at the oral production (disfluences) and lexical
(part-of-speech and lemmas) levels. This study is part of a larger project,
which includes other levels of analyses (syntax and discourse). The obtained
results should help us rebut or identify new linguistic evidence participating
in the manifestation of a dysfunction at these different levels. The corpus
contains more than 375,000 words, its analysis therefore required that we use
Natural Language Processing (NLP) and lexicometric tools. In particular, we
processed disfluencies and parts-of-speech separately, which allowed us to
demonstrate that if schizophrenic patients do produce more disfluencies than
control, their lexical richness is not significatively different.

There is an urgent need for economical blood based, noninvasive molecular
biomarkers to assist in the detection and diagnosis of cancers in a cost
effective manner at an early stage, when curative interventions are still
possible. Serum autoantibodies are attractive biomarkers for early cancer
detection, but their development has been hindered by the punctuated genetic
nature of the ten million known cancer mutations. A recent study of 50,000
patients (Pedersen et al., 2013) showed p53 15mer epitopes are much more
sensitive colon cancer biomarkers than p53, which in turn is a more sensitive
cancer biomarker than any other protein. The function of p53 as a nearly
universal tumor suppressor is well established, because of its strong
immunogenicity in terms of not only antibody recruitment, but also stimulation
of autoantibodies. Here we examine bioinformatic fractal scaling analysis for
identifying sensitive epitopes from the p53 amino acid sequence, and show how
it could be used for early cancer detection (ECD). We trim 15mers to 7mers, and
identify specific 7mers from other species that could be more sensitive to
aggressive human cancers, such as liver cancer.

We present a meta-analysis of independent studies on the potential
implication in the occurrence of coronary heart disease (CHD) of the
single-nucleotide polymorphism (SNP) at the -308 position of the tumor necrosis
factor alpha (TNF-alpha) gene. We use Bayesian analysis to integrate
independent data sets and to infer statistically robust measurements of
correlation. Bayesian hypothesis testing indicates that there is no preference
for the hypothesis that the -308 TNF-alpha SNP is related to the occurrence of
CHD, in the Caucasian or in the Asian population, over the null hypothesis. As
a measure of correlation, we use the probability of occurrence of CHD
conditional on the presence of the SNP, derived as the posterior probability of
the Bayesian meta-analysis. The conditional probability indicates that CHD is
not more likely to occur when the SNP is present, which suggests that the -308
TNF-alpha SNP is not implicated in the occurrence of CHD.

Alisporivir is a cyclophilin inhibitor with demonstrated in vitro and in vivo
activity against hepatitis C 11 virus (HCV). We estimated antiviral
effectiveness of alisporivir alone or in combination with 12
pegylated-Inteferon (peg-IFN) in 88 patients infected with different HCV
genotypes treated for four 13 weeks. The pharmacokinetics of both drugs were
modeled and used as driving functions for the viral 14 kinetic model. Genotype
was found to significantly affect pegylated-Inteferon effectiveness
($\epsilon$= 86.3% 15 and 99.1% in genotype-1/4 and genotype-2/3, respectively,
p\textless{}10 -7) and infected cells loss rate ($\delta$= 16 0.22 vs 0.39 day
-1 in genotype-1/4 and genotype-2/3, respectively, p\textless{}10 -6).
Alisporivir effectiveness 17 was not significantly different across genotype
and was high for doses $\ge$600 mg QD. We simulated 18 virologic responses with
other alisporivir dosing regimens in HCV genotype-2/3 patients using the 19
model. Our predictions consistently matched the observed responses,
demonstrating that this model 20 could be a useful tool for anticipating
virologic response and optimize alisporivir-based therapies.

We develop a simple three compartment model based on mass balance equations
which quantitatively describes the dynamics of breath methane concentration
profiles during exercise on an ergometer. With the help of this model it is
possible to estimate the endogenous production rate of methane in the large
intestine by measuring breath gas concentrations of methane.

We present a simulation algorithm that accurately propagates a molecule pair
using large time steps without the need to invoke the full exact analytical
solutions of the Smoluchowski diffusion equation. Because the proposed method
only uses uniform and Gaussian random numbers, it allows for position updates
that are two to three orders of magnitude faster than those of a corresponding
scheme based on full solutions, while mantaining the same degree of accuracy.
Neither simplifying nor ad hoc assumptions that are foreign to the underlying
Smoluchowski theory are employed, instead, the algorithm faithfully
incorporates the individual elements of the theoretical model. The method is
flexible and applicable in 1, 2 and 3 dimensions, suggesting that it may find
broad usage in various stochastic simulation algorithms. We demonstrate the
algorithm for the case of a non-reactive, irreversible and reversible reacting
molecule pair.

In the analysis of frozen hydrated biomolecules by single-particle
cryo-electron microscopy, template-based particle picking by a target function
called fast local correlation (FLC) allows a large number of particle images to
be automatically picked from micrographs. A second, independent target function
based on maximum likelihood (ML) can be used to align the images and verify the
presence of signal in the picked particles. Although the paradigm of this
dual-target-function (DTF) evaluation of single-particle selection has been
practiced in recent years, it remains unclear how the performance of this DTF
approach is affected by the signal-to-noise ratio of the images and by the
choice of references for FLC and ML. Here we examine this problem through a
systematic study of simulated data, followed by experimental substantiation. We
quantitatively pinpoint the critical signal-to-noise ratio (SNR), at which the
DTF approach starts losing its ability to select and verify particles from
cryo-EM micrographs. A Gaussian model is shown to be as effective in picking
particles as a single projection view of the imaged molecule in the tested
cases. For both simulated micrographs and real cryo-EM data of the 173-kDa
glucose isomerase complex, we found that the use of a Gaussian model to
initialize the target functions suppressed the detrimental effect of reference
bias in template-based particle selection. Given a sufficient signal-to-noise
ratio in the images and the appropriate choice of references, the DTF approach
can expedite the automated assembly of single-particle data sets.

While the use of technology to provide accurate and objective measurements of
human movement performance is presently an area of great interest, efforts to
quantify the performance of movement are hampered by the lack of a principled
model that describes how a subject goes about making a movement. We put forward
a principled mathematical formalism that describes human movements using an
optimal control model in which the subject controls the jerk of the movement.
We construct the formalism by assuming that the movement a subject chooses to
make is better than the alternatives. We quantify the relative quality of
movements mathematically by specifying a cost functional that assigns a
numerical value to every possible movement; the subject makes the movement that
minimizes the cost functional. We develop the mathematical structure of
movements that minimize a cost functional, and observe that this development
parallels the development of analytical mechanics from the Principle of Least
Action. We derive a constant of the motion for human movements that plays a
role that is analogous to the role that the energy plays in classical
mechanics. We apply the formalism to the description of two movements: (1)
rapid, targeted movements of a computer mouse, and (2) finger-tapping, and show
that the constant of the motion that we have derived provides a useful value
with which we can characterize the performance of the movements. In the case of
rapid, targeted movements of a computer mouse, we show how the model of human
movement that we have developed can be made to agree with Fitts' law, and we
show how Fitts' law is related to the constant of the motion that we have
derived. We finally show that solutions exist within the model of human
movements that exhibit an oscillatory character reminiscent of tremor.

The widening gap between known proteins and their functions has encouraged
the development of methods to automatically infer annotations. Automatic
functional annotation of proteins is expected to meet the conflicting
requirements of maximizing annotation coverage, while minimizing erroneous
functional assignments. This trade-off imposes a great challenge in designing
intelligent systems to tackle the problem of automatic protein annotation. In
this work, we present a system that utilizes rule mining techniques to predict
metabolic pathways in prokaryotes. The resulting knowledge represents
predictive models that assign pathway involvement to UniProtKB entries. We
carried out an evaluation study of our system performance using
cross-validation technique. We found that it achieved very promising results in
pathway identification with an F1-measure of 0.982 and an AUC of 0.987. Our
prediction models were then successfully applied to 6.2 million
UniProtKB/TrEMBL reference proteome entries of prokaryotes. As a result,
663,724 entries were covered, where 436,510 of them lacked any previous pathway
annotations.

Partial differential equations are a convenient way to describe reaction-
advection-diffusion processes of signalling models. If only one cell type is
present, and tissue dynamics can be neglected, the equations can be solved
directly. However, in case of multiple cell types it is not always straight
forward to integrate a continuous description of the tissue dynamics. Here, we
discuss (delayed) differentiation of cells into different cell types and
hypertrophic cell volume change upon differentiation.

Allosteric (long-range) interactions can be surprisingly strong in proteins
of biomedical interest. Here we use bioinformatic scaling to connect prior
results on nonsteroidal anti-inflammatory drugs to promising new drugs that
inhibit cancer cell metabolism. Many parallel features are apparent, which
explain how even one amino acid mutation, remote from active sites, can alter
medical results. The enzyme twins involved are cyclooxygenase (aspirin) and
isocitrate dehydrogenase (IDH). The IDH results are accurate to 1% and are
overdetermined by adjusting a single bioinformatic scaling parameter. It
appears that the final stage in optimizing protein functionality may involve
leveling of the hydrophobic cutoffs of the arms of conformational hydrophilic
hinges.

The risk of false positive results in noninvasive prenatal diagnosis focused
on fetal gender and RhD status determination could be a problem in clinical
routine. This is because these tests are based on detection of presence of DNA
sequences with high population frequency and so there is the risk of sample
contamination during sample collection and processing. In our study the
different fragmentation of fetal and maternal DNA molecules present in maternal
circulation was utilized in identification of contaminated samples.
Amplification of Y-chromosome specific assays different in size was tested on
circulating DNA samples.
  Of the four tested assays two shorter (84 and 177 bp) showed expected qPCR
efficiency and have comparable amplification profiles. The difference in Ct
values between these two assays was found to be statistically significant in
comparison of fetal male and normal male samples (p<0.0001) as well as in
blinded pilot study performed on 10 artificially contaminated and 10
non-contaminated samples (F=34.4, p<0.0001) that were all identified correctly.
  Our results showed that differently sized assays performed well in detection
of external contamination of samples in noninvasive prenatal fetal gender test
and could be of help in clinical laboratories to minimize the risk of false
positive results.

Urban vegetation is of key importance because a large proportion of the human
population lives in cities. Nevertheless, urban vegetation is understudied
outside central Europe and particularly, little is known about the flora of
tropical Asian, African and Latin American cities. We present an estimate of
how the vegetation has changed in the city of San Jos\'e, Costa Rica, after
about one century, with the repeat photography technique (based on a collection
of 19th and early 20th century photographs by Jos\'e Fidel Trist\'an and
others) and with data from the Costa Rican National Herbarium. We found little
vegetation change in the landscape of San Jos\'e during the 20th century, where
a total of 95 families and 458 species were collected in the late 19th and
early 20th century. The families with most species were Asteraceae, Fabaceae,
Poaceae, Lamiaceae, Euphorbiaceae, Solanaceae, Cyperaceae, Acanthaceae,
Malvaceae, Piperaceae and Verbenaceae. Similar results have been found in
Europe, where the number of plant species often is stable for long periods even
when the individual species vary.

Lichens are good bio-indicators of air pollution, but in most tropical
countries there are few studies on the subject; however, in the city of San
Jos\'e, Costa Rica, the relationship between air pollution and lichens has been
studied for decades. In this article we evaluate the hypothesis that air
pollution is lower where the wind enters the urban area (Northeast) and higher
where it exits San Jos\'e (Southwest). We identified the urban parks with a
minimum area of approximately 5 000m2 and randomly selected a sample of 40
parks located along the passage of wind through the city. To measure lichen
coverage, we applied a previously validated 10 x 20cm template with 50 random
points to five trees per park (1.5m above ground, to the side with most
lichens). Our results (years 2008 and 2009) fully agree with the generally
accepted view that lichens reflect air pollution carried by circulating air
masses. The practical implication is that the air enters the city relatively
clean by the semi-rural and economically middle class area of Coronado, and
leaves through the developed neighborhoods of Escaz\'u and Santa Ana with a
significant amount of pollutants. In the dry season, the live lichen coverage
of this tropical city was lower than in the May to December rainy season, a
pattern that contrasts with temperate habitats; but regardless of the season,
pollution follows the pattern of wind movement through the city.

Impedance Spectroscopy resolves electrical properties into uncorrelated
variables, as a function of frequency, with exquisite resolution. Separation is
robust and most useful when the system is linear. Impedance spectroscopy
combined with appropriate structural knowledge provides insight into pathways
for current flow, with more success than other methods. Biological applications
of impedance spectroscopy are often not useful since so much of biology is
strongly nonlinear in its essential features, and impedance spectroscopy is
fundamentally a linear analysis. All cells and tissues have cell membranes and
its capacitance is both linear and important to cell function. Measurements
proved straightforward in skeletal muscle, cardiac muscle, and lens of the eye.
In skeletal muscle, measurements provided the best estimates of the predominant
(cell) membrane system that dominates electrical properties. In cardiac muscle,
measurements showed definitively that classical microelectrode voltage clamp
could not control the potential of the predominant membranes, that were in the
tubular system separated from the extracellular space by substantial
distributed resistance. In the lens of the eye, impedance spectroscopy changed
the basis of all recording and interpretation of electrical measurements and
laid the basis for Rae and Mathias extensive later experimental work. Many
tissues are riddled with extracellular space as clefts and tubules, for
example, cardiac muscle, the lens of the eye, most epithelia, and of course
frog muscle. These tissues are best analyzed with a bidomain theory that arose
from the work on electrical structure described here. There has been a great
deal of work since then on the bi-domain and this represents the most important
contribution to biology of the analysis of electrical structure in my view.

Reticulate evolutionary processes result in phylogenetic histories that
cannot be modeled using a tree topology. Here, we apply methods from
topological data analysis to molecular sequence data with reticulations. Using
a simple example, we demonstrate the correspondence between nontrivial higher
homology and reticulate evolution. We discuss the sensitivity of the standard
filtration and show cases where reticulate evolution can fail to be detected.
We introduce an extension of the standard framework and define the median
complex as a construction to recover signal of the frequency and scale of
reticulate evolution by inferring and imputing putative ancestral states.
Finally, we apply our methods to two datasets from phylogenetics. Our work
expands on earlier ideas of using topology to extract important evolutionary
features from genomic data.

Over the past few decades, magnetoreception has been discovered in several
species of teleost and elasmobranch fishes by employing varied experimental
methods including conditioning experiments, observations of alignment with
external fields, and experiments with magnetic deterrents. Biogenic magnetite
has been confirmed to be an important receptor mechanism in some species, but
there is ongoing debate regarding whether other mechanisms are at work. This
paper presents evidence for magnetoreception in three additional species, red
drum (Sciaenops ocellatus), black drum (Pogonias cromis), and sea catfish
(Ariopsis felis), by employing experiments to test whether fish respond
differently to bait on a magnetic hook than on a control. In red drum, the
control hook outcaught the magnetic hook by 32 - 18 for chi-squared = 3.92 and
a P-value of 0.048. Black drum showed a significant attraction for the magnetic
hook, which prevailed over the control hook by 11 - 3 for chi-squared = 4.57
and a P-value of 0.033. Gafftopsail catfish (Bagre marinus) showed no
preference with a 31 - 35 split between magnetic hook and control for
chi-squared = 0.242 and a P-value of 0.623. In a sample of 100 sea catfish in
an analogous experiment using smaller hooks, the control hook was preferred
62-38 for chi-squared = 5.76 and a P-value of < 0.001. Such a simple method for
identifying magnetoreceptive species may quickly expand the number of known
magnetoreceptive species and allow for easier access to magnetoreceptive
species and thus facilitate testing of magnetoreceptive hypotheses.

Biogeographical regions (bioregions) reveal how different sets of species are
spatially grouped and therefore are important units for conservation,
historical biogeography, ecology and evolution. Several methods have been
developed to identify bioregions based on species distribution data rather than
expert opinion. One approach successfully applies network theory to simplify
and highlight the underlying structure in species distributions. However, this
method lacks tools for simple and efficient analysis. Here we present Infomap
Bioregions, an interactive web application that inputs species distribution
data and generates bioregion maps. Species distributions may be provided as
georeferenced point occurrences or range maps, and can be of local, regional or
global scale. The application uses a novel adaptive resolution method to make
best use of often incomplete species distribution data. The results can be
downloaded as vector graphics, shapefiles or in table format. We validate the
tool by processing large datasets of publicly available species distribution
data of the world's amphibians using species ranges, and mammals using point
occurrences. We then calculate the fit between the inferred bioregions and WWF
ecoregions. As examples of applications, researchers can reconstruct ancestral
ranges in historical biogeography or identify indicator species for targeted
conservation.

Cryo-electron tomography enables 3D visualization of cells in a near native
state at molecular resolution. The produced cellular tomograms contain detailed
information about all macromolecular complexes, their structures, their
abundances and their specific spatial locations in the cell. However,
extracting this information is very challenging and current methods usually
rely on templates of known structure. Here, we formulate a template-free visual
proteomics analysis as a de novo pattern mining problem and propose a new
framework called "Multi Pattern Pursuit" for supporting proteome-scale de novo
discovery of macromolecular complexes in cellular tomograms without using
templates of known structures. Our tests on simulated and experimental
tomograms show that our method is a promising tool for template-free visual
proteomics analysis.

Participant needs to achieve a given power are frequently underestimated.
This is particularly problematic when effect sizes are small, such as is common
in neuroscience and psychology. We provide tools to make these demands
immediately obvious in the form of a powerscape visualization.

Infectious diseases are notorious for their complex dynamics, which make it
difficult to fit models to test hypotheses. Methods based on state-space
reconstruction have been proposed to infer causal interactions in noisy,
nonlinear dynamical systems. These "model-free" methods are collectively known
as convergent cross-mapping (CCM). Although CCM has theoretical support,
natural systems routinely violate its assumptions. To identify the practical
limits of causal inference under CCM, we simulated the dynamics of two pathogen
strains with varying interaction strengths. The original method of CCM is
extremely sensitive to periodic fluctuations, inferring interactions between
independent strains that oscillate with similar frequencies. This sensitivity
vanishes with alternative criteria for inferring causality. However, CCM
remains sensitive to high levels of process noise and changes to the
deterministic attractor. This sensitivity is problematic because it remains
challenging to gauge noise and dynamical changes in natural systems, including
the quality of reconstructed attractors that underlie cross-mapping. We
illustrate these challenges by analyzing time series of reportable childhood
infections in New York City and Chicago during the pre-vaccine era. We comment
on the statistical and conceptual challenges that currently limit the use of
state-space reconstruction in causal inference.

Time course measurement of single molecules on a cell surface provides
detailed information on the dynamics of the molecules, which is otherwise
inaccessible. To extract the quantitative information, single particle tracking
(SPT) is typically performed. However, trajectories extracted by SPT inevitably
have linking errors when the diffusion speed of single molecules is high
compared to the scale of the particle density. To circumvent this problem we
developed an algorithm to estimate diffusion constants without relying on SPT.
We demonstrated that the proposed algorithm provides reasonable estimation of
diffusion constants even when other methods fail due to high particle density
or inhomogeneous particle distribution. In addition, our algorithm can be used
for visualization of time course data from single molecular measurements.

Background: The increasing volume and variety of genotypic and phenotypic
data is a major defining characteristic of modern biomedical sciences. At the
same time, the limitations in technology for generating data and the inherently
stochastic nature of biomolecular events have led to the discrepancy between
the volume of data and the amount of knowledge gleaned from it. A major
bottleneck in our ability to understand the molecular underpinnings of life is
the assignment of function to biological macromolecules, especially proteins.
While molecular experiments provide the most reliable annotation of proteins,
their relatively low throughput and restricted purview have led to an
increasing role for computational function prediction. However, accurately
assessing methods for protein function prediction and tracking progress in the
field remain challenging. Methodology: We have conducted the second Critical
Assessment of Functional Annotation (CAFA), a timed challenge to assess
computational methods that automatically assign protein function. One hundred
twenty-six methods from 56 research groups were evaluated for their ability to
predict biological functions using the Gene Ontology and gene-disease
associations using the Human Phenotype Ontology on a set of 3,681 proteins from
18 species. CAFA2 featured significantly expanded analysis compared with CAFA1,
with regards to data set size, variety, and assessment metrics. To review
progress in the field, the analysis also compared the best methods
participating in CAFA1 to those of CAFA2. Conclusions: The top performing
methods in CAFA2 outperformed the best methods from CAFA1, demonstrating that
computational function prediction is improving. This increased accuracy can be
attributed to the combined effect of the growing number of experimental
annotations and improved methods for function prediction.

A biological experiment is the most reliable way of assigning function to a
protein. However, in the era of high-throughput sequencing, scientists are
unable to carry out experiments to determine the function of every single gene
product. Therefore, to gain insights into the activity of these molecules and
guide experiments, we must rely on computational means to functionally annotate
the majority of sequence data. To understand how well these algorithms perform,
we have established a challenge involving a broad scientific community in which
we evaluate different annotation methods according to their ability to predict
the associations between previously unannotated protein sequences and Gene
Ontology terms. Here we discuss the rationale, benefits and issues associated
with evaluating computational methods in an ongoing community-wide challenge.

Mast fruiting represents a synchronous population behaviour which can spread
on large landscape areas. This reproductive pattern is generally perceived as a
synchronous periodic production of large seed crops and has a significant
practical importance to forest natural regeneration in order to synchronize
cuttings. The mechanisms of masting are still argued and models of this
phenomenon are uncommon, so a stochastic approach can cast significant light on
some particular aspects. Trees manage to get synchronized and coordinate their
reproductive routines. But is it possible that trees get synchronized by
chance, absolutely random? Using a Monte Carlo simulation of seeding years and
a theoretical masting pattern, a stochastic analysis is performed in order to
assess the chance of random mast fruiting. Two populations of 100 trees, with
different fruiting periodicity of 2-3 years and 4-6 years, were set and the
fruition dynamic was simulated for 100 years. The results show that periodicity
itself cannot induce by chance the masting effect, but periodicity
mathematically influences the reproductive pattern.

How long people live depends on their health, and how it changes with age.
Individual health can be tracked by the accumulation of age-related health
deficits. The fraction of age-related deficits is a simple quantitative measure
of human aging. This quantitative frailty index (F) is as good as chronological
age in predicting mortality. In this paper, we use a dynamical network model of
deficits to explore the effects of interactions between deficits, deficit
damage and repair processes, and the connection between the F and mortality.
With our model, we qualitatively reproduce Gompertz's law of increasing human
mortality with age, the broadening of the F distribution with age, the
characteristic non-linear increase of the F with age, and the increased
mortality of high-frailty individuals. No explicit time-dependence in damage or
repair rates is needed in our model. Instead, implicit time-dependence arises
through deficit interactions -- so that the average deficit damage rates
increases, and deficit repair rates decreases, with age . We use a simple
mortality criterion, where mortality occurs when the most connected node is
damaged.

Development of several alternative mathematical models for the biological
system in question and discrimination between such models using experimental
data is the best way to robust conclusions. Models which challenge existing
theories are more valuable than models which support such theories.

Collective movement can be achieved when individuals respond to the local
movements and positions of their neighbours. Some individuals may
disproportionately influence group movement if they occupy particular spatial
positions in the group, for example, positions at the front of the group. We
asked, therefore, what led individuals in moving pairs of fish (Gambusia
holbrooki) to occupy a position in front of their partner. Individuals adjusted
their speed and direction differently in response to their partner's position,
resulting in individuals occupying different positions in the group.
Individuals that were found most often at the front of the pair had greater
mean changes in speed than their partner, and were less likely to turn towards
their partner, compared to those individuals most often found at the back of
the pair. The pair moved faster when led by the individual that was usually at
the front. Our results highlight how differences in the social responsiveness
between individuals can give rise to leadership in free moving groups. They
also demonstrate how the movement characteristics of groups depend on the
spatial configuration of individuals within them.

Time course data are often used to study the changes to a biological process
after perturbation. Statistical methods have been developed to determine
whether such a perturbation induces changes over time, e.g. comparing a
perturbed and unperturbed time course dataset to uncover differences. However,
existing methods do not provide a principled statistical approach to identify
the specific time when the two time course datasets first begin to diverge
after a perturbation; we call this the perturbation time. Estimation of the
perturbation time for different variables in a biological process allows us to
identify the sequence of events following a perturbation and therefore provides
valuable insights into likely causal relationships.
  In this paper, we propose a Bayesian method to infer the perturbation time
given time course data from a wild-type and perturbed system. We use a
non-parametric approach based on Gaussian Process regression. We derive a
probabilistic model of noise-corrupted and replicated time course data coming
from the same profile before the perturbation time and diverging after the
perturbation time. The likelihood function can be worked out exactly for this
model and the posterior distribution of the perturbation time is obtained by a
simple histogram approach, without recourse to complex approximate inference
algorithms. We validate the method on simulated data and apply it to study the
transcriptional change occurring in Arabidopsis following inoculation with P.
syringae pv. tomato DC3000 versus the disarmed strain DC3000hrpA.
  An R package, DEtime, implementing the method is available at
https://github.com/ManchesterBioinference/DEtime along with the data and code
required to reproduce all the results.

We consider a class of biologically-motivated stochastic processes in which a
unicellular organism divides its resources (volume or damaged proteins, in
particular) symmetrically or asymmetrically between its progeny. Assuming the
final amount of the resource is controlled by a growth policy and subject to
additive and multiplicative noise, we derive the "master equation" describing
how the resource distribution evolves over subsequent generations and use it to
study the properties of stable resource distributions. We find conditions under
which a unique stable resource distribution exists and calculate its moments
for the class of affine linear growth policies. Moreover, we apply an
asymptotic analysis to elucidate the conditions under which the stable
distribution (when it exists) has a power-law tail. Finally, we use the results
of this asymptotic analysis along with the moment equations to draw a stability
phase diagram for the system that reveals the counterintuitive result that
asymmetry serves to increase stability while at the same time widening the
stable distribution. We also briefly discuss how cells can divide damaged
proteins asymmetrically between their progeny as a form of damage control. In
the appendix, motivated by the asymmetric division of cell volume in
Saccharomyces cerevisiae, we extend our results to the case wherein mother and
daughter cells follow different growth policies.

Diversity represents a key concept in ecology, and there are various methods
of assessing it. The multitude of diversity indices are quite puzzling and
sometimes difficult to compute for a large volume of data. This paper promotes
a computational tool used to assess the diversity of different entities. The
BIODIV software is a user-friendly tool, developed using Microsoft Visual
Basic. It is capable to compute several diversity indices such as: Shannon,
Simpson, Pielou, Brillouin, Berger-Parker, McIntosh, Margalef, Menhinick and
Gleason. The software tool was tested using real data sets and the results were
analysed in order to make assumption on the indices behaviour. The results
showed a clear segregation of indices in two major groups with similar
expressivity.

We explore the relationship among model fidelity, experimental design, and
parameter estimation in sloppy models. We show that the approximate nature of
mathematical models poses challenges for experimental design in sloppy models.
In many models of complex biological processes it is unknown what are the
relevant physics that must be included to explain collective behaviors. As a
consequence, models are often overly complex, with many practically
unidentifiable parameters. Furthermore, which details are relevant/irrelevant
vary among potential experiments. By selecting complementary experiments,
experimental design may inadvertently make details that were ommitted from the
model become relevant. When this occurs, the model will fail to give a good fit
to the data. We use a simple hyper-model of model error to quantify a model's
inadequacy and apply it to two models of complex biological processes (EGFR
signaling and DNA repair) with optimally selected experiments. We find that
although parameters may be accurately estimated, the error in the model renders
it less predictive than it was in the sloppy regime where model error is small.
We introduce the concept of a \emph{sloppy system}--a sequence of models of
increasing complexity that become sloppy in the limit of microscopic accuracy.
We explore the limits of accurate parameter estimation in sloppy systems and
argue that system identification better approached by considering a hierarchy
of models of varying detail rather than focusing parameter estimation in a
single model.

Use of accelerometers is now widespread within animal biotelemetry as they
provide a means of measuring an animal's activity in a meaningful and
quantitative way where direct observation is not possible. In sequential
acceleration data there is a natural dependence between observations of
movement or behaviour, a fact that has been largely ignored in most analyses.
Analyses of acceleration data where serial dependence has been explicitly
modelled have largely relied on hidden Markov models (HMMs). Depending on the
aim of an analysis, either a supervised or an unsupervised learning approach
can be applied. Under a supervised context, an HMM is trained to classify
unlabelled acceleration data into a finite set of pre-specified categories,
whereas we will demonstrate how an unsupervised learning approach can be used
to infer new aspects of animal behaviour. We will provide the details necessary
to implement and assess an HMM in both the supervised and unsupervised context,
and discuss the data requirements of each case. We outline two applications to
marine and aerial systems (sharks and eagles) taking the unsupervised approach,
which is more readily applicable to animal activity measured in the field. HMMs
were used to infer the effects of temporal, atmospheric and tidal inputs on
animal behaviour. Animal accelerometer data allow ecologists to identify
important correlates and drivers of animal activity (and hence behaviour). The
HMM framework is well suited to deal with the main features commonly observed
in accelerometer data. The ability to combine direct observations of animals
activity and combine it with statistical models which account for the features
of accelerometer data offer a new way to quantify animal behaviour, energetic
expenditure and deepen our insights into individual behaviour as a constituent
of populations and ecosystems.

Analytical ultracentrifugation (AUC) is a classical technique of physical
biochemistry providing information on size, shape, and interactions of
macromolecules from the analysis of their migration in centrifugal fields while
free in solution. A key mechanical element in AUC is the centerpiece, a
component of the sample cell assembly that is mounted between the optical
windows to allow imaging and to seal the sample solution column against high
vacuum while exposed to gravitational forces in excess of 300,000 g. For
sedimentation velocity it needs to be precisely sector-shaped to allow
unimpeded radial macromolecular migration. During the history of AUC a great
variety of centerpiece designs have been developed for different types of
experiments. Here, we report that centerpieces can now be readily fabricated by
3D printing at low cost, from a variety of materials, and with customized
designs. The new centerpieces can exhibit sufficient mechanical stability to
withstand the gravitational forces at the highest rotor speeds and be
sufficiently precise for sedimentation equilibrium and sedimentation velocity
experiments. Sedimentation velocity experiments with bovine serum albumin as a
reference molecule in 3D printed centerpieces with standard double-sector
design result in sedimentation boundaries virtually indistinguishable from
those in commercial double-sector epoxy centerpieces, with sedimentation
coefficients well within the range of published values. The statistical error
of the measurement is slightly above that obtained with commercial epoxy, but
still below 1%. Facilitated by modern open-source design and fabrication
paradigms, we believe 3D printed centerpieces and AUC accessories can spawn a
variety of improvements in AUC experimental design, efficiency and resource
allocation.

This paper describes methodological details used by WHO in 2015 to estimate
TB incidence, prevalence and mortality. Incidence and mortality are
disaggregated by HIV status, age and sex. Methods to derive MDR-TB burden
indicators are detailed. Four main methods were used to derive incidence: (i)
case notification data combined with expert opinion about case detection gaps
(120 countries representing 51% of global incidence); (ii) results from
national TB prevalence surveys (19 countries, 46% of global incidence); (iii)
notifications in high-income countries adjusted by a standard factor to account
for under-reporting and underdiagnosis (73 countries, 3% of global incidence)
and (iv) capture-recapture modelling (5 countries, 0.5% of global incidence).
Prevalence was obtained from results of national prevalence surveys in 21
countries, representing 69% of global prevalence). In other countries,
prevalence was estimated from incidence and disease duration. Mortality was
obtained from national vital registration systems of mortality surveys in 129
countries (43% of global HIV-negative TB mortality). In other countries,
mortality was derived indirectly from incidence and case fatality ratio.

Numerous processes across both the physical and biological sciences are
driven by diffusion. Partial differential equations (PDEs) are a popular tool
for modelling such phenomena deterministically, but it is often necessary to
use stochastic models to accurately capture the behaviour of a system,
especially when the number of diffusing particles is low. The stochastic models
we consider in this paper are `compartment-based': the domain is discretized
into compartments, and particles can jump between these compartments.
Volume-excluding effects (crowding) can be incorporated by blocking movement
with some probability.
  Recent work has established the connection between fine-grained models and
coarse-grained models incorporating volume exclusion, but only for uniform
lattices. In this paper we consider non-uniform, hybrid lattices that
incorporate both fine- and coarse-grained regions, and present two different
approaches to describing the interface of the regions. We test both techniques
in a range of scenarios to establish their accuracy, benchmarking against
fine-grained models, and show that the hybrid models developed in this paper
can be significantly faster to simulate than the fine-grained models in certain
situations, and are at least as fast otherwise.

Resting-state functional MRI (rs-fMRI) is widely used to noninvasively study
human brain networks. Network functional connectivity is often estimated by
calculating the timeseries correlation between blood-oxygen-level dependent
(BOLD) signal from different regions of interest. However, standard correlation
cannot characterize the direction of information flow between regions. In this
paper, we introduce and test a new concept, prediction correlation, to estimate
effective connectivity in functional brain networks from rs-fMRI. In this
approach, the correlation between two BOLD signals is replaced by a correlation
between one BOLD signal and a prediction of this signal via a causal system
driven by another BOLD signal. Three validations are described: (1) Prediction
correlation performed well on simulated data where the ground truth was known,
and outperformed four other methods. (2) On simulated data designed to display
the "common driver" problem, prediction correlation did not introduce false
connections between non-interacting driven ROIs. (3) On experimental data,
prediction correlation recovered the previously identified network organization
of human brain. Prediction correlation scales well to work with hundreds of
ROIs, enabling it to assess whole brain interregional connectivity at the
single subject level. These results provide an initial validation that
prediction correlation can capture the direction of information flow and
estimate the duration of extended temporal delays in information flow between
regions of interest based on BOLD signal. This approach not only maintains the
high sensitivity to network connectivity provided by the correlation analysis,
but also performs well in the estimation of causal information flow in the
brain.

In this study, we analyse a high-frequency movement dataset for a group of
grazing cattle and investigate their spatiotemporal patterns using a simple
two-state `stop-and-move' mobility model. We find that the dispersal kernel in
the moving state is best described by a mixture exponential distribution,
indicating the hierarchical nature of the movement. On the other hand, the
waiting time appears to be scale-invariant below a certain cut-off and is best
described by a truncated power-law distribution, suggesting heterogenous
dynamics in the non-moving state. We explore possible explanations for the
observed phenomena, covering factors that can play a role in the generation of
mobility patterns, such as the context of grazing environment, the intrinsic
decision-making mechanism or the energy status of different activities. In
particular, we propose a new hypothesis that the underlying movement pattern
can be attributed to the most probable observable energy status under the
maximum entropy configuration. These results are not only valuable for
modelling cattle movement but also provide new insights for understanding the
underlying biological basis of grazing behaviour.

Next-generation sequencing technologies allow the measurement of somatic
mutations in a large number of patients from the same cancer type. One of the
main goals in analyzing these mutations is the identification of mutations
associated with clinical parameters, such as survival time. This goal is
hindered by the genetic heterogeneity of mutations in cancer, due to the fact
that genes and mutations act in the context of pathways. To identify mutations
associated with survival time it is therefore crucial to study mutations in the
context of interaction networks.
  In this work we study the problem of identifying subnetworks of a large
gene-gene interaction network that have mutations associated with survival. We
formally define the associated computational problem by using a score for
subnetworks based on the test statistic of the log-rank test, a widely used
statistical test for comparing the survival of two populations. We show that
the computational problem is NP-hard and we propose a novel algorithm, called
Network of Mutations Associated with Survival (NoMAS), to solve it. NoMAS is
based on the color-coding technique, that has been previously used in other
applications to find the highest scoring subnetwork with high probability when
the subnetwork score is additive. In our case the score is not additive;
nonetheless, we prove that under a reasonable model for mutations in cancer
NoMAS does identify the optimal solution with high probability. We test NoMAS
on simulated and cancer data, comparing it to approaches based on single gene
tests and to various greedy approaches. We show that our method does indeed
find the optimal solution and performs better than the other approaches.
Moreover, on two cancer datasets our method identifies subnetworks with
significant association to survival when none of the genes has significant
association with survival when considered in isolation.

In cancer treatment, chemotherapy is administered according a constant
schedule. The chronotherapy approach, considering chronobiological drug
delivery, adapts the chemotherapy profile to the circadian rhythms of the human
organism. This reduces toxicity effects and at the same time enhances
efficiency of chemotherapy. To personalize cancer treatment, chemotherapy
profiles have to be further adapted to individual patients. Therefore, we
present a new model to represent cycle phenomena in circadian rhythms. The
model enables a more precise modelling of the underlying circadian rhythms. In
comparison with the standard model, our model delivers better results in all
defined quality indices. The new model can be used to adapt the chemotherapy
profile efficiently to individual patients. The adaption to individual patients
contributes to the aim of personalizing cancer therapy.

Communication and coordination play a major role in the ability of bacterial
cells to adapt to ever changing environments and conditions. Recent work has
shown that such coordination underlies several aspects of bacterial responses
including their ability to develop antibiotic resistance. Here we develop a new
distributed gradient descent method that helps explain how bacterial cells
collectively search for food in harsh environments using extremely limited
communication and computational complexity. This method can also be used for
computational tasks when agents are facing similarly restricted conditions. We
formalize the communication and computation assumptions required for successful
coordination and prove that the method we propose leads to convergence even
when using a dynamically changing interaction network. The proposed method
improves upon prior models suggested for bacterial foraging despite making
fewer assumptions. Simulation studies and analysis of experimental data
illustrate the ability of the method to explain and further predict several
aspects of bacterial swarm food search.

Biochemical reaction networks are often modelled using discrete-state,
continuous-time Markov chains. System statistics of these Markov chains usually
cannot be calculated analytically and therefore estimates must be generated via
simulation techniques. There is a well documented class of simulation
techniques known as exact stochastic simulation algorithms, an example of which
is Gillespie's direct method. These algorithms often come with high
computational costs, therefore approximate stochastic simulation algorithms
such as the tau-leap method are used. However, in order to minimise the bias in
the estimates generated using them, a relatively small value of tau is needed,
rendering the computational costs comparable to Gillespie's direct method.
  The multi-level Monte Carlo method (Anderson and Higham, Multiscale Model.
Simul. 10:146-179, 2012) provides a reduction in computational costs whilst
minimising or even eliminating the bias in the estimates of system statistics.
This is achieved by first crudely approximating required statistics with many
sample paths of low accuracy. Then correction terms are added until a required
level of accuracy is reached. Recent literature has primarily focussed on
implementing the multi-level method efficiently to estimate a single system
statistic. However, it is clearly also of interest to be able to approximate
entire probability distributions of species counts. We present two novel
methods that combine known techniques for distribution reconstruction with the
multi-level method. We demonstrate the potential of our methods using a number
of examples.

Systems biology approaches to the integrative study of cells, organs and
organisms offer the best means of understanding in a holistic manner the
diversity of molecular assays that can be now be implemented in a high
throughput manner. Such assays can sample the genome, epigenome, proteome,
metabolome and microbiome contemporaneously, allowing us for the first time to
perform a complete analysis of physiological activity. The central problem
remains empowering the scientific community to actually implement such an
integration, across seemingly diverse data types and measurements. One
promising solution is to apply semantic techniques on a self-consistent and
implicitly correct ontological representation of these data types. In this
paper we describe how we have applied one such solution, based around the
InterMine data warehouse platform which uses as its basis the Sequence
Ontology, to facilitate a systems biology analysis of virulence in the
apicomplexan pathogen $Toxoplasma~gondii$, a common parasite that infects up to
half the worlds population, with acute pathogenic risks for immuno-compromised
individuals or pregnant mothers. Our solution, which we named `toxoMine', has
provided both a platform for our collaborators to perform such integrative
analyses and also opportunities for such cyberinfrastructure to be further
developed, particularly to take advantage of possible semantic similarities of
value to knowledge discovery in the Omics enterprise. We discuss these
opportunities in the context of further enhancing the capabilities of this
powerful integrative platform.

This paper will detail the basis of our previously developed predictive model
for pigeon flight paths based on observations of the specific individual being
predicted. We will then describe how this model can be adapted to predict the
flight of a new, unobserved bird, based on observations of other individuals
from the same release site. We will test the accuracy of these predictions
relative to naive models with no previous flight information and those trained
on the focal bird's own previous flights, and discuss the implications of these
results for the nature of navigational cue use in the familiar area. Finally we
will discuss how visual cues may be explicitly encoded in the model in future
work.

PANDA (Passing Attributes between Networks for Data Assimilation) is a gene
regulatory network inference method that uses message-passing to integrate
multiple sources of 'omics data. PANDA was originally coded in C++. In this
application note we describe PyPanda, the Python version of PANDA. PyPanda runs
considerably faster than the C++ version and includes additional features for
network analysis. Availability: The open source PyPanda Python package is
freely available at https://github.com/davidvi/pypanda. Contact: d.g.p.van
ijzendoorn@lumc.nl

Stochastic simulation methods can be applied successfully to model exact
spatio-temporally resolved reaction-diffusion systems. However, in many cases,
these methods can quickly become extremely computationally intensive with
increasing particle numbers. An alternative description of many of these
systems can be derived in the diffusive limit as a deterministic, continuum
system of partial differential equations. Although the numerical solution of
such partial differential equations is, in general, much more efficient than
the full stochastic simulation, the deterministic continuum description is
generally not valid when copy numbers are low and stochastic effects dominate.
Therefore, to take advantage of the benefits of both of these types of models,
each of which may be appropriate in different parts of a spatial domain, we
have developed an algorithm that can be used to couple these two types of model
together. This hybrid coupling algorithm uses an overlap region between the two
modelling regimes. By coupling fluxes at one end of the interface and using a
concentration-matching condition at the other end, we ensure that mass is
appropriately transferred between PDE- and compartment-based regimes. Our
methodology gives notable reductions in simulation time in comparison with
using a fully stochastic model, whilst maintaining the important stochastic
features of the system and providing detail in appropriate areas of the domain.
We test our hybrid methodology robustly by applying it to several biologically
motivated problems including diffusion and morphogen gradient formation. Our
analysis shows that the resulting error is small, unbiased and does not grow
over time.

Despite extensive research during the last decades, coronary artery disease
(CAD) remains the number one cause of death, responsible for near 50% of global
mortality. A main reason for this is that CAD has a complex inheritance and
etiology that unlike rare single gene disorders cannot fully be understood from
studies of of genes one-by-one.In parallel, studies that simultaneously assess
multiple, functionally associated genes are warranted. For this reason we
undertook the Stockholm Atherosclerosis Gene Expression (STAGE) study that
besides careful clinical characterization and genome-wide DNA genotyping also
assessed the global gene expression profiles from seven CAD-relevant vascular
and metabolic tissues. In this thesis report we show that by integrating GWAS
with genetics of gene expression studies like STAGE, we can advance our
understanding from the perspective of multiple genes and gene variants acting
in conjunction to cause CAD in the form of regulatory gene networks. This is
done through developing new bioinformatics tools and applying them to
disease-specific, genetics of global gene expression studies like STAGE. These
tools are necessary to go beyond our current limited single-gene understanding
of complex traits, like CAD.

Computational systems biology has provided plenty of insights into cell
biology. Early on, the focus was on reaction networks between molecular
species. Spatial distribution only began to be considered mostly within the
last decade. However, calculations were restricted to small systems because of
tremendously high computational workloads. To date, application to the cell of
typical size with molecular resolution is still far from realization. In this
article, we present a new parallel stochastic method for particle
reaction-diffusion systems. The program called pSpatiocyte was created bearing
in mind reaction networks in biological cells operating in crowded
intracellular environments as the primary simulation target. pSpatiocyte
employs unique discretization and parallelization algorithms based on a
hexagonal close-packed lattice for efficient execution particularly on large
distributed memory parallel computers. For two-level parallelization, we
introduced isolated subdomain and tri-stage lockstep communication for
process-level, and voxel-locking techniques for thread-level. We performed a
series of parallel runs on RIKEN's K computer. For a fine lattice that had
relatively low occupancy, pSpatiocyte achieved 7686 times speedup with 663552
cores relative to 64 cores from the viewpoint of strong scaling and exhibited
74\% parallel efficiency. As for weak scaling, efficiencies at least 60% were
observed up to 663552 cores. In addition to computational performance,
diffusion and reaction rates were validated by theory and another
well-validated program and had good agreement. Lastly, as a preliminary example
of real-world applications, we present a calculation of the MAPK model, a
typical reaction network motif in cell signaling pathways.

We present a Bayesian methodology for infinite as well as finite dimensional
parameter identification for partial differential equation models. The Bayesian
framework provides a rigorous mathematical framework for incorporating prior
knowledge on uncertainty in the observations and the parameters themselves,
resulting in an approximation of the full probability distribution for the
parameters, given the data. Although the numerical approximation of the full
probability distribution is computationally expensive, parallelised algorithms
can make many practically relevant problems computationally feasible. The
probability distribution not only provides estimates for the values of the
parameters, but also provides information about the inferability of parameters
and the sensitivity of the model. This information is crucial when a
mathematical model is used to study the outcome of real-world experiments.
Keeping in mind the applicability of our approach to tackle real-world
practical problems with data from experiments, in this initial proof of concept
work, we apply this theoretical and computational framework to parameter
identification for a well studied semilinear reaction-diffusion system with
activator-depleted reaction kinetics, posed on evolving and stationary domains.

With the advance of experimental techniques such as time-lapse fluorescence
microscopy, the availability of single-cell trajectory data has vastly
increased, and so has the demand for computational methods suitable for
parameter inference with this type of data. However, most of the currently
available methods treat single-cell trajectories independently, ignoring the
mother-daughter relationships and the information provided by population
structure. This information is however essential if a process of interest
happens at cell division, or if it evolves slowly compared to the duration of
the cell cycle. In this work, we highlight the importance of tracking cell
lineage trees and propose a Bayesian framework for parameter inference on
tree-structured data. Our method relies on a combination of Sequential Monte
Carlo for likelihood approximation and Markov Chain Monte Carlo for parameter
sampling. We demonstrate the capabilities of our inference framework on two
simple examples in which the lineage tree information is necessary: one in
which the cell phenotype can only switch at cell division and another where the
cell type fluctuates randomly over timescales that extend well beyond the
lifetime of a single cell.

Motivation: Alternative splicing is an important mechanism in which the
regions of pre-mRNAs are differentially joined in order to form different
transcript isoforms. Alternative splicing is involved in the regulation of
normal physiological functions but also linked to the development of diseases
such as cancer. We analyse differential expression and splicing using RNA-seq
time series in three different settings: overall gene expression levels,
absolute transcript expression levels and relative transcript expression
levels.
  Results: Using estrogen receptor $\alpha$ signalling response as a model
system, our Gaussian process (GP)-based test identifies genes with differential
splicing and/or differentially expressed transcripts. We discover genes with
consistent changes in alternative splicing independent of changes in absolute
expression and genes where some transcripts change while others stay constant
in absolute level. The results suggest classes of genes with different modes of
alternative splicing regulation during the experiment.
  Availability: R and Matlab codes implementing the method are available at
https://github.com/PROBIC/diffsplicing . An interactive browser for viewing all
model fits is available at http://users.ics.aalto.fi/hande/splicingGP/ .

Animal cells use traction forces to sense the mechanics and geometry of their
environment. Measuring these traction forces requires a workflow combining cell
experiments, image processing and force reconstruction based on elasticity
theory. Such procedures have been established before mainly for planar
substrates, in which case one can use the Green's function formalism. Here we
introduce a worksflow to measure traction forces of cardiac myofibroblasts on
non-planar elastic substrates. Soft elastic substrates with a wave-like
topology were micromolded from polydimethylsiloxane (PDMS) and fluorescent
marker beads were distributed homogeneously in the substrate. Using feature
vector based tracking of these marker beads, we first constructed a hexahedral
mesh for the substrate. We then solved the direct elastic boundary volume
problem on this mesh using the finite element method (FEM). Using data
simulations, we show that the traction forces can be reconstructed from the
substrate deformations by solving the corresponding inverse problem with a
L1-norm for the residue and a L2-norm for 0th order Tikhonov regularization.
Applying this procedure to the experimental data, we find that cardiac
myofibroblast cells tend to align both their shapes and their forces with the
long axis of the deformable wavy substrate.

We address the need for affordable, rapid, and easy to use diagnostic
technologies by coupling an innovative thermocycling system that harnesses
natural convection to perform rapid DNA amplification via the polymerase chain
reaction (PCR) with smartphone-based detection. Our approach offers an
inherently simple design that enables PCR to be completed in 10-20 minutes.
Electrical power requirements are dramatically reduced by harnessing natural
convection to actuate the reaction, allowing the entire system to be operated
from a standard USB connection (5 V) via solar battery packs. Instantaneous
detection and analysis are enabled using an ordinary smartphone camera and
dedicated app interface.

Motivation: Clustering techniques are routinely applied to identify patterns
of co-expression in gene expression data. Co-regulation, and involvement of
genes in similar cellular function, is subsequently inferred from the clusters
which are obtained. Increasingly sophisticated algorithms have been applied to
microarray data, however, less attention has been given to the statistical
significance of the results of clustering studies. We present a technique for
the analysis of commonly used hierarchical linkage-based clustering called
Significance Analysis of Linkage Trees (SALT).
  Results: The statistical significance of pairwise similarity levels between
gene expression profiles, a measure of co-expression, is established using a
surrogate data analysis method. We find that a modified version of the standard
linkage technique, complete-linkage, must be used to generate hierarchical
linkage trees with the appropriate properties. The approach is illustrated
using synthetic data generated from a novel model of gene expression profiles
and is then applied to previously analysed microarray data on the
transcriptional response of human fibroblasts to serum stimulation.

The focus of pancreatic cancer research has been shifted from pancreatic
cancer cells towards their microenvironment, involving pancreatic stellate
cells that interact with cancer cells and influence tumor progression. To
quantitatively understand the pancreatic cancer microenvironment, we construct
a computational model for intracellular signaling networks of cancer cells and
stellate cells as well as their intercellular communication. We extend the
rule-based BioNetGen language to depict intra- and inter-cellular dynamics
using discrete and continuous variables respectively. Our framework also
enables a statistical model checking procedure for analyzing the system
behavior in response to various perturbations. The results demonstrate the
predictive power of our model by identifying important system properties that
are consistent with existing experimental observations. We also obtain
interesting insights into the development of novel therapeutic strategies for
pancreatic cancer.

The development of mechanistic models of biological systems is a central part
of Systems Biology. One major task in developing these models is the inference
of the correct model parameters. Due to the size of most realistic models and
their possibly complex dynamical behaviour one must usually rely on sample
based methods. In this paper we present a novel algorithm that reliably
estimates model parameters for deterministic as well as stochastic models from
trajectory data. Our algorithm samples iteratively independent particles from
the level sets of the likelihood and recovers the posterior from these level
sets. The presented approach is easily parallelizable and, by utilizing density
estimation through Dirichlet Process Gaussian Mixture Models, can deal with
high dimensional parameter spaces. We illustrate that our algorithm is
applicable to large, realistic deterministic and stochastic models and succeeds
in inferring the correct posterior from a given number of observed
trajectories. This algorithm presents a novel, computationally feasible
approach to identify parameters of large biochemical reaction models based on
sample path data.

Electric fishes modulate their electric organ discharges with a remarkable
variability. Some patterns can be easily identified, such as pulse rate
changes, offs and chirps, which are often associated with important behavioral
contexts, including aggression, hiding and mating. However, these behaviors are
only observed when at least two fish are freely interacting. Although their
electrical pulses can be easily recorded by non-invasive techniques,
discriminating the emitter of each pulse is challenging when physically similar
fish are allowed to freely move and interact. Here we optimized a custom-made
software recently designed to identify the emitter of pulses by using automated
chirp detection, adaptive threshold for pulse detection and slightly changing
how the recorded signals are integrated. With these optimizations, we performed
a quantitative analysis of the statistical changes throughout the dominance
contest with respect to Inter Pulse Intervals, Chirps and Offs dyads of freely
moving Gymnotus carapo. In all dyads, chirps were signatures of subsequent
submission, even when they occurred early in the contest. Although offs were
observed in both dominant and submissive fish, they were substantially more
frequent in submissive individuals, in agreement with the idea from previous
studies that offs are electric cues of submission. In general, after the
dominance is established the submissive fish significantly changes its average
pulse rate, while the pulse rate of the dominant remained unchanged.
Additionally, no chirps or offs were observed when two fish were manually kept
in direct physical contact, suggesting that these electric behaviors are not
automatic responses to physical contact.

FRET measurements can provide dynamic spatial information on length scales
smaller than the diffraction limit of light. Several methods exist to measure
FRET between fluorophores, including Fluorescence Lifetime Imaging Microscopy
(FLIM), which relies on the reduction of fluorescence lifetime when a
fluorophore is undergoing FRET. FLIM measurements take the form of histograms
of photon arrival times, containing contributions from a mixed population of
fluorophores both undergoing and not undergoing FRET, with the measured
distribution being a mixture of exponentials of different lifetimes. Here, we
present an analysis method based on Bayesian inference that rigorously takes
into account several experimental complications. We test the precision and
accuracy of our analysis on controlled experimental data and verify that we can
faithfully extract model parameters, both in the low-photon and low-fraction
regimes.

Protein quality assessment (QA) has played an important role in protein
structure prediction. We developed a novel single-model quality assessment
method - Qprob. Qprob calculates the absolute error for each protein feature
value against the true quality scores (i.e. GDT-TS scores) of protein
structural models, and uses them to estimate its probability density
distribution for quality assessment. Qprob has been blindly tested on the 11th
Critical Assessment of Techniques for Protein Structure Prediction (CASP11) as
MULTICOM-NOVEL server. The official CASP result shows that Qprob ranks as one
of the top single-model QA methods. In addition, Qprob makes contributions to
our protein tertiary structure predictor MULTICOM, which is officially ranked
3rd out of 143 predictors. The good performance shows that Qprob is good at
assessing the quality of models of hard targets. These results demonstrate that
this new probability density distribution based method is effective for protein
single-model quality assessment and is useful for protein structure prediction.
The webserver and software packages of Qprob are available at:
http://calla.rnet.missouri.edu/qprob/.

Cohort studies employ pairwise measures of association to quantify
dependencies among conditions and exposures. To reliably use these measures to
draw conclusions about the underlying association strengths requires that the
measures be robust and unbiased. These considerations assume greater
significance when applied to disease networks, where associations among
heterogeneous pairs of diseases are ranked. Using disease diagnoses data from a
large cohort of 5.5 million individuals, we develop a comprehensive methodology
to characterize the bias of standard association measures like relative risk
and $\phi$ correlation. To overcome these biases, we devise a novel measure
based on a stochastic model for disease development. The new measure is
demonstrated to have the least overall bias and hence would be most suitable
for application to heterogeneous disease cohorts.

These notes provide a short, focused introduction to modelling stochastic
gene expression, including a derivation of the master equation, the recovery of
deterministic dynamics, birth-and-death processes, and Langevin theory. The
notes were last updated around 2010 and written for lectures given at summer
schools held at McGill University's Centre for Non-linear Dynamics in 2004,
2006, and 2008.

Sigmoid semilogarithmic functions with shape of Boltzmann equations, have
become extremely popular to describe diverse biological situations. Part of the
popularity is due to the easy avail- ability of software which fits Boltzmann
functions to data, without much knowledge of the fitting procedure or the
statistical properties of the parameters derived from the procedure. The
purpose of this paper is to explore the plasticity of the Boltzmann function to
fit data, some aspects of the optimization procedure to fit the function to
data and how to use this plastic function to differentiate the effect of
treatment on data and to attest the statistical significance of treatment
effect on the data.

The biomechanics of the human body allow humans a range of possible ways of
executing movements to attain specific goals. Nevertheless, humans exhibit
significant patterns in how they execute movements. We propose that the
observed patterns of human movement arise because subjects select those ways to
execute movements that are, in a rigorous sense, optimal. In this project, we
show how this proposition can guide the development of computational models of
movement selection and thereby account for human movement patterns. We proceed
by first developing a movement utility formalism that operationalizes the
concept of a best or optimal way of executing a movement using a utility
function so that the problem of movement selection becomes the problem of
finding the movement that maximizes the utility function. Since the movement
utility formalism includes a contribution of the metabolic energy of the
movement (maximum utility movements try to minimize metabolic energy), we also
develop a metabolic energy formalism that we can use to construct estimators of
the metabolic energies of particular movements. We then show how we can
construct an estimator for the metabolic energies of normal walking gaits and
we use that estimator to construct a movement utility model of the selection of
normal walking gaits and show that the relationship between avg. walking speed
and avg. step length predicted by this model agrees with observation. We
conclude by proposing a physical mechanism that a subject might use to estimate
the metabolic energy of a movement in practice.

In this work, we propose a deep learning approach to improve docking-based
virtual screening. The introduced deep neural network, DeepVS, uses the output
of a docking program and learns how to extract relevant features from basic
data such as atom and residues types obtained from protein-ligand complexes.
Our approach introduces the use of atom and amino acid embeddings and
implements an effective way of creating distributed vector representations of
protein-ligand complexes by modeling the compound as a set of atom contexts
that is further processed by a convolutional layer. One of the main advantages
of the proposed method is that it does not require feature engineering. We
evaluate DeepVS on the Directory of Useful Decoys (DUD), using the output of
two docking programs: AutodockVina1.1.2 and Dock6.6. Using a strict evaluation
with leave-one-out cross-validation, DeepVS outperforms the docking programs in
both AUC ROC and enrichment factor. Moreover, using the output of
AutodockVina1.1.2, DeepVS achieves an AUC ROC of 0.81, which, to the best of
our knowledge, is the best AUC reported so far for virtual screening using the
40 receptors from DUD.

Independent computational reproducibility of scientific results is rapidly
becoming of pivotal importance in scientific progress as computation itself
plays a more and more central role in so many branches of science.
Historically, reproducibility has followed the familiar Popperian [38] model
whereby theory cannot be verified by scientific testing, it can only be
falsified. Ultimately, this implies that if an experiment cannot be reproduced
independently to some satisfactory level of precision, its value is essentially
unquantifiable; put brutally, it is impossible to determine its scientific
value. The burgeoning presence of software in most scientific work adds a new
and particularly opaque layer of complexity [29]. In spite of much recent
interest in many scientific areas, emphasis remains more on procedures,
strictures and discussion [12, 14, 16, 29, 30, 37, 41], reflecting the
inexperience of most scientific journals when it comes to software, rather than
the details of how computational reproducibility is actually achieved, for
which there appear to be relatively few guiding examples [6, 10, 17]. After
considering basic principles, here we show how full computational
reproducibility can be achieved in practice at every stage using a case study
of a multi-gigabyte protein study on the open SwissProt protein database, from
data download all the way to individual figure by figure reproduction as an
exemplar for general scientific computation.

Background: Species abundance distributions in chemical reaction network
models cannot usually be computed analytically. Instead, stochas- tic
simulation algorithms allow sample from the the system configuration. Although
many algorithms have been described, no fast implementation has been provided
for {\tau}-leaping which i) is Matlab-compatible, ii) adap- tively alternates
between SSA, implicit and explicit {\tau}-leaping, and iii) provides summary
statistics necessary for Bayesian inference. Results: We provide a
Matlab-compatible implementation of the adap- tive explicit-implicit
{\tau}-leaping algorithm to address the above-mentioned deficits. matLeap
provides equal or substantially faster results compared to two widely used
simulation packages while maintaining accuracy. Lastly, matLeap yields summary
statistics of the stochastic process unavailable with other methods, which are
indispensable for Bayesian inference. Conclusions: matLeap addresses
shortcomings in existing Matlab-compatible stochastic simulation software,
providing significant speedups and sum- mary statistics that are especially
useful for researchers utilizing particle- filter based methods for Bayesian
inference. Code is available for download at
https://github.com/claassengroup/matLeap. Contact:
justin.feigelman@imsb.biol.ethz.ch

Reaction-diffusion models are widely used to study spatially-extended
chemical reaction systems. In order to understand how the dynamics of a
reaction-diffusion model are affected by changes in its input parameters,
efficient methods for computing parametric sensitivities are required. In this
work, we focus on stochastic models of spatially-extended chemical reaction
systems that involve partitioning the computational domain into voxels.
Parametric sensitivities are often calculated using Monte Carlo techniques that
are typically computationally expensive; however, variance reduction techniques
can decrease the number of Monte Carlo simulations required. By exploiting the
characteristic dynamics of spatially-extended reaction networks, we are able to
adapt existing finite difference schemes to robustly estimate parametric
sensitivities in a spatially-extended network. We show that algorithmic
performance depends on the dynamics of the given network and the choice of
summary statistics. We then describe a hybrid technique that dynamically
chooses the most appropriate simulation method for the network of interest. Our
method is tested for functionality and accuracy in a range of different
scenarios.

Objective: Syncope is a sudden loss of consciousness with loss of postural
tone and spontaneous recovery; it is a common condition, albeit one that is
challenging to accurately diagnose. Uncertainties about the triggering
mechanisms and their underlying pathophysiology have led to various
classifications of patients exhibiting this symptom. This study presents a new
way to classify syncope types using machine learning. Method: we hypothesize
that syncope types can be characterized by analyzing blood pressure and heart
rate time series data obtained from the head-up tilt test procedure. By
optimizing classification rates, we identify a small number of determining
markers which enable data clustering. Results: We apply the proposed method to
clinical data from 157 subjects; each subject was identified by an expert as
being either healthy or suffering from one of three conditions:
cardioinhibitory syncope, vasodepressor syncope and postural orthostatic
tachycardia. Clustering confirms the three disease groups and identifies two
distinct subgroups within the healthy controls. Conclusion: The proposed method
provides evidence to question current syncope classifications; it also offers
means to refine them. Significance: Current syncope classifications are not
based on pathophysiology and have not led to significant improvements in
patient care. It is expected that a more faithful classification will
facilitate our understanding of the autonomic system for healthy subjects,
which is essential in analyzing pathophysiology of the disease groups.

In single-particle cryo-electron microscopy (cryo-EM), K-means clustering
algorithm is widely used in unsupervised 2D classification of projection images
of biological macromolecules. 3D ab initio reconstruction requires accurate
unsupervised classification in order to separate molecular projections of
distinct orientations. Due to background noise in single-particle images and
uncertainty of molecular orientations, traditional K-means clustering algorithm
may classify images into wrong classes and produce classes with a large
variation in membership. Overcoming these limitations requires further
development on clustering algorithms for cryo-EM data analysis. We propose a
novel unsupervised data clustering method building upon the traditional K-means
algorithm. By introducing an adaptive constraint term in the objective
function, our algorithm not only avoids a large variation in class sizes but
also produces more accurate data clustering. Applications of this approach to
both simulated and experimental cryo-EM data demonstrate that our algorithm is
a significantly improved alterative to the traditional K-means algorithm in
single-particle cryo-EM analysis.

In this paper we propose a workflow to detect and track mitotic cells in
time-lapse microscopy image sequences. In order to avoid the requirement for
cell lines expressing fluorescent markers and the associated phototoxicity,
phase contrast microscopy is often preferred over fluorescence microscopy in
live-cell imaging. However, common specific image characteristics complicate
image processing and impede use of standard methods. Nevertheless, automated
analysis is desirable due to manual analysis being subjective, biased and
extremely time-consuming for large data sets. Here, we present the following
workflow based on mathematical imaging methods. In the first step, mitosis
detection is performed by means of the circular Hough transform. The obtained
circular contour subsequently serves as an initialisation for the tracking
algorithm based on variational methods. It is sub-divided into two parts: in
order to determine the beginning of the whole mitosis cycle, a backwards
tracking procedure is performed. After that, the cell is tracked forwards in
time until the end of mitosis. As a result, the average of mitosis duration and
ratios of different cell fates (cell death, no division, division into two or
more daughter cells) can be measured and statistics on cell morphologies can be
obtained. All of the tools are featured in the user-friendly
MATLAB$^{\circledR}$ Graphical User Interface MitosisAnalyser.

Wild populations of Medicago ciliaris and Medicago polymorpha were subjected
to four salt treatments 0, 50, 100 and 150 mM NaCl, plant growth and proline
concentration in leaves were assessed. The analyzed data revealed significant
variability in salt response within and between the two species, depending on
the salinity level. It was found that high NaCl concentrations affected all the
growth parameters. However, the reduction was more important at higher NaCl
concentrations and the highest reduction was obtained for the populations of
Medicago polymorpha where it reached around 90% in root length at 150 mM NaCl
for Pmar. The Tunisian population of Medicago ciliaris, prospected on soils
affected by salinity, was the best tolerant in all ecotypes studied in this
work. This population, exhibits a particular adaptability to salt environment
at both germination and seedling stage. Furthermore, the correlation among the
studied plants sensitivity and leaf proline concentration showed that high
proline contents were related to their reactivity to salt. Consequently, it
appeared that proline biosynthesis occurred presumably as a consequence of
disturbance in cell homoeostasis and reflected poor performance and greater
damage in response to salt stress. These findings indicated that this osmolytes
content may be used as another useful criterion to differentiate salt-tolerant
from salt sensitive plant in annual medics.

The biomechanics of the human body allow humans a range of possible ways of
executing movements to attain specific goals. This range of movement is limited
by a number of mechanical, biomechanical, or cognitive constraints. Shifts in
these limits result in changes available possible movements from which a
subject can select and can affect which movements a subject selects. Therefore
by understanding the limits on the range of movement we can come to a better
understanding of declines in movement performance due to disease or aging. In
this project, we look at how models for the limits on the range of movement can
be derived in a principled manner from a model of the movement. Using the
example of normal walking gaits, we develop a lower limit on the avg. walking
speed by examining the process by which the body restores mechanical energy
lost during walking, and we develop an upper limit on the avg. step length by
examining the forces the body can exert doing external mechanical work, in this
case, pulling a cart. Making slight changes to the model for normal walking
gaits, we develop a model of very slow walking gaits with avg. walking speeds
below the lower limit on normal walking gaits but that also has a lower limit
on the avg. walking speed. We note that the lowest avg. walking speeds observed
clinically fall into the range of very slow walking gaits so defined, and argue
that forms of bipedal locomotion with still lower speeds should be considered
distinct from walking gaits.

STATCHECK is an R algorithm designed to scan papers automatically for
inconsistencies between test statistics and their associated p values (Nuijten
et al., 2016). The goal of this comment is to point out an important and
well-documented flaw in this busily applied algorithm: It cannot handle
corrected p values. As a result, statistical tests applying appropriate
corrections to the p value (e.g., for multiple tests, post-hoc tests,
violations of assumptions, etc.) are likely to be flagged as reporting
inconsistent statistics, whereas papers omitting necessary corrections are
certified as correct. The STATCHECK algorithm is thus valid for only a subset
of scientific papers, and conclusions about the quality or integrity of
statistical reports should never be based solely on this program.

Production of chemicals from engineered organisms in a batch culture involves
an inherent trade-off between productivity, yield, and titer. Existing
strategies for strain design typically focus on designing mutations that
achieve the highest yield possible while maintaining growth viability. While
these methods are computationally tractable, an optimum productivity could be
achieved by a dynamic strategy in which the intracellular division of resources
is permitted to change with time. New methods for the design and implementation
of dynamic microbial processes, both computational and experimental, have
therefore been explored to maximize productivity. However, solving for the
optimal metabolic behavior under the assumption that all fluxes in the cell are
free to vary is a challenging numerical task. This work presents an efficient
method for the calculation of a maximum theoretical productivity of a batch
culture system using a dynamic optimization framework. This metric is analogous
to the maximum theoretical yield, a measure that is well established in the
metabolic engineering literature and whose use helps guide strain and pathway
selection. The proposed method follows traditional assumptions of dynamic flux
balance analysis: (1) that internal metabolite fluxes are governed by a
pseudo-steady state, and (2) that external metabolite fluxes are dynamically
bounded. The optimization is achieved via collocation on finite elements, and
accounts explicitly for an arbitrary number of flux changes. The method can be
further extended to explicitly solve for the trade-off curve between maximum
productivity and yield. We demonstrate the method on succinate production in
two common microbial hosts, Escherichia coli and Actinobacillus succinogenes,
revealing that nearly optimal yields and productivities can be achieved with
only two discrete flux stages.

Thermodynamic scaling explains the dramatic successes of CTP fused human
growth proteins as regards lifetime in vivo and enhanced functionality compared
to their wild-type analogues, like Biogen. The theory is semi-quantitative and
contains no adjustable parameters. It shows how hydrophilic terminal spheres
orient fused proteins in the neighborhood of a membrane surface, extending
lifetimes and improving functionality.

Background: The huge quantity of data produced in Biomedical research needs
sophisticated algorithmic methodologies for its storage, analysis, and
processing. High Performance Computing (HPC) appears as a magic bullet in this
challenge. However, several hard to solve parallelization and load balancing
problems arise in this context. Here we discuss the HPC-oriented implementation
of a general purpose learning algorithm, originally conceived for DNA analysis
and recently extended to treat uncertainty on data (U BRAIN). The U-BRAIN
algorithm is a learning algorithm that finds a Boolean formula in disjunctive
normal form (DNF), of approximately minimum complexity, that is consistent with
a set of data (instances) which may have missing bits. The conjunctive terms of
the formula are computed in an iterative way by identifying, from the given
data, a family of sets of conditions that must be satisfied by all the positive
instances and violated by all the negative ones; such conditions allow the
computation of a set of coefficients (relevances) for each attribute (literal),
that form a probability distribution, allowing the selection of the term
literals. The great versatility that characterizes it, makes U-BRAIN applicable
in many of the fields in which there are data to be analyzed. However the
memory and the execution time required by the running are of O(n3) and of O(n5)
order, respectively, and so, the algorithm is unaffordable for huge data sets.

Modern technologies are enabling scientists to collect extraordinary amounts
of complex and sophisticated data across a huge range of scales like never
before. With this onslaught of data, we can allow the focal point to shift
towards answering the question of how we can analyze and understand the massive
amounts of data in front of us. Unfortunately, lack of standardized sharing
mechanisms and practices often make reproducing or extending scientific results
very difficult. With the creation of data organization structures and tools
which drastically improve code portability, we now have the opportunity to
design such a framework for communicating extensible scientific discoveries.
Our proposed solution leverages these existing technologies and standards, and
provides an accessible and extensible model for reproducible research, called
"science in the cloud" (sic). Exploiting scientific containers, cloud computing
and cloud data services, we show the capability to launch a computer in the
cloud and run a web service which enables intimate interaction with the tools
and data presented. We hope this model will inspire the community to produce
reproducible and, importantly, extensible results which will enable us to
collectively accelerate the rate at which scientific breakthroughs are
discovered, replicated, and extended.

Interpretability of machine learning models is critical for data-driven
precision medicine efforts. However, highly predictive models are generally
complex and are difficult to interpret. Here using Model-Agnostic Explanations
algorithm, we show that complex models such as random forest can be made
interpretable. Using MIMIC-II dataset, we successfully predicted ICU mortality
with 80% balanced accuracy and were also were able to interpret the relative
effect of the features on prediction at individual level.

In this paper, we study the "free exploration" of individual ants of the
species Atta insularis, i.e., their motion on a featureless flat, horizontal
surface. Two basic preliminary results emerge from our work (a) the free
exploration is super-diffusive and (b) ants tend to turn more frequently to the
left than to the right -so we call them "let-handed"-. More statistics is
needed, however, to confirm those findings.

Although somatic mutations are the main contributor to cancer, underlying
germline alterations may increase the risk of cancer, mold the somatic
alteration landscape and cooperate with acquired mutations to promote the tumor
onset and/or maintenance. Therefore, both tumor genome and germline sequence
data have to be analyzed to have a more complete picture of the overall genetic
foundation of the disease. To reinforce such notion we quantitatively assess
the bias of restricting the analysis to somatic mutation data using mutational
data from well-known cancer genes which displays both types of alterations,
inherited and somatically acquired mutations.

Metagenome, a mixture of different genomes (as a rule, bacterial), represents
a pattern, and the analysis of its composition is, currently, one of the
challenging problems of bioinformatics. In the present study, the possibility
of evaluating metagenome composition by DNA-marker methods is investigated.
These methods are based on using primers, short nucleic acid fragments. Each
primer picks out of the tested genome the fragment set specific just for this
genome, which is called its spectrum (for the given primer) and is used for
identifying the genome. The DNA-marker method, applied to a metagenome, also
gives its spectrum, which, obviously, represents the union of the spectra of
all genomes belonging to the metagenome. Thus each primer provides a projection
of the genomes and of the metagenome onto the corresponding spectra set. Here
we propose to apply the random projection (random primer) approach for
analyzing metagenome composition and present some estimates of the method
effectiveness for the case of Random Amplified Polymorphic DNA (RAPD)
technology.

Heparin is an important anticoagulant drug, about one billion doses are
produced annually. It is a polydisperse sulfated polysaccharide, and the
inherent heterogeneity makes the analysis of heparin difficult. The global
crisis resulting from adulterated heparin in 2008 has drawn renewed attention
to the challenges that are associated with the quality control and
characterization of this complex biological medicine from natural sources. The
present study addresses the need for simple and user-friendly analytical
methods for the fast and accurate quantification of heparin in complex
matrices. Direct quantification of heparin in the low microgram per mL range
was accomplished using a specific commercially available assay based on the
fluorescent molecular probe Heparin Red, simply by mixing the heparin
containing sample and a reagent solution in a 96-well microplate followed by
fluorescence readout. A screening of typical impurities in raw heparin
(selected other glycosaminoglycans, residual nucleic acids and proteins),
related to the extraction from animal tissues, as well as of components of the
urine matrix (inorganic salts, amino acids, trace proteins) revealed that these
compounds even in large excess have no or very little effect on the accuracy of
heparin determination. Heparin spike detection in urine, a biological
multicomponent matrix, also showed good accuracy. We envision applications of
this mix-and-read assay in the process and quality control in heparin
manufacturing, but also in pharmacokinetic studies as a convenient tool for
measuring of the urinary excretion of heparins.

Human movements are physical processes combining the classical mechanics of
the human body moving in space and the biomechanics of the muscles generating
the forces acting on the body under sophisticated sensory-motor control. One
way to characterize movement performance is through measures of energy
efficiency that relate the mechanical energy of the body and metabolic energy
expended by the muscles. We expect the practical utility of such measures to be
greater when human subjects execute movements that maximize energy efficiency.
We therefore seek to understand if and when subjects select movements with that
maximizing energy efficiency. We proceed using a model-based approach to
describe movements which perform a task requiring the body to add or remove
external mechanical work to or from an object. We use the specific example of
walking gaits doing external mechanical work by pulling a cart, and estimate
the relationship between the avg. walking speed and avg. step length. In the
limit where no external work is done, we find that the estimated maximum energy
efficiency walking gait is much slower than the walking gaits healthy adults
typically select. We then modify the situation of the walking gait by
introducing an idealized mechanical device that creates an adjustable
mechanical advantage. The walking gaits that maximize the energy efficiency
using the optimal mechanical advantage are again much slower than the walking
gaits healthy adults typically select. We finally modify the situation so that
the avg. walking speed is fixed and derive the pattern of the avg. step length
and mechanical advantage that maximize energy efficiency.

Both the weighted and unweighted Unifrac distances have been very
successfully employed to assess if two communities differ, but do not give any
information about how two communities differ. We take advantage of recent
observations that the Unifrac metric is equivalent to the so-called earth
mover's distance (also known as the Kantorovich-Rubinstein metric) to develop
an algorithm that not only computes the Unifrac distance in linear time and
space, but also simultaneously finds which operational taxonomic units are
responsible for the observed differences between samples. This allows the
algorithm, called EMDUnifrac, to determine why given samples are different, not
just if they are different, and with no added computational burden. EMDUnifrac
can be utilized on any distribution on a tree, and so is particularly suitable
to analyzing both operational taxonomic units derived from amplicon sequencing,
as well as community profiles resulting from classifying whole genome shotgun
metagenomes. The EMDUnifrac source code (written in python) is freely available
at: https://github.com/dkoslicki/EMDUnifrac.

Motivation:
  Flux balance analysis, and its variants, are widely used methods for
predicting steady-state reaction rates in biochemical reaction networks. The
exploration of high dimensional networks with such methods is currently
hampered by software performance limitations.
  Results:
  DistributedFBA.jl is a high-level, high-performance, open-source
implementation of flux balance analysis in Julia. It is tailored to solve
multiple flux balance analyses on a subset or all the reactions of large and
huge-scale networks, on any number of threads or nodes.
  Availability:
  The code and benchmark data are freely available on
http://github.com/opencobra/COBRA.jl. The documentation can be found at
http://opencobra.github.io/COBRA.jl

Dyslexia is a developmental learning disorder of single word reading accuracy
and/or fluency, with compelling research directed towards understanding the
contributions of the visual system. While dyslexia is not an oculomotor
disease, readers with dyslexia have shown different eye movements than
typically developing students during text reading. Readers with dyslexia
exhibit longer and more frequent fixations, shorter saccade lengths, more
backward refixations than typical readers. Furthermore, readers with dyslexia
are known to have difficulty in reading long words, lower skipping rate of
short words, and high gaze duration on many words. It is an open question
whether it is possible to harness these distinctive oculomotor scanning
patterns observed during reading in order to develop a screening tool that can
reliably identify struggling readers, who may be candidates for dyslexia. Here,
we introduce a novel, fast, objective, non-invasive method, named Rapid
Assessment of Difficulties and Abnormalities in Reading (RADAR) that screens
for features associated with the aberrant visual scanning of reading text seen
in dyslexia. Eye tracking parameter measurements that are stable under retest
and have high discriminative power, as indicated by their ROC curves, were
obtained during silent text reading. These parameters were combined to derive a
total reading score (TRS) that can reliably separate readers with dyslexia from
typical readers. We tested TRS in a group of school-age children ranging from
8.5 to 12.5 years of age. TRS achieved 94.2% correct classification of children
tested. Specifically, 35 out of 37 control (specificity 94.6%) and 30 out of 32
readers with dyslexia (sensitivity 93.8%) were classified correctly using
RADAR, under a circular validation condition where the individual evaluated was
not included in the test construction group.

Burkholderia is an important genus encompassing a variety of species,
including pathogenic strains as well as strains that promote plant growth. We
have carried out a global strategy, which combined two complementary
approaches. The first one is genome guided with deep analysis of genome
sequences and the second one is assay guided with experiments to support the
predictions obtained in silico. This efficient screening for new secondary
metabolites, performed on 48 gapless genomes of Burkholderia species, revealed
a total of 161 clusters containing nonribosomal peptide synthetases (NRPSs),
with the potential to synthesize at least 11 novel products. Most of them are
siderophores or lipopeptides, two classes of products with potential
application in biocontrol. The strategy led to the identification, for the
first time, of the cluster for cepaciachelin biosynthesis in the genome of
Burkholderia ambifaria AMMD and a cluster corresponding to a new
malleobactin-like siderophore, called phymabactin, was identified in
Burkholderia phymatum STM815 genome. In both cases, the siderophore was
produced when the strain was grown in iron-limited conditions. Elsewhere, the
cluster for the antifungal burkholdin was detected in the genome of B.
ambifaria AMMD and also Burkholderia sp. KJ006. Burkholderia pseudomallei
strains harbor the genetic potential to produce a novel lipopeptide called
burkhomycin, containing a peptidyl moiety of 12 monomers. A mixture of
lipopeptides produced by Burkholderia rhizoxinica lowered the surface tension
of the supernatant from 70 to 27 mN/m. The production of nonribosomal secondary
metabolites seems related to the three phylogenetic groups obtained from 16S
rRNA sequences. Moreover, the genome-mining approach gave new insights into the
nonribosomal synthesis exemplified by the identification of dual C/E domains in
lipopeptide NRPSs, up to now essentially found in Pseudomonas strains.

Metabarcoding on amplicons is rapidly expanding as a method to produce
molecular based inventories of microbial communities. Here, we work on
freshwater diatoms, which are microalgae possibly inventoried both on a
morphological and a molecular basis. We have developed an algorithm, in a
program called diagno-syst, based a the notion of informative read, which
carries out supervised clustering of reads by mapping them exactly one by one
on all reads of a well curated and taxonomically annotated reference database.
This program has been run on a HPC (and HTC) infrastructure to address
computation load. We compare optical and molecular based inventories on 10
samples from L\'eman lake, and 30 from Swedish rivers. We track all
possibilities of mismatches between both approaches, and compare the results
with standard pipelines (with heuristics) like Mothur. We find that the
comparison with optics is more accurate when using exact calculations, at the
price of a heavier computation load. It is crucial when studying the long tail
of biodiversity, which may be overestimated by pipelines or algorithms using
heuristics instead (more false positive). This work supports the analysis that
these methods will benefit from progress in, first, building an agreement
between molecular based and morphological based systematics and, second, having
as complete as possible publicly available reference databases.

The \emph{community}, the assemblage of organisms co-existing in a given
space and time, has the potential to become one of the unifying concepts of
biology, especially with the advent of high-throughput sequencing experiments
that reveal genetic diversity exhaustively. In this spirit we show that a tool
from community ecology, the Rank Abundance Distribution (RAD), can be turned by
the new MaxRank normalization method into a generic, expressive descriptor for
quantitative comparison of communities in many areas of biology. To illustrate
the versatility of the method, we analyze RADs from various \emph{generalized
communities}, i.e.\ assemblages of genetically diverse cells or organisms,
including human B cells, gut microbiomes under antibiotic treatment and of
different ages and countries of origin, and other human and environmental
microbial communities. We show that normalized RADs enable novel quantitative
approaches that help to understand structures and dynamics of complex
generalize communities.

Background. Wearable accelerometry devices allow collection of high-density
activity data in large epidemiological studies both in-the-lab as well as
in-the-wild (free-living). Such data can be used to detect and identify periods
of sustained harmonic walking. This report aims to establish whether the micro-
and macro-features of walking identified in the laboratory and free-living
environments are associated with measures of physical function, mobility,
fatigability, and fitness.
  Methods. Fifty-one older adults (median age 77.5) enrolled in the
Developmental Epidemiologic Cohort Study in Pittsburgh, Pennsylvania were
included in the analyses. The study included an in-the-lab component as well as
7 days of monitoring in-the-wild. Participants were equipped with hip-worn
Actigraph GT3X+ activity monitors, which collect high-density raw accelerometry
data. We applied a walking identification algorithm to the data and defined
features of walking, such as participant-specific walking acceleration and
cadence. The association between these walking features and physical function,
mobility, fatigability, and fitness was quantified using linear regression
analysis.
  Results. Micro-scale features of walking (acceleration and cadence) estimated
from in-the-lab and in-the-wild data were associated with measures of physical
function, mobility, fatigability, and fitness. In-the-lab median walking
acceleration was strongly inversely associated with physical function,
mobility, fatigability and fitness. Additionally, in-the-wild daily walking
time was inversely associated with usual- and fast-paced 400m walking time.
  Conclusions. The proposed accelerometry-derived walking features are
significantly associated with measures of physical function, mobility,
fatigability, and fitness, which provides evidence of convergent validity.

1. Electronic telemetry is frequently used to document animal movement
through time. Methods that can identify underlying behaviors driving specific
movement patterns can help us understand how and why animals use available
space, thereby aiding conservation and management efforts. For aquatic animal
tracking data with significant measurement error, a Bayesian state-space model
called the first-Difference Correlated Random Walk with Switching (DCRWS) has
often been used for this purpose. However, for aquatic animals, highly accurate
tracking data of animal movement are now becoming more common.
  2. We developed a new Hidden Markov Model (HMM) for identifying behavioral
states from animal tracks with negligible error, which we called the Hidden
Markov Movement Model (HMMM). We implemented as the basis for the HMMM the
process equation of the DCRWS, but we used the method of maximum likelihood and
the R package TMB for rapid model fitting.
  3. We compared the HMMM to a modified version of the DCRWS for highly
accurate tracks, the DCRWSnome, and to a common HMM for animal tracks fitted
with the R package moveHMM. We show that the HMMM is both accurate and suitable
for multiple species by fitting it to real tracks from a grey seal, lake trout,
and blue shark, as well as to simulated data.
  4. The HMMM is a fast and reliable tool for making meaningful inference from
animal movement data that is ideally suited for ecologists who want to use the
popular DCRWS implementation for highly accurate tracking data. It additionally
provides a groundwork for development of more complex modelling of animal
movement with TMB. To facilitate its uptake, we make it available through the R
package swim.

Quantification of system-wide perturbations from time series -omic data (i.e.
a large number of variables with multiple measures in time) provides the basis
for many downstream hypothesis generating tools. Here we propose a method,
Massively Parallel Analysis of Time Series (MPATS) that can be applied to
quantify transcriptome-wide perturbations. The proposed method characterizes
each individual time series through its $\ell_1$ distance to every other time
series. Application of MPATS to compare biological conditions produces a ranked
list of time series based on their magnitude of differences in their $\ell_1$
representation, which then can be further interpreted through enrichment
analysis. The performance of MPATS was validated through its application to a
study of IFN$\alpha$ dendritic cell responses to viral and bacterial infection.
In conjunction with Gene Set Enrichment Analysis (GSEA), MPATS produced
consistently identified signature gene sets of anti-bacterial and anti-viral
response. Traditional methods such as EDGE and GSEA Time Series (GSEA-TS)
failed to identify the relevant signature gene sets. Furthermore, the results
of MPATS highlighted the crucial functional difference between STAT1/STAT2
during anti-viral and anti-bacterial response. In our simulation study, MPATS
exhibited acceptable performance with small group size (n = 3), when the
appropriate effect size is considered. This method can be easily adopted for
other -omic data types.

The Gene Ontology (GO) is a major bioinformatics ontology that provides
structured controlled vocabularies to classify gene and proteins function and
role. The GO and its annotations to gene products are now an integral part of
functional analysis. Recently, the evaluation of similarity among gene products
starting from their annotations (also referred to as semantic similarities) has
become an increasing area in bioinformatics. While many research on updates to
the structure of GO and on the annotation corpora have been made, the impact of
GO evolution on semantic similarities is quite unobserved. Here we extensively
analyze how GO changes that should be carefully considered by all users of
semantic similarities. GO changes in particular have a big impact on
information content (IC) of GO terms. Since many semantic similarities rely on
calculation of IC it is obvious that the study of these changes should be
deeply investigated. Here we consider GO versions from 2005 to 2014 and we
calculate IC of all GO Terms considering five different formulation. Then we
compare these results. Analysis confirm that there exists a statistically
significant difference among different calculation on the same version of the
ontology (and this is quite obvious) and there exists a statistically
difference among the results obtained with different GO version on the same IC
formula. Results evidence there exist a remarkable bias due to the GO evolution
that has not been considered so far. Possible future works should keep into
account this consideration.

In the stochastic formulation of chemical kinetics, the stationary moments of
the population count of species can be described via a set of linear equations.
However, except for some specific cases such as systems with linear reaction
propensities, the moment equations are underdetermined as a lower order moment
might depend upon a higher order moment. Here, we propose a method to find
lower, and upper bounds on stationary moments of molecular counts in a chemical
reaction system. The method exploits the fact that statistical moments of any
positive-valued random variable must satisfy some constraints. Such constraints
can be expressed as nonlinear inequalities on moments in terms of their lower
order moments, and solving them in conjugation with the stationary moment
equations results in bounds on the moments. Using two examples of biochemical
systems, we illustrate that not only one obtains upper and lower bounds on a
given stationary moment, but these bounds also improve as one uses more moment
equations and utilizes the inequalities for the corresponding higher order
moments. Our results provide avenues for development of moment approximations
that provide explicit bounds on moment dynamics for systems whose dynamics are
otherwise intractable.

Assessing the performance and the characteristics (e.g. yield, quality,
disease resistance, abiotic stress tolerance) of new varieties is a key
component of crop performance improvement. However, the variety testing process
is presently exclusively based on experimental field approaches which
inherently reduces the number and the diversity of experienced combinations of
varieties x environmental conditions in regard of the multiplicity of growing
conditions within the cultivation area. Our aim is to make a greater and faster
use of the information issuing from these trials using crop modeling and
simulation to amplify the environmental and agronomic conditions in which the
new varieties are tested.
  In this study, we present a model-based approach to assist variety testing
and implement this approach on sunflower crop, using the SUNFLO simulation
model and a subset of 80 trials from a large multi-environment trial (MET)
conducted each year by agricultural extension services to compare newly
released sunflower hybrids. After estimating parameter values (using plant
phenotyping) to account for new genetic material, we independently evaluated
the model prediction capacity on the MET (model accuracy was 54.4 %) and its
capacity to rank commercial hybrids for performance level (Kendall's $\tau$ =
0.41, P < 0.01). We then designed a numerical experiment by combining the
previously tested genetic and new cropping conditions (2100 virtual trials) to
determine the best varieties and related management in representative French
production regions.
  We suggest that this approach could find operational outcomes to recommend
varieties according to environment types. Such spatial management of genetic
resources could potentially improve crop performance by reducing the
genotype-phenotype mismatch in farming environments.

The evolutionary success of ants and other social insects is considered to be
intrinsically linked to division of labor and emergent collective intelligence.
The role of the brains of individual ants in generating these processes,
however, is poorly understood. One genus of ant of special interest is
Pheidole, which includes more than a thousand species, most of which are
dimorphic, i.e. their colonies contain two subcastes of workers: minors and
majors. Using confocal imaging and manual annotations, it has been demonstrated
that minor and major workers of different ages of three species of Pheidole
have distinct patterns of brain size and subregion scaling. However, these
studies require laborious effort to quantify brain region volumes and are
subject to potential bias. To address these issues, we propose a group-wise 3D
registration approach to build for the first time bias-free brain atlases of
intra- and inter-subcaste individuals and automatize the segmentation of new
individuals.

Identification and alignment of three-dimensional folding of proteins may
yield useful information about relationships too remote to be detected by
conventional methods, such as sequence comparison, and may potentially lead to
prediction of patterns and motifs in mutual structural fragments. With the
exponential increase of structural proteomics data, the methods that scale with
the rate of increase of data lose efficiency. Hence, new methods that reduce
the computational expense of this problem should be developed. We present a
novel framework through which we are able to find and align protein structure
neighbors via hierarchical clustering and entropy-based query search, and
present a web-based protein database search and alignment tool to demonstrate
the applicability of our approach. The resulting method replicates the results
of the current gold standard with a minimal loss in sensitivity in a
significantly shorter amount of time, while ameliorating the existing web
workspace of protein structure comparison with a customized and dynamic
web-based environment. Our tool serves as both a functional industrial means of
protein structure comparison and a valid demonstration of heuristics in
proteomics.

A key challenge in drug delivery systems is the real time monitoring of
delivered drug and subsequent response. Recent advancement in nanotechnology
has enabled the design and preclinical implementation of novel drug delivery
systems (DDS) with theranostic abilities. Herein, fluorescent cerium fluoride
(CeF3) nanoparticles (nps) were synthesized and their surface modified with a
coat of polyethylenimine (PEI). Thereafter, Methotrexate was conjugated upon it
through glutaraldehyde crosslinking for a pH-sensitive release. This was
followed by the addition of a Hyaluronic acid (HA) receptor via
1-Ethyl-3-(3-dimethylaminopropyl)-carbodiimide and N-hydroxysuccinimide
(EDC-NHS) chemistry to achieve a possible active drug targeting system. The
obtained drug delivery nano-agent retains and exhibits unique photo-luminescent
properties attributed to the nps while exhibiting potential theranostic
capabilities.

Fire propagation is a major concern in the world in general and in
Argentinian northwestern Patagonia in particular where every year hundreds of
hectares are affected by both natural and anthropogenic forest fires. We
developed an efficient cellular automata model in Graphic Processing Units
(GPUs) to simulate fire propagation. The graphical advantages of GPUs were
exploded by overlapping wind direction maps, as well as vegetation, slope and
aspect maps, taking into account relevant landscape characteristics for fire
propagation. Stochastic propagation was performed with a probability model that
depends on aspect, slope, wind direction and vegetation type. Implementing a
genetic algorithm search strategy we show, using simulated fires, that we
recover the five parameter values that characterize fire propagation. The
efficiency of the fire simulation procedure allowed us to also estimate the
fire ignition point when it is unknown as well as its associated uncertainty,
making this approach suitable for the analysis of fire spread based on maps of
burned areas without knowing the point of origin of the fires or how they
spread.

This paper surveys various distance measures for networks and graphs that
were introduced in persistent homology. The scope of the paper is limited to
network distances that were actually used in brain networks but the methods can
be easily adapted to any weighted graph in other fields. The network version of
Gromov-Hausdorff, bottleneck, kernel distances are introduced. We also
introduce a recently developed KS-test like distance based on monotonic
topology features such as the zeroth Betti number. Numerous toy examples and
the result of applying many different distances to the brain networks of
different clinical status and populations are given.

Cellular Electron Cryotomography (CryoET) offers the ability to look inside
cells and observe macromolecules frozen in action. A primary challenge for this
technique is identifying and extracting the molecular components within the
crowded cellular environment. We introduce a method using neural networks to
dramatically reduce the time and human effort required for subcellular
annotation and feature extraction. Subsequent subtomogram classification and
averaging yields in-situ structures of molecular components of interest.

Extreme climatic events have been shown to be strong drivers of tree growth,
forest dynamics, and range contraction. Here we study the climatic drivers of
Picea crassifolia Kom., an endemic to northwest China where climate has
significantly warmed. Picea crassifolia was sampled from its lower
distributional margin to its upper distributional margin on the Helan Mountains
to test the hypothesis that 1) growth at the upper limit is limited by cool
temperatures and 2) is limited by drought at its lower limit. We found that
trees at the lower distributional margin have experienced a higher rate of
stem-growth cessation events since 2001 compared to trees at other elevations.
While all populations have a similar climatic sensitivity, stem-growth
cessation events in trees at lower distributional margin appear to be driven by
low precipitation in June as the monsoon begins to deliver moisture to the
region. Evidence indicates that mid-summer (July) vapor pressure deficit (VPD)
exacerbates the frequency of these events. These data and our analysis makes it
evident that an increase in severity and frequency of drought early in the
monsoon season could increase the frequency and severity of stem-growth
cessation in Picea crassifolia trees at lower elevations. Increases in VPD and
warming would likely exacerbate the growth stress of this species on Helan
Mountain. Hypothetically, if the combinations of low moisture and increased VPD
stress becomes more common, the mortality rate of lower distributional margin
trees could increase, especially of those that are already experiencing events
of temporary growth cessation.

Estimates of age-specific natural (M) and fishing (F) mortalities among
economically important stocks are required to determine sustainable yields and,
ultimately, facilitate effective resource management. Here we used hazard
functions to estimate mortality rates for eastern sea garfish, Hyporhamphus
australis, a pelagic species that forms the basis of an Australian commercial
lampara-net fishery. Data describing annual (2004 to 2015) age frequencies (0-1
to 5-6 years), yield, effort (boat-days), and average weights at age were used
to fit various stochastic models to estimate mortality rates by maximum
likelihood. The model best supported by the data implied: (i) the escape of
fish aged 0-1 years increased from approximately 90 to 97% as a result of a
mandated increase in stretched mesh opening from 25 to 28 mm; (ii) full
selectivity among older age groups; (iii) a constant M of 0.52 +- 0.06 per
year; and (iv) a decline in F between 2004 and 2015. Recruitment and biomass
were estimated to vary, but increased during the sampled period. The results
reiterate the utility of hazard functions to estimate and partition mortality
rates, and support traditional input controls designed to reduce both accounted
and unaccounted F.

Motivation: Cellular Electron CryoTomography (CECT) enables 3D visualization
of cellular organization at near-native state and in sub-molecular resolution,
making it a powerful tool for analyzing structures of macromolecular complexes
and their spatial organizations inside single cells. However, high degree of
structural complexity together with practical imaging limitations make the
systematic de novo discovery of structures within cells challenging. It would
likely require averaging and classifying millions of subtomograms potentially
containing hundreds of highly heterogeneous structural classes. Although it is
no longer difficult to acquire CECT data containing such amount of subtomograms
due to advances in data acquisition automation, existing computational
approaches have very limited scalability or discrimination ability, making them
incapable of processing such amount of data.
  Results: To complement existing approaches, in this paper we propose a new
approach for subdividing subtomograms into smaller but relatively homogeneous
subsets. The structures in these subsets can then be separately recovered using
existing computation intensive methods. Our approach is based on supervised
structural feature extraction using deep learning, in combination with
unsupervised clustering and reference-free classification. Our experiments show
that, compared to existing unsupervised rotation invariant feature and
pose-normalization based approaches, our new approach achieves significant
improvements in both discrimination ability and scalability. More importantly,
our new approach is able to discover new structural classes and recover
structures that do not exist in training data.

Human movements are physical processes combining the classical mechanics of
the human body moving in space and the biomechanics of the muscles generating
the forces acting on the body under sophisticated sensory-motor control. The
characterization of the performance of human movements is a problem with
important applications in clinical and sports research. One way to characterize
movement performance is through measures of energy efficiency that relate the
mechanical energy of the body and metabolic energy expended by the muscles.
Such a characterization provides information about the performance of a
movement insofar as subjects select movements with the aim of maximizing the
energy efficiency. We examine the case of the energy efficiency of asynchronous
arm-cranking doing external mechanical work, that is, using the arms to turn an
asynchronous arm-crank that performs external mechanical work. We construct a
metabolic energy model and use it estimate how cranking may be performed with
maximum energy efficiency, and recover the intuitive result that for larger
external forces the crank-handles should be placed as far from the center of
the crank as is comfortable for the subject to turn. We further examine
mechanical advantage in asynchronous arm-cranking by constructing an idealized
system that is driven by a crank and which involves an adjustable mechanical
advantage, and analyze the case in which the avg. frequency is fixed and derive
the mechanical advantages that maximize energy efficiency.

Heparan sulfate (HS) is a linear, polydisperse sulfated polysaccharide
belonging to the glycosaminoglycan family. HS proteoglycans are ubiquitously
found at the cell surface and extracellular matrix in animal species. HS is
involved in the interaction with a wide variety of proteins and the regulation
of many biological activities. In certain pathologic conditions, expression and
shedding of HS proteoglycans is overregulated, or enzymatic degradation of HS
in lysosomes is deficient, both leading to excess circulating free HS chains in
blood plasma. HS has therefore been suggested as a biomarker for various severe
disease states. The structural heterogeneity makes the quantification of
heparan sulfate in complex matrices such as human plasma challenging. HS plasma
levels are usually quantified by either disaccharide analysis or enzyme linked
immunosorbent assay(ELISA). Both methods require time-consuming
multistep-protocols. We describe here the instant detection of heparan sulfate
in spiked plasma samples by the Heparin Red Kit, a commercial mix-and-read
fluorescence microplate assay. The method enables HS quantification in the low
microgram per mL range without sample pretreatment. Heparin Red appears to be
sufficiently sensitive for the detection of highly elevated HS levels as
reported for mucopolysaccharidosis, graft versus host disease after
transplantation, dengue infection or septic shock. This study is a significant
step toward the development of a convenient and fast method for the
quantification of HS in human plasma, with the potential to simplify the
detection and advance the acceptance of HS as a biomarker.

Low grade gliomas (LGGs) are infiltrative and incurable primary brain tumours
with typically slow evolution. These tumours usually occur in young and
otherwise healthy patients, bringing controversies in treatment planning since
aggressive treatment may lead to undesirable side effects. Thus, for management
decisions it would be valuable to obtain early estimates of LGG growth
potential. Here we propose a simple mathematical model of LGG growth and its
response to chemotherapy which allows the growth of LGGs to be described in
real patients. The model predicts, and our clinical data confirms, that the
speed of response to chemotherapy is related to tumour aggressiveness.
Moreover, we provide a formula for the time to radiological progression, which
can be possibly used as a measure of tumour aggressiveness. Finally, we suggest
that the response to a few chemotherapy cycles upon diagnosis might be used to
predict tumour growth and to guide therapeutical actions on the basis of the
findings.

In scientific literature, there are many programs that predict linear B-cell
epitopes from a protein sequence. Each program generates multiple B-cell
epitopes that can be individually studied. This paper defines a function called
<C> that combines results from five different prediction programs concerning
the linear B-cell epitopes (ie., BebiPred, EPMLR, BCPred, ABCPred and Emini
Prediction) for selecting the best B-cell epitopes. We obtained 17 potential
linear B cells consensus epitopes from Glycoprotein E from serotype IV of the
dengue virus for exploring new possibilities in vaccine development. The direct
implication of the results obtained is to open the way to experimentally
validate more epitopes to increase the efficiency of the available treatments
against dengue and to explore the methodology in other diseases.

The increasing capacity of high-throughput genomic technologies for
generating time-course data has stimulated a rich debate on the most
appropriate methods to highlight crucial aspects of data structure. In this
work, we address the problem of sparse co-expression network representation of
several time-course stress responses in {\it Saccharomyces cerevisiae}. We
quantify the information preserved from the original datasets under a
graph-theoretical framework and evaluate how cross-stress features can be
identified. This is performed both from a node and a network community
organization point of view. Cluster analysis, here viewed as a problem of
network partitioning, is achieved under state-of-the-art algorithms relying on
the properties of stochastic processes on the constructed graphs. Relative
performance with respect to a metric-free Bayesian clustering analysis is
evaluated and possible extensions are discussed. We further cluster the
stress-induced co-expression networks generated independently by using their
community organization at multiple scales. This type of protocol allows for an
integration of multiple datasets that may not be immediately comparable, either
due to diverse experimental variations or because they represent different
types of information about the same genes.

To understand the nature of a cell, one needs to understand the structure of
its genome. For this purpose, experimental techniques such as Hi-C detecting
chromosomal contacts are used to probe the three-dimensional genomic structure.
These experiments yield topological information, consistently showing a
hierarchical subdivision of the genome into self-interacting domains across
many organisms. Current methods for detecting these domains using the Hi-C
contact matrix, i.e. a doubly-stochastic matrix, are mostly based on the
assumption that the domains are distinct, thus non-overlapping. For overcoming
this simplification and for being able to unravel a possible nested domain
structure, we developed a probabilistic graphical model that makes no a priori
assumptions on the domain structure. Within this approach, the Hi-C contact
matrix is analyzed using an Ising like probabilistic graphical model whose
coupling constant is proportional to each lattice point (entry in the contact
matrix). The results show clear boundaries between identified domains and the
background. These domain boundaries are dependent on the coupling constant, so
that one matrix yields several clusters of different sizes, which show the
self-interaction of the genome on different scales.

1. Animal movement patterns contribute to our understanding of variation in
breeding success and survival of individuals, and the implications for
population dynamics. 2. Over time, sensor technology for measuring movement
patterns has improved. Although older technologies may be rendered obsolete,
the existing data are still valuable, especially if new and old data can be
compared to test whether a behaviour has changed over time. 3. We used
simulated data to assess the ability to quantify and correctly identify
patterns of seabird flight lengths under observational regimes used in
successive generations of tracking technology. 4. Care must be taken when
comparing data collected at differing time-scales, even when using inference
procedures that incorporate the observational process, as model selection and
parameter estimation may be biased. In practice, comparisons may only be valid
when degrading all data to match the lowest resolution in a set. 5. Changes in
tracking technology that lead to aggregation of measurements at different
temporal scales make comparisons challenging. We therefore urge ecologists to
use synthetic data to assess whether accurate parameter estimation is possible
for models comparing disparate data sets before conducting analyses such as
responses to environmental changes or the assessment of management actions.

Camera-traps is a relatively new but already popular instrument in the
estimation of abundance of non-identifiable animals. Although camera-traps are
convenient in application, there remain both theoretical complications such as
spatial autocorrelation or false negative problem and practical difficulties,
for example, laborious random sampling. In the article we propose an
alternative way to bypass the mentioned problems.
  In the proposed approach, the raw video information collected from the
camera-traps situated at the spots of natural attraction is turned into the
frequency of visits, and the latter is transformed into the desired abundance
estimate. The key for such a transformation is the application of the
correction coefficients, computed for each particular observation environment
using the Bayesian approach and the massive database (DB) of observations under
various conditions.
  The main result of the article is a new method of census based on video-data
from camera-traps at the spots of natural attraction and information from a
special community-driven database.
  The proposed method is based on automated video-capturing at a moderate
number of easy to reach spots, so in the long term many laborious census works
may be conducted easier, cheaper and cause less disturbance for the wild life.
Information post-processing is strictly formalized, which leaves little chance
for subjective alterations. However, the method heavily relies on the volume
and quality of the DB, which in its turn heavily relies on the efforts of the
community. There is realistic hope that the community of zoologists and
environment specialists could create and maintain a DB similar to the proposed
one. Such a rich DB of visits might benefit not only censuses, but also many
behavioral studies.

Motivation: Metabolomics data is typically scaled to a common reference like
a constant volume of body fluid, a constant creatinine level, or a constant
area under the spectrum. Such normalization of the data, however, may affect
the selection of biomarkers and the biological interpretation of results in
unforeseen ways.
  Results: First, we study how the outcome of hypothesis tests for differential
metabolite concentration is affected by the choice of scale. Furthermore, we
observe this interdependence also for different classification approaches.
Second, to overcome this problem and establish a scale-invariant biomarker
discovery algorithm, we extend linear zero-sum regression to the logistic
regression framework and show in two applications to ${}^1$H NMR-based
metabolomics data how this approach overcomes the scaling problem.
  Availability: Logistic zero-sum regression is available as an R package as
well as a high-performance computing implementation that can be downloaded at
https://github.com/rehbergT/zeroSum

Most human protein-coding genes can be transcribed into multiple possible
distinct mRNA isoforms. These alternative splicing patterns encourage molecular
diversity and dysregulation of isoform expression plays an important role in
disease etiology. However, isoforms are difficult to characterize from
short-read RNA-seq data because they share identical subsequences and exist in
tissue- and sample-specific frequencies. Here, we develop BIISQ, a Bayesian
nonparametric model to discover Isoforms and Individual Specific Quantification
from RNA-seq data. BIISQ does not require known isoform reference sequences but
instead estimates isoform composition directly with an isoform catalog shared
across samples. We develop a stochastic variational inference approach for
efficient and robust posterior inference and demonstrate superior precision and
recall for short read RNA-seq simulations and simulated short read data from
PacBio long read sequencing when compared to state-of-the-art isoform
reconstruction methods. BIISQ achieves the most significant gains for longer
(in terms of exons) isoforms and isoforms that are lowly expressed (over 500%
more transcripts correctly inferred at low coverage in simulations). Finally,
we estimate isoforms in the GEUVADIS RNA-seq data, identify genetic variants
that regulate transcript ratios, and demonstrate variant enrichment in
functional elements related to mRNA splicing regulation.

Advances in molecular biology are enabling rapid and efficient analyses for
effective intervention in domains such as biology research, infectious disease
management, food safety, and biodefense. The emergence of microfluidics and
nanotechnologies has enabled both new capabilities and instrument sizes
practical for point-of-care. It has also introduced new functionality, enhanced
sensitivity, and reduced the time and cost involved in conventional molecular
diagnostic techniques. This chapter reviews the application of microfluidics
for molecular diagnostics methods such as nucleic acid amplification,
next-generation sequencing, high resolution melting analysis, cytogenetics,
protein detection and analysis, and cell sorting. We also review microfluidic
sample preparation platforms applied to molecular diagnostics and targeted to
sample-in, answer-out capabilities.

The Drosophila melanogaster white-eyed w1118 line serves as a blank control,
allowing genetic recombination of any gene of interest along with a readily
recognizable marker. w1118 flies display behavioral susceptibility to
environmental stimulation such as light. It is of great importance to
characterize the behavioral performance of w1118 flies because this would
provide a baseline from which the effect of the gene of interest could be
differentiated. Little work has been performed to characterize the walking
behavior in adult w1118 flies. Here we show that pulsed light stimulation
increased the regularity of walking trajectories of w1118 flies in circular
arenas. We statistically modeled the distribution of distances to center and
extracted the walking structures of w1118 flies. Pulsed light stimulation
redistributed the time proportions for individual walking structures.
Specifically, pulsed light stimulation reduced the episodes of crossing over
the central region of the arena. An addition of four genomic copies of
mini-white, a common marker gene for eye color, mimicked the effect of pulsed
light stimulation in reducing crossing in a circular arena. The reducing effect
of mini-white was copy-number-dependent. These findings highlight the rhythmic
light stimulation-evoked modifications of walking behavior in w1118 flies and
an unexpected behavioral consequence of mini-white in transgenic flies carrying
w1118 isogenic background.

Motivation: Site directed mutagenesis is widely used to understand the
structure and function of biomolecules. Computational prediction of protein
mutation impacts offers a fast, economical and potentially accurate alternative
to laboratory mutagenesis. Most existing methods rely on geometric
descriptions, this work introduces a topology based approach to provide an
entirely new representation of protein mutation impacts that could not be
obtained from conventional techniques. Results: Topology based mutation
predictor (T-MP) is introduced to dramatically reduce the geometric complexity
and number of degrees of freedom of proteins, while element specific persistent
homology is proposed to retain essential biological information. The present
approach is found to outperform other existing methods in globular protein
mutation impact predictions. A Pearson correlation coefficient of 0.82 with an
RMSE of 0.92 kcal/mol is obtained on a test set of 350 mutation samples. For
the prediction of membrane protein stability changes upon mutation, the
proposed topological approach has a 84% higher Pearson correlation coefficient
than the current state-of-the-art empirical methods, achieving a Pearson
correlation of 0.57 and an RMSE of 1.09 kcal/mol in a 5-fold cross validation
on a set of 223 membrane protein mutation samples.

Epistasis, or the context-dependence of the effects of mutations, limits our
ability to predict the functional impact of combinations of mutations, and
ultimately our ability to predict evolutionary trajectories. Information about
the context-dependence of mutations can essentially be obtained in two ways:
First, by experimental measurement the functional effects of combinations of
mutations and calculating the epistatic contributions directly, and second, by
statistical analysis of the frequencies and co-occurrences of protein residues
in a multiple sequence alignment of protein homologs. In this manuscript, we
derive the mathematical relationship between epistasis calculated on the basis
of functional measurements, and the covariance calculated from a multiple
sequence alignment. There is no one-to-one mapping between covariance and
epistatic terms: covariance implies epistasis, but epistasis does not
necessarily lead to covariance, indicating that covariance in itself is not the
directly relevant quantity for functional prediction. Having calculated
epistatic contributions from the alignment, we can directly obtain a functional
prediction from the alignment statistics by applying a Walsh-Hadamard
transform, fully analogous to the transformation that reconstructs functional
data from measured epistatic contributions. This embedding into the Hadamard
framework is directly relevant for solidifying our theoretical understanding of
statistical methods that predict function and three-dimensional structure from
natural alignments.

Although deep learning approaches have had tremendous success in image, video
and audio processing, computer vision, and speech recognition, their
applications to three-dimensional (3D) biomolecular structural data sets have
been hindered by the entangled geometric complexity and biological complexity.
We introduce topology, i.e., element specific persistent homology (ESPH), to
untangle geometric complexity and biological complexity. ESPH represents 3D
complex geometry by one-dimensional (1D) topological invariants and retains
crucial biological information via a multichannel image representation. It is
able to reveal hidden structure-function relationships in biomolecules. We
further integrate ESPH and convolutional neural networks to construct a
multichannel topological neural network (TopologyNet) for the predictions of
protein-ligand binding affinities and protein stability changes upon mutation.
To overcome the limitations to deep learning arising from small and noisy
training sets, we present a multitask topological convolutional neural network
(MT-TCNN). We demonstrate that the present TopologyNet architectures outperform
other state-of-the-art methods in the predictions of protein-ligand binding
affinities, globular protein mutation impacts, and membrane protein mutation
impacts.

Particle tracking is a powerful biophysical tool that requires conversion of
large video files into position time series, i.e. traces of the species of
interest for data analysis. Current tracking methods, based on a limited set of
input parameters to identify bright objects, are ill-equipped to handle the
spectrum of spatiotemporal heterogeneity and poor signal-to-noise ratios
typically presented by submicron species in complex biological environments.
Extensive user involvement is frequently necessary to optimize and execute
tracking methods, which is not only inefficient but introduces user bias. To
develop a fully automated tracking method, we developed a convolutional neural
network for particle localization from image data, comprised of over 6,000
parameters, and employed machine learning techniques to train the network on a
diverse portfolio of video conditions. The neural network tracker provides
unprecedented automation and accuracy, with exceptionally low false positive
and false negative rates on both 2D and 3D simulated videos and 2D experimental
videos of difficult-to-track species.

Identifying disease genes from human genome is an important and fundamental
problem in biomedical research. Despite many publications of machine learning
methods applied to discover new disease genes, it still remains a challenge
because of the pleiotropy of genes, the limited number of confirmed disease
genes among whole genome and the genetic heterogeneity of diseases. Recent
approaches have applied the concept of 'guilty by association' to investigate
the association between a disease phenotype and its causative genes, which
means that candidate genes with similar characteristics as known disease genes
are more likely to be associated with diseases. However, due to the imbalance
issues (few genes are experimentally confirmed as disease related genes within
human genome) in disease gene identification, semi-supervised approaches, like
label propagation approaches and positive-unlabeled learning, are used to
identify candidate disease genes via making use of unknown genes for training -
typically in the scenario of a small amount of confirmed disease genes (labeled
data) with a large amount of unknown genome (unlabeled data). The performance
of Disease gene prediction models are limited by potential bias of single
learning models and incompleteness and noise of single biological data sources,
therefore ensemble learning models are applied via combining multiple diverse
biological sources and learning models to obtain better predictive performance.
In this thesis, we propose three computational models for identifying candidate
disease genes.

Objective: Due to the non-linearity of numerous biomedical signals,
non-linear analysis of multi-channel time series, notably multivariate
multiscale entropy (mvMSE), has been extensively used in biomedical signal
processing. However, mvMSE has three drawbacks: 1) mvMSE values are either
undefined or unreliable for short signals; 2) mvMSE is not fast enough for
real-time applications; and 3) the computation of mvMSE for signals with a
large number of channels requires the storage of a huge number of elements.
Methods: To deal with these problems and improve the stability of mvMSE, we
introduce multivariate multiscale dispersion entropy (MDE - mvMDE) as an
extension of our recently developed MDE, to quantify the complexity of
multivariate time series. Results: We assess mvMDE, in comparison with mvMSE
and multivariate multiscale fuzzy entropy (mvMFE), on correlated and
uncorrelated multi-channel noise signals, bivariate autoregressive processes,
and three biomedical datasets. The results show that mvMDE takes into account
dependencies in patterns across both the time and spatial domains. The mvMDE,
mvMSE, and mvMFE methods are consistent in that they lead to similar
conclusions about the underlying physiological conditions. However, the
proposed mvMDE discriminates various physiological states of the biomedical
recordings better than mvMSE and mvMFE. In addition, for both the short and
long time series, the mvMDE-based results are noticeably more stable than the
mvMSE- and mvMFE-based ones. Conclusion: For short multivariate time series,
mvMDE, unlike mvMSE, does not result in undefined values. Furthermore, mvMDE is
noticeably faster than mvMFE and mvMSE and also needs to store a considerably
smaller number of elements. Significance: Due to its ability to detect
different kinds of dynamics of multivariate signals, mvMDE has great potential
to analyse various physiological signals.

The aim of this paper was to develop statistical models to estimate
individual breed composition based on the previously proposed idea of
regressing discrete random variables corresponding to counts of reference
alleles of biallelic molecular markers located across the genome on the allele
frequencies of each marker in the pure (base) breeds. Some of the existing
regression-based methods do not guarantee that estimators of breed composition
will lie in the appropriate parameter space and none of them account for
uncertainty about allele frequencies in the pure breeds, that is, uncertainty
about the design matrix. In order to overcome these limitations, we proposed
two Bayesian generalized linear models. For each individual, both models assume
that the counts of the reference allele at each marker locus follow independent
Binomial distributions, use the logit link, and pose a Dirichlet prior over the
vector of regression coefficients (which corresponds to breed composition).
This prior guarantees that point estimators of breed composition like the
posterior mean pertain to the appropriate space. The difference between these
models is that model termed BIBI does not account for uncertainty about the
design matrix, while model termed BIBI2 accounts for such an uncertainty by
assigning independent Beta priors to the entries of this matrix. We implemented
these models in a multibreed Angus-Brahman population. Posterior means were
used as point estimators of breed composition. In addition, the ordinary least
squares estimator proposed by Kuehn et al. (2011) (OLSK) was also computed.
BIBI and BIBI2 estimated breed composition more accurately than OLSK, and BIBI2
had an 8.3% improvement in accuracy as compared to BIBI.

Vector tomography methods intend to reconstruct and visualize vector fields
in restricted domains by measuring line integrals of projections of these
vector fields. Here, we deal with the reconstruction of irrotational vector
functions from boundary measurements. As the majority of inverse problems,
vector field recovery is an ill posed in the continuous domain and therefore
further assumptions, measurements and constraints should be imposed for the
full vector field estimation. The reconstruction idea in the discrete domain
relies on solving a numerical system of linear equations which derives from the
approximation of the line integrals along lines which trace the bounded domain.
This work presents an extensive description of a vector field recovery, the
fundamental assumptions and the ill conditioning of this inverse problem. More
importantly we show that this inverse problem is regularized via the domain
discretization, i.e. we show that the recovery of an irrotational vector field
within a discrete grid employing a finite set of longitudinal line integrals,
leads to a consistent linear system which has bounded solution errors. We
elaborate on the estimation of the solution's error and we prove that this
relative error is finite and therefore a stable vector field reconstruction is
ensured. Such theoretical aspects are critical for future implementations of
vector tomography in practical applications like the inverse bioelectric field
problem. We validate our theoretical results by performing simulations that
reconstruct smooth irrotational fields based solely on a finite number of
boundary measurements and without the need of any additional or prior
information (e.g. transversal line integrals or source free assumption).

The household secondary attack risk (SAR), often called the secondary attack
rate or secondary infection risk, is the probability of infectious contact from
an infectious household member A to a given household member B, where we define
infectious contact to be a contact sufficient to infect B if he or she is
susceptible. Estimation of the SAR is an important part of understanding and
controlling the transmission of infectious diseases. In practice, it is most
often estimated using binomial models such as logistic regression, which
implicitly attribute all secondary infections in a household to the primary
case. In the simplest case, the number of secondary infections in a household
with m susceptibles and a single primary case is modeled as a binomial(m, p)
random variable where p is the SAR. Although it has long been understood that
transmission within households is not binomial, it is thought that multiple
generations of transmission can be safely neglected when p is small. We use
probability generating functions and simulations to show that this is a
mistake. The proportion of susceptible household members infected can be
substantially larger than the SAR even when p is small. As a result, binomial
estimates of the SAR are biased upward and their confidence intervals have poor
coverage probabilities even if adjusted for clustering. Accurate point and
interval estimates of the SAR can be obtained using longitudinal chain binomial
models or pairwise survival analysis, which account for multiple generations of
transmission within households, the ongoing risk of infection from outside the
household, and incomplete follow-up. We illustrate the practical implications
of these results in an analysis of household surveillance data collected by the
Los Angeles County Department of Public Health during the 2009 influenza A
(H1N1) pandemic.

We investigate usage of dynamic time warping (DTW) algorithm for aligning raw
signal data from MinION sequencer. DTW is mostly using for fast alignment for
selective sequencing to quickly determine whether a read comes from sequence of
interest.
  We show that standard usage of DTW has low discriminative power mainly due to
problem with accurate estimation of scaling parameters. We propose a simple
variation of DTW algorithm, which does not suffer from scaling problems and has
much higher discriminative power.

Developing an accurate and reliable injury predictor is central to the
biomechanical studies of traumatic brain injury. State-of-the-art efforts
continue to rely on empirical, scalar metrics based on kinematics or
model-estimated tissue responses explicitly pre-defined in a specific brain
region of interest. They could suffer from loss of information. A single
training dataset has also been used to evaluate performance but without
cross-validation. In this study, we developed a deep learning approach for
concussion classification using implicit features of the entire voxel-wise
white matter fiber strains. Using reconstructed American National Football
League (NFL) injury cases, leave-one-out cross-validation was employed to
objectively compare injury prediction performances against two baseline machine
learning classifiers (support vector machine (SVM) and random forest (RF)) and
four scalar metrics via univariate logistic regression (Brain Injury Criterion
(BrIC), cumulative strain damage measure of the whole brain (CSDM-WB) and the
corpus callosum (CSDM-CC), and peak fiber strain in the CC). Feature-based deep
learning and machine learning classifiers consistently outperformed all scalar
injury metrics across all performance categories in cross-validation (e.g.,
average accuracy of 0.844 vs. 0.746, and average area under the receiver
operating curve (AUC) of 0.873 vs. 0.769, respectively, based on the testing
dataset). Nevertheless, deep learning achieved the best cross-validation
accuracy, sensitivity, and AUC (e.g., accuracy of 0.862 vs. 0.828 and 0.842 for
SVM and RF, respectively). These findings demonstrate the superior performances
of deep learning in concussion prediction, and suggest its promise for future
applications in biomechanical investigations of traumatic brain injury.

In recent years, deep learning algorithms have outperformed the state-of-the
art methods in several areas thanks to the efficient methods for training and
for preventing overfitting, advancement in computer hardware, the availability
of vast amount data. The high performance of multi-task deep neural networks in
drug discovery has attracted the attention to deep learning algorithms in
bioinformatics area. Here, we proposed a hierarchical multi-task deep neural
network architecture based on Gene Ontology (GO) terms as a solution to protein
function prediction problem and investigated various aspects of the proposed
architecture by performing several experiments. First, we showed that there is
a positive correlation between performance of the system and the size of
training datasets. Second, we investigated whether the level of GO terms on GO
hierarchy related to their performance. We showed that there is no relation
between the depth of GO terms on GO hierarchy and their performance. In
addition, we included all annotations to the training of a set of GO terms to
investigate whether including noisy data to the training datasets change the
performance of the system. The results showed that including less reliable
annotations in training of deep neural networks increased the performance of
the low performed GO terms, significantly. We evaluated the performance of the
system using hierarchical evaluation method. Mathews correlation coefficient
was calculated as 0.75, 0.49 and 0.63 for molecular function, biological
process and cellular component categories, respectively. We showed that deep
learning algorithms have a great potential in protein function prediction area.
We plan to further improve the DEEPred by including other types of annotations
from various biological data sources. We plan to construct DEEPred as an open
access online tool.

Dynamic cerebral autoregulation, that is the transient response of cerebral
blood flow to changes in arterial blood pressure, is currently assessed using a
variety of different time series methods and data collection protocols. In the
continuing absence of a gold standard for the study of cerebral autoregulation
it is unclear to what extent does the assessment depend on the choice of a
computational method and protocol. We use continuous measurements of blood
pressure and cerebral blood flow velocity in the middle cerebral artery from
the cohorts of 18 normotensive subjects performing sit-to-stand manoeuvre. We
estimate cerebral autoregulation using a wide variety of black-box approaches
(ARI, Mx, Sx, Dx, FIR and ARX) and compare them in the context of
reproducibility and variability. For all autoregulation indices, considered
here, the ICC was greater during the standing protocol, however, it was
significantly greater (Fisher's Z-test) for Mx (p < 0.03), Sx (p<0.003)$ and Dx
(p<0.03). In the specific case of the sit-to-stand manoeuvre, measurements
taken immediately after standing up greatly improve the reproducibility of the
autoregulation coefficients. This is generally coupled with an increase of the
within-group spread of the estimates.

Accurate predictions of peptide retention times (RT) in liquid chromatography
have many applications in mass spectrometry-based proteomics. Herein, we
present DeepRT, a deep learning based software for peptide retention time
prediction. DeepRT automatically learns features directly from the peptide
sequences using the deep convolutional Neural Network (CNN) and Recurrent
Neural Network (RNN) model, which eliminates the need to use hand-crafted
features or rules. After the feature learning, principal component analysis
(PCA) was used for dimensionality reduction, then three conventional machine
learning methods were utilized to perform modeling. Two published datasets were
used to evaluate the performance of DeepRT and we demonstrate that DeepRT
greatly outperforms previous state-of-the-art approaches ELUDE and GPTime.

Prediction of poly(lactic co glycolic acid) (PLGA) micro- and nanoparticles'
dissolution rates plays a significant role in pharmaceutical and medical
industries. The prediction of PLGA dissolution rate is crucial for drug
manufacturing. Therefore, a model that predicts the PLGA dissolution rate could
be beneficial. PLGA dissolution is influenced by numerous factors (features),
and counting the known features leads to a dataset with 300 features. This
large number of features and high redundancy within the dataset makes the
prediction task very difficult and inaccurate. In this study, dimensionality
reduction techniques were applied in order to simplify the task and eliminate
irrelevant and redundant features. A heterogeneous pool of several regression
algorithms were independently tested and evaluated. In addition, several
ensemble methods were tested in order to improve the accuracy of prediction.
The empirical results revealed that the proposed evolutionary weighted ensemble
method offered the lowest margin of error and significantly outperformed the
individual algorithms and the other ensemble techniques.

Segmentation, the process of delineating tumor apart from healthy tissue, is
a vital part of both the clinical assessment and the quantitative analysis of
brain cancers. Here, we provide an open-source algorithm (MITKats), built on
the Medical Imaging Interaction Toolkit, to provide user-friendly and expedient
tools for semi-automatic segmentation. To evaluate its performance against
competing algorithms, we applied MITKats to 38 high-grade glioma cases from
publicly available benchmarks. The similarity of the segmentations to
expert-delineated ground truths approached the discrepancies among different
manual raters, the theoretically maximal precision. The average time spent on
each segmentation was 5 minutes, making MITKats between 4 and 11 times faster
than competing semi-automatic algorithms, while retaining similar accuracy.

Borneo contains some of the world's most biodiverse and carbon dense tropical
forest, but this 750,000-km2 island has lost 62% of its old-growth forests
within the last 40 years. Efforts to protect and restore the remaining forests
of Borneo hinge on recognising the ecosystem services they provide, including
their ability to store and sequester carbon. Airborne Laser Scanning (ALS) is a
remote sensing technology that allows forest structural properties to be
captured in great detail across vast geographic areas. In recent years ALS has
been integrated into state-wide assessment of forest carbon in Neotropical and
African regions, but not yet in Asia. For this to happen new regional models
need to be developed for estimating carbon stocks from ALS in tropical Asia, as
the forests of this region are structurally and compositionally distinct from
those found elsewhere in the tropics. By combining ALS imagery with data from
173 permanent forest plots spanning the lowland rain forests of Sabah, on the
island of Borneo, we develop a simple-yet-general model for estimating forest
carbon stocks using ALS-derived canopy height and canopy cover as input
metrics. An advanced feature of this new model is the propagation of
uncertainty in both ALS- and ground-based data, allowing uncertainty in
hectare-scale estimates of carbon stocks to be quantified robustly. We show
that the model effectively captures variation in aboveground carbons stocks
across extreme disturbance gradients spanning tall dipterocarp forests and
heavily logged regions, and clearly outperforms existing ALS-based models
calibrated for the tropics, as well as currently available satellite-derived
products. Our model provides a simple, generalised and effective approach for
mapping forest carbon stocks in Borneo, providing a key tool to support the
protection and restoration of its tropical forests.

The stochastic simulation algorithm commonly known as Gillespie's algorithm
is now used ubiquitously in the modelling of biological processes in which
stochastic effects play an important role. In well-mixed scenarios at the
sub-cellular level it is often reasonable to assume that times between
successive reaction/interaction events are exponentially distributed and can be
appropriately modelled as a Markov process and hence simulated by the Gillespie
algorithm. However, Gillespie's algorithm is routinely applied to model
biological systems for which it was never intended. In particular, processes in
which cell proliferation is important should not be simulated naively using the
Gillespie algorithm since the history-dependent nature of the cell cycle breaks
the Markov process. The variance in experimentally measured cell cycle times is
far less than in an exponential cell cycle time distribution with the same
mean.
  Here we suggest a method of modelling the cell cycle that restores the
memoryless property to the system and is therefore consistent with simulation
via the Gillespie algorithm. By breaking the cell cycle into a number of
independent exponentially distributed stages we can restore the Markov property
at the same time as more accurately approximating the appropriate cell cycle
time distributions. The consequences of our revised mathematical model are
explored analytically. We demonstrate the importance of employing the correct
cell cycle time distribution by considering two models incorporating cellular
proliferation (one spatial and one non-spatial) and demonstrating that changing
the cell cycle time distribution makes quantitative and qualitative differences
to their outcomes. Our adaptation will allow modellers and experimentalists
alike to appropriately represent cellular proliferation, whilst still being
able to take advantage of the Gillespie algorithm.

The Gaussian scale mixture model (GSM) is a simple yet powerful probabilistic
generative model of natural image patches. In line with the well-established
idea that sensory processing is adapted to the statistics of the natural
environment, the GSM has also been considered a model of the early visual
system, as a reasonable "first-order" approximation of the internal model that
the primary visual cortex (V1) implements. According to this view, neural
activities in V1 represent the posterior distribution under the GSM given a
particular visual stimulus. Indeed, (approximate) inference under the GSM has
successfully accounted for various nonlinearities in the mean (trial-average)
responses of V1 neurons, as well as the dependence of (across-trial) response
variability with stimulus contrast found in V1 recordings. However, previous
work almost exclusively relied on numerical simulations to obtain these
results. Thus, for a deeper insight into the realm of possible behaviours the
GSM can (and cannot) exhibit and predict, here we present analytical
derivations for the limiting behaviour of the mean and (co)variance of the GSM
posterior at very low and very high contrast levels. These results should guide
future work exploring neural circuit dynamics appropriate for implementing
inference under the GSM.

Spatio-temporal systems exhibiting multi-scale behaviour are common in
applications ranging from cyber-physical systems to systems biology, yet they
present formidable challenges for computational modelling and analysis. Here we
consider a prototypic scenario where spatially distributed agents decide their
movement based on external inputs and a fast-equilibrating internal
computation. We propose a generally applicable strategy based on statistically
abstracting the internal system using Gaussian Processes, a powerful class of
non-parametric regression techniques from Bayesian Machine Learning. We show on
a running example of bacterial chemotaxis that this approach leads to accurate
and much faster simulations in a variety of scenarios.

The goals of the Triple Aim of health care and the goals of P4 medicine
outline objectives that require a significant health informatics component.
However, the goals do not provide specifications about how all of the new
individual patient data will be combined in meaningful ways and with data from
other sources, like epidemiological data, to promote the health of individuals
and society. We seem to have more data than ever before but few resources and
means to use it efficiently. We need a general, extensible solution that
integrates and homogenizes data of disparate origin, incompatible formats, and
multiple spatial and temporal scales. To address this problem, we introduce the
Scientific Knowledge Extraction from Data (SKED) architecture, as a
technology-agnostic framework to minimize the overhead of data integration,
permit reuse of analytical pipelines, and guarantee reproducible quantitative
results. The SKED architecture consists of a Resource Allocation Service to
locate resources, and the definition of data primitives to simplify and
harmonize data. SKED allows automated knowledge discovery and provides a
platform for the realization of the major goals of modern health care.

Background: The past few years have seen a tremendous increase in the size
and complexity of datasets. Scientific and clinical studies must to incorporate
datasets that cross multiple spatial and temporal scales to describe a
particular phenomenon. The storage and accessibility of these heterogeneous
datasets in a way that is useful to researchers and yet extensible to new data
types is a major challenge.
  Methods: In order to overcome these obstacles, we propose the use of data
primitives as a common currency between analytical methods. The four data
primitives we have identified are time series, text, annotated graph and
triangulated mesh, with associated metadata. Using only data primitives to
store data and as algorithm input, output, and intermediate results, promotes
interoperability, scalability, and reproducibility in scientific studies.
  Results: Data primitives were used in a multi-omic, multi-scale systems
biology study of malaria infection in non-human primates to perform many types
of integrative analysis quickly and efficiently.
  Conclusions: Using data primitives as a common currency for both data storage
and for cross talk between analytical methods enables the analysis of complex
multi-omic, multi-scale datasets in a reproducible modular fashion.

A critical component of preventing the spread of vector borne diseases such
as Chagas disease are door-to-door campaigns by public health officials that
implement insecticide application in order to eradicate the vector infestation
of households. The success of such campaigns depends on adequate household
participation during the active phase as well as on sufficient follow-up during
the surveillance phase when newly infested houses or infested houses that had
not participated in the active phase will receive treatment. Queueing models
which are widely used in operations management give us a mathematical
representation of the operational efforts needed to contain the spread of
infestation. By modeling the queue as consisting of all infested houses in a
given locality, we capture the dynamics of the insect population due to
prevalence of infestation and to the additional growth of infestation by
redispersion, i.e. by the spread of infestation to previously uninfested houses
during the wait time for treatment. In contrast to traditional queueing models,
houses waiting for treatment are not known but must be identified through a
search process by public health workers. Thus, both the arrival rate of houses
to the queue as well as the removal rate from the queue depend on the current
level of infestation. We incorporate these dependencies through a load
dependent queueing model which allows us to estimate the long run average rate
of removing houses from the queue and therefore the cost associated with a
given surveillance program. The model is motivated by and applied to an ongoing
Chagas disease control campaign in Arequipa, Peru.

Many single-cell observables are highly heterogeneous. A part of this
heterogeneity stems from age-related phenomena: the fact that there is a
nonuniform distribution of cells with different ages. This has led to a renewed
interest in analytic methodologies including use of the "von Foerster equation"
for predicting population growth and cell age distributions. Here we discuss
how some of the most popular implementations of this machinery assume a strong
condition on the ergodicity of the cell cycle duration ensemble. We show that
one common definition for the term ergodicity, "a single individual observed
over many generations recapitulates the behavior of the entire ensemble" is
implied by the other, "the probability of observing any state is conserved
across time and over all individuals" in an ensemble with a fixed number of
individuals but that this is not true when the ensemble is growing. We further
explore the impact of generational correlations between cell cycle durations on
the population growth rate. Finally, we explore the "growth rate gain" - the
phenomenon that variations in the cell cycle duration lead to an improved
population-level growth rate - in this context. We highlight that,
fundamentally, this effect is due to asymmetric division.

In many situations, the gene expression signature is a unique marker of the
biological state. We study the modification of the gene expression distribution
function when the biological state of a system experiences a change. This
change may be the result of a selective pressure, as in the Long Term Evolution
Experiment with E. Coli populations, or the progression to Alzheimer disease in
aged brains, or the progression from a normal tissue to the cancer state. The
first two cases seem to belong to a class of transitions, where the initial and
final states are relatively close to each other, and the distribution function
for the differential expressions is short ranged, with a tail of only a few
dozens of strongly varying genes. In the latter case, cancer, the initial and
final states are far apart and separated by a low-fitness barrier. The
distribution function shows a very heavy tail, with thousands of silenced and
over-expressed genes. We characterize the biological states by means of their
principal component representations, and the expression distribution functions
by their maximal and minimal differential expression values and the exponents
of the Pareto laws describing the tails.

Sunflower (Helianthus annuus L.) grain and oil quality are defined by grain
weight and oil percentage, oil fatty acid composition and the amount of
antioxidants. The aim of this work was to establish and validate a simple
model, based on published relationships, which can estimate not only yield and
its components, but also grain and oil quality aspects which are of relevance
for industrial processes or human health. The model we developed provided good
estimations of grain yield (similar to those of a more complex model) and oil
quality from independent experiments. It explained known differences in
potential yield and grain and oil quality between locations, in terms of
differences in incident radiation, mean or minimum temperature. Simulations
showed that recent climatic changes could have caused a decrease in sunflower
yield and changes in oil quality. Our results suggest that at locations at
lower latitudes, sunflower oil with high nutritious value and oxidative
stability could compensate for relatively low yields, while at higher
latitudes, high-linoleic acid oil production should be compatible with high
yield potentials. Our model could facilitate the selection of the best
location, sowing date or density for the production of sunflower oil with
specific quality characteristics.

In this paper, I introduce a Sequence-based Multiscale Model (SeqMM) for the
biomolecular data analysis. With the combination of spectral graph method, I
reveal the essential difference between the global scale models and local scale
ones in structure clustering, i.e., different optimization on Euclidean (or
spatial) distances and sequential (or genomic) distances. More specifically,
clusters from global scale models optimize Euclidean distance relations. Local
scale models, on the other hand, result in clusters that optimize the genomic
distance relations. For a biomolecular data, Euclidean distances and sequential
distances are two independent variables, which can never be optimized
simultaneously in data clustering. However, sequence scale in my SeqMM can work
as a tuning parameter that balances these two variables and deliver different
clusterings based on my purposes. Further, my SeqMM is used to explore the
hierarchical structures of chromosomes. I find that in global scale, the
Fiedler vector from my SeqMM bears a great similarity with the principal vector
from principal component analysis, and can be used to study genomic
compartments. In TAD analysis, I find that TADs evaluated from different scales
are not consistent and vary a lot. Particularly when the sequence scale is
small, the calculated TAD boundaries are dramatically different. Even for
regions with high contact frequencies, TAD regions show no obvious consistence.
However, when the scale value increases further, although TADs are still quite
different, TAD boundaries in these high contact frequency regions become more
and more consistent. Finally, I find that for a fixed local scale, my method
can deliver very robust TAD boundaries in different cluster numbers.

Cellular processes are governed by macromolecular complexes inside the cell.
Study of the native structures of macromolecular complexes has been extremely
difficult due to lack of data. With recent breakthroughs in Cellular electron
cryo tomography (CECT) 3D imaging technology, it is now possible for
researchers to gain accesses to fully study and understand the macromolecular
structures single cells. However, systematic recovery of macromolecular
structures from CECT is very difficult due to high degree of structural
complexity and practical imaging limitations. Specifically, we proposed a deep
learning based image classification approach for large-scale systematic
macromolecular structure separation from CECT data. However, our previous work
was only a very initial step towards exploration of the full potential of deep
learning based macromolecule separation. In this paper, we focus on improving
classification performance by proposing three newly designed individual CNN
models: an extended version of (Deep Small Receptive Field) DSRF3D, donated as
DSRF3D-v2, a 3D residual block based neural network, named as RB3D and a
convolutional 3D(C3D) based model, CB3D. We compare them with our previously
developed model (DSRF3D) on 12 datasets with different SNRs and tilt angle
ranges. The experiments show that our new models achieved significantly higher
classification accuracies. The accuracies are not only higher than 0.9 on
normal datasets, but also demonstrate potentials to operate on datasets with
high levels of noises and missing wedge effects presented.

Bioinformatics research depends on high-quality databases to provide accurate
results. In silico experiments, correctly performed, may prospect novel
discoveries and elucidates pathways for biological experiments through data
analysis in large scale. However, most biological databases have presented
mistakes, such as data incorrectly classified or incomplete information. Also,
sometimes, data mining algorithms cannot treat these errors, leading to serious
problems for the in silico analysis. Manual curation of data extracted from
literature is a possible solution for this problem. Systematic Literature
Review (SLR), or Systematic Review, is a method to identify, evaluate and
summarize the state-of-the-art of a specific theme. Moreover, SLR allows the
collection from databases restrictively, which allows an analysis with lower
bias than traditional reviews. The SRL approaches have been widely used for
decision-making in medical and environmental studies. However, other research
areas, such as bioinformatics, do not have a specific step-by-step to guide
researchers undertaking the procedures of an SLR. In this study, we propose a
guideline, called BiSRL, to perform SLR in bioinformatics. Our procedures cover
the most traditional guides to produce SLRs adapted to bioinformatics. To
evaluate our method, we propose a case study to detect and summarize SLRs
developed for bioinformatics data. We used two databases: PubMed and
ScienceDirect. A total of 207 papers were screened in four steps: title,
abstract, diagonal and full-text reading. Four evaluators performed the SLR
independently to reduce bias risk. A total of 8 papers was included in the SLR
case study. The case study demonstrates how to implement the methods of BiSLR
to procedure SLR for bioinformatics. BiSLR may guide bioinformaticians to
perform systematic reviews reproducible to collect accurate data for higher
quality analysis.

As very large studies of complex neuroimaging phenotypes become more common,
human quality assessment of MRI-derived data remains one of the last major
bottlenecks. Few attempts have so far been made to address this issue with
machine learning. In this work, we optimize predictive models of quality for
meshes representing deep brain structure shapes. We use standard vertex-wise
and global shape features computed homologously across 19 cohorts and over 7500
human-rated subjects, training kernelized Support Vector Machine and Gradient
Boosted Decision Trees classifiers to detect meshes of failing quality. Our
models generalize across datasets and diseases, reducing human workload by
30-70\%, or equivalently hundreds of human rater hours for datasets of
comparable size, with recall rates approaching inter-rater reliability.

Kinetic rate constants fundamentally characterize the dynamics of the
chemical interaction of macromolecules, and thus their study sets a major
direction in experimental biochemistry. The estimation of such constants is
often challenging, partly due to the noisiness of data, and partly due to the
theoretical framework. Novel and qualitatively reasonable methods are presented
for the estimation of the rate constants of complex formation and dissociation
in Kinetic Capillary Electrophoresis (KCE). This also serves the broader effort
to resolve the inverse problem of KCE, where these estimates pose as initial
starting points in the non-linear optimization space, along with the asymmetric
Gaussian parameters describing the injected plug concentration profiles, which
is also hereby estimated. This rate constant estimation method is also compared
to an earlier one.

Determining kinetic rate constants is a highly relevant problem in
biochemistry, so various methods have been designed to extract them from
experimental data. Such methods have two main components: the experimental
apparatus and the subsequent analysis, the latter often dependent on
mathematical theory. Thus the theoretical approach taken influences the
effectiveness of constant determination. A computational inverse problem
approach is hereby presented, which does not merely give a single rough
approximation of the sought constants, but is inherently capable of determining
them from exact signals to arbitrary accuracy. This approach is thus not merely
novel, but opens a whole new category of solution approaches in the field,
enabled primarily by an efficient direct solver.

The discrete chemical master equation (dCME) provides a fundamental framework
for studying stochasticity in mesoscopic networks. Because of the multi-scale
nature of many networks where reaction rates have large disparity, directly
solving dCMEs is intractable due to the exploding size of the state space. It
is important to truncate the state space effectively with quantified errors, so
accurate solutions can be computed. It is also important to know if all major
probabilistic peaks have been computed. Here we introduce the Accurate CME
(ACME) algorithm for obtaining direct solutions to dCMEs. With multi-finite
buffers for reducing the state space by O(n!), exact steady-state and
time-evolving network probability landscapes can be computed. We further
describe a theoretical framework of aggregating microstates into a smaller
number of macrostates by decomposing a network into independent aggregated
birth and death processes, and give an a priori method for rapidly determining
steady-state truncation errors. The maximal sizes of the finite buffers for a
given error tolerance can also be pre-computed without costly trial solutions
of dCMEs. We show exactly computed probability landscapes of three multi-scale
networks, namely, a 6-node toggle switch, 11-node phage-lambda epigenetic
circuit, and 16-node MAPK cascade network, the latter two with no known
solutions. We also show how probabilities of rare events can be computed from
first-passage times, another class of unsolved problems challenging for
simulation-based techniques due to large separations in time scales. Overall,
the ACME method enables accurate and efficient solutions of the dCME for a
large class of networks.

The discrete chemical master equation (dCME) provides a general framework for
studying stochasticity in mesoscopic reaction networks. Since its direct
solution rapidly becomes intractable due to the increasing size of the state
space, truncation of the state space is necessary for solving most dCMEs. It is
therefore important to assess the consequences of state space truncations so
errors can be quantified and minimized. Here we describe a novel method for
state space truncation. By partitioning a reaction network into multiple
molecular equivalence groups (MEG), we truncate the state space by limiting the
total molecular copy numbers in each MEG. We further describe a theoretical
framework for analysis of the truncation error in the steady state probability
landscape using reflecting boundaries. By aggregating the state space based on
the usage of a MEG and constructing an aggregated Markov process, we show that
the truncation error of a MEG can be asymptotically bounded by the probability
of states on the reflecting boundary of the MEG. Furthermore, truncating states
of an arbitrary MEG will not undermine the estimated error of truncating any
other MEGs. We then provide an error estimate for networks with multiple MEGs.
To rapidly determine the appropriate size of an arbitrary MEG, we introduce an
a priori method to estimate the upper bound of its truncation error, which can
be rapidly computed from reaction rates, without costly trial solutions of the
dCME. We show results of applying our methods to four stochastic networks. We
demonstrate how truncation errors and steady state probability landscapes can
be computed using different sizes of the MEG(s) and how the results validate
out theories. Overall, the novel state space truncation and error analysis
methods developed here can be used to ensure accurate direct solutions to the
dCME for a large class of stochastic networks.

The metabolism of an organism is regulated at the cellular level, yet is
strongly influenced by its environment. The precise metabolomic study of living
organisms is currently hampered by measurement sensitivity: most metabolomic
measurement techniques involve some compromise, in that averaging is performed
over a volume significantly larger than a single cell, or require invasion of
the organism, or arrest the state of the organism. NMR is an inherently
non-invasive chemometric and imaging method, and hence in principle suitable
for metabolomic measurements. The digital twin of metabolomics is computational
systems biology, so that NMR microscopy is potentially a viable approach with
which to join the theoretical and experimental exploration of the metabolomic
and behavioural response of organisms. This prospect paper considers the
challenge of performing in vivo NMR-based metabolomics on the small organism C.
elegans, points the way towards possible solutions created using MEMS
techniques, and highlights currently insurmountable challenges.

In this work, we consider the problem of estimating summary statistics to
characterise biochemical reaction networks of interest. Such networks are often
described using the framework of the Chemical Master Equation (CME). For
physically-realistic models, the CME is widely considered to be analytically
intractable. A variety of Monte Carlo algorithms have therefore been developed
to explore the dynamics of such networks empirically. Amongst them is the
multi-level method, which uses estimates from multiple ensembles of sample
paths of different accuracies to estimate a summary statistic of interest. {In
this work, we develop the multi-level method in two directions: (1) to increase
the robustness, reliability and performance of the multi-level method, we
implement an improved variance reduction method for generating the sample paths
of each ensemble; and (2) to improve computational performance, we demonstrate
the successful use of a different mechanism for choosing which ensembles should
be included in the multi-level algorithm.

Analyzing the relation between a set of biological sequences can help to
identify and understand the evolutionary history of these sequences and the
functional relations among them. Multiple Sequence Alignment (MSA) is the main
obstacle to proper design and develop homology and evolutionary modeling
applications since these kinds of applications require an effective MSA
technique with high accuracy. This work proposes a novel Position-based
Multiple Sequence Alignment (PoMSA) technique -- which depends on generating a
position matrix for a given set of biological sequences. This position matrix
can be used to reconstruct the given set of sequences in more aligned format.
On the contrary of existing techniques, PoMSA uses position matrix instead of
distance matrix to correctly adding gaps in sequences which improve the
efficiency of the alignment operation. We have evaluated the proposed technique
with different datasets benchmarks such as BAliBASE, OXBench, and SMART. The
experiments show that PoMSA technique satisfies higher alignment score compared
to existing state-of-art algorithms: Clustal-Omega, MAFTT, and MUSCLE.

Imaging data has become widely available to study biological systems at
various scales, for example the motile behaviour of bacteria or the transport
of mRNA, and it has the potential to transform our understanding of key
transport mechanisms. Often these imaging studies require us to compare
biological species or mutants, and to do this we need to quantitatively
characterise their behaviour. Mathematical models offer a quantitative
description of a system that enables us to perform this comparison, but to
relate these mechanistic mathematical models to imaging data, we need to
estimate the parameters of the models. In this work, we study the impact of
collecting data at different temporal resolutions on parameter inference for
biological transport models by performing exact inference for simple velocity
jump process models in a Bayesian framework. This issue is prominent in a host
of studies because the majority of imaging technologies place constraints on
the frequency with which images can be collected, and the discrete nature of
observations can introduce errors into parameter estimates. In this work, we
avoid such errors by formulating the velocity jump process model within a
hidden states framework. This allows us to obtain estimates of the
reorientation rate and noise amplitude for noisy observations of a simple
velocity jump process. We demonstrate the sensitivity of these estimates to
temporal variations in the sampling resolution and extent of measurement noise.
We use our methodology to provide experimental guidelines for researchers
aiming to characterise motile behaviour that can be described by a velocity
jump process. In particular, we consider how experimental constraints resulting
in a trade-off between temporal sampling resolution and observation noise may
affect parameter estimates.

Metabolomics is a key approach in modern functional genomics and systems
biology. Due to the complexity of metabolomics data, the variety of
experimental designs, and the variety of existing bioinformatics tools,
providing experimenters with a simple and efficient resource to conduct
comprehensive and rigorous analysis of their data is of utmost importance. In
2014, we launched the Workflow4Metabolomics (W4M,
http://workflow4metabolomics.org) online infrastructure for metabolomics built
on the Galaxy environment, which offers user-friendly features to build and run
data analysis workflows including preprocessing, statistical analysis, and
annotation steps. Here we present the new W4M 3.0 release, which contains twice
as many tools as the first version, and provides two features which are, to our
knowledge, unique among online resources. First, data from the four major
metabolomics technologies (i.e., LC-MS, FIA-MS, GC-MS, and NMR) can be analyzed
on a single platform. By using three studies in human physiology, alga
evolution, and animal toxicology, we demonstrate how the 40 available tools can
be easily combined to address biological issues. Second, the full analysis
(including the workflow, the parameter values, the input data and output
results) can be referenced with a permanent digital object identifier (DOI).
Publication of data analyses is of major importance for robust and reproducible
science. Furthermore, the publicly shared workflows are of high-value for
e-learning and training. The Workflow4Metabolomics 3.0 e-infrastructure thus
not only offers a unique online environment for analysis of data from the main
metabolomics technologies, but it is also the first reference repository for
metabolomics workflows.

Motivation: Flow Injection Analysis coupled to High-Resolution Mass
Spectrometry (FIA-HRMS) is a promising approach for high-throughput
metabolomics. FIA-HRMS data, however, cannot be preprocessed with current
software tools which rely on liquid chromatography separation, or handle low
resolution data only. Results: We thus developed the proFIA package, which
implements a suite of innovative algorithms to preprocess FIA-HRMS raw files,
and generates the table of peak intensities. The workflow consists of 3 steps:
i) noise estimation, peak detection and quantification, ii) peak grouping
across samples, and iii) missing value imputation. In addition, we have
implemented a new indicator to quantify the potential alteration of the feature
peak shape due to matrix effect. The preprocessing is fast (less than 15 s per
file), and the value of the main parameters (ppm and dmz) can be easily
inferred from the mass resolution of the instrument. Application to two
metabolomics datasets (including spiked serum samples) showed high precision
(96%) and recall (98%) compared with manual integration. These results
demonstrate that proFIA achieves very efficient and robust detection and
quantification of FIA-HRMS data, and opens new opportunities for
high-throughput phenotyping. Availability: The proFIA software (as well as the
plasFIA data set) is available as an R package on the Bioconductor repository
(http://bioconductor.org/packages/proFIA), and as a Galaxy module on the Main
Toolshed (https://toolshed.g2.bx.psu.edu/) and on the Workflow4Metabolomics
online infrastructure (http://workflow4metabolomics.org). Contacts:
alexis.delabriere@cea.fr and etienne.thevenot@cea.fr.

Reaction-diffusion systems are used to represent many biological and physical
phenomena. They model the random motion of particles (diffusion) and
interactions between them (reactions). Such systems can be modelled at multiple
scales with varying degrees of accuracy and computational efficiency. When
representing genuinely multiscale phenomena, fine-scale models can be
prohibitively expensive, whereas coarser models, although cheaper, often lack
sufficient detail to accurately represent the phenomenon at hand. Spatial
hybrid methods couple two or more of these representations in order to improve
efficiency without compromising accuracy.
  In this paper, we present a novel spatial hybrid method, which we call the
auxiliary region method (ARM), which couples PDE and Brownian-based
representations of reaction-diffusion systems. Numerical PDE solutions on one
side of an interface are coupled to Brownian-based dynamics on the other side
using compartment-based "auxiliary regions". We demonstrate that the hybrid
method is able to simulate reaction-diffusion dynamics for a number of
different test problems with high accuracy. Further, we undertake error
analysis on the ARM which demonstrates that it is robust to changes in the free
parameters in the model, where previous coupling algorithms are not. In
particular, we envisage that the method will be applicable for a wide range of
spatial multi-scales problems including, filopodial dynamics, intracellular
signalling, embryogenesis and travelling wave phenomena.

Common wheat (Triticum aestivum L.) is one of the most important cereal
crops. Wheat powdery mildew caused by Blumeria graminis f. sp. tritici (Bgt) is
a continuing threat to wheat production. The Pm21 gene, originating from
Dasypyrum villosum, confers high resistance to all known Bgt races and has been
widely applied in wheat breeding in China. In this research, we identify Pm21
as a typical coiled-coil, nucleotide-binding site, leucine-rich repeat gene by
an integrated strategy of resistance gene analog (RGA)-based cloning via
comparative genomics, physical and genetic mapping, BSMV-induced gene silencing
(BSMV-VIGS), large-scale mutagenesis and genetic transformation.

Standard techniques for studying biological systems largely focus on their
dynamical, or, more recently, their informational properties, usually taking
either a reductionist or holistic perspective. Yet, studying only individual
system elements or the dynamics of the system as a whole disregards the
organisational structure of the system - whether there are subsets of elements
with joint causes or effects, and whether the system is strongly integrated or
composed of several loosely interacting components. Integrated information
theory (IIT), offers a theoretical framework to (1) investigate the
compositional cause-effect structure of a system, and to (2) identify causal
borders of highly integrated elements comprising local maxima of intrinsic
cause-effect power. Here we apply this comprehensive causal analysis to a
Boolean network model of the fission yeast (Schizosaccharomyces pombe)
cell-cycle. We demonstrate that this biological model features a non-trivial
causal architecture, whose discovery may provide insights about the real cell
cycle that could not be gained from holistic or reductionist approaches. We
also show how some specific properties of this underlying causal architecture
relate to the biological notion of autonomy. Ultimately, we suggest that
analysing the causal organisation of a system, including key features like
intrinsic control and stable causal borders, should prove relevant for
distinguishing life from non-life, and thus could also illuminate the origin of
life problem.

A mutation in a protein-coding gene in DNA can alter the protein structure
coded by the same gene. Structurally altered proteins usually lose their
functions and sometimes gain an undesirable function instead. These types of
mutations and their effects can result in genetic diseases or antibiotic
resistant bacteria, among other health issues. Important curing methods have
been developed for detecting mutations against AIDS as well as genetic
diseases. Another example is the influenza virus. The reasons why a vaccination
developed to fight against influenza does not work the following year are (a)
the mutation of its DNA and (b) the outbreak of the virus after it has been
mutated especially if it is a virus that escaped the vaccinations target. Due
to such reasons, it is highly important to know in advance the location of a
potential mutation in a protein as well as the problems it might cause the
medical sciences. In this study we have used artificial neural networks, which
are one of the latest artificial intelligence technologies, to determine the
effects of cancer mutations. The model we developed has given more successful
results compared to other methods. We foresee that our model will bring a new
dimension to medical research and the medical industry.

This article seeks to address the prevailing issue of how to measure specific
process components of psychobiological stress responses. Particularly the
change of cortisol secretion due to stress exposure has been discussed as an
endophenotype of many psychosomatic health outcomes. To assess its process
components, a large variety of non-compartmental parameters (i.e., composite
measures of substance concentrations at different points in time) like the area
under the concentration-time curve (AUC) are utilized. However, a systematic
evaluation and validation of these parameters based on a physiologically
plausible model of cortisol secretion has not been performed so far. Thus, a
population pharmacokinetic (mixed-effects SDE) model was developed and fitted
to densely sampled salivary cortisol data of 10 males from Montreal, Canada,
and sparsely sampled data of 200 mixed-sex participants from Dresden, Germany,
who completed the Trier Social Stress Test (TSST). Besides the two major
process components representing (1) stress-related cortisol secretion
(reactivity) and (2) cortisol elimination (recovery), the model incorporates
two additional, often disregarded components: (3) the secretory delay after
stress onset, and (4) deviations from the projected steady-state concentration.
The fitted model (R2 = 99%) was thereafter used to investigate the correlation
structure of the four individually varying, and readily interpretable model
parameters and eleven popular non-compartmental parameters. Based on these
analyses, we recommend to use the minimum-maximum cortisol difference and the
minimum concentration as proxy measures of reactivity and recovery,
respectively. Finally, statistical power analyses of the reactivity-related sex
effect illustrate the consequences of using impure non-compartmental measures
of the different process components that underlie the cortisol stress response.

Flow-Imaging Microscopy (FIM) is commonly used in both academia and industry
to characterize subvisible particles (those $\le 25 \mu m$ in size) in protein
therapeutics. Pharmaceutical companies are required to record vast volumes of
FIM data on protein therapeutic products, but are only mandated under US FDA
regulations (i.e., USP $\big \langle 788 \big \rangle$) to control the number
of particles exceeding $10$ and $25 \mu m$ in delivered products. Hence, a vast
amount of digital images are available to analyze. Current state-of-the-art
methods rely on a relatively low-dimensional list of "morphological features"
to characterize particles, but these methods ignore an enormous amount of
information encoded in the existing large digital image repositories. Deep
Convolutional Neural Networks (CNNs or "ConvNets") have demonstrated the
ability to extract predictive information from raw macroscopic image data
without requiring the selection or specification of "morphological features" in
a variety of tasks. However, the heterogeneity, polydispersity of protein
therapeutics, and optical phenomena associated with subvisible FIM particle
measurements introduce new challenges regarding the application of CNNs to FIM
image analysis. In this article, we demonstrate a supervised learning technique
leveraging CNNs to extract information from raw images in order to predict the
process conditions or stress states (freeze-thaw, mechanical shaking, etc.)
that produced a variety of different protein images. We demonstrate that our
new classifier (in combination with a sample "image pooling" strategy) can
obtain nearly perfect predictions using as few as 20 FIM images from a given
protein formulation in a variety of scenarios of relevance to protein
therapeutics quality control and process monitoring.

Backgr: Digital pathology images are increasingly used both for diagnosis and
research, because slide scanners are nowadays broadly available and because the
quantitative study of these images yields new insights in systems biology.
However, such virtual slides build up a technical challenge since the images
occupy often several gigabytes and cannot be fully opened in a computer's
memory. Moreover, there is no standard format. Therefore, most common open
source tools such as ImageJ fail at treating them, and the others require
expensive hardware while still being prohibitively slow.
  Res: We have developed several cross-platform open source software tools to
overcome these limitations. The NDPITools provide a way to transform microscopy
images initially in the loosely supported NDPI format into one or several
standard TIFF files, and to create mosaics (division of huge images into small
ones, with or without overlap) in various TIFF and JPEG formats. They can be
driven through ImageJ plugins. The LargeTIFFTools achieve similar functionality
for huge TIFF images which do not fit into RAM. We test the performance of
these tools on several digital slides and compare them, when applicable, to
standard software. A statistical study of the cells in a tissue sample from an
oligodendroglioma was performed on an average laptop computer to demonstrate
the efficiency of the tools.
  Concl: Our open source software enables dealing with huge images with
standard software on average computers. Our tools are cross-platform,
independent of proprietary libraries, and very modular, allowing them to be
used in other open source projects. They have excellent performance in terms of
execution speed and RAM requirements. They open promising perspectives both to
the clinician who wants to study a single slide and to the research team or
data centre who do image analysis of many slides on a computer cluster.

3-carboxy-4-methyl-5-propyl-2-furanpropanoic acid (CMPF) is a major
endogenous ligand found in the human serum albumin (HSA) of renal failure
patients. It gets accumulated in the HSA and its concentration in sera of
patients may reflect the chronicity of renal failure [1-4]. It is considered
uremic toxin due to its damaging effect on the renal cells. The high
concentrations of CMPF inhibit the binding of other ligands to HSA. Removal of
CMPF is difficult through conventional hemodialysis due to its strong binding
affinity. We hypothesized that the competitive inhibition may be helpful in
removal of CMPF binding to HSA. A compound with higher HSA binding affinity
than CMPF could be useful to prevent CMPF from binding so that CMPF could be
excreted by the body through the urine. We studied an active compound
dihydrothymoquinone/ dithymoquinone (DTQ) found in black cumin seed (Nigella
sativa), which has higher binding affinity for HSA. Molecular docking
simulations were performed to find the binding affinity of CMPF and DTQ with
HSA. DTQ was found to have higher binding affinity possessing more interactions
with the binding residues than the CMPF. We studied the binding pocket
flexibility of CMPF and DTQ to analyze the binding abilities of both the
compounds. We have also predicted the ADME properties for DTQ which shows
higher lipophilicity, higher gastrointestinal (GI) absorption, and blood-brain
barrier (BBB) permeability. We discovered that DTQ has potential to act as an
inhibitor of CMPF and can be considered as a candidate for the formation of the
therapeutic drug against CMPF.

Light microscopy as well as image acquisition and processing suffer from
physical and technical prejudices which preclude a correct interpretation of
biological observations which can be reflected in, e.g., medical and
pharmacological praxis. Using the examples of a diffracting microbead and
fluorescently labelled tissue, this article clarifies some ignored aspects of
image build-up in the light microscope and introduce algorithms for maximal
extraction of information from the 3D microscopic experiments. We provided a
correct set-up of the microscope and we sought a voxel (3D pixel) called an
electromagnetic centroid which localizes the information about the object. In
diffraction imaging and light emission, this voxel shows a minimal intensity
change in two consecutive optical cuts. This approach further enabled us to
identify z-stack of a DAPI-stained tissue section where at least one object of
a relevant fluorescent marker was in focus. The spatial corrections (overlaps)
of the DAPI-labelled region with in-focus autofluorescent regions then enabled
us to co-localize these three regions in the optimal way when considering
physical laws and information theory. We demonstrate that superresolution down
to the Nobelish level can be obtained from commonplace widefield bright-field
and fluorescence microscopy and bring new perspectives on co-localization in
fluorescent microscopy.

We develop a method to reconstruct, from measured displacements of an
underlying elastic substrate, the spatially dependent forces that cells or
tissues impart on it. Given newly available high-resolution images of substrate
displacements, it is desirable to be able to reconstruct small scale, compactly
supported focal adhesions which are often localized and exist only within the
footprint of a cell. In addition to the standard quadratic data mismatch terms
that define least-squares fitting, we motivate a regularization term in the
objective function that penalizes vectorial invariants of the reconstructed
surface stress while preserving boundaries. We solve this inverse problem by
providing a numerical method for setting up a discretized inverse problem that
is solvable by standard convex optimization techniques. By minimizing the
objective function subject to a number of important physically motivated
constraints, we are able to efficiently reconstruct stress fields with
localized structure from simulated and experimental substrate displacements.
Our method incorporates the exact solution for the stress tensor accurate to
first-order finite-differences and motivates the use of distance-based cut-offs
for data inclusion and problem sparsification.

Effective methods of fluid transport vary across scale. A commonly used
dimensionless number for quantifying the effective scale of fluid transport is
the Reynolds number, Re, which gives the ratio of inertial to viscous forces.
What may work well for one Re regime may not produce significant flows for
another. These differences in scale have implications for many organisms,
ranging from the mechanics of how organisms move through their fluid
environment to how hearts pump at various stages in development. Some
organisms, such as soft pulsing corals, actively contract their tentacles to
generate mixing currents that enhance photosynthesis. Their unique morphology
and intermediate scale where both viscous and inertial forces are significant
make them a unique model organism for understanding fluid mixing. In this
paper, 3D fluid-structure interaction simulations of a pulsing soft coral are
used to quantify fluid transport and fluid mixing across a wide range of Re.
The results show that net transport is negligible for $Re<10$, and continuous
upward flow is produced for $Re\geq 10$.

BackgroundLowering the gut exposure to antibiotics during treatments can
prevent microbiota disruption. We evaluated the effect of an activated
charcoal-based adsorbent, DAV131A, on fecal free moxifloxacin concentration and
mortality in a hamster model of moxifloxacin-induced C. difficile
infection.Methods215 hamsters receiving moxifloxacin subcutaneously (D1-D5)
were orally infected at D3 with C. difficile spores. They received various
doses (0-1800mg/kg/day) and schedules (BID, TID) of DAV131A (D1-D8).
Moxifloxacin concentration and C. difficile counts were determined at D3, and
mortality at D12. We compared mortality, moxifloxacin concentration and C.
difficile counts according to DAV131A regimens, and modelled the link between
DAV131A regimen, moxifloxacin concentration and mortality. ResultsAll hamsters
that received no DAV131A died, but none of those that received 1800mg/kg/day. A
significant dose-dependent relationship between DAV131A dose and (i) mortality
rates, (ii) moxifloxacin concentration and (iii) C. difficile counts was
evidenced. Mathematical modeling suggested that (i) lowering moxifloxacin
concentration at D3, which was 58$\mu$g/g (95%CI=50-66) without DAV131A, to
17$\mu$g/g (14-21) would reduce mortality by 90% and (ii) this would be
achieved with a daily DAV131A dose of 703mg/kg (596-809).ConclusionsIn this
model of C. difficile infection, DAV131A reduced mortality in a dose-dependent
manner by decreasing fecal free moxifloxacin concentration.

Within animals, oxygen exchange occurs within networks containing potentially
billions of microvessels that are distributed throughout the animal's body.
Innovative imaging methods now allow for mapping of the architecture and blood
flows within real microvascular networks. However, these data streams have so
far yielded little new understanding of the physical principles that underlie
the organization of microvascular networks, which could allow healthy networks
to be quantitatively compared with networks that have been damaged, e.g. due to
diabetes. A natural mathematical starting point for understanding network
organization is to construct networks that are optimized accordingly to
specified functions. Here we present a method for deriving transport networks
that optimize general functions involving the fluxes and conductances within
the network. In our method Kirchoff's laws are imposed via Lagrange
multipliers, creating a large, but sparse system of auxiliary equations. By
treating network conductances as adiabatic variables, we derive a gradient
descent method in which conductances are iteratively adjusted, and auxiliary
variables are solved for by two inversions of O(N^2) sized sparse matrices. In
particular our algorithm allows us to validate the hypothesis that
microvascular networks are organized to uniformly partition the flow of red
blood cells through vessels. The theoretical framework can also be used to
consider more general sets of objective functions and constraints within
transport networks, including incorporating the non-Newtonian rheology of blood
(i.e. the Fahraeus-Lindqvist effect). More generally by forming linear
combinations of objective functions, we can explore tradeoffs between different
optimization functions, giving more insight into the diversity of biological
transport networks seen in nature.

Manuka honey (MH) is used as an antibacterial agent in bioactive wound
dressings via direct impregnation onto a suitable substrate. MH provides unique
antibacterial activity when compared with conventional honeys, owing partly to
one of its constituents, methylglyoxal (MGO). Aiming to investigate an
antibiotic-free antimicrobial strategy, we studied the antibacterial activity
of both MH and MGO (at equivalent MGO concentrations) when applied as a
physical coating to a nonwoven fabric wound dressing. When physically coated on
to a cellulosic hydroentangled nonwoven fabric, it was found that
concentrations of 0.0054 mg cm-2 of MGO in the form of MH and MGO was
sufficient to achieve 100 CFU% bacteria reduction against gram-positive
Staphylococcus aureus and gram-negative Klebsiella pneumoniae, based on BS EN
ISO 20743:2007. A 3- to 20- fold increase in MGO concentration (0.0170 - 0.1 mg
cm-2) was required to facilitate a good antibacterial effect (based on BS EN
ISO 20645:2004) in terms of zone of inhibition and lack of growth under the
sample. The minimum inhibitory concentration (MIC) and minimum bactericidal
concentration (MBC) was also assessed for MGO in liquid form against three
prevalent wound and healthcare-associated pathogens, i.e. Staphylococcus
aureus, gram-negative Pseudomonas aeruginosa and gram-positive Enterococcus
faecalis. Other than the case of MGO-containing fabrics, solutions with much
higher MGO concentrations (128 mg L-1 - 1024 mg L-1) were required to provide
either a bacteriostatic or bactericidal effect. The results presented in this
study therefore demonstrate the relevance of MGO-based coating as an
environment-friendly strategy for the design of functional dressings with
antibiotic-free antimicrobial chemistries.

Anti-staphylococcal penicillins (ASPs) are recommended as first-line agents
in methicillin-susceptible Staphylococcus aureus (MSSA) bacteraemia. Concerns
about their safety profile have contributed to the increased use of cefazolin.
The comparative clinical effectiveness and safety profile of cefazolin versus
ASPs for such infections remain unclear. Furthermore, uncertainty persists
concerning the use of cefazolin due to controversies over its efficacy in deep
MSSA infections and its possible negative ecological impact.

Alzheimer disease (AD) is the leading cause of dementia, accounts for 60 to
80 percent cases. Two main factors called beta amyloid plaques and tangles are
prime suspects in damaging and killing nerve cells. However, oxidative stress,
the process which produces free radicals in cells, is believed to promote its
progression to the extent that it may responsible for the cognitive and
functional decline observed in AD. As of today there are few FDA approved drugs
in the market for treatment, but their cholinergic adverse effect, potentially
distressing toxicity and limited targets in AD pathology limits their use.
Therefore, it is crucial to find an effective compounds to combat AD. We choose
45 plant derived natural compounds that have antioxidant properties to slow
down disease progression by quenching free redicals or promoting endogenous
antioxidant capacity. However, we performed molecular docking studies to
investigate the binding interactions between natural compounds and 13 various
anti Alzheimer drug targets. Three known Cholinesterase inhibitors (Donepezil,
Galantamine and Rivastigmine) were taken as reference drugs over natural
compounds for comparison and drug likeness studies. Few of these compounds
showed good inhibitory activity besides anti oxidant activity. Most of these
compounds followed pharmacokinetics properties that make them potentially
promising drug candidates for the treatment of Alzheimer disease.

In genomics, pattern matching against a sequence of nucleotides plays a
pivotal role for DNA sequence alignment and comparing genomes. This helps
tackling some diseases, such as cancer in humans. The complexity of searching
biological sequences in big databases has transformed sequence alignment
problem into a challenging field of research in bioinformatics. A large number
of research has been carried to solve this problem based on electronic
computers. The required extensive amount of computations for handling this huge
database in electronic computers leads to vast amounts of energy consumption
for electrical processing and cooling. On the other hand, optical processing
due to its parallel nature is much faster than electrical counterpart at a
fraction of energy consumption level and cost. In this paper, an algorithm
based on optical parallel processing is proposed that not only locate
similarity between sequences but also determines the exact location of edits.
The proposed algorithm is based on partitioning the read sequence into some
parts, namely, windows, then, computing their correlation with reference
sequence in parallel. Multiple metamaterial based optical correlators are used
in parallel to optically implement the architecture. Design limitations and
challenges of the architecture are also discussed in details. The simulation
results, comparing with the well-known BLAST algorithm, demonstrate superior
speed, accuracy, and much lower power consumption.

Whole-cell computational models aim to predict cellular phenotypes from
genotype by representing the entire genome, the structure and concentration of
each molecular species, each molecular interaction, and the extracellular
environment. Whole-cell models have great potential to transform bioscience,
bioengineering, and medicine. However, numerous challenges remain to achieve
whole-cell models. Nevertheless, researchers are beginning to leverage recent
progress in measurement technology, bioinformatics, data sharing, rule-based
modeling, and multi-algorithmic simulation to build the first whole-cell
models. We anticipate that ongoing efforts to develop scalable whole-cell
modeling tools will enable dramatically more comprehensive and more accurate
models, including models of human cells.

Far-from-equilibrium thermodynamics underpins the emergence of life, but how
has been a long-outstanding puzzle. Best candidate theories based on the
maximum entropy production principle could not be unequivocally proven, in part
due to complicated physics, unintuitive stochastic thermodynamics, and the
existence of alternative theories such as the minimum entropy production
principle. Here, we use a simple, analytically solvable, one-dimensional
bistable chemical system to demonstrate the validity of the maximum entropy
production principle. To generalize to multistable stochastic system, we use
the stochastic least-action principle to derive the entropy production and its
role in the stability of nonequilibrium steady states. This shows that in a
multistable system, all else being equal, the steady state with the highest
entropy production is favored, with a number of implications for the evolution
of biological, physical, and geological systems.

During organogenesis tissue grows and deforms. The growth processes are
controlled by diffusible proteins, so-called morphogens. Many different
patterning mechanisms have been proposed. The stereotypic branching program
during lung development can be recapitulated by a receptor-ligand based Turing
model. Our group has previously used the Arbitrary Lagrangian-Eulerian (ALE)
framework for solving the receptor-ligand Turing model on growing lung domains.
However, complex mesh deformations which occur during lung growth severely
limit the number of branch generations that can be simulated. A new Phase-Field
implementation avoids mesh deformations by considering the surface of the
modelling domains as interfaces between phases, and by coupling the
reaction-diffusion framework to these surfaces. In this paper, we present a
rigorous comparison between the Phase-Field approach and the ALE-based
simulation.

Recent experimental and theoretical work on neural populations belonging to
two separate early sensory systems, olfaction and vision, has challenged the
notion that the two operate under different computational paradigms by
providing evidence for the respective neural population codes having three
central, common features: they are highly redundant; they are organized such
that information is carried in the identity, and not the relative timing, of
the active neurons; they are capable of error correction. We present the first
model that captures these three properties in a general manner, making it
possible to investigate whether similar structure is present in other
population codes. Our model also makes specific predictions about additional,
as yet unseen, structure in such codes. If these predictions are found in real
data, this would provide new evidence that such population codes are operating
under more general computational principles.

COnstraint-Based Reconstruction and Analysis (COBRA) provides a molecular
mechanistic framework for integrative analysis of experimental data and
quantitative prediction of physicochemically and biochemically feasible
phenotypic states. The COBRA Toolbox is a comprehensive software suite of
interoperable COBRA methods. It has found widespread applications in biology,
biomedicine, and biotechnology because its functions can be flexibly combined
to implement tailored COBRA protocols for any biochemical network. Version 3.0
includes new methods for quality controlled reconstruction, modelling,
topological analysis, strain and experimental design, network visualisation as
well as network integration of chemoinformatic, metabolomic, transcriptomic,
proteomic, and thermochemical data. New multi-lingual code integration also
enables an expansion in COBRA application scope via high-precision,
high-performance, and nonlinear numerical optimisation solvers for multi-scale,
multi-cellular and reaction kinetic modelling, respectively. This protocol can
be adapted for the generation and analysis of a constraint-based model in a
wide variety of molecular systems biology scenarios. This protocol is an update
to the COBRA Toolbox 1.0 and 2.0. The COBRA Toolbox 3.0 provides an
unparalleled depth of constraint-based reconstruction and analysis methods.

In most models of collective motion in animal groups each individual updates
its heading based on the current positions and headings of its neighbors.
Several authors have investigated the effects of including anticipation into
models of this type, and have found that anticipation inhibits polarized
collective motion in alignment based models and promotes milling and swarming
in the one attraction-repulsion model studied. However, it was recently
reported that polarized collective motion does emerge in an alignment based
asynchronous lattice model with mutual anticipation. To our knowledge this is
the only reported case where polarized collective motion has been observed in a
model with anticipation. Here we show that including anticipation induces
polarized collective motion in a synchronous, off lattice, attraction based
model. This establishes that neither asynchrony, mutual anticipation nor motion
restricted to a lattice environment are strict requirements for anticipation to
promote polarized collective motion. In addition, unlike alignment based models
the attraction based model used here does not produce any type of polarized
collective motion in the absence of anticipation. Here anticipation is a direct
polarization inducing mechanism. We believe that utilizing anticipation instead
of frequently used alternatives such as explicit alignment terms, asynchronous
updates and asymmetric interactions to generate polarized collective motion may
be advantageous in some cases.

Surface Enhanced Laser Desorption/Ionization-Time Of Flight Mass Spectrometry
(SELDI-TOF MS) is a variant of the MALDI. It is uses in many cases especially
for the analysis of protein profiling and for preliminary screening tasks of
complex sample aimed for the searching of biomarker. Unfortunately, these
analysis are time consuming and strictly limited about the protein
identification. Seldi analysis of mass spectra (SELYMATRA) is a Web Application
(WA) developed with the aim of reduce these lacks automating the identification
processes and introducing the possibility to predict the proteins present in
complex mixtures from cells and tissues analysed by Mass Spectrometry.
SELYMATRA has the following characteristics. The architectural pattern used to
develop the WA is the Model-View-Controller (MVC), extremely used in the
development of software system. The WA expects an user to upload data in a
Microsoft Excel spreadsheet file format, usually generated by means of the
proprietary Mass Spectrometry softwares. Several parameters can be set such as
experiment conditions, range of isoelectric point, range of pH, relative errors
and so on. The WA compare the mass value among two mass spectra (sample vs
control) to extract differences, and according to the parameters set, it
queries a local database for the prediction of the most likely proteins related
to the masses differently expressed. The WA was validated in a cellular model
overexpressing a tagged NURR1 receptor. SELYMATRA is available at
http://140.164.61.23:8080/SELYMATRA.

We revisit the size distribution of finite components in infinite
Configuration Model networks. We provide an elementary combinatorial proof
about the sizes of birth-death trees which is more intuitive than previous
proofs. We use this to rederive the component size distribution for
Configuration Model networks. Our derivation provides a more intuitive
interpretation of the formula as contrasted with the previous derivation based
on contour integrations. We demonstrate that the formula performs well, even on
networks with heavy tails which violate assumptions of the derivation. We
explain why the result should remain robust for these networks.

The master equation plays an important role in many scientific fields
including physics, chemistry, systems biology, physical finance, and
sociodynamics. We consider the master equation with periodic transition rates.
This may represent an external periodic excitation like the 24h solar day in
biological systems or periodic traffic lights in a model of vehicular traffic.
Using tools from systems and control theory, we prove that under mild technical
conditions every solution of the master equation converges to a periodic
solution with the same period as the rates. In other words, the master equation
entrains (or phase locks) to periodic excitations. We describe two applications
of our theoretical results to important models from statistical mechanics and
epidemiology.

One of the great challenges in biology is to observe, at sufficient detail,
the real-time workings of the cell. Many methods exist to do cell measurements
invasively. For example, mass spectrometry has tremendous mass sensitivity but
destroys the cell. Molecular tagging can reveal exquisite detail using STED
microscopy, but is currently neither relevant for a large number of different
molecules, nor is it applicable to very small molecules. For marker free
non-invasive measurements, only magnetic resonance has sufficient molecular
specificity, but the technique suffers from low sensitivity and resolution. In
this presentation we will consider the roadmap for achieving in vivo
metabolomic measurements with more sensitivity and resolution. The roadmap will
point towards the technological advances that are necessary for magnetic
resonance microscopy to answer questions relevant to cell biology.

In Schiebinger et al. (2017), the authors use optimal transport of measures
on empirical distributions arising from biological experiments to relate the
single cell RNA sequencing profiles for induced pluripotent stem cells
differentiating. But such algorithms could be arbitrarily applied to any
datasets from any collection of experiments. We consider here a natural
question that arises: in a manner consistent with conventionally accepted
assumptions about biology, in which cases can the results of two experiments be
mapped to each other in this manner? The answer to this question is of
fundamental practical importance in developing algorithms that use this method
for analysing and integrating complex datasets collected as part of the Human
Cell Atlas. Here, we develop a formulation of biology in terms of sheaves of
$C^*(X)$-modules for a smooth manifold $X$ equipped with certain structures,
that enables this question to be formally answered, leading to formal
statements about experimental inference and phenotypic identifiability. These
structures capture a perspective on biology that is consistent with a standard,
widely accepted biological perspective and is mathematically intuitive. Our
methods provide a framework in which to design complex experiments and the
algorithms to analyse them in a way that their conclusions can be believed.

Mathematical models are essential tools to study how the cardiovascular
system maintains homeostasis. The utility of such models is limited by the
accuracy of their predictions, which can be determined by uncertainty
quantification (UQ). A challenge associated with the use of UQ is that many
published methods assume that the underlying model is identifiable (e.g. that a
one-to-one mapping exists from the parameter space to the model output). In
this study we present a novel methodology that is used here to calibrate a
lumped-parameter model to left ventricular pressure and volume time series data
sets. Key steps include using (1) literature and available data to determine
nominal parameter values; (2) sensitivity analysis and subset selection to
determine a set of identifiable parameters; (3) optimization to find a point
estimate for identifiable parameters; and (4) frequentist and Bayesian UQ
calculations to assess the predictive capability of the model. Our results show
that it is possible to determine 5 identifiable model parameters that can be
estimated to our experimental data from three rats, and that computed UQ
intervals capture the measurement and model error.

Fluorescence spectroscopy is an image correlation technique to analyze and
characterize the molecular dynamics from a sequence of fluorescence images.
Many image correlation techniques have been developed for different
applications [1]. But in practice the use of these techniques is often limited
to a manually selected region of analysis where it is assumed that the observed
molecules have homogeneous and constant behavior over time. Due to the spatial
and temporal complexity of biological objects, this assumption is frequently at
fault. It is then necessary to propose a robust method for discriminating the
different behaviors over time from experience, as well as identification of the
type of dynamics (diffusion or flow). This paper presents an original system of
automatic discrimination and identification of spatially and temporally uniform
regions for the analysis of molecular different dynamics over time by
calculating STICS (Spatio-Temporal Image Correlation Spectroscopy) at different
time lags. An evaluation of system performance is presented on simulated images
and images acquired by fluorescence microscopy on actin cytoskeleton.

Treadmill walking is a convenient tool for studying the human gait; however,
a common gait parameter, stride length, can be difficult to calculate directly
because relevant reference points continually move backwards. Although there is
no direct calculation of stride length itself, we can use positional
heel-marker data to directly determine a similar parameter, step length, and we
can sum two step lengths to result in one stride length. This proposed method
of calculation is simple but seems to be unexplored in other literature, so
this paper displays the details of the calculation. Our experimental results
differed from the expected values by 2.2% and had a very low standard
deviation, suggesting that this method is viable for practical use. The ability
to calculate stride length for treadmill walking using heel-marker data may
allow for quick and accurate gait calculations that further contribute to the
versatility of heel data as a tool for gait analysis.

Advances in OMICS technologies emerged both massive expression data sets and
huge networks modelling the molecular interplay of genes, RNAs, proteins and
metabolites. Network enrichment methods combine these two data types to extract
subnetwork responses from case/control setups. However, no methods exist to
integrate time series data with networks, thus preventing the identification of
time-dependent systems biology responses. We close this gap with Time Course
Network Enrichment (TiCoNE). It combines a new kind of human-augmented
clustering with a novel approach to network enrichment. It finds temporal
expression prototypes that are mapped to a network and investigated for
enriched prototype pairs interacting more often than expected by chance. Such
patterns of temporal subnetwork co-enrichment can be compared between different
conditions. With TiCoNE, we identified the first distinguishing temporal
systems biology profiles in time series gene expression data of human lung
cells after infection with Influenza and Rhino virus. TiCoNE is available
online (https://ticone.compbio.sdu.dk) and as Cytoscape app in the Cytoscape
App Store (http://apps.cytoscape.org/).

Discrete-state, continuous-time Markov models are becoming commonplace in the
modelling of biochemical processes. The mathematical formulations that such
models lead to are opaque, and, due to their complexity, are often considered
analytically intractable. As such, a variety of Monte Carlo simulation
algorithms have been developed to explore model dynamics empirically. Whilst
well-known methods, such as the Gillespie Algorithm, can be implemented to
investigate a given model, the computational demands of traditional simulation
techniques remain a significant barrier to modern research.
  In order to further develop and explore biologically relevant stochastic
models, new and efficient computational methods are required. In this thesis,
high-performance simulation algorithms are developed to estimate summary
statistics that characterise a chosen reaction network. The algorithms make use
of variance reduction techniques, which exploit statistical properties of the
model dynamics, to improve performance.
  The multi-level method is an example of a variance reduction technique. The
method estimates summary statistics of well-mixed, spatially homogeneous models
by using estimates from multiple ensembles of sample paths of different
accuracies. In this thesis, the multi-level method is developed in three
directions: firstly, a nuanced implementation framework is described; secondly,
a reformulated method is applied to stiff reaction systems; and, finally,
different approaches to variance reduction are implemented and compared.
  The variance reduction methods that underpin the multi-level method are then
re-purposed to understand how the dynamics of a spatially-extended Markov model
are affected by changes in its input parameters. By exploiting the inherent
dynamics of spatially-extended models, an efficient finite difference scheme is
used to estimate parametric sensitivities robustly.

Motivation: Mass spectrometry-based proteomics is among the most commonly
used methods for scrutinizing proteomic profiles in different organs for
biological or medical researches. All the proteomic analyses including
peptide/protein identification and quantification, differential expression
analysis, biomarker discovery and so on are all based on the matching of mass
spectra with peptide sequences, which is significantly influenced by the
quality of the spectra, such as the peak numbers, noisy peaks, signal-to-noise
ratios, etc. Hence, it is crucial to assess the quality of the spectra in order
for filtering and/or post-processing after identification. The handcrafted
features representing spectra quality, however, need human expertise to design
and are difficult to optimize, and thus the existing assessing algorithms are
still lacking in accuracy. Thus, there is a critical need for the robust and
adaptive algorithm for mass spectra quality assessment. Results: We have
developed a novel mass spectrum assessment software DeepQuality, based on the
state-of-the-art compressed sensing and deep learning algorithms. We evaluated
the algorithm on two publicly available tandem MS data sets, resulting in the
AUC of 0.96 and 0.92, respectively, a significant improvement compared with the
AUC of 0.85 and 0.91 of the existing method SpectrumQuality v2.0. Availability:
Software available at https://github.com/horsepurve/DeepQuality

A fully automatic prediction for peptide retention time (RT) in liquid
chromatography (LC), termed as DeepRT, was developed using deep learning
approach, an ensemble of Residual Network (ResNet) and Long Short-Term Memory
(LSTM). In contrast to the traditional predictor based on the hand-crafted
features for peptides, DeepRT learns features from raw amino acid sequences and
makes relatively accurate prediction of peptide RTs with 0.987 R2 for
unmodified peptides. Furthermore, by virtue of transfer learning, DeepRT
enables utilization of the peptides datasets generated from different LC
conditions and of different modification status, resulting in the RT prediction
of 0.992 R2 for unmodified peptides and 0.978 R2 for post-translationally
modified peptides. Even though chromatographic behaviors of peptides are quite
complicated, the study here demonstrated that peptide RT prediction could be
largely improved by deep transfer learning. The DeepRT software is freely
available at https://github.com/horsepurve/DeepRT, under Apache2 open source
License.

Breast cancer is the most common type of cancer among women worldwide. The
standard histopathology of breast tissue, the primary means of disease
diagnosis, involves manual microscopic examination of stained tissue by a
pathologist. Because this method relies on qualitative information, it can
result in inter-observer variation. Furthermore, for difficult cases the
pathologist often needs additional markers of malignancy to help in making a
diagnosis. We present a quantitative method for label-free tissue screening
using Spatial Light Interference Microscopy (SLIM). By extracting tissue
markers of malignancy based on the nanostructure revealed by the optical
path-length, our method provides an objective and potentially automatable
method for rapidly flagging suspicious tissue. We demonstrated our method by
imaging a tissue microarray comprising 68 different subjects - 34 with
malignant and 34 with benign tissues. Three-fold cross validation results
showed a sensitivity of 94% and specificity of 85% for detecting cancer. The
quantitative biomarkers we extract provide a repeatable and objective basis for
determining malignancy. Thus, these disease signatures can be automatically
classified through machine learning packages, since our images do not vary from
scan to scan or instrument to instrument, i.e., they represent intrinsic
physical attributes of the sample, independent of staining quality.

Cryosurgery has been consistently used as an effective treatment to eradicate
irregular tumor tissues. During this process, many difficulties occur such as
intense cooling may also damage the neighboring normal tissues due to the
release of large amount of cold from the cooling probe. In order to protect the
normal tissues in the vicinity of target tumor tissues, coolant was released in
a regulated manner accompanied with the nanoparticle to regulate the size and
orientation of ice balls formed together with improved probe capacity. The
phase change occurs in the target tumor tissues during cryosurgery treatment.
The effective heat capacity method is used for simulation of phase change in
bio-heat transfer equation to take into account the latent heat of phase
transition. The bio-heat transfer equation is solved by using element free
Galerkin method (EFGM) to simulate the phase change problem of biological
tissues subject to nano cryosurgery. In this study, Murshed model with
cylindrical nanoparticles is used for the high thermal conductivity of
nanofluids as compared to Leong Model with the spherical nanoparticle. The
important effects of the interfacial layer at the mushy region (i.e. liquid to
the solid interface), size and concentration of nanoparticles are shown on the
freezing process. This type of problem has applications in biomedical treatment
such as drug delivery. Application of cryosurgery in bio-fluids used for drug
delivery in cancer therapy can be made more efficient in the presence of
nanoparticles (such as Iron oxide ($Fe_{3}O_{4}$), alumina ($Al_{2}O_{3}$) and
gold ($Au$)).

Behavior of natural and artificial agents consists of behavioral episodes or
acts. This study introduces a quantitative measure of behavioral acts -- their
apparent complexity. The measure is based on the concept of the Kolmogorov
complexity. It is an apparent measure because it is determined solely by the
readings of the signals that directly encode percepts and actions during
behavior. The article describes an algorithm of generating behavioral acts of
predetermined apparent complexity. Such acts can be used to evaluate and
develop learning abilities of artificial agents.

The possibility of carbohydrate separation in BEH HILIC (Ethylene Bridged
Hybride, Hydrophilic Interaction Liquid Chromatography) column was studied by
ultra-performance liquid chromatography (UPLC) with evaporative light
scattering detector (ELSD) and mobile phase containing amine compounds as
modifiers. The chromatography conditions and ELSD parameters were optimized to
separate five typical carbohydrates and applied to analysis of four infant milk
powders. The linear ranges of carbohydrate determination were 20-300mg/L for
fructose and glucose, 20-250mg/L for sucrose and lactose, and 35-180mg/L for
fructo-oligosaccharide. The LODs were 16.4mg/L for fructose and glucose,
17.3mg/L for sucrose, 20.0mg/L for lactose, and 46.7mg/L for
fructo-oligosaccharide. Relative standard deviations (RSDs) ranged between
3.45-4.23%, 1.46-4.17%, 4.14-5.60%, 1.39-4.09%, and 2.49-3.61% for fructose,
glucose, sucrose, lactose, and fructo-oilgosaccharide, respectively and
recoveries ranged between 95.0 and 105.4%

Ordinary differential equation models have become a standard tool for the
mechanistic description of biochemical processes. If parameters are inferred
from experimental data, such mechanistic models can provide accurate
predictions about the behavior of latent variables or the process under new
experimental conditions. Complementarily, inference of model structure can be
used to identify the most plausible model structure from a set of candidates,
and thus gain novel biological insight. Several toolboxes can infer model
parameters and structure for small- to medium-scale mechanistic models out of
the box. However, models for highly multiplexed datasets can require hundreds
to thousands of state variables and parameters. For the analysis of such
large-scale models, most algorithms require intractably high computation times.
This chapter provides an overview of state-of-the-art methods for parameter and
model inference, with an emphasis on scalability.

Given the recent controversies in some neuroimaging statistical methods, we
compare the most frequently used functional Magnetic Resonance Imaging (fMRI)
analysis packages: AFNI, FSL and SPM, with regard to temporal autocorrelation
modeling. This process, sometimes known as pre-whitening, is conducted in
virtually all task fMRI studies. We employ eleven datasets containing 980 scans
corresponding to different fMRI protocols and subject populations. Though
autocorrelation modeling in AFNI is not perfect, its performance is much higher
than the performance of autocorrelation modeling in FSL and SPM. The residual
autocorrelated noise in FSL and SPM leads to heavily confounded first level
results, particularly for low-frequency experimental designs. Our results show
superior performance of SPM's alternative pre-whitening: FAST, over SPM's
default. The reliability of task fMRI studies would increase with more accurate
autocorrelation modeling. Furthermore, reliability could increase if the
packages provided diagnostic plots. This way the investigator would be aware of
pre-whitening problems.

The adaptive immune system recognizes antigens via an immense array of
antigen-binding antibodies and T-cell receptors, the immune repertoire. The
interrogation of immune repertoires is of high relevance for understanding the
adaptive immune response in disease and infection (e.g., autoimmunity, cancer,
HIV). Adaptive immune receptor repertoire sequencing (AIRR-seq) has driven the
quantitative and molecular-level profiling of immune repertoires thereby
revealing the high-dimensional complexity of the immune receptor sequence
landscape. Several methods for the computational and statistical analysis of
large-scale AIRR-seq data have been developed to resolve immune repertoire
complexity in order to understand the dynamics of adaptive immunity. Here, we
review the current research on (i) diversity, (ii) clustering and network,
(iii) phylogenetic and (iv) machine learning methods applied to dissect,
quantify and compare the architecture, evolution, and specificity of immune
repertoires. We summarize outstanding questions in computational immunology and
propose future directions for systems immunology towards coupling AIRR-seq with
the computational discovery of immunotherapeutics, vaccines, and
immunodiagnostics.

The identification of reproducible biological patterns from high-dimensional
data is a bottleneck for understanding the biology of complex illnesses such as
schizophrenia. To address this, we developed a biologically informed,
multi-stage machine learning (BioMM) framework. BioMM incorporates biological
pathway information to stratify and aggregate high-dimensional biological data.
We demonstrate the utility of this method using genome-wide DNA methylation
data and show that it substantially outperforms conventional machine learning
approaches. Therefore, the BioMM framework may be a fruitful machine learning
strategy in high-dimensional data and be the basis for future, integrative
analysis approaches.

Integrative biological simulations have a varied and controversial history in
the biological sciences. From computational models of organelles, cells, and
simple organisms, to physiological models of tissues, organ systems, and
ecosystems, a diverse array of biological systems have been the target of
large-scale computational modeling efforts. Nonetheless, these research agendas
have yet to prove decisively their value among the broader community of
theoretical and experimental biologists. In this commentary, we examine a range
of philosophical and practical issues relevant to understanding the potential
of integrative simulations. We discuss the role of theory and modeling in
different areas of physics and suggest that certain sub-disciplines of physics
provide useful cultural analogies for imagining the future role of simulations
in biological research. We examine philosophical issues related to modeling
which consistently arise in discussions about integrative simulations and
suggest a pragmatic viewpoint that balances a belief in philosophy with the
recognition of the relative infancy of our state of philosophical
understanding. Finally, we discuss community workflow and publication practices
to allow research to be readily discoverable and amenable to incorporation into
simulations. We argue that there are aligned incentives in widespread adoption
of practices which will both advance the needs of integrative simulation
efforts as well as other contemporary trends in the biological sciences,
ranging from open science and data sharing to improving reproducibility.

Chronic Kidney Disease (CKD) is an increasingly prevalent condition affecting
13% of the US population. The disease is often a silent condition, making its
diagnosis challenging. Identifying CKD stages from standard office visit
records can help in early detection of the disease and lead to timely
intervention. The dataset we use is highly imbalanced. We propose a
hierarchical meta-classification method, aiming to stratify CKD by severity
levels, employing simple quantitative non-text features gathered from office
visit records, while addressing data imbalance. Our method effectively
stratifies CKD severity levels obtaining high average sensitivity, precision
and F-measure (~93%). We also conduct experiments in which the dimensionality
of the data is significantly reduced to include only the most salient features.
Our results show that the good performance of our system is retained even when
using the reduced feature sets, as well as under much reduced training sets,
indicating that our method is stable and generalizable.

Many biological and physical systems exhibit behaviour at multiple spatial,
temporal or population scales. Multiscale processes provide challenges when
they are to be simulated using numerical techniques. While coarser methods such
as partial differential equations are typically fast to simulate, they lack the
individual-level detail that may be required in regions of low concentration or
small spatial scale. However, to simulate at such an individual-level
throughout a domain and in regions where concentrations are high can be
computationally expensive. Spatially-coupled hybrid methods provide a bridge,
allowing for multiple representations of the same species in one spatial domain
by partitioning space into distinct modelling subdomains. Over the past twenty
years, such hybrid methods have risen to prominence, leading to what is now a
very active research area across multiple disciplines including chemistry,
physics and mathematics.
  There are three main motivations for undertaking this review. Firstly, we
have collated a large number of spatially-extended hybrid methods and presented
them in a single coherent document, while comparing and contrasting them, so
that anyone with a need for a multi-scale hybrid method will be able to find
the most appropriate one for their need. Secondly, we have provided canonical
examples with algorithms and accompanying code, serving to demonstrate how
these types of methods work in practice. Finally, we have presented papers that
employ these methods on real biological and physical problems, demonstrating
their utility. We also consider some open research questions in the area of
hybrid method development and the future directions for the field.

The understanding of toxicity is of paramount importance to human health and
environmental protection. Quantitative toxicity analysis has become a new
standard in the field. This work introduces element specific persistent
homology (ESPH), an algebraic topology approach, for quantitative toxicity
prediction. ESPH retains crucial chemical information during the topological
abstraction of geometric complexity and provides a representation of small
molecules that cannot be obtained by any other method. To investigate the
representability and predictive power of ESPH for small molecules, ancillary
descriptors have also been developed based on physical models. Topological and
physical descriptors are paired with advanced machine learning algorithms, such
as deep neural network (DNN), random forest (RF) and gradient boosting decision
tree (GBDT), to facilitate their applications to quantitative toxicity
predictions. A topology based multi-task strategy is proposed to take the
advantage of the availability of large data sets while dealing with small data
sets. Four benchmark toxicity data sets that involve quantitative measurements
are used to validate the proposed approaches. Extensive numerical studies
indicate that the proposed topological learning methods are able to outperform
the state-of-the-art methods in the literature for quantitative toxicity
analysis. Our online server for computing element-specific topological
descriptors (ESTDs) is available at http://weilab.math.msu.edu/TopTox/

This paper reports some experimental results validating in a broader context
a variant of PCR, called XPCR, previously introduced and tested on relatively
short synthetic DNA sequences. Basic XPCR technique confirmed to work as
expected, to concatenate two genes of different lengths, while a library of all
permutations of three different genes (extracted from the bacterial strain
Bulkolderia fungorum DBT1) has been realized in one step by multiple XPCR.
  Limits and potentialities of the protocols have been discussed, and tested in
several experimental conditions, by aside showing that overlap concatenation of
multiple copies of one only gene is not realizable by these procedures, due to
strand displacement phenomena. In this case, in fact, one copy of the gene is
obtained as a unique amplification product.

Seagrass meadows, one of the worlds most important and productive coastal
habitats, are threatened by a range of anthropogenic actions. Burial of
seagrass plants due to coastal activities is one important anthropogenic
pressure leading to decline of local populations. In our study, we assessed the
response of eelgrass Zostera marina to sediment burial from physiological,
morphological, and population parameters. In a full factorial field experiment,
burial level (5-20 cm) and burial duration (4-16 weeks) were manipulated.
Negative effects were visible even at the lowest burial level (5 cm) and
shortest duration (4 weeks), with increasing effects over time and burial
level. Buried seagrasses showed higher shoot mortality, delayed growth and
flowering and lower carbohydrate storage. The observed effects will likely have
an impact on next years survival of buried plants. Our results have
implications for the management of this important coastal plant.

In a recent paper we presented a simple two compartment model which describes
the influence of inhaled concentrations on exhaled breath concentrations for
volatile organic compounds (VOCs) with small Henry constants. In this paper we
extend this investigation concerning the influence of inhaled concentrations on
exhaled breath concentrations for VOCs with higher Henry constants.
  To this end we extend our model with an additional compartment which takes
into account the influence of the upper airways on exhaled breath VOC
concentrations.

Sulfated polysaccharides constitute a large and complex group of
macromolecules which possess a wide range of important biological properties.
Many of them hold promise as new therapeutics, but determination of their blood
levels during pharmacokinetic studies can be challenging. Heparin Red, a
commercial mix-and-read fluorescence assay, has recently emerged as a tool in
clinical drug development and pharmacokinetic analysis for the quantification
of sulfated polysaccharides in human plasma. The present study describes the
application of Heparin Red to the detection of heparin, a highly sulfated
polysaccharide, and fucoidan, a less sulfated polysaccharide, in spiked mouse
and rat plasmas. While the standard assay protocol for human plasma matrix gave
less satisfactory results, a modified protocol was developed that provides
within a detection range 0 to 10 micrograms per mL better limits of
quantification, 1.1 to 2.3 micrograms per mL for heparin, and 1.7 to 3.4
micrograms per mL for fucoidan. The required plasma sample volume of only 20
microliters is advantegous in particular when blood samples need to be
collected from mice. Our results suggest that Heparin Red is a promising tool
for the preclinical evaluation of sulfated polysaccharides with varying
sulfation degrees in mouse and rat models.

In this work, we examine effects of large permanent charges on ionic flow
through ion channels based on a quasi-one dimensional Poisson-Nernst-Planck
model. It turns out large positive permanent charges inhibit the flux of cation
as expected, but strikingly, as the transmembrane electrochemical potential for
anion increases in a particular way, the flux of anion decreases. The latter
phenomenon was observed experimentally but the cause seemed to be unclear. The
mechanisms for these phenomena are examined with the help of the profiles of
the ionic concentrations, electric fields and electrochemical potentials. The
underlying reasons for the near zero flux of cation and for the decreasing flux
of anion are shown to be different over different regions of the permanent
charge. Our model is oversimplified. More structural detail and more
correlations between ions can and should be included. But the basic finding
seems striking and important and deserving of further investigation.

BioDynaMo is a biological processes simulator developed by an international
community of researchers and software engineers working closely with
neuroscientists. The authors have been working on gene expression, i.e. the
process by which the heritable information in a gene - the sequence of DNA base
pairs - is made into a functional gene product, such as protein or RNA.
Typically, gene regulatory models employ either statistical or analytical
approaches, being the former already well understood and broadly used. In this
paper, we utilize analytical approaches representing the regulatory networks by
means of differential equations, such as Euler and Runge-Kutta methods. The two
solutions are implemented and have been submitted for inclusion in the
BioDynaMo project and are compared for accuracy and performance.

Due to the heterogeneity of the phenotype defined by Diagnostic and
Statistical Manual of Mental Disorders (DSM) IV, it is not an optimal option to
identify the genetic variation that underlies the risk for alcohol dependence
(AD) and identifying subtypes of AD becomes an important topic. Traditional
unsupervised cluster analysis and latent class analysis are the most commonly
used methods to obtain the subtypes, but without the guidance of the genetic
information, all these methods may lead to subtypes of little utility in
genetic analysis. Recently, some multi-view co-clustering methods are proposed
to ameliorate this drawback. However, these new methods did not take the
missing values inside the data into consideration. To get around this
limitation, we extended one of the multi-view methods to dealing with the
missing values and clustering simultaneously. We applied this method to 2230
European-American sample and found that the well-known generic variant
rs1229984 (in the ADH1B candidate gene) for the subtype is more significant
than that corresponding to case-control association test. Finally, we verify it
on the 1707 replication sample and find it significant, too.

Electronic health records (EHR) contain a large variety of information on the
clinical history of patients such as vital signs, demographics, diagnostic
codes and imaging data. The enormous potential for discovery in this rich
dataset is hampered by its complexity and heterogeneity.
  We present the first study to assess unsupervised homogenization pipelines
designed for EHR clustering. To identify the optimal pipeline, we tested
accuracy on simulated data with varying amounts of redundancy, heterogeneity,
and missingness. We identified two optimal pipelines: 1) Multiple Imputation by
Chained Equations (MICE) combined with Local Linear Embedding; and 2) MICE,
Z-scoring, and Deep Autoencoders.

Aqueous solubility and partition coefficient are important physical
properties of small molecules. Accurate theoretical prediction of aqueous
solubility and partition coefficient plays an important role in drug design and
discovery. The prediction accuracy depends crucially on molecular descriptors
which are typically derived from theoretical understanding of the chemistry and
physics of small molecules. The present work introduces an algebraic topology
based method, called element specific persistent homology (ESPH), as a new
representation of small molecules that is entirely different from conventional
chemical and/or physical representations. ESPH describes molecular properties
in terms of multiscale and multicomponent topological invariants. Such
topological representation is systematical, comprehensive, and scalable with
respect to molecular size and composition variations. However, it cannot be
literally translated into a physical interpretation. Fortunately, it is readily
suitable for machine learning methods, rendering topological learning
algorithms. Due to the inherent correlation between solubility and partition
coefficient, a uniform ESPH representation is developed for both properties,
which facilitates multi-task deep neural networks for their simultaneous
predictions. This strategy leads to more accurate prediction of relatively
small data sets. A total of six data sets is considered in the present work to
validate the proposed topological and multi-task deep learning approaches. It
is demonstrate that the proposed approaches achieve some of the most accurate
predictions of aqueous solubility and partition coefficient. Our software is
available online at {\url{http://weilab.math.msu.edu/TopP-S/}}

BACKGROUND: Most studies of CAD revascularization have been based on and
reported according to angiographic criteria which don't consider the relation
between the resulting effective flow distal to the stenosis and the demand of a
hypertrophied myocardial tissue.
  MODEL: Mathematical model of the myocardial perfusion in comorbid CAD and
ventricular hypertrophy using Poiseuille's law. The analysis yields that the
curve, which represents the relation between the perfusion and the severity of
CAD depending on angiographic and/or angiophysiologic criteria, is shifted to
the right by the effect of myocardial tissue hypertrophy. The right shift of
said curve, which is directly proportional to the degree of ventricular
hypertrophy, indicates that the perfusion of the corresponding myocardial
tissue is compromised at angiographically and/or angiophysiologically
subsignificant stenosis of the supplying epicardial vessel.
  RESULTS: Patients with comorbid CAD and left ventricular hypertrophy are more
sensitive to CAD-related hemodynamic changes. They are more prone to develop
ischemic complications, than their peers with isolated CAD regarding the same
degree of coronary stenosis.
  CONCLUSION: Patients with comorbid CAD and ventricular hypertrophy suffer
from myocardial hypoperfusion at angiographically and/or angiophysiologically
subcritical epicardial stenosis. Accordingly; the comorbidity of both diseases
should be considered upon designing of the treatment regime.

Within the diverse interdisciplinary life sciences domains, semantic,
workflow, and methodological ambiguities can prevent the appreciation of
explanations of phenomena, handicap the use of computational models, and hamper
communication among scientists, engineers, and the public. Members of the life
sciences community commonly, and too often loosely, draw on "mechanistic model"
and similar phrases when referring to the processes of discovering and
establishing causal explanations of biological phenomena. Ambiguities in
modeling and simulation terminology and methods diminish clarity, credibility,
and the perceived significance of research findings. To encourage improved
semantic and methodological clarity, we describe the spectrum of
Mechanism-oriented Models being used to develop explanations of biological
phenomena. We cluster them into three broad groups. We then expand the three
groups into a total of seven workflow-related model types having clearly
distinguishable features. We name each type and illustrate with diverse
examples drawn from the literature. These model types are intended to
contribute to the foundation of an ontology of mechanism-based simulation
research in the life sciences. We show that it is within the model-development
workflows that the different model types manifest and exert their scientific
usefulness by enhancing and extending different forms and degrees of
explanation. The process starts with knowledge about the phenomenon and
continues with explanatory and mathematical descriptions. Those descriptions
are transformed into software and used to perform experimental explorations by
running and examining simulation output. The credibility of inferences is thus
linked to having easy access to the scientific and technical provenance from
each workflow stage.

Genetic sequence data of pathogens are increasingly used to investigate
transmission dynamics in both endemic diseases and disease outbreaks; such
research can aid in development of appropriate interventions and in design of
studies to evaluate them. Several methods have been proposed to infer
transmission chains from sequence data; however, existing methods do not
generally reliably reconstruct transmission trees because genetic sequence data
or inferred phylogenetic trees from such data are insufficient for accurate
inference regarding transmission chains. In this paper, we demonstrate the lack
of a one-to-one relationship between phylogenies and transmission trees, and
also show that information regarding infection times together with genetic
sequences permit accurate reconstruction of transmission trees. We propose a
Bayesian inference method for this purpose and demonstrate that precision of
inference regarding these transmission trees depends on precision of the
estimated times of infection. We also illustrate the use of these methods to
study features of epidemic dynamics, such as the relationship between
characteristics of nodes and average number of outbound edges or inbound
edges-- signifying possible transmission events from and to nodes. We study the
performance of the proposed method in simulation experiments and demonstrate
its superiority in comparison to an alternative method. We apply them to a
transmission cluster in San Diego and investigate the impact of biological,
behavioral, and demographic factors.

The gene expression profile of a tissue averages the expression profiles of
all cells in this tissue. Digital tissue deconvolution (DTD) addresses the
following inverse problem: Given the expression profile $y$ of a tissue, what
is the cellular composition $c$ of that tissue? If $X$ is a matrix whose
columns are reference profiles of individual cell types, the composition $c$
can be computed by minimizing $\mathcal L(y-Xc)$ for a given loss function
$\mathcal L$. Current methods use predefined all-purpose loss functions. They
successfully quantify the dominating cells of a tissue, while often falling
short in detecting small cell populations.
  Here we learn the loss function $\mathcal L$ along with the composition $c$.
This allows us to adapt to application-specific requirements such as focusing
on small cell populations or distinguishing phenotypically similar cell
populations. Our method quantifies large cell fractions as accurately as
existing methods and significantly improves the detection of small cell
populations and the distinction of similar cell types.

Oscillatory processes are central for the understanding of the neural bases
of cognition and behaviour. To analyse these processes, time-frequency (TF)
decomposition methods are applied and non-parametric cluster-based statistical
procedure are used for comparing two or more conditions. While this combination
is a powerful method, it has two drawbacks. One the unreliable estimation of
signals outside the cone-of-influence and the second relates to the length of
the time frequency window used for the analysis. Both impose constrains on the
non-parametric statistical procedure for inferring an effect in the TF domain.
Here we extend the method to reliably infer oscillatory differences within the
full TF map and to test single conditions. We show that it can be applied in
small time windows irrespective of the cone-of-influence and we further develop
its application to single-condition case for testing the hypothesis of the
presence or not of time-varying signals. We present tests of this new method on
real EEG and behavioural data and show that its sensitivity to single-condition
tests is at least as good as classic Fourier analysis. Statistical inference in
the full TF map is available and efficient in detecting differences between
conditions as well as the presence of time-varying signal in single condition.

Models of biological systems often have many unknown parameters that must be
determined in order for model behavior to match experimental observations.
Commonly-used methods for parameter estimation that return point estimates of
the best-fit parameters are insufficient when models are high dimensional and
under-constrained. As a result, Bayesian methods, which treat model parameters
as random variables and attempt to estimate their probability distributions
given data, have become popular in systems biology. Bayesian parameter
estimation often relies on Markov Chain Monte Carlo (MCMC) methods to sample
model parameter distributions, but the slow convergence of MCMC sampling can be
a major bottleneck. One approach to improving performance is parallel tempering
(PT), a physics-based method that uses swapping between multiple Markov chains
run in parallel at different temperatures to accelerate sampling. The
temperature of a Markov chain determines the probability of accepting an
unfavorable move, so swapping with higher temperatures chains enables the
sampling chain to escape from local minima. In this work we compared the MCMC
performance of PT and the commonly-used Metropolis-Hastings (MH) algorithm on
six biological models of varying complexity. We found that for simpler models
PT accelerated convergence and sampling, and that for more complex models, PT
often converged in cases MH became trapped in non-optimal local minima. We also
developed a freely-available MATLAB package for Bayesian parameter estimation
called PTempEst (http://github.com/RuleWorld/ptempest), which is closely
integrated with the popular BioNetGen software for rule-based modeling of
biological systems.

Public data archives are the backbone of modern biological and biomedical
research. While archives for biological molecules and structures are
well-established, resources for imaging data do not yet cover the full range of
spatial and temporal scales or application domains used by the scientific
community. In the last few years, the technical barriers to building such
resources have been solved and the first examples of scientific outputs from
public image data resources, often through linkage to existing molecular
resources, have been published. Using the successes of existing biomolecular
resources as a guide, we present the rationale and principles for the
construction of image data archives and databases that will be the foundation
of the next revolution in biological and biomedical informatics and discovery.

In the past decade, digital technologies have started to profoundly influence
healthcare systems. Digital self-tracking has facilitated more precise
epidemiological studies, and in the field of nutritional epidemiology, mobile
apps have the potential to alleviate a significant part of the journaling
burden by, for example, allowing users to record their food intake via a simple
scan of packaged products barcodes. Such studies thus rely on databases of
commercialized products, their barcodes, ingredients, and nutritional values,
which are not yet openly available with sufficient geographical and product
coverage. In this paper, we present FoodRepo (https://www.foodrepo.org), an
open food repository of barcoded food items, whose database is programmatically
accessible through an application programming interface (API). Furthermore, an
open source license gives the appropriate rights to anyone to share and reuse
FoodRepo data, including for commercial purposes. With currently more than
21,000 items available on the Swiss market, our database represents a solid
starting point for large-scale studies in the field of digital nutrition, with
the aim to lead to a better understanding of the intricate connections between
diets and health in general, and metabolic disorders in particular.

Quasi-Monte Carlo methods have proven to be effective extensions of
traditional Monte Carlo methods in, amongst others, problems of quadrature and
the sample path simulation of stochastic differential equations. By replacing
the random number input stream in a simulation procedure by a low-discrepancy
number input stream, variance reductions of several orders have been observed
in financial applications.
  Analysis of stochastic effects in well-mixed chemical reaction networks often
relies on sample path simulation using Monte Carlo methods, even though these
methods suffer from typical slow $\mathcal{O}(N^{-1/2})$ convergence rates as a
function of the number of sample paths $N$. This paper investigates the
combination of (randomised) quasi-Monte Carlo methods with an efficient sample
path simulation procedure, namely $\tau$-leaping. We show that this combination
is often more effective than traditional Monte Carlo simulation in terms of the
decay of statistical errors. The observed convergence rate behaviour is,
however, non-trivial due to the discrete nature of the models of chemical
reactions. We explain how this affects the performance of quasi-Monte Carlo
methods by looking at a test problem in standard quadrature.

The nature of neural codes is central to neuroscience. Do neurons encode
information through relatively slow changes in the emission rates of individual
spikes (rate code), or by the precise timing of every spike (temporal codes)?
Here we compare the loss of information due to correlations for these two
possible neural codes. The essence of Shannon's definition of information is to
combine information with uncertainty: the higher the uncertainty of a given
event, the more information is conveyed by that event. Correlations can reduce
uncertainty or the amount of information, but by how much? In this paper we
address this question by a direct comparison of the information per symbol
conveyed by the words coming from a binary Markov source (temporal codes) with
the information per symbol coming from the corresponding Bernoulli source
(uncorrelated, rate code source). In a previous paper we found that a crucial
role in the relation between Information Transmission Rates (ITR) and Firing
Rates is played by a parameter s, which is the sum of transitions probabilities
from the no-spike-state to the spike-state and vice versa. It turned out that
also in this case a crucial role is played by the same parameter s. We found
bounds of the quotient of ITRs for these sources, i.e. this quotient's minimal
and maximal values. Next, making use of the entropy grouping axiom, we
determined the loss of information in a Markov source in relation to its
corresponding Bernoulli source for a given length of word. Our results show
that in practical situations in the case of correlated signals the loss of
information is relatively small, thus temporal codes, which are more
energetically efficient, can replace the rate code effectively. These phenomena
were confirmed by experiments.

The choice of reference for electroencephalogram (EEG) is a long-lasting
unsolved issue resulting in inconsistent usages and endless debates. Currently,
both average reference (AR) and reference electrode standardization technique
(REST) are two primary, irreconcilable contenders. We propose a theoretical
framework to resolve this issue by formulating both a) estimation of potentials
at infinity, and, b) determination of the reference, as a unified Bayesian
linear inverse problem. We find that AR and REST are very particular cases of
this unified framework: AR results from biophysically non-informative prior;
while REST utilizes the prior of EEG generative model. We develop the
regularized versions of AR and REST, named rAR, and rREST, respectively. Both
depend on a regularization parameter that is the noise to signal ratio.
Traditional and new estimators are evaluated with this framework, by both
simulations and analysis of real EEGs. Generated artificial EEGs, show that
relative error in estimating the EEG potentials at infinity is lowest for
rREST. It also reveals that realistic volume conductor models improve the
performances of REST and rREST. For practical applications, it is shown that
average lead field gives the results comparable to the individual lead field.
Finally, it is shown that the selection of the regularization parameter with
Generalized Cross-Validation (GCV) is close to the 'oracle' choice based on the
ground truth. When evaluated with the real 89 resting state EEGs, rREST
consistently yields the lowest GCV. This study provides a novel perspective on
the EEG reference problem by means of a unified inverse solution framework. It
may allow additional principled theoretical formulations and numerical
evaluation of performance.

Clustering of gene expression time series gives insight into which genes may
be coregulated, allowing us to discern the activity of pathways in a given
microarray experiment. Of particular interest is how a given group of genes
varies with different model conditions or genetic background. Amyotrophic
lateral sclerosis (ALS), an irreversible diverse neurodegenerative disorder
showed consistent phenotypic differences and the disease progression is
heterogeneous with significant variability. This paper demonstrated about
finding some significant gene expression profiles and its associated or
co-regulated cluster of gene expressions from four groups of data with
different genetic background or models conditions. Gene enrichment score
analysis and pathway analysis of judicially selected clusters lead toward
identifying features underlying the differential speed of disease progression.
Gene ontology overrepresentation analysis showed clusters from the proposed
method are less likely to be clustered just by chance. In this paper, we
develop a new clustering method that allows each cluster to be parameterised
according to whether the behaviour of the genes across conditions is correlated
or anti-correlated. Our proposed method unveil the potency of latent
information shared between multiple model conditions and their replicates
during modelling gene expression data.

The local dynamic stability method (maximum Lyapunov exponent) can assess
gait stability. Two variants of the method exist: the short-term divergence
exponent (DE), and the long-term DE. Only the short-term DE can predict fall
risk. The significance of long-term DE has been unclear so far. Some studies
have suggested that the complex, fractal-like structure of fluctuations among
consecutive strides correlates with long-term DE. The aim, therefore, was to
assess whether the long-term DE is a gait complexity index. The study
reanalyzed a dataset of trunk accelerations from 100 healthy adults walking at
preferred speed on a treadmill for 10 minutes. By interpolation, the stride
intervals were modified within the acceleration signals for the purpose of
conserving the original shape of the signal, while imposing a known
stride-to-stride fluctuation structure. 4 types of hybrid signals with
different noise structures were built: constant, anti-correlated, random, and
correlated (fractal). Short- and long-term DEs were then computed. The results
show that long-term DEs, but not short-term DEs, are sensitive to the noise
structure of stride intervals. It was that observed that random hybrid signals
exhibited significantly lower long-term DEs than hybrid correlated signals did
(0.100 vs 0.144, i.e. a 44% difference). Long-term DEs from constant hybrid
signals were close to zero (0.006). Short-term DEs of anti-correlated, random,
and correlated hybrid signals were closely grouped (2.49, 2.50, and 2.51). The
short- and long-term DEs, although they are both computed from divergence
curves, should not be interpreted in a similar way. The long-term DE is very
likely an index of gait complexity, which may be associated with gait
automaticity or cautiousness. To better differentiate between short- and
long-term DEs, the use of the term attractor complexity index (ACI) is proposed
for the latter.

Shape variability represents an important direct response of organisms to
selective environments. Here, we use a combination of geometric morphometrics
and generalised additive mixed models (GAMMs) to identify spatial patterns of
natural shell shape variation in the North Atlantic and Arctic blue mussels,
Mytilus edulis and M. trossulus, with environmental gradients of temperature,
salinity and food availability across 3980 km of coastlines. New statistical
methods and multiple study systems at various geographical scales allowed the
uncoupling of the developmental and genetic contributions to shell shape and
made it possible to identify general relationships between blue mussel shape
variation and environment that are independent of age and species influences.
We find salinity had the strongest effect on the latitudinal patterns of
Mytilus shape, producing shells that were more elongated, narrower and with
more parallel dorsoventral margins at lower salinities. Temperature and food
supply, however, were the main drivers of mussel shape heterogeneity. Our
findings revealed similar shell shape responses in Mytilus to less favourable
environmental conditions across the different geographical scales analysed. Our
results show how shell shape plasticity represents a powerful indicator to
understand the alterations of blue mussel communities in rapidly changing
environments.

Fluid-structure interaction in the developing heart is an active area of
research in developmental biology. However, investigation of heart dynamics is
mostly limited to computational fluid dynamics simulations using heart wall
structure information only, or single plane blood flow information - so there
is a need for 3D + time resolved data to fully understand cardiac function. We
present an imaging platform combining selective plane illumination microscopy
(SPIM) with micro particle image velocimetry ({\textmu}PIV) to enable
3D-resolved flow mapping in a microscopic environment, free from many of the
sources of error and bias present in traditional epifluorescence-based
{\textmu}PIV systems. By using our new system in conjunction with optical heart
beat synchronisation, we demonstrte the ability obtain non-invasive 3D + time
resolved blood flow measurements in the heart of a living zebrafish embryo.

A model of morphogenesis is proposed based on seven explicit postulates. The
mathematical import and biological significance of the postulates are explored
and discussed.

We propose a computational method to quantitatively evaluate the systematic
uncertainties that arise from undetectable sources in biological measurements
using live-cell imaging techniques. We then demonstrate this method in
measuring biological cooperativity of molecular binding networks: in
particular, ligand molecules binding to cell surface receptor proteins. Our
results show how the non-statistical uncertainties lead to invalid
identification of the measured cooperativity. Through this computational
scheme, the biological interpretation can be more objectively evaluated and
understood under a specific experimental configuration of interest.

Discerning how a mutation affects the stability of a protein is central to
the study of a wide range of diseases. Machine learning and statistical
analysis techniques can inform how to allocate limited resources to the
considerable time and cost associated with wet lab mutagenesis experiments. In
this work we explore the effectiveness of using a neural network classifier to
predict the change in the stability of a protein due to a mutation. Assessing
the accuracy of our approach is dependent on the use of experimental data about
the effects of mutations performed in vitro. Because the experimental data is
prone to discrepancies when similar experiments have been performed by multiple
laboratories, the use of the data near the juncture of stabilizing and
destabilizing mutations is questionable. We address this later problem via a
systematic approach in which we explore the use of a three-way classification
scheme with stabilizing, destabilizing, and inconclusive labels. For a
systematic search of potential classification cutoff values our classifier
achieved 68 percent accuracy on ternary classification for cutoff values of
-0.6 and 0.7 with a low rate of classifying stabilizing as destabilizing and
vice versa.

RuleBuilder is a tool for drawing graphs that can be represented by the
BioNetGen language (BNGL), which is used to formulate mathematical, rule-based
models of biochemical systems. BNGL provides an intuitive plain-text, or
string, representation of such systems, which is based on a graphical
formalism. Reactions are defined in terms of graph-rewriting rules that specify
the necessary intrinsic properties of the reactants, a transformation, and a
rate law. Rules may also contain contextual constraints that restrict
application of the rule. In some cases, the specification of contextual
constraints can be verbose, making a rule difficult to read. RuleBuilder is
designed to ease the task of reading and writing individual reaction rules, as
well as individual BNGL patterns similar to those found in rules. The software
assists in the reading of existing models by converting BNGL strings of
interest into a graph-based representation composed of nodes and edges.
RuleBuilder also enables the user to construct de novo a visual representation
of BNGL strings using drawing tools available in its interface. As objects are
added to the drawing canvas, the corresponding BNGL string is generated on the
fly, and objects are similarly drawn on the fly as BNGL strings are entered
into the application. RuleBuilder thus facilitates construction and
interpretation of rule-based models.

Absent experimental evidence, a robust methodology to predict the likelihood
of N-glycosylation in human proteins is essential for guiding experimental
work. Based on the distribution of amino acids in the neighborhood of the NxS/T
sequon (N-site); the structural attributes of the N-site that include
Accessible Surface Area, secondary structural elements, main-chain phi-psi,
turn types; the relative location of the N-site in the primary sequence; and
the nature of the glycan bound, the ridge regression estimated linear
probability model is used to predict this likelihood. This model yields a
Kolmogorov-Smirnov (Gini coefficient) statistic value of about 74% (89%), which
is reasonable.

Identification of patients at high risk for readmission could help reduce
morbidity and mortality as well as healthcare costs. Most of the existing
studies on readmission prediction did not compare the contribution of data
categories. In this study we analyzed relative contribution of 90,101 variables
across 398,884 admission records corresponding to 163,468 patients, including
patient demographics, historical hospitalization information, discharge
disposition, diagnoses, procedures, medications and laboratory test results. We
established an interpretable readmission prediction model based on Logistic
Regression in scikit-learn, and added the available variables to the model one
by one in order to analyze the influences of individual data categories on
readmission prediction accuracy. Diagnosis related groups (c-statistic
increment of 0.0933) and discharge disposition (c-statistic increment of
0.0269) were the strongest contributors to model accuracy. Additionally, we
also identified the top ten contributing variables in every data category.

Recent large cancer studies have measured somatic alterations in an
unprecedented number of tumours. These large datasets allow the identification
of cancer-related sets of genetic alterations by identifying relevant
combinatorial patterns. Among such patterns, mutual exclusivity has been
employed by several recent methods that have shown its effectivenes in
characterizing gene sets associated to cancer. Mutual exclusivity arises
because of the complementarity, at the functional level, of alterations in
genes which are part of a group (e.g., a pathway) performing a given function.
The availability of quantitative target profiles, from genetic perturbations or
from clinical phenotypes, provides additional information that can be leveraged
to improve the identification of cancer related gene sets by discovering groups
with complementary functional associations with such targets.
  In this work we study the problem of finding groups of mutually exclusive
alterations associated with a quantitative (functional) target. We propose a
combinatorial formulation for the problem, and prove that the associated
computation problem is computationally hard. We design two algorithms to solve
the problem and implement them in our tool UNCOVER. We provide analytic
evidence of the effectiveness of UNCOVER in finding high-quality solutions and
show experimentally that UNCOVER finds sets of alterations significantly
associated with functional targets in a variety of scenarios. In addition, our
algorithms are much faster than the state-of-the-art, allowing the analysis of
large datasets of thousands of target profiles from cancer cell lines. We show
that on one such dataset from project Achilles our methods identify several
significant gene sets with complementary functional associations with targets.

Single cell segmentation is critical and challenging in live cell imaging
data analysis. Traditional image processing methods and tools require
time-consuming and labor-intensive efforts of manually fine-tuning parameters.
Slight variations of image setting may lead to poor segmentation results.
Recent development of deep convolutional neural networks(CNN) provides a
potentially efficient, general and robust method for segmentation. Most
existing CNN-based methods treat segmentation as a pixel-wise classification
problem. However, three unique problems of cell images adversely affect
segmentation accuracy: lack of established training dataset, few pixels on cell
boundaries, and ubiquitous blurry features. The problem becomes especially
severe with densely packed cells, where a pixel-wise classification method
tends to identify two neighboring cells with blurry shared boundary as one
cell, leading to poor cell count accuracy and affecting subsequent analysis.
Here we developed a different learning strategy that combines strengths of CNN
and watershed algorithm. The method first trains a CNN to learn Euclidean
distance transform of binary masks corresponding to the input images. Then
another CNN is trained to detect individual cells in the Euclidean distance
transform. In the third step, the watershed algorithm takes the outputs from
the previous steps as inputs and performs the segmentation. We tested the
combined method and various forms of the pixel-wise classification algorithm on
segmenting fluorescence and transmitted light images. The new method achieves
similar pixel accuracy but significant higher cell count accuracy than
pixel-wise classification methods do, and the advantage is most obvious when
applying on noisy images of densely packed cells.

The design of multi-stable RNA molecules has important applications in
biology, medicine, and biotechnology. Synthetic design approaches profit
strongly from effective in-silico methods, which can tremendously impact their
cost and feasibility. We revisit a central ingredient of most in-silico design
methods: the sampling of sequences for the design of multi-target structures,
possibly including pseudoknots. For this task, we present the efficient, tree
decomposition-based algorithm. Our fixed parameter tractable approach is
underpinned by establishing the P-hardness of uniform sampling. Modeling the
problem as a constraint network, our program supports generic
Boltzmann-weighted sampling for arbitrary additive RNA energy models; this
enables the generation of RNA sequences meeting specific goals like expected
free energies or \GCb-content. Finally, we empirically study general properties
of the approach and generate biologically relevant multi-target
Boltzmann-weighted designs for a common design benchmark. Generating seed
sequences with our program, we demonstrate significant improvements over the
previously best multi-target sampling strategy (uniform sampling).Our software
is freely available at: https://github.com/yannponty/RNARedPrint .

We review the general problem of finding a global rotation that transforms a
given set of points and/or coordinate frames (the "test" data) into the best
possible alignment with a corresponding set (the "reference" data). For 3D
point data, this "orthogonal Procrustes problem" is often phrased in terms of
minimizing a root-mean-square deviation or RMSD corresponding to a Euclidean
distance measure relating the two sets of matched coordinates. We focus on
quaternion eigensystem methods that have been exploited to solve this problem
for at least five decades in several different bodies of scientific literature
where they were discovered independently. While numerical methods for the
eigenvalue solutions dominate much of this literature, it has long been
realized that the quaternion-based RMSD optimization problem can also be solved
using exact algebraic expressions based on the form of the quartic equation
solution published by Cardano in 1545; we focus on these exact solutions to
expose the structure of the entire eigensystem for the traditional 3D spatial
alignment problem. We then explore the structure of the less-studied
orientation data context, investigating how quaternion methods can be extended
to solve the corresponding 3D quaternion orientation frame alignment (QFA)
problem, noting the interesting equivalence of this problem to the
rotation-averaging problem, which also has been the subject of independent
literature threads. We conclude with a brief discussion of the combined 3D
translation-orientation data alignment problem. Appendices are devoted to a
tutorial on quaternion frames, a related quaternion technique for extracting
quaternions from rotation matrices, and a review of quaternion
rotation-averaging methods relevant to the orientation-frame alignment problem.
Supplementary Material covers extensions of quaternion methods to the 4D
problem.

The home range of a specific animal describes the geographic area where this
individual spends most of the time while carrying out its usual activities
(eating, resting, reproduction, ...). Although a well-established definition of
this concept is lacking, there is a variety of home range estimators. The first
objective of this work is to review and categorize the statistical
methodologies proposed in the literature to approximate the home range of an
animal, based on a sample of observed locations. The second aim is to address
the open question of choosing the "best" home range from a collection of them
based on the same sample. We introduce a numerical index, based on a
penalization criterion, to rank the estimated home ranges. The key idea is to
balance the excess area covered by the estimator (with respect to the original
sample) and a shape descriptor measuring the over-adjustment of the home range
to the data. To our knowledge, apart from computing the home range area, our
ranking procedure is the first one which is both applicable to real data and to
any type of home range estimator. Further, the optimization of the selection
index provides in fact a way to select the smoothing parameter for the kernel
home range estimator. For clarity of exposition, we have applied all the
estimation procedures and our selection proposal to a set of real locations of
a Mongolian wolf using R as the statistical software. As a byproduct, this
review contains a thorough revision of the implementation of home range
estimators in the R language.

Optimizing amino-acid mutations has been a most challenging task in modern
bio- industrial enzyme designing. It is well known that many successful designs
often hinge on extensive correlations among mutations at different sites within
the enzyme, however, the underpinning mechanism for these correlations is far
from clear. Here, we present a topology-based model to quantitively
characterize correlation effects between mutations. The method is based on the
molecular dynamic simulations and the amino-acid network clique analysis that
simply examines if two single mutation sites belong to some 3-clique. We
analyzed 13 dual mutations of T4 phage lysozyme and found that the clique-based
model successfully distinguishes highly correlated or non-additive double-site
mutations from those with less correlation or additive mutations. We also
applied the model to the protein Eglin c whose topology is significantly
distinct from that of T4 phage lysozyme, and found that the model can, to some
extension, still identify non-additive mutations from additive ones. Our
calculations showed that mutation correlation effects may heavily depend on
topology relationship among mutation sites, which can be quantitatively
characterized using amino-acid network k-cliques. We also showed that
double-site mutation correlations can be significantly altered by exerting a
third mutation, indicating that more detailed physico-chemistry interactions
might be considered with the network model for better understanding of the
elusive mutation-correlation principle.

Understanding intertrochanteric fracture distribution is an important topic
in orthopaedics due to its high morbidity and mortality. The intertrochanteric
fracture can contain high-dimensional information including complicated 3D
fracture lines, which often make it difficult to visualize or to obtain
valuable statistics for clinical diagnosis and prognosis applications. This
paper proposed a map projection technique to map the high-dimensional
information into a 2D parametric space. This method can preserve the 3D
proximal femur surface and structure while visualizing the entire fracture line
with a single plot/view. Using this method and a standardization technique, a
total of 100 patients with different ages and genders are studied based on the
original radiographs acquired by CT scan. The comparison shows that the
proposed map projection representation is more efficient and rich in
information visualization than the conventional heat map technique. Using the
proposed method, a fracture probability can be obtained at any location in the
2D parametric space, from which the most probable fracture region can be
accurately identified. The study shows that age and gender have significant
influences on intertrochanteric fracture frequency and fracture line
distribution.

We suggest an explanation of typical incubation times statistical features
based on the universal behavior of exit times for diffusion models. We give a
mathematically rigorous proof of the characteristic right skewness of the
incubation time distribution for very general one-dimensional diffusion models.
Imposing natural simple conditions on the drift coefficient, we also study
these diffusion models under the assumption of noise smallness and show that
the limiting exit time distributions in the limit of vanishing noise are
Gaussian and Gumbel. Thus they match the existing data as well as the other
existing models do. The character of our models, however, allows us to argue
that the features of the exit time distributions that we describe are universal
and manifest themselves in various other situations where the times involved
can be described as detection or halting times, for example, response times
studied in psychology.

Recently, interest in crassulacean acid metabolism (CAM) photosynthesis has
risen and new, physiologically based CAM models have emerged. These models show
promise, yet unlike the more widely used physiological models of C3 and C4
photosynthesis, their complexity has thus far inhibited their adoption in the
general community. Indeed, most efforts to assess the potential of CAM still
rely on empirically based environmental productivity indices, which makes
uniform comparisons between CAM and non-CAM species difficult. In order to
represent C3, C4, and CAM photosynthesis in a consistent, physiologically based
manner, we introduce the Photo3 model. This work builds on a common
photosynthetic and hydraulic core and adds additional components to depict the
circadian rhythm of CAM photosynthesis and the carbon-concentrating mechanism
of C4 photosynthesis. This allows consistent comparisons of the three
photosynthetic types for the first time. It also allows the representation of
intermediate C3-CAM behavior through the adjustment of a single model
parameter. Model simulations of *Opuntia ficus-indica* (CAM), *Sorghum bicolor*
(C4), and *Triticum aestivum* (C3) capture the diurnal behavior of each species
as well as the cumulative effects of long-term water limitation. The results
show potential for use in understanding CAM productivity, ecology, and climate
feedbacks and in evaluating the tradeoffs between C3, C4, and CAM
photosynthesis.

Monitoring the distribution of microfossils in stratigraphic successions is
an essential tool for biostratigraphic, evolutionary and
paleoecologic/paleoceanographic studies. To estimate the relative abundance (%)
of a given species, it is necessary to estimate in advance the minimum number
of specimens to be used in the count (n). This requires an a priori assumption
about a specified level of confidence, and about the species population
proportion (p). It is common use to apply the binomial distribution to
determine n to detect the presence of more than one species in the same sample,
although the multinomial distribution should necessarily be used instead.
  The mathematical theory of sample size computation using the multinomial
distribution is adapted to the computation of n for any number of species to be
detected together (K) at any level of confidence. Easy-to-use extensive tables
show n, for a combination of K and p. These tables indicate a large difference
for n between that indicated by the binomial and those by the multinomial
distribution when many species are to be detected simultaneously. Counting only
300 specimens (with 95 % confidence level) or 500 (99 %) is not enough to
detect more than one taxon.
  The reconstructed history of the micro-biosphere may therefore, in many
instances, need to be largely revised. This revision should affect our
understanding of the ecological and evolutionary relationships between the past
changes in the biosphere and the other major reservoirs (hydrosphere, geosphere
and atmosphere). In biostratigraphy and biochronology, using a much larger
sample size, when more than one marker species is to be detected in the
neighborhood of the same biozone boundary, may help clarifying the nature of
the apparent inconsistencies given by the observed reversals in the ordinal
(rank) biostratigraphic data shown as intersections of the correlation lines

The development of precision livestock farming which adjusts the food needs
of each animal requires detailed knowledge of its behavior and particularly
physical activity. Individual differences between animals can be observed for
group-housed sows. Accelerometer technology offers opportunities for automatic
monitoring of animal behavior. The aim of the first step was to develop a
methodology to attach the accelerometer on the sow's leg, and an algorithm to
automatically detect standing and lying posture. Accelerometers (Hobo Pendant
G) were put in a metal case and fastened with two cable ties on the leg of 6
group-housed sows. The data loggers recorded the acceleration on one axis every
20 s. Data were then validated by 9 hours of direct observations. The automatic
recording device showed data of high sensitivity (98.8%) and specificity
(99.8%). Then, accelerometers were placed on 12 to 13 group-housed sows for 2
to 4 consecutive days in 6 commercial farms equipped with electronic sow
feeding. On average each day, sows spent 259 minutes ($\pm$ 114) standing and
changed posture 29 ($\pm$ 12) times. The sow's standing time was repeatable day
to day. Differences between sows and herds were significant. Based on
behavioral data, 5 categories of sows were identified. This study suggests that
the consideration of individual behavior of each animal would improve herd
management.

New statistical methods were employed to improve the ability to distinguish
benign from malignant breast tissue ex vivo in a recent study. The ultimately
aim was to improve the intraoperative assessment of positive tumour margins in
breast-conserving surgery (BCS), potentially reducing patient re-operation
rates. A multivariate Bayesian classifier was applied to the waveform samples
produced by a Terahertz Pulsed Imaging (TPI) handheld probe system in order to
discriminate tumour from benign breast tissue, obtaining a sensitivity of 96%
and specificity of 95%.
  We compare these results to traditional and to state-of-the-art methods for
determining resection margins. Given the general nature of the classifier, it
is expected that this method can be applied to other tumour types where
resection margins are also critical.

1. Advances in tracking technology have led to an exponential increase in
animal location data, greatly enhancing our ability to address interesting
questions in movement ecology, but also presenting new challenges related to
data manage- ment and analysis. 2. Step-Selection Functions (SSFs) are commonly
used to link environmental covariates to animal location data collected at fine
temporal resolution. SSFs are estimated by comparing observed steps connecting
successive animal locations to random steps, using a likelihood equivalent of a
Cox proportional hazards model. By using common statistical distributions to
model step length and turn angle distributions, and including habitat- and
movement-related covariates (functions of distances between points, angular
deviations), it is possible to make inference regarding habitat selection and
movement processes, or to control one process while investigating the other.
The fitted model can also be used to estimate utilization distributions and
mechanistic home ranges. 3. Here, we present the R-package amt (animal movement
tools) that allows users to fit SSFs to data and to simulate space use of
animals from fitted models. The amt package also provides tools for managing
telemetry data. 4. Using fisher (Pekania pennanti ) data as a case study, we
illustrate a four-step approach to the analysis of animal movement data,
consisting of data management, exploratory data analysis, fitting of models,
and simulating from fitted models.

PanGeneHome is a web server dedicated to the analysis of available microbial
pangenomes. For any prokaryotic taxon with at least three sequenced genomes,
PanGeneHome provides (i) conservation level of genes, (ii) pangenome and
core-genome curves, estimated pangenome size and other metrics, (iii)
dendrograms based on gene content and average amino acid identity (AAI) for
these genomes, and (iv) functional categories and metabolic pathways
represented in the core, accessory and unique gene pools of the selected taxon.
In addition, the results for these different analyses can be compared for any
set of taxa. With the availability of 615 taxa, covering 182 species and 49
orders, PanGeneHome provides an easy way to get a glimpse on the pangenome of a
microbial group of interest. The server and its documentation are available at
http://pangenehome.lmge.uca.fr.

In a recent study entitled "Cell nuclei have lower refractive index and mass
density than cytoplasm", we provided strong evidence indicating that the
nuclear refractive index (RI) is lower than the RI of the cytoplasm for several
cell lines. In a complementary study in 2017, entitled "Is the nuclear
refractive index lower than cytoplasm? Validation of phase measurements and
implications for light scattering technologies", Steelman et al. observed a
lower nuclear RI also for other cell lines and ruled out methodological error
sources such as phase wrapping and scattering effects. Recently, Yurkin
composed a comment on these 2 publications, entitled "How a phase image of a
cell with nucleus refractive index smaller than that of the cytoplasm should
look like?", putting into question the methods used for measuring the cellular
and nuclear RI in the aforementioned publications by suggesting that a lower
nuclear RI would produce a characteristic dip in the measured phase profile in
situ. We point out the difficulty of identifying this dip in the presence of
other cell organelles, noise, or blurring due to the imaging point spread
function. Furthermore, we mitigate Yurkin's concerns regarding the ability of
the simple-transmission approximation to compare cellular and nuclear RI by
analyzing a set of phase images with a novel, scattering-based approach. We
conclude that the absence of a characteristic dip in the measured phase
profiles does not contradict the usage of the simple-transmission approximation
for the determination of the average cellular or nuclear RI. Our response can
be regarded as an addition to the response by Steelman, Eldridge and Wax. We
kindly ask the reader to attend to their thorough ascertainment prior to
reading our response.

The human electroencephalogram (EEG) of sleep undergoes profound changes with
age. These changes can be conceptualized as "brain age", which can be compared
to an age norm to reflect the deviation from normal aging process. Here, we
develop an interpretable machine learning model to predict brain age based on
two large sleep EEG datasets: the Massachusetts General Hospital sleep lab
dataset (MGH, N = 2,621) covering age 18 to 80; and the Sleep Hearth Health
Study (SHHS, N = 3,520) covering age 40 to 80. The model obtains a mean
absolute deviation of 8.1 years between brain age and chronological age in the
healthy participants in the MGH dataset. As validation, we analyze a subset of
SHHS containing longitudinal EEGs 5 years apart, which shows a 5.5 years
difference in brain age. Participants with neurological and psychiatric
diseases, as well as diabetes and hypertension medications show an older brain
age compared to chronological age. The findings raise the prospect of using
sleep EEG as a biomarker for healthy brain aging.

At the physiological level, aging is neither rigid nor unchangeable. Instead,
the molecular and mechanisms driving aging are sufficiently plastic that a
variety of diverse interventions--dietary, pharmaceutical, and genetic--have
been developed to radically manipulate aging. These interventions, shown to
increase the health and lifespan of laboratory animals, are now being explored
for therapeutic applications in humans.
  This clinical potential makes it especially important to understand how,
quantitatively, aging is altered by lifespan-extending interventions. Do
interventions delay the onset of aging? Slow it down? Ameliorate only its
symptoms? Perhaps some interventions will alter only a subset of aging
mechanisms, leading to complex and unintuitive systemic outcomes. Statistical
and analytic models provide a crucial framework in which to interpret the
physiological responses to interventions in aging.
  This review covers a range of quantitative models of lifespan data and places
them in the context of recent experimental work. The careful application of
these models can help experimentalists move beyond merely identifying
statistically significant differences in lifespan, to instead use lifespan data
as a versatile means for probing the underlying physiological dynamics of
aging.

Defining subtypes of complex diseases such as cancer and stratifying patient
groups with the same disease but different subtypes for targeted treatments is
important for personalized and precision medicine. Approaches that incorporate
multi-omic data are more advantageous to those using only one data type for
patient clustering and disease subtype discovery. However, it is challenging to
integrate multi-omic data as they are heterogeneous and noisy. In this paper,
we present Affinity Network Fusion (ANF) to integrate multi-omic data for
patient clustering. ANF first constructs patient affinity networks for each
omic data type, and then calculates a fused network for spectral clustering. We
applied ANF to a processed harmonized cancer dataset downloaded from GDC data
portal consisting of 2193 patients, and generated promising results on
clustering patients into correct disease types. Moreover, we developed a
semi-supervised model combining ANF and neural network for few-shot learning.
In several cases, the model can achieve greater than 90% acccuracy on test set
with training less than 1% of the data. This demonstrates the power of ANF in
learning a good representation of patients, and shows the great potential of
semi-supervised learning in cancer patient clustering.

The likelihood of O-GlcNAc glycosylation in human proteins is predicted using
the ridge regression estimated linear probability model (LPM). To achieve this,
sequences from three similar post-translational modifications (PTMs) of
proteins occurring at, or very near, the S or T site are analyzed:
N-glycosylation, O-mucin type (O-GalNAc) glycosylation, and phosphorylation.
Results found include: 1) The consensus composite sequon for O-glycosylation
does NOT have W on either side of the glycosylation site. 2) The same holds for
the consensus sequon for phosphorylation. 3) For LPM estimation, N-glycosylated
sequences are found to be good approximations to non-O-glycosylatable
sequences. 4) The selective positioning of an amino acid along the sequence,
differentiates the PTMs of proteins. 5) Some N-glycosylated sequences are also
phosphorylated at the S or T site. 6) ASA values for N-glycosylated sequences
are stochastically larger than those for O-GlcNAc glycosylated sequences. 7)
Structural attributes (beta turn II, II', helix, beta bridges, beta hairpin,
and the phi angle) are significant LPM predictors of O-GlcNAc glycosylation.
The LPM with sequence and structural data as explanatory variables yields a
Kolmogorov-Smirnov (KS) statistic value of 99%. 8) With only sequence data, the
KS statistic erodes to 80%, underscoring the germaneness of structural
information, which is sparse on O-glycosylated sequences. With 50% as the
cutoff probability for predicting O-GlcNAc glycosylation, this LPM mispredicts
21% of out-of-sample O-GlcNAc glycosylated sequences as not being glycosylated.
The 95% confidence interval around this mispredictions rate is 16% to 26%

While neural networks can be trained to map from one specific dataset to
another, they usually do not learn a generalized transformation that can
extrapolate accurately outside the space of training. For instance, a
generative adversarial network (GAN) exclusively trained to transform images of
black-haired men to blond-haired men might not have the same effect on images
of black-haired women. This is because neural networks are good at generation
within the manifold of the data that they are trained on. However, generating
new samples outside of the manifold or extrapolating "out-of-sample" is a much
harder problem that has been less well studied. To address this, we introduce a
technique called neuron editing that learns how neurons encode an edit for a
particular transformation in a latent space. We use an autoencoder to decompose
the variation within the dataset into activations of different neurons and
generate transformed data by defining an editing transformation on those
neurons. By performing the transformation in a latent trained space, we encode
fairly complex and non-linear transformations to the data with much simpler
distribution shifts to the neuron's activations. We motivate our technique on
an image domain and then move to our two main biological applications: removal
of batch artifacts representing unwanted noise and modeling the effect of drug
treatments to predict synergy between drugs.

The movement of ionic solutions is an essential part of biology and
technology. Fluidics, from nano- to micro- to microfluidics, is a burgeoning
area of technology which is all about the movement of ionic solutions, on
various scales. Many cells, tissues, and organs of animals and plants depend on
osmosis, as the movement of fluids is called in biology. Indeed, the movement
of fluids through channel proteins (that have a hole down their middle) is
fluidics on an atomic scale. Ionic fluids are complex fluids, with energy
stored in many ways. Ionic fluids flow driven by gradients of concentration,
chemical and electrical potential, and hydrostatic pressure. Each flow is
classically described by its own field theory, independent of the others, but
of course, in reality every gradient drives every kind of flow to a varying
extent. Combining field equations is tricky and so the theory of complex fluids
derives the equations, rather than assumes their interactions. When field
equations are derived, rather than assumed, their variables are consistent.
That is to say all variables satisfy all equations under all conditions with
one set of parameters. Here we treat a classical osmotic cell in this spirit,
using a sharp interface method to derive boundary conditions consistent with
all flows and fields. We allow volume to change with concentration, since
changes of volume are a property of ionic solutions known to all who make them
in the laboratory. We consider flexible and inflexible membranes. We show how
to combine the energetics of the membrane with the energetics of the
surrounding complex fluids. The results seem general but need application to
specific situations of technological, biological and experimental importance
before the consequences of consistency can be understood.

In this paper, we provide an extensive overview of machine learning
techniques applied to structural magnetic resonance imaging (MRI) data to
obtain clinical classifiers. We specifically address practical problems
commonly encountered in the literature, with the aim of helping researchers
improve the application of these techniques in future works. Additionally, we
survey how these algorithms are applied to a wide range of diseases and
disorders (e.g. Alzheimer's disease (AD), Parkinson's disease (PD), autism,
multiple sclerosis, traumatic brain injury, etc.) in order to provide a
comprehensive view of the state of the art in different fields.

Increasingly sophisticated experiments, coupled with large-scale
computational models, have the potential to systematically test biological
hypotheses to drive our understanding of multicellular systems. In this short
review, we explore key challenges that must be overcome to achieve robust,
repeatable data-driven multicellular systems biology. If these challenges can
be solved, we can grow beyond the current state of isolated tools and datasets
to a community-driven ecosystem of interoperable data, software utilities, and
computational modeling platforms. Progress is within our grasp, but it will
take community (and financial) commitment.

Animal telemetry data are often analysed with discrete time movement models
assuming rotation in the movement. These models are defined with equidistant
distant time steps. However, telemetry data from marine animals are observed
irregularly. To account for irregular data, a time-irregularised
first-difference correlated random walk model with drift is introduced. The
model generalizes the commonly used first-difference correlated random walk
with regular time steps by allowing irregular time steps, including a drift
term, and by allowing different autocorrelation in the two coordinates. The
model is applied to data from a ringed seal collected through the Argos
satellite system, and is compared to related movement models through
simulations. Accounting for irregular data in the movement model results in
accurate parameter estimates and reconstruction of movement paths. Measured by
distance, the introduced model can provide more accurate movement paths than
the regular time counterpart. Extracting accurate movement paths from uncertain
telemetry data is important for evaluating space use patterns for marine
animals, which in turn is crucial for management. Further, handling irregular
data directly in the movement model allows efficient simultaneous analysis of
several animals.

There is an ever growing need to ensure the quality of food and assess
specific quality parameters in all the links of the food chain, ranging from
processing, distribution and retail to preparing food. Various imaging and
sensing technologies, including X-ray imaging, ultrasound, and near infrared
reflectance spectroscopy have been applied to the problem. Cost and other
constraints restrict the application of some of these technologies. In this
study we test a novel Multiplexing Electric Field Sensor (MEFS), an approach
that allows for a completely non-invasive and non-destructive testing approach.
Our experiments demonstrate the reliable detection of certain foreign objects
and provide evidence that this sensor technology has the capability of
measuring fat content in minced meat. Given the fact that this technology can
already be deployed at very low cost, low maintenance and in various different
form factors, we conclude that this type of MEFS is an extremely promising
technology for addressing specific food quality issues.

Motivation: The scratch assay is a standard experimental protocol used to
characterize cell migration. It can be used to identify genes that regulate
migration and evaluate the efficacy of potential drugs that inhibit cancer
invasion. In these experiments, a scratch is made on a cell monolayer and
recolonisation of the scratched region is imaged to quantify cell migration
rates. A drawback of this methodology is the lack of its reproducibility
resulting in irregular cell-free areas with crooked leading edges. Existing
quantification methods deal poorly with such resulting irregularities present
in the data. Results: We introduce a new quantification method that can analyse
low quality experimental data. By considering in-silico and in-vitro data, we
show that the method provides a more accurate statistical classification of the
migration rates than two established quantification methods. The application of
this method will enable the quantification of migration rates of scratch assay
data previously unsuitable for analysis. Availability and Implementation: The
source code and the implementation of the algorithm as a GUI along with an
example dataset and user instructions, are available in
https://bitbucket.org/anavictoria-ponce/local_migration_quantification_scratch_assays/src/master/.
The datasets are available in
https://ganymed.math.uni-heidelberg.de/~victoria/publications.shtml.

Protein activity is a significant characteristic for recombinant proteins
which can be used as biocatalysts. High activity of proteins reduces the cost
of biocatalysts. A model that can predict protein activity from amino acid
sequence is highly desired, as it aids experimental improvement of proteins.
However, only limited data for protein activity are currently available, which
prevents the development of such models. Since protein activity and solubility
are correlated for some proteins, the publicly available solubility dataset may
be adopted to develop models that can predict protein solubility from sequence.
The models could serve as a tool to indirectly predict protein activity from
sequence. In literature, predicting protein solubility from sequence has been
intensively explored, but the predicted solubility represented in binary values
from all the developed models was not suitable for guiding experimental designs
to improve protein solubility. Here we propose new machine learning models for
improving protein solubility in vivo. We first implemented a novel approach
that predicted protein solubility in continuous numerical values instead of
binary ones. After combing it with various machine learning algorithms, we
achieved a prediction accuracy of 76.28 percent when Support Vector Machine
algorithm was used. Continuous values of solubility are more meaningful in
protein engineering, as they enable researchers to choose proteins with higher
predicted solubility for experimental validation, while binary values fail to
distinguish proteins with the same value. There are only two possible values so
many proteins have the same one.

Accurate gene regulatory networks can be used to explain the emergence of
different phenotypes, disease mechanisms, and other biological functions. Many
methods have been proposed to infer networks from gene expression data but have
been hampered by problems such as low sample size, inaccurate constraints, and
incomplete characterizations of regulatory dynamics. Since expression
regulation is dynamic, time-course data can be used to infer causality, but
these datasets tend to be short or sparsely sampled. In addition, temporal
methods typically assume that the expression of a gene at a time point depends
on the expression of other genes at only the immediately preceding time point,
while other methods include additional time points without any constraints to
account for their temporal distance. These limitations can contribute to
inaccurate networks with many missing and anomalous links.
  We adapted the time-lagged Ordered Lasso, a regularized regression method
with temporal monotonicity constraints, for \textit{de novo} reconstruction. We
also developed a semi-supervised method that embeds prior network information
into the Ordered Lasso to discover novel regulatory dependencies in existing
pathways. We evaluated these approaches on simulated data for a repressilator,
time-course data from past DREAM challenges, and a HeLa cell cycle dataset to
show that they can produce accurate networks subject to the dynamics and
assumptions of the time-lagged Ordered Lasso regression.

Although co/multi-morbidities are associated with significant increase in
mortality, the lack of appropriate quantitative exploratory techniques often
impede their analysis. In the current study, we study the clustering of
multimorbid patients in the Texas patient population. To this end we employ
agglomerative hierarchical clustering to find clusters within the patient
population. The analysis revealed the presence of nine distinct, clinically
relevant clusters of co/multi-morbidities within the study population of
interest. This technique provides a quantitative exploratory analysis of the
co/multi-morbidities present in a specific population.

The objective of this paper is to present the design, construction and
operation of 3 full scale semi-closed, horizontal tubular photobioreactors
(PBR) used to remove nutrients of a mixture of agricultural run-off (90%) and
treated domestic wastewater (10%). The microalgal biomass produced in the PBRs
was harvested in a static lamella settling tank. Each PBR treated in average
2.3 m3/d. PBRs were submitted to strong seasonal changes regarding solar
radiation and temperature, which had a direct impact in the activity of
microalgae and the efficiency of the system. Higher mixed liquor pH values were
registered in summer (daily average> 10). Most of the influent and effluent
nitrogen content was inorganic (average of 9.0 mg N/L and 3.17 mg N/L,
respectively), and in the form of nitrate (62% and 50%, respectively). Average
nitrogen removal efficiency was 65%, with values of around 90% in summer, 80%
in autumn, 50 % in winter and 60% in spring. Most of the influent and effluent
phosphorus content was in the form of ortophosphate. Influent average was 0.62
mg P/L, but with great variations and in a considerable number of samples not
detected. Removal efficiency (when influent values were detected) was very high
during all the study, usually greater than 95%, and there were not clear
seasonal trends for efficiency as observed for TIN. Volumetric biomass
production greatly changed between seasons with much lower values in winter (7
g VSS/m3d) than in summer (43 g VSS/m3d). Biomass separation efficiency of the
settler was very good in either terms of turbidity and total suspended solids,
being most of the time lower than 5 UNT and 15 mg/L, respectively. Overall this
study demonstrated the reliable and good effectiveness of microalgae based
technologies such as the PBR to remove nutrients at a full scale size.

Due to the recent advances in high-throughput sequencing technologies, it
becomes possible to directly analyze microbial communities in the human body
and in the environment. Knowledge of how microbes interact with each other and
form functional communities can provide a solid foundation to understand
microbiome related diseases; this can serve as a key step towards precision
medicine. In order to understand how microbes form communities, we propose a
two step approach: First, we infer the microbial co-occurrence network by
integrating a graph inference algorithm with phylogenetic information obtained
directly from metagenomic data. Next, we utilize a network-based community
detection algorithm to cluster microbes into functional groups where microbes
in each group are highly correlated. We also curate a "gold standard" network
based on the microbe-metabolic relationships which are extracted directly from
the metagenomic data. Utilizing community detection on the resulting microbial
metabolic pathway bipartite graph, the community membership for each microbe
can be viewed as the true label when evaluating against other existing methods.
Overall, our proposed framework Phylogenetic Graphical Lasso (PGLasso)
outperforms existing methods with gains larger than 100% in terms of Adjusted
Rand Index (ARI) which is commonly used to quantify the goodness of
clusterings.

Purpose: subject motion and static field (B$_0$) drift are known to reduce
the quality of single voxel MR spectroscopy data due to incoherent averaging.
Retrospective correction has previously been shown to improve data quality by
adjusting the phase and frequency offset of each average to match a reference
spectrum. In this work, a new method (RATS) is developed to be tolerant to
large frequency shifts (greater than 7Hz) and baseline instability resulting
from inconsistent water suppression. Methods: in contrast to previous
approaches, the variable-projection method and baseline fitting is incorporated
into the correction procedure to improve robustness to fluctuating baseline
signals and optimization instability. RATS is compared to an alternative
method, based on time-domain spectral registration (TDSR), using simulated data
to model frequency, phase and baseline instability. In addition, a J-difference
edited glutathione in-vivo dataset is processed using both approaches and
compared. Results: RATS offers improved accuracy and stability for large
frequency shifts and unstable baselines. Reduced subtraction artifacts are
demonstrated for glutathione edited MRS when using RATS, compared with
uncorrected or TDSR corrected spectra. Conclusion: the RATS algorithm has been
shown to provide accurate retrospective correction of SVS MRS data in the
presence of large frequency shifts and baseline instability. The method is
rapid, generic and therefore readily incorporated into MRS processing pipelines
to improve lineshape, SNR and aid quality assessment.

Data science has emerged from the proliferation of digital data, coupled with
advances in algorithms, software and hardware (e.g., GPU computing).
Innovations in structural biology have been driven by similar factors, spurring
us to ask: can these two fields impact one another in deep and hitherto
unforeseen ways? We posit that the answer is yes. New biological knowledge lies
in the relationships between sequence, structure, function and disease, all of
which play out on the stage of evolution, and data science enables us to
elucidate these relationships at scale. Here, we consider the above question
from the five key pillars of data science: acquisition, engineering, analytics,
visualization and policy, with an emphasis on machine learning as the premier
analytics approach.

Gut microbes play a key role in colorectal carcinogenesis, yet reaching a
consensus on microbial signatures remains a challenge. This is in part due to a
reliance on mean value estimates. We present an extreme value analysis for
overcoming these limitations. By characterizing a power law fit to the relative
abundances of microbes, we capture the same microbial signatures as more
complex meta-analyses. Importantly, we show that our method is robust to the
variations inherent in microbial community profiling and point to future
directions for developing sensitive, reliable analytical methods.

The dynamics of complex systems generally include high-dimensional,
non-stationary and non-linear behavior, all of which pose fundamental
challenges to quantitative understanding. To address these difficulties we
detail a new approach based on local linear models within windows determined
adaptively from the data. While the dynamics within each window are simple,
consisting of exponential decay, growth and oscillations, the collection of
local parameters across all windows provides a principled characterization of
the full time series. To explore the resulting model space, we develop a novel
likelihood-based hierarchical clustering and we examine the eigenvalues of the
linear dynamics. We demonstrate our analysis with the Lorenz system undergoing
stable spiral dynamics and in the standard chaotic regime. Applied to the
posture dynamics of the nematode $C. elegans$ our approach identifies
fine-grained behavioral states and model dynamics which fluctuate close to an
instability boundary, and we detail a bifurcation in a transition from forward
to backward crawling. Finally, we analyze whole-brain imaging in $C. elegans$
and show that the stability of global brain states changes with oxygen
concentration.

Parameter estimation is a major challenge in computational modeling of
biological processes. This is especially the case in image-based modeling where
the inherently quantitative output of the model is measured against image data,
which is typically noisy and non-quantitative. In addition, these models can
have a high computational cost, limiting the number of feasible simulations,
and therefore rendering most traditional parameter estimation methods
unsuitable. In this paper, we present a pipeline that uses Gaussian process
learning to estimate biological parameters from noisy, non-quantitative image
data when the model has a high computational cost. This approach is first
successfully tested on a parametric function with the goal of retrieving the
original parameters. We then apply it to estimating parameters in a biological
setting by fitting artificial in-situ hybridization (ISH) data of the
developing murine limb bud. We expect that this method will be of use in a
variety of modeling scenarios where quantitative data is missing and the use of
standard parameter estimation approaches in biological modeling is prohibited
by the computational cost of the model.

Alzheimer s disease (AD) pathophysiology is still imperfectly understood and
current paradigms have not led to curative outcome. Omics technologies offer
great promises for improving our understanding and generating new hypotheses.
However, integration and interpretation of such data pose major challenges,
calling for adequate knowledge models. AlzPathway is a disease map that gives a
detailed and broad account of AD pathophysiology. However, AlzPathway lacks
formalism, which can lead to ambiguity and misinterpretation. Ontologies are an
adequate framework to overcome this limitation, through their axiomatic
definitions and logical reasoning properties. We introduce the AD Map Ontology
(ADMO) an ontological upper model based on systems biology terms. We then
propose to convert AlzPathway into an ontology and to integrate it into ADMO.
We demonstrate that it allows one to deal with issues related to redundancy,
naming, consistency, process classification and pathway relationships. Further,
it opens opportunities to expand the model using elements from other resources,
such as generic pathways from Reactome or clinical features contained in the
ADO (AD Ontology). A version of the ontology will be made freely available to
the community on Bioportal at the time of the confer-ence.

Models for human running performances of various complexities and underlying
principles have been proposed, often combining data from world record
performances and bio-energetic facts of human physiology. Here we present a
novel, minimal and universal model for human running performance that employs a
relative metabolic power scale. The main component is a self-consistency
relation for the time dependent maximal power output. The analytic approach
presented here is the first to derive the observed logarithmic scaling between
world (and other) record running speeds and times from basic principles of
metabolic power supply. Various female and male record performances (world,
national) and also personal best performances of individual runners for
distances from 800m to the marathon are excellently described by this model,
with mean errors of (often much) less than 1%. The model defines endurance in a
way that demonstrates symmetry between long and short racing events that are
separated by a characteristic time scale comparable to the time over which a
runner can sustain maximal oxygen uptake. As an application of our model, we
derive personalized characteristic race speeds for different durations and
distances.

Cloud computing offers on-demand, scalable computing and storage, and has
become an essential resource for the analyses of big biomedical data. The usual
approach to cloud computing requires users to reserve and provision virtual
servers. An emerging alternative is to have the provider allocate machine
resources dynamically. This type of serverless computing has tremendous
potential for biomedical research in terms of ease-of-use, instantaneous
scalability and cost effectiveness. In our proof of concept example, we
demonstrate how serverless computing provides low cost access to hundreds of
CPUs, on demand, with little or no setup. In particular, we illustrate that the
all-against-all pairwise comparison among all unique human proteins can be
accomplished in approximately 2 minutes, at a cost of less than $1, using
Amazon Web Services Lambda. This is a 250x speedup compared to running the same
task on a typical laptop computer.

Searching for local sequence patterns is one of the basic tasks in
bioinformatics. Sequence patterns might have structural, functional or some
other relevance, and numerous methods have been developed to detect and analyze
them. These methods often depend on the wealth of information already
collected. The explosion in the number of newly available sequences calls for
novel methods to explore local sequence similarity. We have developed a high
sensitivity web-based iterative local similarity scanner, that finds sequence
patterns similar to a submitted query.

Ubiquitous in eukaryotic organisms, the flagellum is a well-studied organelle
that is well-known to be responsible for motility in a variety of organisms.
Commonly necessitated in their study is the capability to image and
subsequently track the movement of one or more flagella using videomicroscopy,
requiring digital isolation and location of the flagellum within a sequence of
frames. Such a process in general currently requires some researcher input,
providing some manual estimate or reliance on an experiment-specific heuristic
to correctly identify and track the motion of a flagellum. Here we present a
fully-automated method of flagellum identification from videomicroscopy based
on the fact that the flagella are of approximately constant width when viewed
by microscopy. We demonstrate the effectiveness of the algorithm by application
to captured videomicroscopy of Leishmania mexicana, a parasitic monoflagellate
of the family Trypanosomatidae. ImageJ Macros for flagellar identification are
provided, and high accuracy and remarkable throughput are achieved via this
unsupervised method, obtaining results comparable in quality to previous
studies of closely-related species but achieved without the need for precursory
measurements or the development of a specialised heuristic, enabling in general
the automated generation of digitised kinematic descriptions of flagellar
beating from videomicroscopy.

Continuous-time models have been developed to capture features of animal
movement across temporal scales. In particular, one popular model is the
continuous-time correlated random walk, in which the velocity of an animal is
formulated as an Ornstein-Uhlenbeck process, to capture the autocorrelation in
the speed and direction of its movement. In telemetry analyses, discrete-time
state-switching models (such as hidden Markov models) have been increasingly
popular to identify behavioural phases from animal tracking data. We propose a
multistate formulation of the continuous-time correlated random walk, with an
underlying Markov process used as a proxy for the animal's behavioural state
process. We present a Markov chain Monte Carlo algorithm to carry out Bayesian
inference for this multistate continuous-time model. Posterior samples of the
hidden state sequence, of the state transition rates, and of the
state-dependent movement parameters can be obtained. We investigate the
performance of the method in a simulation study, and we illustrate its use in a
case study of grey seal (Halichoerus grypus) tracking data. The method we
present makes use of the state-space model formulation of the continuous-time
correlated random walk, and can accommodate irregular sampling frequency and
measurement error. It will facilitate the use of continuous-time models to
estimate movement characteristics and infer behavioural states from animal
telemetry data.

Unveiling pathological brain changes associated with Alzheimer's disease (AD)
is a challenging task especially that people do not show symptoms of dementia
until it is late. Over the past years, neuroimaging techniques paved the way
for computer-based diagnosis and prognosis to facilitate the automation of
medical decision support and help clinicians identify cognitively intact
subjects that are at high-risk of developing AD. As a progressive
neurodegenerative disorder, researchers investigated how AD affects the brain
using different approaches: 1) image-based methods where mainly neuroimaging
modalities are used to provide early AD biomarkers, and 2) network-based
methods which focus on functional and structural brain connectivities to give
insights into how AD alters brain wiring. In this study, we reviewed
neuroimaging-based technical methods developed for AD and mild-cognitive
impairment (MCI) classification and prediction tasks, selected by screening all
MICCAI proceedings published between 2010 and 2016. We included papers that fit
into image-based or network-based categories. The majority of papers focused on
classifying MCI vs. AD brain states, which has enabled the discovery of
discriminative or altered brain regions and connections. However, very few
works aimed to predict MCI progression based on early neuroimaging-based
observations. Despite the high importance of reliably identifying which early
MCI patient will convert to AD, remain stable or reverse to normal over
months/years, predictive models are still lagging behind.

The volume tensor provides robust estimate of object shape and orientation in
space. The tensor is estimated from 3D data set by the Fakir probe, an
interactive method using intersections of the objects boundary with a virtual
lines. The method thus can be applied to objects that cannot be segmented
automatically. Marking the intersections instead of segmenting the whole object
reduces the workload required for obtaining sufficiently precise results. We
present theoretical results on the variance of estimate of integrals by
systematic sampling that enable calculation of the shape estimate precision. To
demonstrate the ability of Fakir technique, we measure the changes in shape and
orientation of pheasant brain compartments during development.

The majority of the proteins encoded in the genomes of eukaryotes contain
more than one domain. Reasons for high prevalence of multi-domain proteins in
various organisms have been attributed to higher stability and functional and
folding advantages over single-domain proteins. Despite these advantages, many
proteins are composed of only one domain while their homologous domains are
part of multi-domain proteins. In the study presented here, differences in the
properties of protein domains in single-domain and multi-domain systems and
their influence on functions are discussed. We studied 20 pairs of identical
protein domains, which were crystallized in two forms (a) tethered to other
proteins domains and (b) tethered to fewer protein domains than (a) or not
tethered to any protein domain. Results suggest that tethering of domains in
multi-domain proteins influences the structural, dynamic and energetic
properties of the constituent protein domains. 50% of the protein domain pairs
show significant structural deviations while 90% of the protein domain pairs
show differences in dynamics and 12% of the residues show differences in the
energetics. To gain further insights on the influence of tethering on the
function of the domains, 4 pairs of homologous protein domains, where one of
them is a full-length single-domain protein and the other protein domain is a
part of a multi-domain protein, were studied. Analyses showed that identical
and structurally equivalent functional residues show differential dynamics in
homologous protein domains, though comparable dynamics between in-silico
generated chimera protein and multi-domain proteins were observed. From these
observations, the differences observed in the functions of homologous proteins
could be attributed to the presence of tethered domain. Overall, we conclude
that tethered domains in multi-domain proteins not only provide stability or
folding advantages but also influence pathways resulting in differences in
function or regulatory properties.

In movement ecology, the few works that have taken collective behaviour into
account are data-driven and rely on simplistic theoretical assumptions, relying
in metrics that may or may not be measuring what is intended. In the present
paper, we focus on pairwise joint-movement behaviour, where individuals move
together during at least a segment of their path. We investigate the adequacy
of twelve metrics introduced in previous works for assessing joint movement by
analysing their theoretical properties and confronting them with contrasting
case scenarios. Three criteria are taken into account for review of those
metrics: 1) practical use, 2) dependence on parameters and underlying
assumptions, and 3) computational cost. When analysing the similarities between
the metrics as defined, we show how some of them can be expressed using general
mathematical forms. In addition, we evaluate the ability of each metric to
assess specific aspects of joint-movement behaviour: proximity (closeness in
space-time) and coordination (synchrony) in direction and speed. We found that
some metrics are better suited to assess proximity and others are more
sensitive to coordination. To help readers choose metrics, we elaborate a
graphical representation of the metrics in the coordination and proximity space
based on our results, and give a few examples of proximity and coordination
focus in different movement studies.

Alignment of structural RNAs is an important problem with a wide range of
applications. Since function is often determined by molecular structure, RNA
alignment programs should take into account both sequence and base-pairing
information for structural homology identi^Lcation. A number of successful
alignment programs are heuristic versions of Sanko^K's optimal algorithm. Most
of them require O(n4) run time. This paper describes C++ software,
RNAmountAlign, for RNA sequence/structure alignment that runs in O(n3) time and
O(n2) space; moreover, our software returns a p-value (transformable to expect
value E) based on Karlin-Altschul statistics for local alignment, as well as
parameter ^Ltting for local and global alignment. Using incremental mountain
height, a representation of structural information computable in cubic time,
RNAmountAlign implements quadratic time pairwise local, global and
global/semiglobal (query search) alignment using a weighted combination of
sequence and structural similarity. RNAmountAlign is capable of performing
progressive multiple alignment as well. Benchmarking of RNAmountAlign against
LocARNA, LARA, FOLDALIGN, DYNALIGN and STRAL shows that RNAmountAlign has
reasonably good accuracy and much faster run time supporting all alignment
types.

Background: Disulfide bonds are crucial to protein structural formation.
Developing an effective method topredict disulfide bonding formation is
important for protein structural modeling and functional study. Mostcurrent
methods still have shortcomings, including low accuracy and strict requirements
for the selection ofdiscriminative features. Results: In this study, we
introduced a nested structure of Bidirectional Long-short Term
Memory(BLSTM)neural network called Global-Local-BLSTM (GL-BLSTM) for disulfide
bonding state prediction. Based on thepatterns of disulfide bond formation, a
BLSTM network called Local-BLSTM is used to extract context-basedfeatures
around every Cys residue. Another BLSTM network called Global-BLSTM is
introduced aboveLocal-BLSTM layer to integrate context-based features of all
Cys residues in the same protein chain, therebyinvolving inter-residue
relationships in the training process. According to our experimental
results,GL-BLSTMnetwork performs much better than other methods, GL-BLSTM
reached 90.26% accuracy at residue-level and 83.66% at protein-level. This
model has reached the state of the art in this field. Conclusion: GL-BLSTMs
special structure and mechanisms are beneficial to disulfide bonding state
prediction.By applying bidirectional LSTM, it can extract context based
features by processing protein sequences, therebyobtain more discriminative
information than traditional machine learning methods. Whats more,
GL-BLSTMsspecial two-layer structure enables it to extract both local and
global features, in which global features areplaying important roles in
improving prediction accuracy, especially at protein-level. Keywords: Disulfide
bonds; Prediction; Deep learning; Bidirectional long short term memory

P-values and null-hypothesis significance testing are popular data-analytical
tools in functional neuroimaging. Sparked by the analysis of resting-state fMRI
data, there has been a resurgence of interest in the validity of some of the
p-values evaluated with the widely used software SPM in recent years. The
default parametric p-values reported in SPM are based on the application of
results from random field theory to statistical parametric maps, a framework
commonly referred to as RFT. While RFT was established two decades ago and has
since been applied in a plethora of fMRI studies, there does not exist a
unified documentation of the mathematical and computational underpinnings of
RFT as implemented in current versions of SPM. Here, we provide such a
documentation with the aim of contributing to contemporary efforts towards
higher levels of computational transparency in functional neuroimaging.

Mathematical methods of information theory constitute essential tools to
describe how stimuli are encoded in activities of signaling effectors.
Exploring the information-theoretic perspective, however, remains conceptually,
experimentally and computationally challenging. Specifically, existing
computational tools enable efficient analysis of relatively simple systems,
usually with one input and output only. Moreover, their robust and readily
applicable implementations are missing. Here, we propose a novel algorithm to
analyze signaling data within the framework of information theory. Our approach
enables robust as well as statistically and computationally efficient analysis
of signaling systems with high-dimensional outputs and a large number of input
values. Analysis of the NF-kB single - cell signaling responses to TNF-a
uniquely reveals that the NF-kB signaling dynamics improves discrimination of
high concentrations of TNF-a with a modest impact on discrimination of low
concentrations. Our readily applicable R-package, SLEMI - statistical learning
based estimation of mutual information, allows the approach to be used by
computational biologists with only elementary knowledge of information theory.

This tutorial introduces participants to the design and implementation of an
agent-based model using NetLogo through one of two different projects:
modelling T cell movement within a lymph node or modelling the progress of a
viral infection in an in vitro cell culture monolayer. Each project is broken
into a series of incremental steps of increasing complexity. Each step is
described in detail and the code to type in is initially provided. However,
each project has room to grow in complexity and biological realism so
participants are encouraged to expand their project beyond the scope of the
tutorial or to develop a project of their own.

Gut microbial composition has been linked to multiple health outcomes. Yet,
temporal analysis of this composition had been limited to deterministic models.
In this paper, we introduce a probabilistic model for the dynamics of
intestinal microbiomes that takes into account interaction among bacteria as
well as external effects such as antibiotics. The model successfully deals with
pragmatic issues such as random measurement error and varying time intervals
between measurements through latent space modeling. We demonstrate utility of
the model by using latent state features to predict the clinical events of
intestinal domination and bacteremia, improving accuracy over existing methods.
We further leverage this framework to validate known links between antibiotics
and clinical outcomes, while discovering new ones.

Statistical and mathematical modeling are crucial to describe, interpret,
compare and predict the behavior of complex biological systems including the
organization of hematopoietic stem and progenitor cells in the bone marrow
environment. The current prominence of high-resolution and live-cell imaging
data provides an unprecedented opportunity to study the spatiotemporal dynamics
of these cells within their stem cell niche and learn more about aberrant, but
also unperturbed, normal hematopoiesis. However, this requires careful
quantitative statistical analysis of the spatial and temporal behavior of cells
and the interaction with their microenvironment. Moreover, such quantification
is a prerequisite for the construction of hypothesis-driven mathematical models
that can provide mechanistic explanations by generating spatiotemporal dynamics
that can be directly compared to experimental observations. Here, we provide a
brief overview of statistical methods in analyzing spatial distribution of
cells, cell motility, cell shapes and cellular genealogies. We also describe
cell- based modeling formalisms that allow researchers to simulate emergent
behavior in a multicellular system based on a set of hypothesized mechanisms.
Together, these methods provide a quantitative workflow for the analytic and
synthetic study of the spatiotemporal behavior of hematopoietic stem and
progenitor cells.

A rapid, cost-effective and easy method that allows on-site determination of
the concentration of live and dead bacterial cells using a fibre-based
spectroscopic device (the optrode system) is proposed and demonstrated.
Identification of live and dead bacteria was achieved by using the commercially
available dyes SYTO 9 and propidium iodide, and fluorescence spectra were
measured by the optrode. Three spectral processing methods were evaluated for
their effectiveness in predicting the original bacterial concentration in the
samples: principal components regression (PCR), partial least squares
regression (PLSR) and support vector regression (SVR). Without any sample
pre-concentration, PCR achieved the most reliable results. It was able to
quantify live bacteria from $10^{8}$ down to $10^{6.2}$ bacteria/mL and showed
the potential to detect as low as $10^{5.7}$ bacteria/mL. Meanwhile,
enumeration of dead bacteria using PCR was achieved between $10^{8}$ and
$10^{7}$ bacteria/mL. The general procedures described in this article can be
applied or modified for the enumeration of bacteria within populations stained
with fluorescent dyes. The optrode is a promising device for the enumeration of
live and dead bacterial populations particularly where rapid, on-site
measurement and analysis is required.

Missing values are largely inevitable in gene expression microarray studies.
Data sets often have significant omissions due to individuals dropping out of
experiments, errors in data collection, image corruptions, and so on. Missing
data could potentially undermine the validity of research results - leading to
inaccurate predictive models and misleading conclusions. Imputation - a
relatively flexible, general purpose approach towards dealing with missing data
- is now available in massive numbers, making it possible to handle missing
data. While these estimation methods are becoming increasingly more effective
in resolving the discrepancies between true and estimated values, its effect on
clustering outcomes is largely disregarded.
  This study seeks to reveal the vast differences in agglomerative hierarchal
clustering outcomes estimation methods can construct in comparison to the
precision exhibited (presented through the cophenetic correlation coefficient)
in comparison to their high efficiency and effectivity in value preservation of
true and imputed values (presented through the root-mean-squared error). We
argue against the traditional approach towards the development of imputation
methods and instead, advocate towards methods that reproduce a data set's
original, natural cluster.
  By using a number of advanced imputation methods, we reveal extensive
differences between original and reconstructed clusters that could
significantly transform the interpretations of the data as a whole.

Many biological, psychological and economic experiments have been designed
where an organism or individual must choose between two options that have the
same expected reward but differ in the variance of reward received. In this
way, designed empirical approaches have been developed for evaluating risk
preferences. Here, however, we show that if the experimental subject is
inferring the reward distribution (to optimize some process), they will never
agree in finite time that the expected rewards are equal. In turn, we argue
that this makes discussions of risk preferences, and indeed the motivations of
behaviour, not so simple or straightforward to interpret. We use this
particular experiment to highlight the serious need to consider the frame of
reference of the experimental subject in studies of behaviour.

We introduce Ordinal Synchronization ($OS$) as a new measure to quantify
synchronization between dynamical systems. $OS$ is calculated from the
extraction of the ordinal patterns related to two time series, their
transformation into $D$-dimensional ordinal vectors and the adequate
quantification of their alignment. $OS$ provides a fast and robust-to noise
tool to assess synchronization without any implicit assumption about the
distribution of data sets nor their dynamical properties, capturing in-phase
and anti-phase synchronization. Furthermore, varying the length of the ordinal
vectors required to compute $OS$ it is possible to detect synchronization at
different time scales. We test the performance of $OS$ with data sets coming
from unidirectionally coupled electronic Lorenz oscillators and brain imaging
datasets obtained from magnetoencephalographic recordings, comparing the
performance of $OS$ with other classical metrics that quantify synchronization
between dynamical systems.

If Electronic Health Records contain a large amount of information about the
patients condition and response to treatment, which can potentially
revolutionize the clinical practice, such information is seldom considered due
to the complexity of its extraction and analysis. We here report on a first
integration of an NLP framework for the analysis of clinical records of lung
cancer patients making use of a telephone assistance service of a major Spanish
hospital. We specifically show how some relevant data, about patient
demographics and health condition, can be extracted; and how some relevant
analyses can be performed, aimed at improving the usefulness of the service. We
thus demonstrate that the use of EHR texts, and their integration inside a data
analysis framework, is technically feasible and worth of further study.

RKappa is a framework for the development, simulation and analysis of
rule-base models within the mature statistically empowered R environment. It is
designed for model editing, parameter identification, simulation, sensitivity
analysis and visualisation. The framework is optimised for high-performance
computing platforms and facilitates analysis of large-scale systems biology
models where knowledge of exact mechanisms is limited and parameter values are
uncertain. The RKappa software is an open source (GLP3 license) package for R,
which is freely available online ( https://github.com/lptolik/R4Kappa ).

BioNetFit is a software tool designed for solving parameter identification
problems that arise in the development of rule-based models. It solves these
problems through curve fitting (i.e., nonlinear regression). BioNetFit is
compatible with deterministic and stochastic simulators that accept BioNetGen
language (BNGL)-formatted files as inputs, such those available within the
BioNetGen framework. BioNetFit can be used on a laptop or standalone multicore
workstation as well as on many Linux clusters, such as those that use the Slurm
Workload Manager to schedule jobs. BioNetFit implements a metaheuristic
population-based global optimization procedure, an evolutionary algorithm (EA),
to minimize a user-defined objective function, such as a residual sum of
squares (RSS) function. BioNetFit also implements a bootstrapping procedure for
determining confidence intervals for parameter estimates. Here, we provide
step-by-step instructions for using BioNetFit to estimate the values of
parameters of a BNGL-encoded model and to define bootstrap confidence
intervals. The process entails the use of several plain-text files, which are
processed by BioNetFit and BioNetGen. In general, these files include 1) one or
more EXP files, which each contains (experimental) data to be used in parameter
identification/bootstrapping; 2) a BNGL file containing a model section, which
defines a (rule-based) model, and an actions section, which defines simulation
protocols that generate GDAT and/or SCAN files with model predictions
corresponding to the data in the EXP file(s); and 3) a CONF file that
configures the fitting/bootstrapping job and that defines algorithmic parameter
settings.

This chapter provides a brief introduction to the theory and practice of
spatial stochastic simulations. It begins with an overview of different methods
available for biochemical simulations highlighting their strengths and
limitations. Spatial stochastic modeling approaches are indicated when
diffusion is relatively slow and spatial inhomogeneities involve relatively
small numbers of particles. The popular software package MCell allows
particle-based stochastic simulations of biochemical systems in complex three
dimensional (3D) geometries, which are important for many cell biology
applications. Here, we provide an overview of the simulation algorithms used by
MCell and the underlying theory. We then give a tutorial on building and
simulating MCell models using the CellBlender graphical user interface, that is
built as a plug-in to Blender, a widely-used and freely available software
platform for 3D modeling. The tutorial starts with simple models that
demonstrate basic MCell functionality and then advances to a number of more
complex examples that demonstrate a range of features and provide examples of
important biophysical effects that require spatially-resolved stochastic
dynamics to capture.

Focused Ion Beam Scanning Electron Microscope (FIB-SEM) imaging is a
technique that image materials section-by-section at nano-resolution, e.g.,5
nanometer width voxels. FIB-SEM is well suited for imaging ultrastructures in
cells. Unfortunately, typical setups will introduce a slight sub-pixel
translation from section to section typically referred to as drift. Over
multiple sections, drift compound to skew distance measures and geometric
structures significantly from the pre-imaged stage. Popular correction
approaches often involve standard image registration methods available in
packages such as ImageJ or similar software. These methods transform the images
to maximize the similarity between consecutive two-dimensional sections under
some measure. We show how these standard approaches will both significantly
underestimate the drift, as well as producing biased corrections as they tend
to align the images such that the normal of planar biological structures are
perpendicular to the sectioning direction causing poor or incorrect correction
of the images. In this paper, we present a highly accurate correction method
for estimating drift in isotropic electron microscope images with visible
vesicles.

The drive for reproducibility in the computational sciences has provoked
discussion and effort across a broad range of perspectives: technological,
legislative/policy, education, and publishing. Discussion on these topics is
not new, but the need to adopt standards for reproducibility of claims made
based on computational results is now clear to researchers, publishers and
policymakers alike. Many technologies exist to support and promote reproduction
of computational results: containerisation tools like Docker, literate
programming approaches such as Sweave, knitr, iPython or cloud environments
like Amazon Web Services. But these technologies are tied to specific
programming languages (e.g. Sweave/knitr to R; iPython to Python) or to
platforms (e.g. Docker for 64-bit Linux environments only). To date, no single
approach is able to span the broad range of technologies and platforms
represented in computational biology and biotechnology.
  To enable reproducibility across computational biology, we demonstrate an
approach and provide a set of tools that is suitable for all computational work
and is not tied to a particular programming language or platform. We present
published examples from a series of papers in different areas of computational
biology, spanning the major languages and technologies in the field
(Python/R/MATLAB/Fortran/C/Java). Our approach produces a transparent and
flexible process for replication and recomputation of results. Ultimately, its
most valuable aspect is the decoupling of methods in computational biology from
their implementation. Separating the 'how' (method) of a publication from the
'where' (implementation) promotes genuinely open science and benefits the
scientific community as a whole.

In this work we study the characteristics of the heart rate variability (HRV)
as a function of age and gender. The analyzed data include previous results
reported in the literature. The data obtained in this work expand the range of
age studied until now revealing new behaviors not reported before. We analyze
some measurements in the time domain,in the frequency domain and nonlinear
measurements. We report scaling behaviors and abrupt changes in some
measurements. There is also a progressive decrease in the dimensionality of the
dynamic system governing the HRV, with the increase in age that is interpreted
in terms ofautonomic regulation of cardiac activity.

This paper addresses the problem of reconstructing the motion trajectories of
the individuals in a large collection of flying objects using two temporally
synchronized and geometrically calibrated cameras. The 3D trajectory
reconstruction problem involves two challenging tasks - stereo matching and
temporal tracking. Existing methods separate the two and process them one at a
time sequentially, and suffer from frequent irresolvable ambiguities in stereo
matching and in tracking. We unify the two tasks, and propose an optimization
approach to solving stereo matching and temporal tracking simultaneously. It
treats 3D trajectory acquisition problem as selecting appropriate stereo
correspondence out of all possible ones for each object via minimizing a cost
function. Experiment results show that the proposed method offers significant
performance advantage over existing approaches. The proposed method has
successfully been applied to reconstruct 3D motion trajectories of hundreds of
simultaneously flying fruit flies (Drosophila Melanogaster), which could
facilitate the study the insect's collective behavior.

Optogenetics is a revolutionary new field of biotechnology, achieving optical
control over biological functions in living cells by genetically inserting
light sensitive proteins into cellular signaling pathways. Applications of
optogenetic switches are expanding rapidly, but the technique is hampered by
spectral cross-talk: the broad absorption spectra of compatible biochemical
chromophores limits the number of switches that can be independently controlled
and restricts the dynamic range of each switch. In the present work we develop
and implement a non-linear optical photoswitching capability, Stimulated
Depletion Quenching (SDQ), is used to overcome spectral cross-talk by
exploiting the molecules' unique dynamic response to ultrashort laser pulses.
SDQ is employed to enhance the control of Cph8, a photo-reversible phytochrome
based optogenetic switch designed to control gene expression in E. Coli
bacteria. The Cph8 switch can not be fully converted to it's biologically
inactive state ($P_{FR}$) by linear photos-witching, as spectral cross-talk
causes a reverse photoswitching reaction to revert to it back to the active
state ($P_{R}$). SDQ selectively halts this reverse reaction while allowing the
forward reaction to proceed. The results of this proof of concept experiment
lay the foundation for future experiments that will use optimal pulse shaping
to further enhance control of Cph8 and enable simultaneous, multiplexed control
of multiple optogenetic switches.

The interaction of the actin cytoskeleton with cell-substrate adhesions is
necessary for cell migration. While the trajectories of motile cells have a
stochastic character, investigations of cell motility mechanisms rarely
elaborate on the origins of the observed randomness. Here, guided by a few
fundamental attributes of cell motility, we construct a minimal stochastic cell
migration model from ground-up. The resulting model couples a deterministic
actomyosin contractility mechanism with stochastic cell-substrate adhesion
kinetics, and yields a well-defined piecewise deterministic process. Numerical
simulations reproduce several experimentally observed results, including
anomalous diffusion, tactic migration, and contact guidance. This work provides
a basis for the development of cell-cell collision and population migration
models.

We consider the problem of estimating an unbiased and reference-free ab-inito
model for non-symmetric molecules from images generated by single-particle
cryo-electron microscopy. The proposed algorithm finds the globally optimal
assignment of orientations that simultaneously respects all common lines
between all images. The contribution of each common line to the estimated
orientations is weighted according to a statistical model for common lines'
detection errors. The key property of the proposed algorithm is that it finds
the global optimum for the orientations given the common lines. In particular,
any local optima in the common lines energy landscape do not affect the
proposed algorithm. As a result, it is applicable to thousands of images at
once, very robust to noise, completely reference free, and not biased towards
any initial model. A byproduct of the algorithm is a set of measures that allow
to asses the reliability of the obtained ab-initio model. We demonstrate the
algorithm using class averages from two experimental data sets, resulting in
ab-initio models with resolutions of 20A or better, even from class averages
consisting of as few as three raw images per class.

Nitrous oxide (N2O) is a potent greenhouse gas emitted during biological
wastewater treatment. A pseudo-mechanistic model describing three biological
pathways for nitric oxide (NO) and N2O production was calibrated for mixed
culture biomass from an activated sludge process using laboratory-scale
experiments. The model (NDHA) comprehensively describes N2O producing pathways
by both autotrophic ammonium oxidizing bacteria and heterotrophic bacteria.
Extant respirometric assays and anaerobic batch experiments were designed to
calibrate endogenous and exogenous processes (heterotrophic denitrification and
autotrophic ammonium/nitrite oxidation) together with the associated net N2O
production. Ten parameters describing heterotrophic processes and seven for
autotrophic processes were accurately estimated (variance/mean < 25%). The
model predicted NO and N2O dynamics at varying dissolved oxygen, ammonium and
nitrite levels and was validated against an independent set of experiments with
the same biomass. Aerobic ammonium oxidation experiments at two oxygen levels
used for model evaluation (2 and 0.5 mg/L) indicated that both the nitrifier
denitrification (42, 64%) and heterotrophic denitrification (7, 17%) pathways
increased and dominated N2O production at high nitrite and low oxygen
concentrations; while the nitrifier nitrification pathway showed the largest
contribution at high dissolved oxygen levels (51, 19%). The uncertainty of the
biological parameter estimates was propagated to N2O model outputs via Monte
Carlo simulations as 95% confidence intervals. The accuracy of the estimated
parameters resulted in a low uncertainty of the N2O emission factors (4.6 +-
0.6% and 1.2 +- 0.1%).

Seed priming is one of the well-established and low cost method to improve
seed germination properties, productivity, and stress tolerance in different
crops. It is a pre-germination treatment that partially hydrates the seed and
allows controlled imbibition. This stimulates and induces initial germination
process, but prevents radicle emergence. Consequently, treated seeds are
fortified with enhanced germination characteristics, improved physiological
parameters, uniformity in growth, and improved capability to cope up with
different biotic and abiotic stresses. Existing techniques for evaluating the
effectiveness of seed priming suffer from several drawbacks, including very
high operating time, indirect and destructive analysis, bulky experimental
arrangement, high cost, and require extensive analytical expertise. To
circumvent these drawbacks, we propose a biospeckle based technique to analyse
the effects of different priming treatments on germination characteristics of
seeds. The study employs non-primed (T0) and priming treatments (T1-T75),
including hydropriming and chemical priming (using three chemical agents namely
sodium chloride, potassium nitrate, and urea) for different time durations and
solution concentrations. The results conclusively establish biospeckle analysis
as an efficient active tool for seed priming analysis. Furthermore, the
proposed setup is extremely simple, low-cost, involves non-mechanical scanning
and is highly stable.

This work considers the method of uniformisation for continuous-time Markov
chains in the context of chemical reaction networks. Previous work in the
literature has shown that uniformisation can be beneficial in the context of
time-inhomogeneous models, such as chemical reaction networks incorporating
extrinsic noise. This paper lays focus on the understanding of uniformisation
from the viewpoint of sample paths of chemical reaction networks. In
particular, an efficient pathwise stochastic simulation algorithm for
time-homogeneous models is presented which is complexity-wise equal to
Gillespie's direct method. This new approach therefore enlarges the class of
problems for which the uniformisation approach forms a computationally
attractive choice. Furthermore, as a new application of the uniformisation
method, we provide a novel variance reduction method for (raw) moment
estimators of chemical reaction networks based upon the combination of
stratification and uniformisation.

Video analysis is currently the main non-intrusive method for the study of
collective behavior. However, 3D-to-2D projection leads to overlapping of
observed objects. The situation is further complicated by the absence of stall
shapes for the majority of living objects. Fortunately, living objects often
possess a certain symmetry which was used as a basis for morphological
fingerprinting. This technique allowed us to record forms of symmetrical
objects in a pose-invariant way. When combined with image skeletonization, this
gives a robust, nonlinear, optimization-free, and fast method for detection of
overlapping objects, even without any rigid pattern. This novel method was
verified on fish (European bass, Dicentrarchus labrax, and tiger barbs, Puntius
tetrazona) swimming in a reasonably small tank, which forced them to exhibit a
large variety of shapes. Compared with manual detection, the correct number of
objects was determined for up to almost $90 \%$ of overlaps, and the mean
Dice-Sorensen coefficient was around $0.83$. This implies that this method is
feasible in real-life applications such as toxicity testing.

Reliable determination of sensory thresholds is the holy grail of signal
detection theory. However, there exists no gold standard for the estimation of
thresholds based on neurophysiological parameters, although a reliable
estimation method is crucial for both scientific investigations and clinical
diagnosis. Whenever it is impossible to communicate with the subjects, as in
studies with animals or neonatales, thresholds have to be derived from neural
recordings. In such cases when the threshold is estimated based on neuronal
measures, the standard approach is still the subjective setting of the
threshold to the value where at least a "clear" neuronal signal is detectable.
These measures are highly subjective, strongly depend on the noise, and
fluctuate due to the low signal-to-noise ratio near the threshold. Here we show
a novel method to reliably estimate physiological thresholds based on
neurophysiological parameters. Using surrogate data, we demonstrate that
fitting the responses to different stimulus intensities with a hard sigmoid
function, in combination with subsampling, provides a robust threshold value as
well as an accurate uncertainty estimate. This method has no systematic
dependence on the noise and does not even require samples in the full dynamic
range of the sensory system. It is universally applicable to all types of
sensory systems, ranging from somatosensory stimulus processing in the cortex
to auditory processing in the brain stem.

Diffusion kurtosis imaging (DKI), is an imaging modality that yields novel
disease biomarkers and in combination with nervous tissue modeling, provides
access to microstructural parameters. Recently, DKI and subsequent estimation
of microstructural model parameters has been used for assessment of tissue
changes in neurodegenerative diseases and their animal models. In this study,
mouse spinal cords from the experimental autoimmune encephalomyelitis (EAE)
model of multiple sclerosis (MS) were investigated for the first time using DKI
in combination with biophysical modeling to study the relationship between
microstructural metrics and degree of animal dysfunction. Thirteen spinal cords
were extracted from animals of variable disability and scanned in a high-field
MRI scanner along with five control specimen. Diffusion weighted data were
acquired together with high resolution T2* images. Diffusion data were fit to
estimate diffusion and kurtosis tensors and white matter modeling parameters,
which were all used for subsequent statistical analysis using a linear mixed
effects model. T2* images were used to delineate focal
demyelination/inflammation. Our results unveil a strong relationship between
disability and measured microstructural parameters in normal appearing white
matter and gray matter. The changes we found in biophysical modeling parameters
and in particular in extra-axonal axial diffusivity were clearly different from
previous studies employing other animal models of MS. In conclusion, our data
suggest that DKI and microstructural modeling can provide a unique contrast
capable of detecting EAE-specific changes correlating with clinical disability.
These findings could close the gap between MRI findings and clinical
presentation in patients and deepen our understanding of EAE and the MS
mechanisms.

Persistent homology has been applied to brain network analysis for finding
the shape of brain networks across multiple thresholds. In the persistent
homology, the shape of networks is often quantified by the sequence of
$k$-dimensional holes and Betti numbers.The Betti numbers are more widely used
than holes themselves in topological brain network analysis. However, the holes
show the local connectivity of networks, and they can be very informative
features in analysis. In this study, we propose a new method of measuring
network differences based on the dissimilarity measure of harmonic holes (HHs).
The HHs, which represent the substructure of brain networks, are extracted by
the Hodge Laplacian of brain networks. We also find the most contributed HHs to
the network difference based on the HH dissimilarity. We applied our proposed
method to clustering the networks of 4 groups, normal control (NC), stable and
progressive mild cognitive impairment (sMCI and pMCI), and Alzheimer's disease
(AD). The results showed that the clustering performance of the proposed method
was better than that of network distances based on only the global change of
topology.

Optogenetics is a rapidly growing field of biotechnology, potentially
allowing a deeper understanding and control of complex biological networks. The
major challenge is the multiplexed control of several optogenetic components in
the presence of significant spectral cross talk. We propose and demonstrate
through simulations a new control approach of Stimulated Depletion Quenching.
This approach is applied to the phytochrome Cph8 bidirectional optogenetic
switch, and the results show significant improvement of its dynamic range.

Protein solubility plays a critical role in improving production yield of
recombinant proteins in biocatalyst and pharmaceutical field. To some extent,
protein solubility can represent the function and activity of biocatalysts
which are mainly composed of recombinant proteins. Highly soluble proteins are
more effective in biocatalytic processes and can reduce the cost of
biocatalysts. Screening proteins by experiments in vivo is time-consuming and
expensive. In literature, large amounts of machine learning models have been
investigated, whereas parameters of those models are underdetermined with
insufficient data of protein solubility. A data augmentation algorithm that can
enlarge the dataset of protein solubility and improve the performance of
prediction model is highly desired, which can alleviate the common issue of
insufficient data in biotechnology applications for developing machine learning
models. We first implemented a novel approach that a data augmentation
algorithm, conditional WGAN was used to improve prediction performance of DNN
for protein solubility from protein sequence by generating artificial data.
After adding mimic data produced from conditional WGAN, the prediction
performance represented by $R^{2}$ was improved compared with the $R^{2}$
without data augmentation. After tuning the hyperparameters of two algorithms
and organizing the dataset, we achieved a $R^{2}$ value of $45.04\%$, which
enhanced $R^{2}$ about $10\%$ compared with the previous study using the same
dataset. Data augmentation opens the door to applications of machine learning
models on biological data, as machine learning models always fail to be well
trained by small datasets.

Over the last decades, honeybees have been a fascinating model to study
insect navigation. While there is some controversy about the complexity of
underlying neural correlates, the research of honeybee navigation makes
progress through both the analysis of flight behavior and the synthesis of
agent models. Since visual cues are believed to play a crucial role for the
behavioral output of a navigating bee we have developed a realistic
3-dimensional virtual world, in which simulated agents can be tested, or in
which the visual input of experimentally traced animals can be reconstructed.
In this paper we present implementation details on how we reconstructed a large
3-dimensional world from aerial imagery of one of our field sites, how the
distribution of ommatidia and their view geometry was modeled, and how the
system samples from the scene to obtain realistic bee views. This system is
made available as an open-source project to the community on
\url{http://github.com/bioroboticslab/bee_view}.

Approximate Bayesian Computation is widely used to infer the parameters of
discrete-state continuous-time Markov networks. In this work, we focus on
models that are governed by the Chemical Master Equation (the CME). Whilst
originally designed to model biochemical reactions, CME-based models are now
frequently used to describe a wide range of biological phenomena
mathematically. We describe and implement an efficient multi-level ABC method
for investigating model parameters. In short, we generate sample paths of
CME-based models with varying time resolutions. We start by generating
low-resolution sample paths, which require only limited computational resources
to construct. Those sample paths that compare well with experimental data are
selected, and the temporal resolutions of the chosen sample paths are
recursively increased. Those sample paths unlikely to aid in parameter
inference are discarded at an early stage, leading to an optimal use of
computational resources. The efficacy of the multi-level ABC is demonstrated
through two case studies.

Quantification of sulfated polysaccharides in urine samples is relevant to
pharmacokinetic studies in drug development projects and to the non-invasive
diagnosis and therapy monitoring of mucopolysaccharidoses. The Heparin Red Kit
is a particularly simple and user friendly fluorescence assay for the
quantification of sulfated polysaccharides and has recently emerged as a novel
tool for the monitoring of their blood levels during pharmacokinetic studies in
clinical trials. The standard protocol for the blood plasma matrix is, however,
not suited for urine samples due to matrix interference. The present study
identifies inorganic sulfate as the interfering component in urine. The sulfate
level of urine is typically 1-2 orders of magnitude higher compared with the
blood plasma level. Addition of either hydrochloric acid or magnesium chloride
counteracts sulfate interference but still enables sensitive detection of
sulfated polysaccharides such as heparin, heparan sulfate and dermatan sulfate
at low microgramm per milliliter levels. This study extends the application
range of the Heparin Red Kit by a simple modification of the assay protocol to
the direct quantification of various sulfated polysaccharides in human urine.

Identification of nearly all proteins in a system using data-dependent
acquisition (DDA) mass spectrometry has become routine for simple organisms,
such as bacteria and yeast. Still, quantification of the identified proteins
may be a complex process and require multiple different software packages. This
protocol describes identification and label-free quantification of proteins
from bottom-up proteomics experiments. This method can be used to quantify all
the detectable proteins in any DDA dataset collected with high-resolution
precursor scans. This protocol may be used to quantify proteome remodeling in
response to a drug treatment or a gene knockout. Notably, the method uses the
latest and fastest freely-available software, and the entire protocol can be
completed in a few hours with data from organisms with relatively small
genomes, such as yeast or bacteria.

Movement-based indices such as moves per minute (MPM) and proportion time
moving (PTM) are common methodologies to quantify foraging behaviour. Hundreds
of studies have reported these indices without specifying the temporal
resolution of their original data, despite the likelihood that the minimal stop
and move durations can affect MPM and PTM estimates. Our goal was to
empirically determine the sensitivity of these foraging indices to changes in
the temporal resolution of the observation. We used a high speed camera to
record movement sequences of 20 Acanthodactylus boskianus lizards. We gradually
decreased the data resolution by ignoring short stops and either ignoring,
elongating or leaving short moves unchanged. We then used the manipulated data
to calculate the foraging indices at different temporal resolutions. We found
that movement-based indices are very sensitive to the observation resolution,
so that realistic variation in the minimal duration of stops and moves could
lead to 68 percent and 48 percent difference in MPM and PTM estimates,
respectively. When using the highest resolution, our estimate of MPM was an
order of magnitude higher than all prior reported values for lizards. Also, the
distribution of stop durations was well described by a single heavy tailed
distribution above 0.35 seconds. This suggests that for A. boskianus there is
no reason to ignore short stops above this threshold. Our results raise major
concerns regarding the use of already published movement based indices, and
enable us to recommend how new foraging data should be collected.

Several scalp EEG functional connectivity studies, mostly clinical, seem to
overlook the reference electrode impact. The subsequent interpretation of brain
connectivity is thus often biased by the choice a non-neutral reference. This
study aims at systematically investigating these effects. As EEG reference, we
examined: the vertex electrode (Cz), the digitally linked mastoids (DLM), the
average reference (AVE), and the Reference Electrode Standardization Technique
(REST). As a connectivity metric, we used the imaginary part of coherency. We
tested simulated and real data (eyes open resting state), by evaluating the
influence of electrode density, effect of head model accuracy in the REST
transformation, and impact on the characterization of the topology of
functional networks from graph analysis. Simulations demonstrated that REST
significantly reduced the distortion of connectivity patterns when compared to
AVE, Cz and DLM references. Moreover, the availability of high-density EEG
systems and an accurate knowledge of the head model are crucial elements to
improve REST performance. For real data, a systematic change of the spatial
pattern of functional connectivity depending on the chosen reference was also
observed. The distortion of connectivity patterns was larger for the Cz
reference, and progressively decreases when using the DLM, the AVE, the REST.
Strikingly, we also showed that network attributes derived from graph analysis,
i.e., node degree and local efficiency, are significantly influenced by the EEG
reference choice. Overall, this study highlights that significant differences
arise in scalp EEG functional connectivity and graph network properties, in
dependence of the chosen reference. We hope our study will convey the message
that caution should be taken when interpreting and comparing results obtained
from different laboratories when using different reference schemes.

Synthetic DNA can in principle be used for the archival storage of arbitrary
data. Because errors are introduced during DNA synthesis, storage, and
sequencing, an error-correcting code (ECC) is necessary for error-free recovery
of the data. Previous work has utilized ECCs that can correct substitution
errors, but not insertion or deletion errors (indels), instead relying on
sequencing depth and multiple alignment to detect and correct indels -- in
effect an inefficient multiple-repetition code. This paper describes an ECC,
termed "HEDGES", that corrects simultaneously for substitutions, insertions,
and deletions in a single read. Varying code rates allow for correction of up
to ~10% nucleotide errors and achieve 50% or better of the estimated Shannon
limit.

Recent clinical trials have shown that the adaptive drug therapy can be more
efficient than a standard MTD-based policy in treatment of cancer patients. The
adaptive therapy paradigm is not based on a preset schedule; instead, the doses
are administered based on the current state of tumor. But the adaptive
treatment policies examined so far have been largely ad hoc. In this paper we
propose a method for systematically optimizing the rules of adaptive policies
based on an Evolutionary Game Theory model of cancer dynamics. Given a set of
treatment objectives, we use the framework of dynamic programming to find the
optimal treatment strategies. In particular, we optimize the total drug usage
and time to recovery by solving a Hamilton-Jacobi-Bellman equation based on a
mathematical model of tumor evolution. We compare adaptive/optimal treatment
strategy with MTD-based treatment policy. We show that optimal treatment
strategies can dramatically decrease the total amount of drugs prescribed as
well as increase the fraction of initial tumour states from which the recovery
is possible. We also examine the optimization trade-offs between the total
administered drugs and recovery time. The adaptive therapy combined with
optimal control theory is a promising concept in the cancer treatment and
should be integrated into clinical trial design.

Radiotherapy plays a vital role in cancer treatment, for which accurate
prognosis is important for guiding sequential treatment and improving the
curative effect for patients. An issue of great significance in radiotherapy is
to assess tumor radiosensitivity for devising the optimal treatment strategy.
Previous studies focused on gene expression in cells closely associated with
radiosensitivity, but factors such as the response of a cancer patient to
irradiation and the patient survival time are largely ignored. For clinical
cancer treatment, a specific pre-treatment indicator taking into account cancer
cell type and patient radiosensitivity is of great value but it has been
missing. Here, we propose an effective indicator for radiosensitivity:
radiosensitive gene group centrality (RSGGC), which characterizes the
importance of the group of genes that are radiosensitive in the whole gene
correlation network. We demonstrate, using both clinical patient data and
experimental cancer cell lines, which RSGGC can provide a quantitative estimate
of the effect of radiotherapy, with factors such as the patient survival time
and the survived fraction of cancer cell lines under radiotherapy fully taken
into account. Our main finding is that, for patients with a higher RSGGC score
before radiotherapy, cancer treatment tends to be more effective. The RSGGC can
have significant applications in clinical prognosis, serving as a key measure
to classifying radiosensitive and radioresistant patients.

Epileptic seizures detection and forecasting is nowadays widely recognized as
a problem of great significance and social resonance, and still remains an
open, grand challenge. Furthermore, the development of mobile warning systems
and wearable, non invasive, advisory devices are increasingly and strongly
requested, from the patient community and their families and also from
institutional stakeholders. According to the many recent studies, exploiting
machine learning capabilities upon intracranial EEG (iEEG), in this work we
investigate a combination of novel game theory dynamical model on networks for
brain electrical activity and nonlinear time series analysis based on
recurrences quantification. These two methods are then melted together within a
supervised learning scheme and finally, prediction performances are assessed
using EEG scalp datasets, specifically recorded for this study. Our study
achieved mean sensitivity of 70.9% and a mean time in warning of 20.3%, thus
showing an increase of the improvement over chance metric from 42%, reported in
the most recent study, to 50.5%. Moreover, the real time implementation of the
proposed approach is currently under development on a prototype of a wearable
device.

Epicormic branches arise from dormant buds patterned during the growth of
previous years. Dormant epicormic buds remain on the surface of trees, pushed
outward from the pith during secondary growth, but maintaining vascular
connections. Epicormic buds can be reactivated, either through natural
processes or intentionally, to rejuvenate orchards and control tree
architecture. Because epicormic structures are embedded within secondary
growth, tomographic approaches are a useful method to study them and understand
their development.
  We apply techniques from image processing to determine the locations of
epicormic vascular traces embedded within secondary growth of sweet cherry
(Prunus avium L.), revealing the juvenile phyllotactic pattern in the trunk of
an adult tree. Techniques include breadth-first search to find the pith of the
tree, edge detection to approximate the radius, and a conversion to polar
coordinates to threshold and segment phyllotactic features. Intensity values
from Magnetic Resonance Imaging (MRI) of the trunk are projected onto the
surface of a perfect cylinder to find the locations of traces in the "boundary
image". Mathematical phyllotaxy provides a means to capture the patterns in the
boundary image by modeling phyllotactic parameters. Our cherry tree specimen
has the conspicuous parastichy pair $(2,3)$, phyllotactic fraction 2/5, and
divergence angle of approximately 143 degrees.
  The methods described not only provide a framework to study phyllotaxy, but
for image processing of volumetric image data in plants. Our results have
practical implications for orchard rejuvenation and directed approaches to
influence tree architecture. The study of epicormic structures, which are
hidden within secondary growth, using tomographic methods also opens the
possibility of studying the genetic and environmental basis of such structures.

Potts statistical models have become a popular and promising way to analyze
mutational covariation in protein Multiple Sequence Alignments (MSAs) in order
to understand protein structure, function and fitness. But the statistical
limitations of these models, which can have millions of parameters and are fit
to MSAs of only thousands or hundreds of effective sequences using a procedure
known as inverse Ising inference, are incompletely understood. In this work we
predict how model quality degrades as a function of the number of sequences
$N$, sequence length $L$, amino-acid alphabet size $q$, and the degree of
conservation of the MSA, in different applications of the Potts models: In
"fitness" predictions of individual protein sequences, in predictions of the
effects of single-point mutations, in "double mutant cycle" predictions of
epistasis, and in 3-d contact prediction in protein structure. We show how as
MSA depth $N$ decreases an "overfitting" effect occurs such that sequences in
the training MSA have overestimated fitness, and we predict the magnitude of
this effect and discuss how regularization can help correct for it, use a
regularization procedure motivated by statistical analysis of the effects of
finite sampling. We find that as $N$ decreases the quality of point-mutation
effect predictions degrade least, fitness and epistasis predictions degrade
more rapidly, and contact predictions are most affected. However, overfitting
becomes negligible for MSA depths of more than a few thousand effective
sequences, as often used in practice, and regularization becomes less
necessary. We discuss the implications of these results for users of Potts
covariation analysis.

Observability is a modelling property that describes the possibility of
inferring the internal state of a system from observations of its output. A
related property, structural identifiability, refers to the theoretical
possibility of determining the parameter values from the output. In fact,
structural identifiability becomes a particular case of observability if the
parameters are considered as constant state variables. It is possible to
simultaneously analyse the observability and structural identifiability of a
model using the conceptual tools of differential geometry. Many complex
biological processes can be described by systems of nonlinear ordinary
differential equations, and can therefore be analysed with this approach. The
purpose of this review article is threefold: (I) to serve as a tutorial on
observability and structural identifiability of nonlinear systems, using the
differential geometry approach for their analysis; (II) to review recent
advances in the field; and (III) to identify open problems and suggest new
avenues for research in this area.

Colon cancer is the second leading cause of cancer-related death in the
United States of America. Its prognosis has significantly improved with the
advancement of targeted therapies based on underlying molecular changes. The
KRAS mutation is one of the most frequent molecular alterations seen in colon
cancer and its presence can affect treatment selection. We attempted to use
Apple machine learning algorithms to diagnose colon cancer and predict the KRAS
mutation status from histopathological images. We captured 250 colon cancer
images and 250 benign colon tissue images. Half of colon cancer images were
captured from KRAS mutation-positive tumors and another half from KRAS
mutation-negative tumors. Next, we created Image Classifier Model using Apple
CreateML machine learning module. The trained and validated model was able to
successfully differentiate between colon cancer and benign colon tissue images
with 98 % recall and 98 % precision. However, our model failed to reliably
identify KRAS mutations, with the highest realized accuracy of 66 %. Although
not yet perfected, in the near future Apple CreateML modules can be used in
diagnostic smartphone-based applications and potentially alleviate shortages of
medical professionals in understaffed parts of the world.

Models of coupled oscillators are useful in describing a wide variety of
phenomena in physics, biology and economics. These models typically rest on the
premise that the oscillators are weakly coupled, meaning that amplitudes can be
assumed to be constant and dynamics can therefore be described purely in terms
of phase differences. Whilst mathematically convenient, the restrictive nature
of the weak coupling assumption can limit the explanatory power of these
phase-coupled oscillator models. We therefore propose an extension to the
weakly-coupled oscillator model that incorporates both amplitude and phase as
dependent variables. We use the bilinear neuronal state equations of dynamic
causal modelling as a foundation in deriving coupled differential equations
that describe the activity of both weakly and strongly coupled oscillators. We
show that weakly-coupled oscillator models are inadequate in describing the
processes underlying the temporally variable signals observed in a variety of
systems. We demonstrate that phase-coupled models perform well on simulations
of weakly coupled systems but fail when connectivity is no longer weak. On the
other hand, using Bayesian model selection, we show that our phase-amplitude
coupling model can describe non-weakly coupled systems more effectively despite
the added complexity associated with using amplitude as an extra dependent
variable. We demonstrate the advantage of our phase-amplitude model in the
context of model-generated data, as well as of a simulation of inter-connected
pendula, neural local field potential recordings in rodents under anaesthesia
and international economic gross domestic product data.

Traumatic brain injury (TBI) is a complex injury that is hard to predict and
diagnose, with many studies focused on associating head kinematics to brain
injury risk. Recently, there has been a push towards using computationally
expensive finite element (FE) models of the brain to create tissue deformation
metrics of brain injury. Here, we developed a 3 degree-of-freedom
lumped-parameter brain model, built based on the measured natural frequencies
of a FE brain model simulated with live human impact data, to be used to
rapidly estimate peak brain strains experienced during head rotational
accelerations. On our dataset, the simplified model correlates with peak
principal FE strain by an R2 of 0.80. Further, coronal and axial model
displacement correlated with fiber-oriented peak strain in the corpus callosum
with an R2 of 0.77. Using the maximum displacement predicted by our brain
model, we propose an injury criteria and compare it against a number of
existing rotational and translational kinematic injury metrics on a dataset of
head kinematics from 27 clinically diagnosed injuries and 887 non-injuries. We
found that our proposed metric performed comparably to peak angular
acceleration, linear acceleration, and angular velocity in classifying injury
and non-injury events. Metrics which separated time traces into their
directional components had improved deviance to those which combined components
into a single time trace magnitude. Our brain model can be used in future work
as a computationally efficient alternative to FE models for classifying
injuries over a wide range of loading conditions.

Microchip electrokinetic methods are capable of increasing the sensitivity of
molecular assays by enriching and purifying target analytes. However, their use
is currently limited to assays that can be performed under a high external
electric field, as spatial separation and focusing is lost when the electric
field is removed. We present a novel method that uses two-phase encapsulation
to overcome this limitation. The method uses passive filling and pinning of an
oil phase in hydrophobic channels to encapsulate electrokinetically separated
and focused analytes with a brief pressure pulse. The resulting encapsulated
sample droplet maintains its concentration over long periods of time without
requiring an electric field and can be manipulated for further analysis, either
on- or off- chip. We demonstrate the method by encapsulating DNA
oligonucleotides in a 240 pL aqueous segment after isotachophoresis (ITP)
focusing, and show that the concentration remains at 60% of the initial value
for tens of minutes, a 22-fold increase over free diffusion after 20 minutes.
Furthermore, we demonstrate manipulation of a single droplet by selectively
encapsulating amplicon after ITP purification from a polymerase chain reaction
(PCR) mix, and performing parallel off-chip detection reactions using the
droplet. We provide geometrical design guidelines for devices implementing the
encapsulation method, and show how the method can be scaled to multiple analyte
zones.

Across diverse biological systems -- ranging from neural networks to
intracellular signaling and genetic regulatory networks -- the information
about changes in the environment is frequently encoded in the full temporal
dynamics of the network nodes. A pressing data-analysis challenge has thus been
to efficiently estimate the amount of information that these dynamics convey
from experimental data. Here we develop and evaluate decoding-based estimation
methods to lower bound the mutual information about a finite set of inputs,
encoded in single-cell high-dimensional time series data. For biological
reaction networks governed by the chemical Master equation, we derive
model-based information approximations and analytical upper bounds, against
which we benchmark our proposed model-free decoding estimators. In contrast to
the frequently-used k-nearest-neighbor estimator, decoding-based estimators
robustly extract a large fraction of the available information from
high-dimensional trajectories with a realistic number of data samples. We apply
these estimators to previously published data on Erk and Ca signaling in
mammalian cells and to yeast stress-response, and find that substantial amount
of information about environmental state can be encoded by non-trivial response
statistics even in stationary signals. We argue that these single-cell,
decoding-based information estimates, rather than the commonly-used tests for
significant differences between selected population response statistics,
provide a proper and unbiased measure for the performance of biological
signaling networks.

Determining the best model or models for a particular data set, a process
known as Bayesian model comparison, is a critical part of probabilistic
inference. Typically, this process assumes a fixed model-space (that is, a
fixed set of candidate models). However, it is also possible to perform
Bayesian inference over model-spaces themselves, thus determining which spaces
provide the best explanation for observed data. Model-space inference (MSI)
allows the effective exclusion of poorly performing models (a process analogous
to Automatic Relevance Detection), and thus mitigates against the well-known
phenomenon of model dilution, resulting in posterior probability estimates that
are, on average, more accurate than those produced when using a fixed
model-space. We focus on model comparison in the context of multiple
independent data sets (as produced, for example, by multi-subject behavioural
or neuroimaging studies), and cast our proposal as a development of
random-effects Bayesian Model Selection, the current state-of-the-art in the
field. We demonstrate the increased accuracy of MSI using simulated behavioural
and neuroimaging data, as well as by assessing predictive performance in
previously-acquired empirical data. Additionally, we explore other applications
of MSI, including formal testing for a diversity of models within a population,
and comparison of model-spaces between populations. Our approach thus provides
an important new tool for model comparison.

Microtubules are inherently dynamic sub-cellular filamentuous polymers that
are spatially organized within the cell by motor proteins which cross-link and
move microtubules. In-vitro microtubule motility assays, in which motors
attached to a surface move microtubules along it, have been used traditionally
to study motor function. However, the way in which microtubule-microtubule
interactions affect microtubule movement remains largely unexplored. To address
this question, time-lapse image series of in-vitro microtubule motility assays
were obtained using total internal reflection fluorescence (TIRF) microscopy.
Categorized as a general problem of multiple object tracking (MOT), particular
challenges arising in this project include low feature diversity, dynamic
instability, sudden changes in microtubules motility patterns, as well as their
instantaneous appearance/disappearance. This work describes a new application
of piecewise-stationary multiple motion model Kalman smoother (PMMS) for
modeling individual microtubules motility trends. To both evaluate the
capability of this procedure and optimize its hyper-parameters, a large dataset
simulating the series of time-lapse images was used first. Next, we applied it
to the sequence of frames from the real data. Results of our analyses provide a
quantitative description of microtubule velocity which, in turn, enumerates the
occurrence of microtubule-microtubule interactions per frame.

In this review we make the statement that hybrid models in oncology are
required as a mean for enhanced data integration. In the context of systems
oncology, experimental and clinical data need to be at the heart of the models
developments from conception to validation to ensure a relevant use of the
models in the clinical context. The main applications pursued are to improve
diagnosis and to optimize therapies.We first present the Successes achieved
thanks to hybrid modelling approaches to advance knowledge, treatments or drug
discovery. Then we present the Challenges than need to be addressed to allow
for a better integration of the model parts and of the data into the models.
And Finally, the Hopes with a focus towards making personalised medicine a
reality. Mathematics Subject Classification. 35Q92, 68U20, 68T05, 92-08, 92B05.

Point 1: Shape characterizers are metrics that quantify aspects of the
overall geometry of a 3D digital surface. When computed for biological objects,
the values of a shape characterizer are largely independent of homology
interpretations and often contain a strong ecological and functional signal.
Thus shape characterizers are useful for understanding evolutionary processes.
Dirichlet Normal Energy (DNE) is a widely used shape characterizer in
morphological studies.
  Point 2: Recent studies found that DNE is sensitive to various procedures for
preparing 3D mesh from raw scan data, raising concerns regarding comparability
and objectivity when utilizing DNE in morphological research. We provide a
robustly implemented algorithm for computing the Dirichlet energy of the normal
(ariaDNE) on 3D meshes.
  Point 3: We show through simulation that the effects of preparation-related
mesh surface attributes such as triangle count, mesh representation, noise,
smoothing and boundary triangles are much more limited on ariaDNE than DNE.
Furthermore, ariaDNE retains the potential of DNE for biological studies,
illustrated by its effectiveness in differentiating species by dietary
preferences.
  Point 4: Use of ariaDNE can dramatically enhance assessment of ecological
aspects of morphological variation by its stability under different 3D model
acquisition methods and preparation procedure. Towards this goal, we provide
scripts for computing ariaDNE and ariaDNE values for specimens used in
previously published DNE analyses.

Fetal heart rate variability (fHRV) is an important indicator of health and
disease, yet its physiological origins, neural contributions in particular, are
not well understood. We aimed to develop novel experimental and data analytical
approaches to identify fHRV measures reflecting the vagus nerve contributions
to fHRV. In near-term ovine fetuses, a comprehensive set of 46 fHRV measures
was computed from fetal pre-cordial electrocardiogram recorded during surgery
and 72 hours later without (n=24) and with intra-surgical bilateral cervical
vagotomy (n=15). The fetal heart rate did not change due to vagotomy. We
identify fHRV measures specific to the vagal modulation of fHRV: Multiscale
time irreversibility asymmetry index (AsymI), Detrended fluctuation analysis
(DFA) alpha1, Kullback-Leibler permutation entropy (KLPE) and Scale dependent
Lyapunov exponent slope (SDLE alpha). We provide a systematic delineation of
vagal contributions to fHRV across signal-analytical domains which should be
relevant for the emerging field of bioelectronic medicine and the deciphering
of the vagus code. Our findings also have clinical significance for in utero
monitoring of fetal health during surgery.

Calcium (Ca2+) signalling is one of the most important mechanisms of
information propagation in the body. In embryogenesis the interplay between
Ca2+ signalling and mechanical forces is critical to the healthy development of
an embryo but poorly understood. Several types of embryonic cells exhibit
calcium-induced contractions and many experiments indicate that Ca2+ signals
and contractions are coupled via a two-way mechanochemical coupling. We present
a new analysis of experimental data that supports the existence of this
coupling during Apical Constriction in Neural Tube Closure. We then propose a
mechanochemical model, building on early models that couple Ca2+ dynamics to
cell mechanics and replace the bistable Ca2+ release with modern,
experimentally validated Ca2+ dynamics. We assume that the cell is a linear
viscoelastic material and model the Ca2+-induced contraction stress with a Hill
function saturating at high Ca2+ levels. We also express, for the first time,
the "stretch-activation" Ca2+ flux in the early mechanochemical models as a
bottom-up contribution from stretch-sensitive Ca2+ channels on the cell
membrane. We reduce the model to three ordinary differential equations and
analyse its bifurcation structure semi-analytically as the $IP_3$
concentration, and the "strength" of stretch activation, $\lambda$ vary. The
Ca2+ system ($\lambda=0$, no mechanics) exhibits relaxation oscillations for a
certain range of $IP_3$ values. As $\lambda$ is increased the range of $IP_3$
values decreases, the oscillation amplitude decreases and the frequency
increases. Oscillations vanish for a sufficiently high value of $\lambda$.
These results agree with experiments in embryonic cells that also link the loss
of Ca2+ oscillations to embryo abnormalities. The work addresses a very
important and understudied question on the coupling of chemical and mechanical
signalling in embryogenesis.

The fluorescence spectra of bacterial samples stained with SYTO 9 and
propidium iodide (PI) were used to monitor bacterial viability. Stained
mixtures of live and dead Escherichia coli with proportions of live:dead cells
varying from 0 to 100% were measured using the optrode, a cost effective and
convenient fibre-based spectroscopic device. We demonstrated several approaches
to obtaining the proportions of live:dead E. coli in a mixture of both live and
dead, from analyses of the fluorescence spectra collected by the optrode. To
find a suitable technique for predicting the percentage of live bacteria in a
sample, four analysis methods were assessed and compared: SYTO 9:PI
fluorescence intensity ratio, an adjusted fluorescence intensity ratio,
single-spectrum support vector regression (SVR) and multi-spectra SVR. Of the
four analysis methods, multi-spectra SVR obtained the most reliable results and
was able to predict the percentage of live bacteria in 10^8 bacteria/mL samples
between c. 7% and 100% live, and in 10^7 bacteria/mL samples between c. 7% and
73% live. By demonstrating the use of multi-spectra SVR and the optrode to
monitor E. coli viability, we raise points of consideration for spectroscopic
analysis of SYTO 9 and PI and aim to lay the foundation for future work that
use similar methods for different bacterial species.

BACKGROUND Indicators of relative inequality of lifespans are important
because they capture the dimensionless shape of aging. They are markers of
inequality at the population level and express the uncertainty at the time of
death at the individual level. In particular, Keyfitz' entropy $\bar{H}$
represents the elasticity of life expectancy to a change in mortality and it
has been used as an indicator of lifespan variation. However, it is unknown how
this measure changes over time and whether a threshold age exists, as it does
for other lifespan variation indicators.
  RESULTS The time derivative of $\bar{H}$ can be decomposed into changes in
life disparity $e^\dagger$ and life expectancy at birth $e_o$. Likewise,
changes over time in $\bar{H}$ are a weighted average of age-specific rates of
mortality improvements. These weights reflect the sensitivity of $\bar{H}$ and
show how mortality improvements can increase (or decrease) the relative
inequality of lifespans. Further, we prove that $\bar{H}$, as well as
$e^\dagger$, in the case that mortality is reduced in every age, has a
threshold age below which saving lives reduces entropy, whereas improvements
above that age increase entropy.
  CONTRIBUTION We give a formal expression for changes over time of $\bar{H}$
and provide a formal proof of the threshold age that separates reductions and
increases in lifespan inequality from age-specific mortality improvements.

Diseases involve complex processes and modifications to the cellular
machinery. The gene expression profile of the affected cells contains
characteristic patterns linked to a disease. Hence, biological knowledge
pertaining to a disease can be derived from a patient cell's profile, improving
our diagnosis ability, as well as our grasp of disease risks. This knowledge
can be used for drug re-purposing, or by physicians to evaluate a patient's
condition and co-morbidity risk. Here, we look at differential gene expression
obtained from microarray technology for patients diagnosed with various
diseases. Based on this data and cellular multi-scale organization, we aim to
uncover disease--disease links, as well as disease-gene and disease--pathways
associations. We propose neural networks with structures inspired by the
multi-scale organization of a cell. We show that these models are able to
correctly predict the diagnosis for the majority of the patients. Through the
analysis of the trained models, we predict and validate disease-disease,
disease-pathway, and disease-gene associations with comparisons to known
interactions and literature search, proposing putative explanations for the
novel predictions that come from our study.

Models of misfolded proteins (MP) aim at discovering the bio-mechanical
propagation properties of neurological diseases (ND) by identifying plausible
associated dynamical systems. Solving these systems along the full disease
trajectory is usually challenging, due to the lack of a well defined time axis
for the pathology. This issue is addressed by disease progression models (DPM)
where long-term progression trajectories are estimated via time
reparametrization of individual observations. However, due to their loose
assumptions on the dynamics, DPM do not provide insights on the bio-mechanical
properties of MP propagation. Here we propose a unified model of
spatio-temporal protein dynamics based on the joint estimation of long-term MP
dynamics and time reparameterization of individuals observations. The model is
expressed within a Gaussian Process (GP) regression setting, where constraints
on the MP dynamics are imposed through non--linear dynamical systems. We use
stochastic variational inference on both GP and dynamical system parameters for
scalable inference and uncertainty quantification of the trajectories.
Experiments on simulated data show that our model accurately recovers
prescribed rates along graph dynamics and precisely reconstructs the underlying
progression. When applied to brain imaging data our model allows the
bio-mechanical interpretation of amyloid deposition in Alzheimer's disease,
leading to plausible simulations of MP propagation, and achieving accurate
predictions of individual MP deposition in unseen data.

This postdoctoral thesis starts by reviewing the historic development of
airplane structures and high lift devices from an engineering point of view.
However, the main purpose of this document is the development of a novel
concept for shape changing, gapless high lift devices that is inspired by the
nastic movement of plants. A particular focus is put on the efficient
simulation and optimization of compliant pressure actuated cellular structures.

Recent advances in electron microscopy have enabled the imaging of single
cells in 3D at nanometer length scale resolutions. An uncharted frontier for in
silico biology is the ability to simulate cellular processes using these
observed geometries. Enabling such simulations requires watertight meshing of
electron micrograph images into 3D volume meshes, which can then form the basis
of computer simulations of such processes using numerical techniques such as
the Finite Element Method. In this paper, we describe the use of our recently
rewritten mesh processing software, GAMer 2, to bridge the gap between poorly
conditioned meshes generated from segmented micrographs and boundary marked
tetrahedral meshes which are compatible with simulation. We demonstrate the
application of a workflow using GAMer 2 to a series of electron micrographs of
neuronal dendrite morphology explored at three different length scales and show
that the resulting meshes are suitable for finite element simulations. This
work is an important step towards making physical simulations of biological
processes in realistic geometries routine. Innovations in algorithms to
reconstruct and simulate cellular length scale phenomena based on emerging
structural data will enable realistic physical models and advance discovery at
the interface of geometry and cellular processes. We posit that a new frontier
at the intersection of computational technologies and single cell biology is
now open.

Background. Most surgical procedures involve structures deeper than the skin.
However, the difference in surgical noxious stimulation between skin incision
and laparoscopic trocar insertion is unknown. By analyzing instantaneous heart
rate (IHR) calculated from the electrocardiogram, in particular the transient
bradycardia in response to surgical stimuli, this study investigates surgical
noxious stimuli arising from skin incision and laparoscopic trocar insertion.
Methods. Thirty-five patients undergoing laparoscopic cholecystectomy were
enrolled in this prospective observational study. Sequential surgical steps
including umbilical skin incision (11 mm), umbilical trocar insertion (11 mm),
xiphoid skin incision (5 mm), xiphoid trocar insertion (5 mm), subcostal skin
incision (3 mm), and subcostal trocar insertion (3 mm) were investigated. IHR
was derived from electrocardiography and calculated by the modern time-varying
power spectrum. Similar to the classical heart rate variability analysis, the
time-varying low frequency power (tvLF), time-varying high frequency power
(tvHF), and tvLF-to-tvHF ratio (tvLHR) were calculated. Prediction probability
(PK) analysis and global pointwise F-test were used to compare the performance
between indices and the heart rate readings from the patient monitor. Results.
Analysis of IHR showed that surgical stimulus elicits a transient bradycardia,
followed by the increase of heart rate. Transient bradycardia is more
significant in trocar insertion than skin incision. The IHR change quantifies
differential responses to different surgical intensity. Serial PK analysis
demonstrates de-sensitization in skin incision, but not in laparoscopic trocar
insertion. Conclusions. Quantitative indices present the transient bradycardia
introduced by noxious stimulation. The results indicate different effects
between skin incision and trocar insertion.

Neuroanatomical segmentation in magnetic resonance imaging (MRI) of the brain
is a prerequisite for volume, thickness and shape measurements. This work
introduces a new highly accurate and versatile method based on 3D convolutional
neural networks for the automatic segmentation of neuroanatomy in T1-weighted
MRI. In combination with a deep 3D fully convolutional architecture, efficient
linear registration-derived spatial priors are used to incorporate additional
spatial context into the network. An aggressive data augmentation scheme using
random elastic deformations is also used to regularize the networks, allowing
for excellent performance even in cases where only limited labelled training
data are available. Applied to hippocampus segmentation in an elderly
population (mean Dice coefficient = 92.1%) and sub-cortical segmentation in a
healthy adult population (mean Dice coefficient = 89.5%), we demonstrate new
state-of-the-art accuracies and a high robustness to outliers with the same
architecture. Further validation on a multi-structure segmentation task in a
scan-rescan dataset demonstrates accuracy (mean Dice coefficient = 86.6%)
similar to the scan-rescan reliability of expert manual segmentations (mean
Dice coefficient = 86.9%), and improved reliability compared to both expert
manual segmentations and automated segmentations using FIRST. Furthermore, our
method maintains a highly competitive runtime performance (e.g. requiring only
10 seconds for left/right hippocampal segmentation in 1x1x1 MNI stereotaxic
space), orders of magnitude faster than conventional multi-atlas segmentation
methods.

Darwin is a genomics co-processor that achieved a 15000x acceleration on long
read assembly through innovative hardware and algorithm co-design. Darwins
algorithms and hardware implementation were specifically designed for DNA
analysis pipelines. This paper analyzes the feasibility of applying Darwins
algorithms to the problem of protein sequence alignment. In addition to a
behavioral analysis of Darwin when aligning proteins, we propose an algorithmic
improvement to Darwins alignment algorithm, GACT, in the form of a multi-pass
variant that increases its accuracy on protein sequence alignment. Concretely,
our proposed multi-pass variant of GACT achieves on average 14\% better
alignment scores.

Key processes in biological and chemical systems are described by networks of
chemical reactions. From molecular biology to biotechnology applications,
computational models of reaction networks are used extensively to elucidate
their non-linear dynamics. Model dynamics are crucially dependent on parameter
values which are often estimated from observations. Over past decade, the
interest in parameter and state estimation in models of (bio-)chemical reaction
networks (BRNs) grew considerably. Statistical inference problems are also
encountered in many other tasks including model calibration, discrimination,
identifiability and checking as well as optimum experiment design, sensitivity
analysis, bifurcation analysis and other. The aim of this review paper is to
explore developments of past decade to understand what BRN models are commonly
used in literature, and for what inference tasks and inference methods. Initial
collection of about 700 publications excluding books in computational biology
and chemistry were screened to select over 260 research papers and 20 graduate
theses concerning estimation problems in BRNs. The paper selection was
performed as text mining using scripts to automate search for relevant keywords
and terms. The outcome are tables revealing the level of interest in different
inference tasks and methods for given models in literature as well as recent
trends. In addition, a brief survey of general estimation strategies is
provided to facilitate understanding of estimation methods which are used for
BRNs. Our findings indicate that many combinations of models, tasks and methods
are still relatively sparse representing new research opportunities to explore
those that have not been considered - perhaps for a good reason. The paper
concludes by discussing future research directions including research problems
which cannot be directly deduced from presented tables.

Heart rate variability studies depend on the robust calculation of the
tachogram, the heart rate times series, usually by the detection of R peaks in
the electrocardiogram (ECG). ECGs however are subject to a number of sources of
noise which are difficult to filter and therefore reduce the tachogram
accuracy. We describe a pipeline for fast calculation of tachograms from noisy
ECGs of several hours' length. The pipeline consists of three stages. A neural
network (NN) trained to detect R peaks and distinguish these from noise; a
measure to robustly detect false positives (FPs) and negatives (FNs) produced
by the NN; a simple "alarm" algorithm for automatically removing FPs and
interpolating FNs. In addition, we introduce the approach of encoding ECGs,
tachograms and other cardiac time series in the form of raster images, which
greatly speeds and eases their visual inspection and analysis.

Lead is a naturally-occurring element. It has been known to man for a long
time, and it is one of the longest established poisons. The current consensus
is that no level of lead exposure should be deemed "safe." New evidence
regarding the blood levels at which morbidities occur has prompted the CDC to
reduce the screening guideline of 10 $\mu$g/dl to 2 $\mu$g/dl. Measurable
cognitive decline (reduced IQ, academic deficits) have been found to occur at
levels below 10mg/dl.
  Knowledge of lead pharmacology allows us to better understand its absorption
and metabolization, mechanisms that produce its medical consequences. Based
upon an original and very simplified compartmental model of Rabinowitz (1973)
with only three major compartments (blood, bone and soft tissue), extensive
biophysical models sprouted over the following two decades. However, none of
these models have been specifically designed to use new knowledge of lead
molecular dynamics to understand its deleterious effects on the brain. We build
and analyze a compartmental model of lead pharmacokinetics, focused
specifically on addressing neurotoxicity. We use traditional phase space
methods, parameter sensitivity analysis and bifurcation theory to study the
transitions in the system's behavior in response to various physiological
parameters.
  We conclude that modeling the complex interaction of lead and calcium along
their dynamic trajectory may successfully explain counter-intuitive effects on
systemic function and neural behavior which could not be addressed by existing
linear models. Our results encourage further efforts towards using nonlinear
phenomenology in conjunction with empirically driven system parameters, to
obtain a biophysical model able to provide clinical assessments and
predictions.

A perturbed gut microbiome has recently been linked with multiple disease
processes, yet researchers currently lack tools that can provide in vivo,
quantitative, and real-time insight into these processes and associated
host-microbe interactions. We propose an in vivo wireless implant for
monitoring gastrointestinal tract redox states using oxidation-reduction
potentials (ORP). The implant is powered and conveniently interrogated via
ultrasonic waves. We engineer the sensor electronics, electrodes, and
encapsulation materials for robustness in vivo, and integrate them into an
implant that endures autoclave sterilization and measures ORP for 12 days
implanted in the cecum of a live rat. The presented implant platform paves the
way for long-term experimental testing of biological hypotheses, offering new
opportunities for understanding gut redox pathophysiology mechanisms, and
facilitating translation to disease diagnosis and treatment applications.

Timing features such as the silence gaps between vocal units -- inter-call
intervals (ICIs) -- often correlate with biological information such as context
or genetic information. Such correlates between the ICIs and biological
information have been reported for a diversity of animals. Yet, few
quantitative approaches for investigating timing exist to date. Here, we
propose a novel approach for quantitatively comparing timing in animal
vocalisations in terms of the typical ICIs. As features, we use the
distribution of silence gaps parametrised with a kernel density estimate (KDE)
and compare the distributions with the symmetric Kullback-Leibler divergence
(sKL-divergence). We use this technique to compare timing in vocalisations of
two frog species, a group of zebra finches and calls from parrots of the same
species. As a main finding, we demonstrate that in our dataset, closely related
species have more similar distributions than species genetically more distant,
with sKL-divergences across-species larger than within-species distances.
Compared with more standard methods such as Fourier analysis, the proposed
method is more robust to different durations present in the data samples,
flexibly applicable to different species and easy to interpret. Investigating
timing in animal vocalisations may thus contribute to taxonomy, support
conservation efforts by helping monitoring animals in the wild and may shed
light onto the origins of timing structures in animal vocal communication.

Single-particle trajectories measured in microscopy experiments contain
important information about dynamic processes undergoing in a range of
materials including living cells and tissues. However, extracting that
information is not a trivial task due to the stochastic nature of particles'
movement and the sampling noise. In this paper, we adopt a deep-learning method
known as a convolutional neural network (CNN) to classify modes of diffusion
from given trajectories. We compare this fully automated approach working with
raw data to classical machine learning techniques that require data
preprocessing and extraction of human-engineered features from the trajectories
to feed classifiers like random forest or gradient boosting. All methods are
tested using simulated trajectories for which the underlying physical model is
known. From the results it follows that CNN is usually slightly better than the
feature-based methods, but at the costs of much longer processing times.
Moreover, there are still some borderline cases, in which the classical methods
perform better than CNN.

Recent experimental evidence suggests that interactions in flocks of birds do
not involve a characteristic length scale. Bird flocks have also been revealed
to have an inhomogeneous density distribution, with the density of birds near
the border greater than near the centre. We introduce a strictly metric-free
model for collective behaviour that incorporates a distributed motional bias,
providing control of the density distribution. A simple version of this model
is then able to provide a good fit to published data for the density variation
across flocks of Starlings. We find that it is necessary for individuals on the
edge of the flock to have an inward motional bias but that birds in the
interior of the flock instead must have an outward bias. We discuss the ability
of individuals to determine their depth within a flock and show how this might
be achieved by relatively simple analysis of their visual environment.

The Categorical Compositional Distributional (DisCoCat) Model is a powerful
mathematical model for composing the meaning of sentences in natural languages.
Since we can think of biological sequences as the "language of life", it is
attempting to apply the DisCoCat model on the language of life to see if we can
obtain new insights and a better understanding of the latter. In this work, we
took an initial step towards that direction. In particular, we choose to focus
on proteins as the linguistic features of protein are the most prominent as
compared with other macromolecules such as DNA or RNA. Concretely, we treat
each protein as a sentence and its constituent domains as words. The meaning of
a word or the sentence is just its biological function, and the arrangement of
domains in a protein corresponds to the syntax. Putting all those into the
DisCoCat framework, we can "compute" the function of a protein based on the
functions of its domains with the grammar rules that combine them together.
Since the functions of both the protein and its domains are represented in
vector spaces, we provide a novel way to formalize the functional
representation of proteins.

Better understanding of feeding behaviour will be vital in reducing obesity
and metabolic syndrome, but we lack a standard model that captures the
complexity of feeding behaviour. We construct an accurate stochastic model of
rodent feeding at the bout level in order to perform quantitative behavioural
analysis. Analysing the different effects on feeding behaviour of PYY 3-36,
lithium chloride, GLP-1 and leptin shows the precise behavioural changes caused
by each anorectic agent, and demonstrates that these changes do not mimic
satiety. In the ad libitum fed state during the light period, meal initiation
is governed by complete stomach emptying, whereas in all other conditions there
is a graduated response. We show how robust homeostatic control of feeding
thwarts attempts to reduce food intake, and how this might be overcome. In
silico experiments suggest that introducing a minimum intermeal interval or
modulating gastric emptying can be as effective as anorectic drug
administration.

Objective: We hypothesized that prenatal stress (PS) exerts lasting impact on
fetal heart rate (fHR). We sought to validate the presence of such PS signature
in fHR by measuring coupling between maternal HR (mHR) and fHR. Study design:
Prospective observational cohort study in stressed group (SG) mothers with
controls matched for gestational age during screening at third trimester using
Cohen Perceived Stress Scale (PSS) questionnaire with PSS-10 equal or above 19
classified as SG. Women with PSS-10 less than 19 served as control group (CG).
Setting: Klinikum rechts der Isar of the Technical University of Munich.
Population: Singleton 3rd trimester pregnant women. Methods: Transabdominal
fetal electrocardiograms (fECG) were recorded. We deployed a signal processing
algorithm termed bivariate phase-rectified signal averaging (BPRSA) to quantify
coupling between mHR and fHR resulting in a fetal stress index (FSI). Maternal
hair cortisol was measured at birth. Differences were assumed to be significant
for p value less than 0.05. Main Outcome Measures: Differences for FSI between
both groups. Results: We screened 1500 women enrolling 538 of which 16.5 %
showed a PSS-10 score equal or above 19 at 34+0 weeks. Fifty five women
eventually comprised the SG and n=55 served as CG. Median PSS was 22.0 (IQR
21.0-24.0) in the SG and 9.0 (6.0-12.0) in the CG, respectively. Maternal hair
cortisol was higher in SG than CG at 86.6 (48.0-169.2) versus 53.0 (34.4-105.9)
pg/mg. At 36+5 weeks, FSI was significantly higher in fetuses of stressed
mothers when compared to controls [0.43 (0.18-0.85) versus 0.00 (-0.49-0.18)].
Conclusion: Our findings show a persistent effect of PS affecting fetuses in
the last trimester.

This tutorial provides a worked example of using Dynamic Causal Modelling
(DCM) and Parametric Empirical Bayes (PEB) to characterise inter-subject
variability in neural circuitry (effective connectivity). This involves
specifying a hierarchical model with two or more levels. At the first level,
state space models (DCMs) are used to infer the effective connectivity that
best explains a subject's neuroimaging timeseries (e.g. fMRI, MEG, EEG).
Subject-specific connectivity parameters are then taken to the group level,
where they are modelled using a General Linear Model (GLM) that partitions
between-subject variability into designed effects and additive random effects.
The ensuing (Bayesian) hierarchical model conveys both the estimated connection
strengths and their uncertainty (i.e., posterior covariance) from the subject
to the group level; enabling hypotheses to be tested about the commonalities
and differences across subjects. This approach can also finesse parameter
estimation at the subject level, by using the group-level parameters as
empirical priors. We walk through this approach in detail, using data from a
published fMRI experiment that characterised individual differences in
hemispheric lateralization in a semantic processing task. The preliminary
subject specific DCM analysis is covered in detail in a companion paper. This
tutorial is accompanied by the example dataset and step-by-step instructions to
reproduce the analyses.

Here we present a novel approach to protein design and phenotypic inference
using a generative model for protein sequences. BioSeqVAE, a variational
autoencoder variant, can hallucinate syntactically valid protein sequences that
are likely to fold and function. BioSeqVAE is trained on the entire known
protein sequence space and learns to generate valid examples of protein
sequences in an unsupervised manner. The model is validated by showing that its
latent feature space is useful and that it accurately reconstructs sequences.
Its usefulness is demonstrated with a selection of relevant downstream design
tasks. This work is intended to serve as a computational first step towards a
general purpose structure free protein design tool.

A. Bornh\"oft, R. Hanke-Rauschenbach, and K. Sundmacher, [Nonlinear Dyn., 73
(2013), pp. 535-549] introduced a qualitative simplification to the ADM1 model
for anaerobic digestion. We obtain global results for this model by first
analyzing the limiting system, a model of single species growth in the
chemostat in which the response function is non-monotone and the species decay
rate is included. Using a Lyapunov function argument and the theory of
asymptotically autonomous systems, we prove that even in the parameter regime
where there is bistability, no periodic orbits exist and every solution
converges to one of the equilibrium points. We then describe two algorithms for
stochastically perturbing the parameters of the model. Simulations done with
these two algorithms are compared with simulations done using the Gillespie and
tau-leaping algorithms. They illustrate the severe impact environmental factors
may have on anaerobic digestion in the transient phase.

Colour patterning contributes to important plant traits that influence
ecological interactions, horticultural breeding, and agricultural performance.
High-throughput phenotyping of colour is valuable for understanding plant
biology and selecting for traits related to colour during plant breeding. Here
we present ColourQuant, an automated high-throughput pipeline that allows users
to extract colour phenotypes from images. This pipeline includes methods for
colour phenotyping using mean pixel values, Gaussian density estimator of Lab
colour, and the analysis of shape-independent colour patterning by circular
deformation.

A variety of microparticles have been proposed for the sustained and
localized delivery of drugs whit the objective of increasing therapeutic
indexes by circumventing filtering organs and biological barriers. Yet, the
geometrical, mechanical and therapeutic properties of such microparticles
cannot be simultaneously and independently tailored during the fabrication
process in order to optimize their performance. In this work, a top-down
approach is employed to realize micron-sized polymeric particles, called
microPlates (uPLs), for the sustained release of therapeutic agents. uPLs are
square hydrogel particles, with an edge length of 20 um and a height of 5 um,
made out of poly (lactic co glycolic acid) (PLGA). During the synthesis
process, the uPL Young's modulus can be varied from 0.6 to 5 MPa by changing
PLGA amounts from 1 to 7.5 mg, without affecting the uPL geometry. Within the
porous uPL matrix, different classes of therapeutic payloads can be
incorporated including molecular agents, such as the anti-inflammatory
dexamethasone (DEX), and nanoparticles, containing themselves imaging and
therapeutic molecules. As a proof of principle, uPLs are loaded with free DEX
and 200 nm spherical polymeric nanoparticles, carrying DEX molecules
(DEX-SPNs). Electron and fluorescent confocal microscopy analyses document the
uniform distribution and stability of molecular and nano agents within the uPL
matrix. This multiscale, hierarchical microparticle releases DEX for at least
10 days. The inclusion of DEX-SPNs serves to minimize the initial burst release
and modulate the diffusion of DEX molecules out of the uPL matrix. The
pharmacological and therapeutic properties together with the fine tuning of
geometry and mechanical stiffness make uPLs a unique polymeric depot for the
potential treatment of cancer, cardiovascular and chronic, inflammatory
diseases.

The relation between ecological conditions and geomorphological factors is
considered the basis for species distribution in Romania. In this context, the
location of each species within parts of the mountain slopes is difficult on a
medium to brad scale level. The paper presents methodology to combine
vegetation data, obtained from IKONOS satellite images, and Digital Elevation
Model obtained from digitized topographic maps. The study area is a northern
slope of the Stanisoarei Mountains with a gradient of species from beech mixed
and coniferous stands.

Branching in vascular networks and in overall organismic form is one of the
most common and ancient features of multicellular plants, fungi, and animals.
By combining machine-learning techniques with new theory that relates vascular
form to metabolic function, we enable novel classification of diverse branching
networks--mouse lung, human head and torso, angiosperm and gymnosperm plants.
We find that ratios of limb radii--which dictate essential biologic functions
related to resource transport and supply--are best at distinguishing branching
networks. We also show how variation in vascular and branching geometry
persists despite observing a convergent relationship across organisms for how
metabolic rate depends on body mass.

Stability landscapes are useful for understanding the properties of dynamical
systems. These landscapes can be calculated from the system's dynamical
equations using the physical concept of scalar potential. Unfortunately, for
most biological systems with two or more state variables such potentials do not
exist. Here we use an analogy with art to provide an accessible explanation of
why this happens. Additionally, we introduce a numerical method for decomposing
differential equations into two terms: the gradient term that has an associated
potential, and the non-gradient term that lacks it. In regions of the state
space where the magnitude of the non-gradient term is small compared to the
gradient part, we use the gradient term to approximate the potential as
quasi-potential. The non-gradient to gradient ratio can be used to estimate the
local error introduced by our approximation. Both the algorithm and a
ready-to-use implementation in the form of an R package are provided.

Summary: The AptaBlocks Web Interface is focused on providing graphical,
intuitive, and platform independent access to AptaBlocks, an experimentally
validated algorithmic approach for the in-silico design of oligonucleotide
sticky bridges. The availability of AptaBlocks online to the nucleic acid
research community at large makes this software a highly effective tool for
accelerating the design and development of novel oligonucleotide based drugs
and other biotechnologies.
  Availability: The AptaBlocks Web Interface is freely available at
www.ncbi.nlm.nih.gov/CBBresearch/Przytycka/index.cgi\#aptablocks

Over the last years, the SWATH data-independent acquisition protocol
(Sequential Window acquisition of All THeoretical mass spectra) has become a
cornerstone for the worldwide proteomics community. In this approach, a
high-resolution quadrupole-ToF mass spectrometer acquires thousands of MS/MS
data by selecting not just a single precursor at a time, but by allowing a
broad m/z range to be fragmented. This acquisition window is then sequentially
moved from the lowest to the highest mass selection range. This technique
enables the acquisition of thousands of high-resolution MS/MS spectra per
minute in a standard LC-MS run. In the subsequent data analysis phase, the
corresponding dataset is searched in a triple quadrupole-like mode, thus not
considering the whole MS/MS scan spectrum, but by searching for several
precursor to fragment transitions that identify and quantify the corresponding
peptide. This search is made possible with the use of an ion library,
previously acquired in a classical data dependent, full-spectrum mode. The
SWATH protocol, combining the protein identification power of high-resolution
MS/MS spectra with the robustness and accuracy in analyte quantification of
triple-quad targeted workflows, has become very popular in proteomics research.
The major drawback lies in the ion library itself, which is normally demanding
and time-consuming to build. Conversely, through the realignment of
chromatographic retention times, an ion library of a given proteome can
relatively easily be tailored upon any proteomics experiment done on the same
proteome. We are thus hereby sharing with the worldwide proteomics community
our newly acquired ion library of mouse adult hippocampal neural stem cells.
Given the growing effort in neuroscience research involving proteomics
experiments, we believe that this data might be of great help for the
neuroscience community.

Deep neural networks have led to state-of-the-art results in many medical
imaging tasks including Alzheimer's disease (AD) detection based on structural
magnetic resonance imaging (MRI) data. However, the network decisions are often
perceived as being highly non-transparent, making it difficult to apply these
algorithms in clinical routine. In this study, we propose using layer-wise
relevance propagation (LRP) to visualize convolutional neural network decisions
for AD based on MRI data. Similarly to other visualization methods, LRP
produces a heatmap in the input space indicating the importance/relevance of
each voxel contributing to the final classification outcome. In contrast to
susceptibility maps produced by guided backpropagation ("Which change in voxels
would change the outcome most?"), the LRP method is able to directly highlight
positive contributions to the network classification in the input space. In
particular, we show that (1) the LRP method is very specific for individuals
("Why does this person have AD?") with high inter-patient variability, (2)
there is very little relevance for AD in healthy controls and (3) areas that
exhibit a lot of relevance correlate well with what is known from literature.
To quantify the latter, we compute size-corrected metrics of the summed
relevance per brain area, e.g., relevance density or relevance gain. Although
these metrics produce very individual "fingerprints" of relevance patterns for
AD patients, a lot of importance is put on areas in the temporal lobe including
the hippocampus. After discussing several limitations such as sensitivity
toward the underlying model and computation parameters, we conclude that LRP
might have a high potential to assist clinicians in explaining neural network
decisions for diagnosing AD (and potentially other diseases) based on
structural MRI data.

This technical note presents a framework for investigating the underlying
mechanisms of neurovascular coupling in the human brain using multi-modal
magnetoencephalography (MEG) and functional magnetic resonance (fMRI)
neuroimaging data. This amounts to estimating the evidence for several
biologically informed models of neurovascular coupling using variational
Bayesian methods and selecting the most plausible explanation using Bayesian
model comparison. First, fMRI data is used to localise active neuronal sources.
The coordinates of neuronal sources are then used as priors in the
specification of a DCM for MEG, in order to estimate the underlying generators
of the electrophysiological responses. The ensuing estimates of neuronal
parameters are used to generate neuronal drive functions, which model the pre
or post synaptic responses to each experimental condition in the fMRI paradigm.
These functions form the input to a model of neurovascular coupling, the
parameters of which are estimated from the fMRI data. This establishes a
Bayesian fusion technique that characterises the BOLD response - asking, for
example, whether instantaneous or delayed pre or post synaptic signals mediate
haemodynamic responses. Bayesian model comparison is used to identify the most
plausible hypotheses about the causes of the multimodal data. We illustrate
this procedure by comparing a set of models of a single-subject auditory fMRI
and MEG dataset. Our exemplar analysis suggests that the origin of the BOLD
signal is mediated instantaneously by intrinsic neuronal dynamics and that
neurovascular coupling mechanisms are region-specific. The code and example
dataset associated with this technical note are available through the
statistical parametric mapping (SPM) software package.

A seagull ({\it Larus crassirostris}) has a high ability to realize its safe,
accurate and smooth landing. We examined how a seagull controls its angle of
attack when landing on a specific target. First, we recorded the landing
behavior of an actual seagull by multiple video cameras and quantified the
flight trajectory and the angle of attack as time series data. Second, we
introduced a mathematical model that describes how a seagull controls its speed
by changing its angle of attack. Based on the numerical simulation combining
the mathematical model and empirical data, we succeeded in qualitatively
explaining the landing behavior of an actual seagull, which demonstrates that
the control the angle of attack is important for landing behavior.

Risk assessment services fulfil the task of generating a risk report from
personal information and are developed for purposes like disease prognosis,
resource utilization prioritization, and informing clinical interventions. A
major component of a risk assessment service is a risk prediction model. For a
model to be easily integrated into risk assessment services, efforts are needed
to design a detailed development roadmap for the intended service at the time
of model development. However, methodology for such design is less described.
We thus reviewed existing literature and formulated a six-stage risk assessment
service development paradigm, from requirements analysis, service development,
model validation, pilot study, to iterative service deployment and assessment
and refinement. The study aims at providing a prototypic development roadmap
with checkpoints for the design of risk assessment services.

Tissue plasminogen activator (tPA) is the sole approved therapeutic molecule
for the treatment of acute ischemic stroke. Yet, only a small percentage of
patients could benefit from this life-saving treatment because of medical
contraindications and severe side effects, including brain hemorrhage,
associated with delayed administration. Here, a nano therapeutic agent is
realized by directly associating the clinical formulation of tPA to the porous
structure of soft discoidal polymeric nanoconstructs (tPA-DPNs). The porous
matrix of DPNs protects tPA from rapid degradation, allowing tPA-DPNs to
preserve over 70 % of the tPA original activity after 3 h of exposure to serum
proteins. Under dynamic conditions, tPA-DPNs dissolve clots more efficiently
than free tPA, as demonstrated in a microfluidic chip where clots are formed
mimicking in vivo conditions. At 60 min post treatment initiation, the clot
area reduces by half (57 + 8 %) with tPA-DPNs, whereas a similar result (56 +
21 %) is obtained only after 90 min for free tPA. In murine mesentery venules,
the intravenous administration of 2.5 mg/kg of tPA-DPNs resolves almost 90 % of
the blood clots, whereas a similar dose of free tPA successfully recanalize
only about 40 % of the treated vessels. At about 1/10 of the clinical dose (1.0
mg/kg), tPA-DPNs still effectively dissolve 70 % of the clots, whereas free tPA
works efficiently only on 16 % of the vessels. In vivo, discoidal tPA-DPNs
outperform the lytic activity of 200 nm spherical tPA-coated nanoconstructs in
terms of both percentage of successful recanalization events and clot area
reduction. The conjugation of tPA with preserved lytic activity, the
deformability and blood circulating time of DPNs together with the faster blood
clot dissolution would make tPA-DPNs a promising nanotool for enhancing both
potency and safety of thrombolytic therapies.

In systems biology modeling, important steps include model parameterization,
uncertainty quantification, and evaluation of agreement with experimental
observations. To help modelers perform these steps, we developed the software
PyBioNetFit. PyBioNetFit is designed for parameterization, and also supports
uncertainty quantification, checking models against known system properties,
and solving design problems. PyBioNetFit introduces the Biological Property
Specification Language (BPSL) for the formal declaration of system properties.
BPSL allows qualitative data to be used alone or in combination with
quantitative data for parameterization model checking, and design. PyBioNetFit
performs parameterization with parallelized metaheuristic optimization
algorithms (differential evolution, particle swarm optimization, scatter
search) that work directly with existing model definition standards: BioNetGen
Language (BNGL) and Systems Biology Markup Language (SBML). We demonstrate
PyBioNetFit's capabilities by solving 31 example problems, including the
challenging problem of parameterizing a model of cell cycle control in yeast.
We benchmark PyBioNetFit's parallelization efficiency on computer clusters,
using up to 288 cores. Finally, we demonstrate the model checking and design
applications of PyBioNetFit and BPSL by analyzing a model of therapeutic
interventions in autophagy signaling.

Artificial Intelligence is set to revolutionize multiple fields in the coming
years. One subset of AI, machine learning, shows immense potential for
application in a diverse set of medical specialties, including diagnostic
pathology. In this study, we investigate the utility of the Apple Create ML and
Google Cloud Auto ML, two machine learning platforms, in a variety of
pathological scenarios involving lung and colon pathology. First, we evaluate
the ability of the platforms to differentiate normal lung tissue from cancerous
lung tissue. Also, the ability to accurately distinguish two subtypes of lung
cancer (adenocarcinoma and squamous cell carcinoma) is examined and compared.
Similarly, the ability of the two programs to differentiate colon
adenocarcinoma from normal colon is assessed as is done with lung tissue. Also,
cases of colon adenocarcinoma are evaluated for the presence or absence of a
specific gene mutation known as KRAS. Finally, our last experiment examines the
ability of the Apple and Google platforms to differentiate between
adenocarcinomas of lung origin versus colon origin. In our trained models for
lung and colon cancer diagnosis, both Apple and Google machine learning
algorithms performed very well individually and with no statistically
significant differences found between the two platforms. However, some critical
factors set them apart. Apple Create ML can be used on local computers but is
limited to an Apple ecosystem. Google Auto ML is not platform specific but runs
only in Google Cloud with associated computational fees. In the end, both are
excellent machine learning tools that have great potential in the field of
diagnostic pathology, and which one to choose would depend on personal
preference, programming experience, and available storage space.

Technological advances in underwater video recording are opening novel
opportunities for monitoring wild fish. However, extracting data from videos is
often challenging. Nevertheless, it has been recently demonstrated that
accurate and precise estimates of density for animals (whose normal activities
are restricted to a bounded area or home range) can be obtained from counts
averaged across a relatively low number of video frames. The method, however,
requires that individual detectability (PID, the probability of detecting a
given animal provided that it is actually within the area surveyed by a camera)
has to be known. Here we propose a Bayesian implementation for estimating PID
after combining counts from cameras with counts from any reference method. The
proposed framework was demonstrated using Serranus scriba as a case-study, a
widely distributed and resident coastal fish. Density and PID were calculated
after combining fish counts from unbaited remote underwater video (RUV) and
underwater visual censuses (UVC) as reference method. The relevance of the
proposed framework is that after estimating PID, fish density can be estimated
accurately and precisely at the UVC scale (or at the scale of the preferred
reference method) using RUV only. This key statement has been extensively
demonstrated using computer simulations yielded by real empirical data.
Finally, we provide a simulation tool-kit for comparing the expected precision
attainable for different sampling effort and for species with different levels
of PID. Overall, the proposed method may contribute to substantially enlarge
the spatio-temporal scope of density monitoring programs for many resident
fish.

We propose a general framework for a collaborative machine learning system to
assist bioscience researchers with the task of labeling specific cell
identities from microscopic still or video imaging. The distinguishing features
of this approach versus prior approaches include: (1) use of a statistical
model of cell features that is iteratively improved, (2) generation of
probabilistic guesses at cell ID rather than single best-guesses for each cell,
(3) tracking of joint probabilities of features within and across cells, and
(4) ability to exploit multi-modal features, such as cell position, morphology,
reporter intensities, and activity. We provide an example implementation of
such a system applicable to labeling fluorescently tagged \textit{C. elegans}
neurons. As a proof of concept, we use a generative spring-mass model to
simulate sequences of cell imaging datasets with variable cell positions and
fluorescence intensities. Training on synthetic data, we find that atlases that
track inter-cell positional correlations give higher labeling accuracies than
those that treat cell positions independently. Tracking an additional feature
type, fluorescence intensity, boosts accuracy relative to a position-only
atlas, suggesting that multiple cell features could be leveraged to improve
automated label predictions.

Estimation of mutual information between (multidimensional) real-valued
variables is used in analysis of complex systems, biological systems, and
recently also quantum systems. This estimation is a hard problem, and
universally good estimators provably do not exist. Kraskov et al. (PRE, 2004)
introduced a successful mutual information estimation approach based on the
statistics of distances between neighboring data points, which empirically
works for a wide class of underlying probability distributions. Here we improve
this estimator by (i) expanding its range of applicability, and by providing
(ii) a self-consistent way of verifying the absence of bias, (iii) a method for
estimation of its variance, and (iv) a criterion for choosing the values of the
free parameter of the estimator. We demonstrate the performance of our
estimator on synthetic data sets, as well as on neurophysiological and systems
biology data sets.

At rest, human brain functional networks display striking modular
architecture in which coherent clusters of brain regions are activated. The
modular account of brain function is pervasive, reliable, and reproducible.
Yet, a complementary perspective posits a core-periphery or rich-club account
of brain function, where hubs are densely interconnected with one another,
allowing for integrative processing. Unifying these two perspectives has
remained difficult due to the fact that the methodological tools to identify
modules are entirely distinct from the methodological tools to identify
core-periphery structure. Here we leverage a recently-developed model-based
approach -- the weighted stochastic block model -- that simultaneously uncovers
modular and core-periphery structure, and we apply it to fMRI data acquired at
rest in 872 youth of the Philadelphia Neurodevelopmental Cohort. We demonstrate
that functional brain networks display rich meso-scale organization beyond that
sought by modularity maximization techniques. Moreover, we show that this
meso-scale organization changes appreciably over the course of
neurodevelopment, and that individual differences in this organization predict
individual differences in cognition more accurately than module organization
alone. Broadly, our study provides a unified assessment of modular and
core-periphery structure in functional brain networks, providing novel insights
into their development and implications for behavior.

Domain growth is a key process in many areas of biology, including embryonic
development, the growth of tissue, and limb regeneration. As a result,
mechanisms for incorporating it into traditional models for cell movement,
interaction, and proliferation are of great importance. A previously well-used
method in order to incorporate domain growth into on-lattice reaction-diffusion
models causes a build up of particles on the boundaries of the domain, which is
particularly evident when diffusion is low in comparison to the rate of domain
growth. Here, we present a new method which addresses this unphysical build up
of particles at the boundaries, and demonstrate that it is accurate even for
scenarios in which the previous method fails. Further, we discuss for which
parameter regimes it is feasible to continue using the original method due to
diffusion dominating the domain growth mechanism.

Deep phenotyping is an emerging conceptual paradigm and experimental approach
that seeks to measure many aspects of phenotypes and link them to understand
the underlying biology. Successful deep phenotyping has mostly been applied in
cultured cells, less so in multicellular organisms. Recently, however, it has
been recognized that such an approach could lead to better understanding of how
genetics, the environment, and stochasticity affect development, physiology,
and behavior of an organism. Over the last 50 years, the nematode
Caenorhabditis elegans has become an invaluable model system for understanding
the role of the genes underlying a phenotypic trait. Recent technological
innovation has taken advantage of the worm physical attributes to increase the
throughput and informational content of experiments. Coupling these technical
advancements with computational or analytical tools has enabled a boom in deep
phenotyping studies of C. elegans. In this review, we highlight how these new
technologies and tools are digging into the biological origins of complex
multidimensional phenotypes seen in the worm.

In this paper, we propose a new framework to analyze the electrical activity
of the uterus recorded by electrohysterography (EHG), from abdominal electrodes
(a grid of 4x4 electrodes) during pregnancy and labor. We evaluate the
potential use of the synchronization between EHG signals in characterizing
electrical activity of the uterus during pregnancy and labor. The complete
processing pipeline consists of i) estimating the correlation between the
different EHG signals, ii) quantifying the connectivity matrices using graph
theory-based analysis and iii) testing the clinical impact of network measures
in pregnancy monitoring and labor detection. We first compared several
connectivity methods to compute the adjacency matrix represented as a graph of
a set of nodes (electrodes) connected by edges (connectivity values). We then
evaluated the performance of different graph measures in the classification of
pregnancy and labor contractions (number of women=35). A comparison with the
already existing parameters used in the state of the art of labor detection and
preterm labor prediction was also performed. Results show higher performance of
connectivity methods when combined with network measures. Denser graphs were
observed during labor than during pregnancy. The network-based metrics showed
the highest classification rate when compared to already existing features.
This network-based approach can be used not only to characterize the
propagation of the uterine contractions, but also may have high clinical impact
in labor detection and likely in the prediction of premature labor.

This paper presents a simple physical model for self-similar (gnomonic, or
first-order) seashell growth which is expressed in coordinate-free terms. The
shell is expressed as the solution of a differential equation which expresses
the growth dynamics, and may be used to investigate shell growth from both the
local viewpoint of the organism building it and moving with the shell opening
(aperture), as well as that of a researcher making global measurements upon a
complete motionless shell. Coordinate systems needed to express the global and
local descriptions of the shell are chosen. The parameters of growth, or their
information equivalent, remain constant in the local system, and are used by
the organism to build the shell, and are likely mirrored in the DNA of the
organism building it. The transformations between local and global
representations are provided. The global model of Cortie, which is very similar
to the present model, is expressed in terms of the present model, and the
global parameters provided by Cortie for various species of mollusk may be used
to calculate the equivalent local parameters.Mathematica code is provided to
implement these transformations, as well as to plot the shells using both
global and local parameters.

Cellular signaling is essential in information processing and decision
making. Therefore, a variety of experimental approaches have been developed to
study signaling on bulk and single-cell level. Single-cell measurements of
signaling molecules demonstrated a substantial cell-to-cell variability,
raising questions about its causes and mechanisms and about how cell
populations cope with or exploit cellular heterogeneity. To gain insights from
single-cell signaling data, analysis and modeling approaches have been
introduced. This review discusses these modeling approaches, with a focus on
recent advances in the development and calibration of mechanistic models.
Additionally, it outlines current and future challenges.

We have advanced a point-process based framework for the regulation of heart
beats by the autonomous nervous system and analyzed the model with and without
feedback. The model without feedback was found amenable to several analytical
results that help develop an intuition about the way the heart interacts with
the nervous system. However, in reality, feedback, baroreflex and chemoreflex
controls are important to model healthy and unhealthy scenarios for the heart.
Based on the Hurst exponent as an index of health of the heart we show how the
state of the nervous system may tune it in health and disease. Monte Carlo
simulation is used to generate RR interval series of the Electrocardiogram
(ECG) for different sympathetic and parasympathetic nerve excitations.

Identifying groups of similar objects using clustering approaches is one of
the most frequently employed first steps in exploratory biomedical data
analysis. Many clustering methods have been developed that pursue different
strategies to identify the optimal clustering for a data set.
  We previously published TiCoNE, an interactive clustering approach coupled
with de-novo network enrichment of identified clusters. However, in this first
version time-series and network analysis remained two separate steps in that
only time-series data was clustered, and identified clusters mapped to and
enriched within a network in a second separate step.
  In this work, we present TiCoNE 2: An extension that can now seamlessly
incorporate multiple data types within its composite clustering model.
Systematic evaluation on 50 random data sets, as well as on 2,400 data sets
containing enriched cluster structure and varying levels of noise, shows that
our approach is able to successfully recover cluster patterns embedded in
random data and that it is more robust towards noise than non-composite models
using only one data type, when applied to two data types simultaneously.
  Herein, each data set was clustered using five different similarity functions
into k=10/30 clusters, resulting to ~5,000 clusterings in total. We evaluated
the quality of each derived clustering with the Jaccard index and an internal
validity score. We used TiCoNE to calculate empirical p-values for all
generated clusters with different permutation functions, resulting in ~80,000
cluster p-values. We show, that derived p-values can be used to reliably
distinguish between foreground and background clusters.
  TiCoNE 2 allows researchers to seamlessly analyze time-series data together
with biological interaction networks in an intuitive way and thereby provides
more robust results than single data type cluster analyses.

Deep phenotyping study has become an emerging field to understand the gene
function and the structure of biological networks. For the living animal C.
elegans, recent advances in genome-editing tools, microfluidic devices and
phenotypic analyses allow for a deeper understanding of the
genotype-to-phenotype pathway. In this article, I reviewed the evolution of
deep phenotyping study in cell development, neuron activity, and the behaviors
of intact animals.

Diabetes in pregnancy (DIP) is an increasing public health priority in the
Australian Capital Territory, particularly due to its impact on risk for
developing Type 2 diabetes. While earlier diagnostic screening results in
greater capacity for early detection and treatment, such benefits must be
balanced with the greater demands this imposes on public health services. To
address such planning challenges, a multi-scale hybrid simulation model of DIP
was built to explore the interaction of risk factors and capture the dynamics
underlying the development of DIP. The impact of interventions on health
outcomes at the physiological, health service and population level is measured.
Of particular central significance in the model is a compartmental model
representing the underlying physiological regulation of glycemic status based
on beta-cell dynamics and insulin resistance. The model also simulated the
dynamics of continuous BMI evolution, glycemic status change during pregnancy
and diabetes classification driven by the individual-level physiological model.
We further modeled public health service pathways providing diagnosis and care
for DIP to explore the optimization of resource use during service delivery.
The model was extensively calibrated against empirical data.

Background: Data preparation, such as missing values imputation and
transformation, is the first step in any data analysis and requires crucial
attention. Particularly, analysis of metabolites demands more preparation since
those small compounds have recently been measurable in large scales with mass
spectrometry techniques. We introduce novel statistical techniques for
metabolite missing values imputation by utilizing replication samples. Results:
To understand the nature of the missing values using replication samples, we
obtained the empirical distribution of missing values and observed that the
rate of missing values is approximately distributed as uniform across the
metabolite range. Therefore, the missing values cannot be imputed with the
lowest values. Using the identified distribution, we illustrated a simulation
study to find an optimal imputation approach for metabolites. Conclusions: We
demonstrated that the missing values in metabolomic data sets might not be
necessarily low value. After identification of the nature of missing values, we
validated K nearest neighborhood as an optimal approach for imputation.

Thalassaemia, triggered by defects in the globin genes, is one of the most
common monogenic diseases. The beta-thalassaemia carrier state is clinically
asymptomatic, thus, making it onerous to diagnose. The current gold standard
technique is implausible to be used for onsite carrier detection as the method
necessitates expensive instruments, skilled manpower and time. In this study,
we have tried to classify the carriers from the healthy samples based on their
blood droplet drying patterns using image analysis based tools and subsequently
develop an in-house program for automated classification of the same. This
automatic, rapid, less laborious and cost-effective technique will
significantly increase the total number of carriers that are screened for
thalassaemia per year in the country, thus, reducing the burden in the state
run advanced health facilities.

Although the open-field test has been widely used, its reliability and
compatibility are frequently questioned. Although many indicating parameters
were introduced for this test, they did not take data distributions into
consideration. This oversight may have caused the problems mentioned above.
Here, an exploratory approach for the analysis of video records of tests of
elderly mice was taken that described the distributions using the least number
of parameters. First, the locomotor activity of the animals was separated into
two clusters: dash and search. The accelerations found in each of the clusters
were distributed normally. The speed and the duration of the clusters exhibited
an exponential distribution. Although the exponential model includes a single
parameter, an additional parameter that indicated instability of the behaviour
was required in many cases for fitting to the data. As this instability
parameter exhibited an inverse correlation with speed, the function of the
brain that maintained stability would be required for a better performance.
According to the distributions, the travel distance, which has been regarded as
an important indicator, was not a robust estimator of the animals' condition.

As biomedical sciences discover new layers of complexity in the mechanisms of
life and disease, mathematical models trying to catch up with these
developments become mathematically intractable. As a result, in the grand
scheme of things, mathematical models have so far played an auxiliary role in
biomedical sciences. We propose a new methodology allowing mathematical
modeling to give, in certain cases, definitive answers to systemic biomedical
questions that elude empirical resolution. Our methodology is based on two
ideas: (1) employing mathematical models that are firmly rooted in established
biomedical knowledge yet so general that they can account for any, or at least
many, biological mechanisms, both known and unknown; (2) finding model
parameters whose likelihood-maximizing values are independent of observations
(existence of such parameters implies that the model must not meet regularity
conditions required for the consistency of maximum likelihood estimator). These
universal parameter values may reveal general patterns (that we call natural
laws) in biomedical processes. We illustrate this approach with the discovery
of a clinically important natural law governing cancer metastasis.
Specifically, we found that under minimal, and fairly realistic, mathematical
and biomedical assumptions the likelihood-maximizing scenario of metastatic
cancer progression in an individual patient is invariably the same: Complete
suppression of metastatic growth before primary tumor resection followed by an
abrupt growth acceleration after surgery. This scenario is widely observed in
clinical practice and supported by a wealth of experimental studies on animals
and clinical case reports published over the last 110 years. The above most
likely scenario does not preclude other possibilities e.g. metastases may start
aggressive growth before primary tumor resection or remain dormant after
surgery.

We present an attention-based Transformer model for automatic retrosynthesis
route planning. Our approach starts from reactants prediction of single-step
organic reactions for given products, followed by Monte Carlo tree search-based
automatic retrosynthetic pathway prediction. Trained on two datasets from the
United States patent literature, our models achieved a top-1 prediction
accuracy of over 54.6% and 63.0% with more than 95% and 99.6% validity rate of
SMILES, respectively, which is the best up to now to our knowledge. We also
demonstrate the application potential of our model by successfully performing
multi-step retrosynthetic route planning for four case products, i.e.,
antiseizure drug Rufinamide, a novel allosteric activator, an inhibitor of
human acute-myeloid-leukemia cells and a complex intermediate of drug
candidate. Further, by using heuristics Monte Carlo tree search, we achieved
automatic retrosynthetic pathway searching and successfully reproduced
published synthesis pathways. In summary, our model has achieved the
state-of-the-art performance on single-step retrosynthetic prediction and
provides a novel strategy for automatic retrosynthetic pathway planning.

Revealing the functional sites of biological sequences, such as evolutionary
conserved, structurally interacting or co-evolving protein sites, is a
fundamental, and yet challenging task. Different frameworks and models were
developed to approach this challenge, including Position-Specific Scoring
Matrices, Markov Random Fields, Multivariate Gaussian models and most recently
Autoencoders. Each of these methods has certain advantages, and while they have
generated a set of insights for better biological predictions, these have been
restricted to the corresponding methods and were difficult to translate to the
complementary domains. Here we propose a unified framework for the
above-mentioned models, that allows for interpretable transformations between
the different methods and naturally incorporates the advantages and insight
gained individually in the different communities. We show how, by using the
unified framework, we are able to achieve state-of-the-art performance for
protein structure prediction, while enhancing interpretability of the
prediction process.

In this chapter, we present a strategy and the technics to approach a
scientific field from a set of articles gathered from the bibliographic
database, Web of Science. The strategy is based on methods developed to analyze
social networks. We illustrate the use of such strategy in studying the
calmodulin field. Such method allows to structure a huge number of articles
when writing a review, to detect the key opinion leaders in a given field and
to locate his own research topic in the landscape of the themes deciphered by
our own community. We show that the free software VosViewer may be used without
knowledge in computing science and with a short learning period. iii.

Cell image classification methods are currently being used in numerous
applications in cell biology and medicine. Applications include understanding
the effects of genes and drugs in screening experiments, understanding the role
and subcellular localization of different proteins, as well as diagnosis and
prognosis of cancer from images acquired using cytological and histological
techniques. We review three different approaches for cell image classification:
numerical feature extraction, end to end classification with neural networks,
and transport-based morphometry. In addition, we provide comparisons on four
different cell imaging datasets to highlight the relative strength of each
method.

Connectivity across landscapes influences a wide range of
conservation-relevant ecological processes, including species movements, gene
flow, and the spread of wildfire, pests, and diseases. Recent improvements in
remote sensing data suggest great potential to advance connectivity models, but
computational constraints hinder these advances. To address this challenge, we
upgraded the widely-used Circuitscape connectivity package to the high
performance Julia programming language. Circuitscape.jl allows users to solve
problems faster via improved parallel processing and solvers, and supports
applications to larger problems (e.g., datasets with hundreds of millions of
cells). We document speed improvements of up to 1800\%. We also demonstrate
scaling of problem sizes up to 437 million grid cells. These improvements allow
modelers to work with higher resolution data, larger landscapes and perform
sensitivity analysis effortlessly. These improvements accelerate the pace of
innovation, helping modelers address pressing challenges like species range
shifts under climate change. Our collaboration between ecologists and computer
scientists has led to the use of connectivity models to inform conservation
decisions. Further, these next generation connectivity models will produce
results faster, facilitating stronger engagement with decision-makers.

Resource Balance Analysis (RBA) is a computational method based on resource
allocation, which performs accurate quantitative predictions of whole-cell
states (i.e. growth rate, meta-bolic fluxes, abundances of molecular machines
including enzymes) across growth conditions. We present an integrated workflow
of RBA together with the Python package RBApy. RBApy builds bacterial RBA
models from annotated genome-scale metabolic models by add-ing descriptions of
cellular processes relevant for growth and maintenance. The package in-cludes
functions for model simulation and calibration and for interfacing to Escher
maps and Proteomaps for visualization. We demonstrate that RBApy faithfully
reproduces results ob-tained by a hand-curated and experimentally validated RBA
model for Bacillus subtilis. We also present a calibrated RBA model of
Escherichia coli generated from scratch, which ob-tained excellent fits to
measured flux values and enzyme abundances. RBApy makes whole-cell modeling
accessible for a wide range of bacterial wild-type and engineered strains, as
il-lustrated with a CO2-fixing Escherichia coli strain.

Metabolomics is becoming a mature part of analytical chemistry as evidenced
by the growing number of publications and attendees of international
conferences dedicated to this topic. Yet, a systematic treatment of the
fundamental structure and properties of metabolomics data is lagging behind. We
want to fill this gap by introducing two fundamental theories concerning
metabolomics data: data theory and measurement theory. Our approach is to ask
simple questions, the answers of which require applying these theories to
metabolomics. We show that we can distinguish at least four different levels of
metabolomics data with different properties and warn against confusing data
with numbers.

Cronobacter sakazakii is an opportunistic pathogen associated with outbreaks
of neonatal necrotizing enterocolitis, septicemia, and meningitis.
Reconstituted powdered infant formulae (PIF) is the most common vehicle of
infection. Plate count methods do not provide direct information on the
physiological status of cells. Flow cytometry (FC) has been used to gain
insights into the physiological states of C. sakazakii after heat treatments,
and to compare FC results with plate counts. The percentage of compromised
cells increased as the percentage of live cells increased after the 100 C
treatment. However, the number of compromised cells after 60 or 65 C treatments
decreased as the percentage of live cells increased, showing that both mild
temperatures would not be completely effective eliminating all bacteria but
compromising their membranes, and showing that mild heat treatments are not
enough to guarantee the safety of PIF. FC was capable to detect C. sakazakii
compromised cells that cannot be detected with classical plate count methods,
thus it could be used to decreasing the risk of pathogenic viable but
non-culturable cells to be in the ingested food. Linear regression analysis
showed good correlations between plate count results vs FC results.

Mathematical models can provide quantitative insight into immunoreceptor
signaling, but require parameterization and uncertainty quantification before
making reliable predictions. We review currently available methods and software
tools to address these problems. We consider gradient-based and gradient-free
methods for point estimation of parameter values, and methods of profile
likelihood, bootstrapping, and Bayesian inference for uncertainty
quantification. We consider recent and potential future applications of these
methods to systems-level modeling of immune-related phenomena.

Microbial metabolism of fugitive hydrocarbons produces greenhouse gas (GHG)
emissions from oil sands tailings ponds (OSTP) and end pit lakes (EPL) that
retain semisolid wastes from surface mining of oil sands ores. Predicting GHG
production, particularly methane (CH4), would help oil sands operators mitigate
tailings emissions and would assist regulators evaluating the trajectory of
reclamation scenarios. Using empirical datasets from laboratory incubation of
OSTP sediments with pertinent hydrocarbons, we developed a stoichiometric model
for CH4 generation by indigenous microbes. This model improved on previous
first-approximation models by considering long-term biodegradation kinetics for
18 relevant hydrocarbons from three different oil sands operations, lag times,
nutrient limitations, and microbial growth and death rates. Laboratory
measurements were used to estimate model parameter values and to validate the
new model. Goodness of fit analysis showed that the stoichiometric model
predicted CH4 production well; normalized mean square error analysis revealed
that it surpassed previous models. Comparison of model predictions with field
measurements of CH4 emissions further validated the new model. Importantly, the
model also identified parameters that are currently lacking but are needed to
enable future robust modeling of CH4 production from OSTP and EPL in situ.

Fontan operation as the current standard of care for the palliation of single
ventricle defects results in significant late complications. Using a mechanical
circulatory device for the right circulation to serve the function of the
missing subpulmonary ventricle could potentially stabilize the failing Fontan
circulation. This study aims to elucidate the hydraulic operating regions that
should be targeted for designing cavopulmonary blood pumps. By integrating
numerical analysis and available clinical information, the interaction of the
cavopulmonary support via the IVC and full assist configurations with a wide
range of simulated adult failing scenarios was investigated; with IVC and full
assist corresponding to the inferior venous return or the entire venous return,
respectively, being routed through the device. We identified the desired
hydraulic operating regions for a cavopulmonary assist device by clustering all
head pressures and corresponding pump flows that result in hemodynamic
improvement for each simulated failing Fontan physiology. Results show that IVC
support can produce beneficial hemodynamics in only a small fraction of failing
Fontan scenarios. Cavopulmonary assist device could increase cardiac index by
35% and decrease the inferior vena cava pressure by 45% depending on the
patient's pre-support hemodynamic state and surgical configuration of the
cavopulmonary assist device (IVC or full support). The desired flow-pressure
operating regions we identified can serve as the performance criteria for
designing cavopulmonary assist devices as well as evaluating off-label use of
commercially available left-side blood pumps for failing Fontan cavopulmonary
support.

Rapid and accurate phenotypic screening of rice germplasms is crucial in
screening for sources of rice sheath blight resistance. However, visual and/or
caliper-based estimations of coalescing, necrotic, ShB disease lesions are
time-consuming, labor-intensive and exposed to human rater subjectivity. Here,
we propose the use of RGB images and image processing techniques to quantify
ShB disease progression in terms of lesion height and diseased area. To be
specific, we developed a pixel color- and coordinate-based K-Means Clustering
(PCC-KMC) algorithm utilizing Mahalanobis metric aimed at accurate segmentation
of symptomatic and non-symptomatic regions within rice stem images. The
performance of PCC-KMC was evaluated using Lin's concordance correlation
coefficient by comparing its results to visual measurements of ShB lesion
height and to lesion/diseased area measured using ImageJ. Low bias and high
precision were observed for absolute lesion height (bias=0.93, precision=0.94)
and absolute symptomatic area (bias=0.98, precision=0.97) studies. Moreover, we
introduced a convolutional neural network (CNN) for the automatic annotation on
clusters, termed PCC-KMC-CNN. Our CNN was trained based on 85%:15% of
composition for training and testing dataset from total 168 ShB-infected stem
sample images, recording 92% accuracy and 0.21 loss. PCC-KMC-CNN also showed
high accuracy and precision for the absolute lesion height (bias=0.86,
precision=0.90) and absolute diseased area (bias=0.99, precision=0.97) studies.
These results demonstrate that the present methodology has great potential and
promise to substitute the traditional visual-based ShB disease severity
assessment.

Summary: Integration of multi-omics data on chemical exposure of cells or
organisms promises a more complete representation of the responding pathways
than single omics data. Data of different omics layers, like transcriptome or
proteome is deposited in different repositories. Additionally, precisely
specifying a chemical of interest that was used in the exposure experiments
suffers from different nomenclatures and non-uniquely mapping of chemical
identifiers. The manual search for corresponding omics data sets of different
layers for exposure with a chemical of interest is thus a tedious task. We have
developed MOD-Finder (Multi-Omics Data set Finder) to efficiently search for
chemical-related omics data sets in several publicly available databases in an
automated manner. A plain and simple presentation of the returned omics data
sets is augmented with effect information that are assumed to be triggered by
the chemical of interest.
  Availability and Implementation: MOD-Finder is implemented in R using the
Shiny package. The web service is available at https://webapp.ufz.de/mod_finder
and the source code under the GNU GPL v3 license at
https://github.com/yigbt/MOD-Finder.
  Supplementary information: Supplementary data are available at
https://www.ufz.de/index.php?en=44919

Quantitative analysis of cell nuclei in microscopic images is an essential
yet challenging source of biological and pathological information. The major
challenge is accurate detection and segmentation of densely packed nuclei in
images acquired under a variety of conditions. Mask R-CNN-based methods have
achieved state-of-the-art nucleus segmentation. However, the current pipeline
requires fully annotated training images, which are time consuming to create
and sometimes noisy. Importantly, nuclei often appear similar within the same
image. This similarity could be utilized to segment nuclei with only partially
labeled training examples. We propose a simple yet effective region-proposal
module for the current Mask R-CNN pipeline to perform few-exemplar learning. To
capture the similarities between unlabeled regions and labeled nuclei, we apply
decomposed self-attention to learned features. On the self-attention map, we
observe strong activation at the centers and edges of all nuclei, including
unlabeled nuclei. On this basis, our region-proposal module propagates partial
annotations to the whole image and proposes effective bounding boxes for the
bounding box-regression and binary mask-generation modules. Our method
effectively learns from unlabeled regions thereby improving detection
performance. We test our method with various nuclear images. When trained with
only 1/4 of the nuclei annotated, our approach retains a detection accuracy
comparable to that from training with fully annotated data. Moreover, our
method can serve as a bootstrapping step to create full annotations of
datasets, iteratively generating and correcting annotations until a
predetermined coverage and accuracy are reached. The source code is available
at https://github.com/feng-lab/nuclei.

Upcoming immunotherapies for cancer treatment rely on the ability of the
immune system to detect and eliminate tumors in the body. A highly simplified
version of this process can be studied in a Petri dish: starting with a random
distribution of immune and tumor cells, it can be observed in detail how
individual immune cells migrate towards nearby tumor cells, establish contact,
and attack. Nevertheless, it remains unclear whether the immune cells find
their targets by chance, or if they approach them 'on purpose', using remote
sensing mechanisms such as chemotaxis. In this work, we present methods to
infer the strength and range of long-range cell-cell interactions from
time-lapse recorded cell trajectories, using a maximum likelihood method to fit
the model parameters. First, we model the interactions as a distance-dependent
'force' that attracts immune cells towards their nearest tumor cell. While this
approach correctly recovers the interaction parameters of simulated cells with
constant migration properties, it detects spurious interactions in the case of
independent cells that spontaneously change their migration behavior over time.
We therefore use an alternative approach that models the interactions by
distance-dependent probabilities for positive and negative turning angles of
the migrating immune cell. We demonstrate that the latter approach finds the
correct interaction parameters even with temporally switching cell migration.

The definition of an innovative therapeutic protocol requires the fine tuning
of all the involved operations in order to maximize the efficiency. In some
cases, the price of the experiments, or their duration, represents a great
obstacle and the full potential of the protocol risks to be reduced or even
hidden by a non-optimal application.
  The implementation of a numerical model of the protocol may represent the
solution, allowing a systematic exploration of all the different alternatives,
shedding the light on the most promising combination and also identifying the
key elements/parameters.
  In this paper, the injection of a plasmid, preceded by a hyaluronidase
injection, is simulated through a mathematical model. Some key elements of the
administration protocol are identified by means of a mathematical optimization
procedure, maximizing the efficacy of the therapy. As a side effect of the
extensive investigation, robust solutions able to reduce the effects of human
errors in the administration are also obtained.

Scaffold proteins organize cellular processes by bringing signaling molecules
into interaction, sometimes by forming large signalosomes. Several of these
scaffolds are known to polymerize. Their assemblies should therefore not be
understood as stoichiometric aggregates, but as combinatorial ensembles. We
analyze the combinatorial interaction of ligands loaded on polymeric scaffolds,
in both a continuum and discrete setting, and compare it with multivalent
scaffolds with fixed number of binding sites. The quantity of interest is the
abundance of ligand interaction possibilities---the catalytic potential
$Q$---in a configurational mixture. Upon increasing scaffold abundance,
scaffolding systems are known to first increase opportunities for ligand
interaction and then to shut them down as ligands become isolated on distinct
scaffolds. The polymerizing system stands out in that the dependency of $Q$ on
protomer concentration switches from being dominated by a first order to a
second order term within a range determined by the polymerization affinity.
This behavior boosts $Q$ beyond that of any multivalent scaffold system. In
addition, the subsequent drop-off is considerably mitigated in that $Q$
decreases with half the power in protomer concentration than for any
multivalent scaffold. We explain this behavior in terms of how the
concentration profile of the polymer length distribution adjusts to changes in
protomer concentration and affinity. The discrete case turns out to be similar,
but the behavior can be exaggerated at small protomer numbers because of a
maximal polymer size, analogous to finite-size effects in bond percolation on a
lattice.

Falls affect a growing number of the population each year. Clinical methods
to identify those at greatest risk for falls usually evaluate individuals while
they perform specific motions such as balancing or Sit-to-Stand (STS).
Unfortunately these techniques have been shown to have poor predictive power
and are unable to identify the magnitude, direction, and timing of
perturbations that can cause an individual to lose stability during motion. To
address this limitation, the recently proposed Stability Basin (SB) aims to
characterize the set of perturbations that will cause an individual to fall
under a specific motor control strategy. The SB is defined as the set of
configurations that do not lead to failure for an individual under their chosen
control strategy. This paper presents a novel method to compute the SB and the
first experimental validation of the SB with an 11-person perturbative STS
experiment involving forwards or backwards pulls from a motor-driven cable. The
individually-constructed SBs are used to identify when a trial fails, i.e.,
when an individual must switch control strategies (indicated by a step or sit)
to recover from a perturbation. The constructed SBs correctly predict the
outcome of trials where failure was observed with over 90% accuracy, and
correctly predict the outcome of successful trials with over 95% accuracy. The
SB was compared to three other methods and was found to estimate the stable
region with over 45% more accuracy in all cases. This study demonstrates that
SBs offer a novel model-based approach for quantifying stability during motion,
which could be used in physical therapy for individuals at risk of falling.

Ensuring that crops use water and nutrients efficiently is an important
strategy for increasing the profitability of farming and reducing the
environmental load from agriculture. Subsurface irrigation can be an
alternative to surface irrigation as a means of losing less irrigation water,
but the application timing and amount are often difficult to determine.
Well-defined soil and crop models are useful for assisting decision support,
but most of the models developed to date have been for surface irrigation. The
present study examines whether the Decision Support System for Agrotechnology
Transfer (DSSAT, version 4.5) cropping system model is applicable for the
production of processing tomatoes with subsurface irrigation, and it revises
the soil module to simulate irrigation schemes with subsurface irrigation. Five
farmed fields in California, USA, are used to test the performance of the
model. The original DSSAT model fails to produce fruit yield by overestimating
the water deficiency. The soil water module is then revised by introducing the
movement of soil moisture due to a vertical soil moisture gradient. Moreover,
an external parameter optimization system is constructed to minimize the error
between the simulation and observations. The revised module reduces the errors
in the soil moisture profile at each field compared to those by the original
DSSAT model. The average soil moisture error decreases from 0.065m^3/m^3 to
0.029m^3/m^3. The yields estimated by the modified model are in a reasonable
range from 80 to 150 ton/ha, which is commonly observed under well-managed
conditions. The present results show that although further testing is required
for yield prediction, the present modification to the original DSSAT model
improves the precision of the soil moisture profile under subsurface irrigation
and can be used for decision support for efficient producting of processing
tomatoes.

Zemblys et al. \cite{gazeNet} reported on a method for the classification of
eye-movements ("gazeNet"). I have found 3 errors and two problems with that
paper that are explained herein. \underline{\textit{\textbf{Error 1}}} The
gazeNet classification method was built assuming that a hand-scored dataset
from Lund University was all collected at 500 Hz, but in fact, six of the 34
recording files were actually collected at 200Hz. Of the six datasets that were
used as the training set for the gazeNet algorithm, 2 were actually collected
at 200Hz. \underline{\textit{\textbf{Problem 1}}} has to do with the fact that
even among the 500Hz data, the inter-timestamp intervals varied widely.
\underline{\textit{\textbf{Problem 2}}} is that there are many unusual
discontinuities in the saccade trajectories from the Lund University dataset
that make it a very poor choice for the construction of an automatic
classification method. \underline{\textit{\textbf{Error 2}}} The gazeNet
algorithm was trained on the Lund dataset, and then compared to other methods,
not trained on this dataset, in terms of performance on this dataset. This is
an inherently unfair comparison, and yet no where in the gazeNet paper is this
unfairness mentioned. \underline{\textit{\textbf{Error 3}}} arises out of the
novel event-related agreement analysis employed by the gazeNet authors.
Although the authors intended to classify unmatched events as either false
positives or false negatives, many are actually being classified as true
negatives. True negatives are not errors, and any unmatched event misclassified
as a true negative is actually driving kappa higher, whereas unmatched events
should be driving kappa lower.

Preferred walking speed is a widely-used performance measure for people with
mobility issues, but is usually measured in straight line walking for fixed
distances or durations. However, daily walking involves walking for bouts of
different distances and walking with turning. Here, we studied walking for
short distances and walking in circles in unilateral lower-limb amputees
wearing an above or below-knee passive prosthesis, specifically, a Jaipur foot
prosthesis. Analogous to earlier results in non-amputees, we found that their
preferred walking speeds are lower for short distances and lower for circles of
smaller radii. Using inverse optimization, we estimated the cost of changing
speeds and turning such that the observed preferred walking speeds in our
experiments minimizes the total energy cost. The inferred costs of changing
speeds and turning were much larger than for non-amputees. These findings could
inform prosthesis design and rehabilitation therapy to better assist changing
speeds and turning tasks in amputee walking. Further, measuring the preferred
speed for a range of distances and radii is a more robust subject-specific
measure of walking performance.

The land crab Cardisoma guanhumi Latreille, 1828 is harvested in several
countries in Latin America, and a critically endangered species. This is the
first study to conduct bootstrapped tagging analysis (BTA) together with
bootstrapped length-frequency analyses (BLFA). Crabs were sampled monthly in a
mangrove patch at Itamaraca Island (Brazil), over 12 months, and marked with
PIT tags. Both methods (BTA and BLFA) indicate very slow growth and Linf far
above Lmax. BTA estimates were K = 0.12 y-1 (95% CI: 0.024 to 0.26 y-1), Linf =
118 mm (95% CI: 81 to 363 mm), Phi' = 1.23 log10(cm y-1) (95% CI: 0.86 to 1.36
log10(cm y-1)). Seasonality in growth was significant (p = 0.006, 95% CI for C:
0.15 to 0.93, median: C = 0.56). Pairs of K and Linf always followed narrow
Phi' isopleths. Total mortality was Z = 2.18 y-1 (95% CI = 1.7 to 4.5 y-1).
Slow growth and a very high Z/K ratio highlight the need for protective
measures. BTA results were 2.2 to 3 times more precise than BLFA. Traditional
length-based methods produced grossly biased results, indicating the urgent
need for new, robust approaches and a critical reevaluation of long-standing
methods and paradigms.

Tardigrades are microscopic animals widely known for their survival
capabilities under extreme conditions. They are the focus of current research
in the fields of taxonomy, biogeography, genomics, proteomics, development,
space biology, evolution, and ecology. Tardigrades, such as Hypsibius
exemplaris, are being advocated as a next-generation model organism for genomic
and developmental studies. The raw culture of H. exemplaris usually contains
tardigrades themselves, their eggs, and algal food and feces. Experimentation
with tardigrades often requires the demanding and laborious separation of
tardigrades from raw samples to prepare pure and contamination-free tardigrade
samples. In this paper, we propose a two-step acousto-microfluidic separation
method to isolate tardigrades from raw samples. In the first step, a passive
microfluidic filter composed of an array of traps is used to remove large algal
clusters in the raw sample. In the second step, a surface acoustic wave-based
active microfluidic separation device is used to continuously deflect
tardigrades from their original streamlines inside the microchannel and thus
selectively isolate them from algae and eggs. The experimental results
demonstrated the efficient tardigrade separation with a recovery rate of 96%
and an algae impurity of 4% on average in a continuous, contactless, automated,
rapid, biocompatible manner.

Cardiovascular waveforms contain information for clinical diagnosis. By
"learning" and organizing the subtle change of waveform morphology from large
amounts of raw waveform data, unsupervised manifold learning helps delineate a
high-dimensional structure and display it as a novel three-dimensional (3D)
image. We investigate the electrocardiography (ECG) waveform for ischemic heart
disease and arterial blood pressure (ABP) waveform in dynamic vasoactive
episodes. We model each beat or pulse to be a point lying on a manifold, like a
surface, and use the diffusion map (DMap) to establish the relationship among
those pulses. For ECG datasets, first we analyzed the non-ST-elevation ECG
waveform distribution from unstable angina to healthy control, and we
investigated intraoperative ST-elevation ECG waveforms to show the dynamic ECG
waveform changes. For ABP datasets, we analyzed waveforms collected under
endotracheal intubation and administration of vasodilator. To quantify the
dynamic separation, we applied the support vector machine (SVM) analysis and
the trajectory analysis. For the non-ST-elevation ECG, a hierarchical tree
structure comprising consecutive ECG waveforms spanning from unstable angina to
healthy control is presented in the 3D image (accuracy=97.6%, macro-F1=96.1%).
The DMap helps quantify and visualize the evolving direction of intraoperative
ST-elevation myocardial episode in a 1-hour period (accuracy=97.58%,
macro-F1=96.06%). The ABP waveform analysis of Nicardipine administration shows
inter-individual difference (accuracy=95.01%, macro-F1=96.9%) and their common
directions from intra-individual moving trajectories. The dynamic change of the
ABP waveform during endotracheal intubation shows a loop-like trajectory
structure, which can be further divided using the knowledge obtained from
Nicardipine. The 3D images provide clues of underneath physiological
mechanisms.

Use of commercial growth chambers for study of biological processes involved
in biomass growth and production pose certain limitations on the nature of
studies that can be performed in them. Optimization of biomass rearing and
production process requires quantitative study of environment influences on the
organism and eventually the products and byproducts consumed and produced. This
work presents a low cost modular system designed to facilitate quantitative
study of growth processes and resource exchanges in organisms such as plants,
fungi and insect larvae. The proposed system constitutes of modular units each
performing a specific function. A novel compact thermoelectric cooler based
unit is designed for conditioning the air. Sensor cluster for measuring gas
concentrations, air properties (temperature, humidity, pressure), and growing
medium properties is implemented and tested. An actuator cluster for resource
exchange and a wiring and control scheme for light spectrum adjustment is
proposed. A three tier hierarchical software framework consisting of an
open-source cloud platform for data aggregation and user interaction, embedded
firmware for microcontroller, and an application development framework for test
automation and experiment regime design is developed and presented. A series of
experiments and tests were performed using the designed hardware and software
to evaluate its capabilities and limitations. This controlled environment was
used to study the photosynthesis and its dependency on temperature and light
intensity in Ocimum basilicum. In a second experiment, evolution of metabolic
activity of Hermetia illucens larvae over its larval phase was studied and the
metabolic products and byproducts were quantitatively measured.

Electroactive-Polymers (EAPs) are one of the best soft materials with great
applications in active microfluidics. Ionic ones (i-EAPs) have more promising
features for being appropriate candidates to use in active microfluidic
devices. Here, as a case study, we have designed and fabricated a microfluidic
micromixer using an i-EAP named Ionic Polymer-Metal Composite (IPMC). In
microfluidics, active devices have more functionality but due to their required
facilities are less effective for Point of Care Tests (POCTs). In the direction
of solving this paradox, we should use some active components that they need
minimum facilities. IPMC can be one of these components, hence by integrating
the IPMC actuator into a microfluidic channel, a micromixer chip was designed
and put to the simulation and experimental tests. The result showed that the
proposed micromixer is able to mix the micro fluids properly and IPMC actuator
has adequate potential to be an active component for POCT-based microfluidic
chips.

Improvements in experimental and computational technologies have led to
significant increases in data available for analysis. Topological data analysis
(TDA) is an emerging area of mathematical research that can identify structures
in these data sets. Here we develop a TDA method to detect physical structures
in a cell that persist over time. In most cells, protein filaments (actin)
interact with motor proteins (myosins) and organize into polymer networks and
higher-order structures. An example of these structures are ring channels that
maintain constant diameters over time and play key roles in processes such as
cell division, development, and wound healing. The interactions of actin with
myosin can be challenging to investigate experimentally in living systems,
given limitations in filament visualization \textit{in vivo}. We therefore use
complex agent-based models that simulate mechanical and chemical interactions
of polymer proteins in cells. To understand how filaments organize into
structures, we propose a TDA method that assesses effective ring generation in
data consisting of simulated actin filament positions through time. We analyze
the topological structure of point clouds sampled along these actin filaments
and propose an algorithm for connecting significant topological features in
time. We introduce visualization tools that allow the detection of dynamic ring
structure formation. This method provides a rigorous way to investigate how
specific interactions and parameters may impact the timing of filamentous
network organization.

Functional magnetic resonance imaging provides rich spatio-temporal data of
human brain activity during task and rest. Many recent efforts have focussed on
characterising dynamics of brain activity. One notable instance is
co-activation pattern (CAP) analysis, a frame-wise analytical approach that
disentangles the different functional brain networks interacting with a
user-defined seed region. While promising applications in various clinical
settings have been demonstrated, there is not yet any centralised, publicly
accessible resource to facilitate the deployment of the technique.
  Here, we release a working version of TbCAPs, a new toolbox for CAP analysis,
which includes all steps of the analytical pipeline, introduces new
methodological developments that build on already existing concepts, and
enables a facilitated inspection of CAPs and resulting metrics of brain
dynamics. The toolbox is available on a public academic repository
(https://c4science.ch/source/CAP_Toolbox.git).
  In addition, to illustrate the feasibility and usefulness of our pipeline, we
describe an application to the study of human cognition. CAPs are constructed
from resting-state fMRI using as seed the right dorsolateral prefrontal cortex,
and, in a separate sample, we successfully predict a behavioural measure of
continuous attentional performance from the metrics of CAP dynamics (R=0.59).

The discovery that repetitive mild traumatic brain injury (mTBI) can result
in chronic traumatic encephalopathy (CTE) in high risk contact sports has led
to increased scrutiny of head protective gear. In this work, we asked if it was
physically possible to prevent mTBI in American football with helmets alone.
Here, we show that modern helmets of several types are unlikely to prevent mTBI
from high speed collisions as might be seen in the NFL, but that introducing
liquid as an energy absorbing medium can dramatically reduce the forces of
impact across a spectrum of impact severities. We hypothesized that a helmet
which transmits a nearly constant force during football impacts is sufficient
to reduce biomechanical loading in the brain below the threshold of mTBI. To
test this hypothesis, we first show that the optimal impact force transmitted
to the head, in terms of brain strain, is in fact a constant force profile.
Then, to generate a constant force with a helmet, we implement a computational
model of a fluid-based shock absorber that adapts passively to any given impact
speed. Computer simulation of head impacts with liquid shock absorption
indicate that, at the highest impact speed, the average brain tissue strain is
reduced by 27.6% $\pm$ 9.3 compared to existing helmet padding that is
available on the market. These simulations are based on the NFL's helmet test
protocol and predict that adding liquid shock absorbers could reduce the number
of concussions by at least 75%. Taken together, these results suggest that the
majority of mTBI in football could be prevented with more efficient helmet
technology.

In this paper, we build a new, simple, and interpretable mathematical model
to estimate and forecast physiology related to the human glucose-insulin
system, constrained by available data. By constructing a simple yet flexible
model class with interpretable parameters, this general model can be
specialized to work in different settings, such as type 2 diabetes mellitus
(T2DM) and intensive care unit (ICU); different choices of appropriate model
functions describing uptake of nutrition and removal of glucose differentiate
between the models. In both cases, the available data is sparse and collected
in clinical settings, major factors that have constrained our model choice to
the simple form adopted.
  The model has the form of a linear stochastic differential equation (SDE) to
describe the evolution of the BG level. The model includes a term quantifying
glucose removal from the bloodstream through the regulation system of the human
body and two other terms representing the effect of nutrition and externally
delivered insulin. The stochastic fluctuations encapsulate model error
necessitated by the simple model form and enable flexible incorporation of
data. The model parameters must be learned in a patient-specific fashion,
leading to personalized models. We present experimental results on
patient-specific parameter estimation and future BG level forecasting in T2DM
and ICU settings. The resulting model leads to the prediction of the BG level
as an expected value accompanied by a band around this value which accounts for
uncertainties in the prediction. Such predictions, then, have the potential for
use as part of control systems that are robust to model imperfections and noisy
data. Finally, the model's predictive capability is compared with two different
models built explicitly for T2DM and ICU contexts.

In patients with depression, the use of 5-HT reuptake inhibitors can improve
the condition. Topological fingerprints, ECFP4, and molecular descriptors were
used. Some SERT and small molecules combined prediction models were established
by using 5 machine learning methods. We selected the higher accuracy models(RF,
SVM, LR) in five-fold cross-validation of training set to establish an
integrated model (VOL_CLF). The training set is from Chembl database and
oversampled by SMOTE algorithm to eliminate data imbalance. The unbalanced data
from same sources (Chembl) was used as Test set 1; the unbalanced data with
different sources(Drugbank) was used as Test set 2 . The prediction accuracy of
SERT inhibitors in Test set 1 was 90.7%~93.3%(VOL_CLF method was the highest);
the inhibitory recall rate was 84.6%-90.1%(RF method was the highest); the
non-inhibitor prediction accuracy rate was 76.1%~80.2%(RF method is the
highest); the non-inhibitor predictive recall rate is 81.2%~87.5% (SVM and
VOL_CLF methods were the highest) The RF model in Test Set 2 performed better
than the other models. The SERT inhibitor predicted accuracy rate, recall rate,
non-inhibitor predicted accuracy rate, recall rate were 42.9%, 85.7%, 95.7%,
73.3%.This study demonstrates that machine learning methods effectively predict
inhibitors of serotonin transporters and accelerate drug screening.

French recommendations for the screening of hepatitis B virus (HBV) infection
were updated in 2019 with the association of three markers: HBs Ag, anti-HBs Ab
and anti-HBc Ab. These three markers allow identification of infected patients,
vaccinated patients and patients who have been in contact with HBV. A positive
HBs Ag is usually associated with HBV infection but this interpretation must
take into account the clinical context. In particular, the absence of anti-HBc
Ab, normal ALAT levels and the absence of jaundice can be associated with
recent HBV vaccination or false-positive HBs Ag. Recent HBV vaccination can
usually be confirmed by patient questioning, while confirmatory tests are
useful to detect false positive HBs Ag. If necessary, a second sample can be
requested to confirm the interpretation.

Tuberculosis (TB) is one of the deadliest diseases worldwide, with 1,5
million fatalities every year along with potential devastating effects on
society, families and individuals. To address this alarming burden, vaccines
can play a fundamental role, even though to date no fully effective TB vaccine
really exists. Current treatments involve several combinations of antibiotics
administered to TB patients for up to two years, leading often to financial
issues and reduced therapy adherence. Along with this, the development and
spread of drug-resistant TB strains is another big complicating matter. Faced
with these challenges, there is an urgent need to explore new vaccination
strategies in order to boost immunity against tuberculosis and shorten the
duration of treatment. Computational modeling represents an extraordinary way
to simulate and predict the outcome of vaccination strategies, speeding up the
arduous process of vaccine pipeline development and relative time to market.
Here, we present EU - funded STriTuVaD project computational platform able to
predict the artificial immunity induced by RUTI and ID93/GLA-SE, two specific
tuberculosis vaccines. Such an in silico trial will be validated through a
phase 2b clinical trial. Moreover, STriTuVaD computational framework is able to
inform of the reasons for failure should the vaccinations strategies against M.
tuberculosis under testing found not efficient, which will suggest possible
improvements.

The family Asteraceae include large number of Centaurea species which have
been applied in folk medicine. One of the family Asteraceae members is the
Centaurea damascena which authentically been tested for its antibacterial and
antioxidant activity as well as its toxicity. The aims of the study were to
determine the antimicrobial and antioxidant activities and toxicity of
methanolic plant extracts of Centaurea damascena. The methanolic extracts were
screened for their antibacterial activity against nine bacteria (Staphylococcus
aureus ATCC 43300, Bacillus subtilis ATCC 6633, Micrococcus luteus ATCC 10240,
and Staphylococcus epidermidis ATCC 12228, Escherichia coli ATCC 11293,
Pseudomonas aerugino and Klebsiella pneumoniae, Enterobacter aerogenes ATCC
13048 and Salmonella typhi ATCC19430). The antibacterial activity was assessed
by using the disc diffusion methods and the minimum inhibition concentrations
(MIC) using microdilution method. The extracts from Centaurea damascena
possessed antibacterial activity against several of the tested microorganisms.
The MIC of methanol extract of C. damascena ranged from 60 to 1100 microgram
per mL. Free radical scavenging capacity of the C. damascena methanol extract
was calculated by DPPH and FRAP test. DPPH radicals were scavenged with an IC50
value of 17.08 microgram per mL. Antioxidant capacities obtained by the FRAP
was 51.9 and expressed in mg Trolox per gram dry weight. The total phenolic
compounds of the methanol extracts of aerial parts, as estimated by
Folin_Ciocalteu reagent method, was about 460 mg GAE per gram. The phenolic
contents in the extracts highly correlate with their antioxidant activity,
confirming that the antioxidant activity of this plant extracts is considerably
phenolic contents-dependent.

It is worth mentioning that the high output of different physiological
responses under the expression of vgb, may have a considerable effect on the
enzyme productivity, dairy industry, heavy metal uptake, biodegradation of
different organic pollutants and other applications. The expression of
bacterial haemoglobin is useful in lessening the load of perceived toxic
conditions such as high oxygen levels. This in turn probably has the same
impact on some peripheral toxic materials. This, hemoglobin biotechnology can
be extended to enhance production of pollutants degrading enzymes or production
of some valuable manufacturing materials on the case by case bases. It is
likely that the mechanism of bacterial hemoglobin (VHb) effects is
intermediated via an oxygen trapping action. This may drive the enrichment of
ATP production, which is mostly required for higher productivity of needed
substances for that activity.

Antibactrial activity of Asteriscus graveolens methanolic extract and its
synergy effect with fungal mediated silver nanoparticles (AgNPs) against some
enteric bacterial human pathogen was conducted. Silver nanoparticles were
synthesized by the fungal strain namely Tritirachium oryzae W5H as reported
early. In this study, MICs of AgNPs against E. aerogenes, Salmonella sp., E.
coli and C. albicans in order were 2.13, 19.15, 0.08 and 6.38 micrograms per
mL, respectively, while the MICs of A. graveolens ethanolic extract against the
same bacteria were 4, 366, 3300 and 40 micrograms per mL, respectively. The MIC
values at concentration less than 19.15 and 40 micrograms per mL indicating the
potent bacteriostatic effect of AgNPs and A. graveolens ethanolic
extract.Increasing in IFA was reported when Nitrofurantion and Trimethoprim
were combined with Etoh extract with maximum increase in IFA by 6 and 12 folds
for, respectively. Also, 10 folds increasing in IFA was reported when
trimethoprim was combined with AgNPs: Etoh extract.But, there were no
synergistic effect between the antifungal agents (Caspofungin and Micafungin)
combined with AgNPs and or A. graveolens ethanolic extract against C.
albicans.The potent synergistic effect of A. graveolens ethanolic extract
and/or NPs with the conventional antibiotics is novel in inhibiting antibiotics
resistant bacteria. In this study, remarkable increasing in the antibacterial
activity, when the most resistant antibiotics combined with A. graveolense
thanolic extract and/or NPs was reported.

The family Asteraceae include large number of Centaurea species which have
been applied in folk medicine. One of the family Asteraceae members is the
Centaurea damascena which authentically been tested for its antibacterial
activity. The aim of the study was to discuss antibacterial activities of
essential oil composition and methanolic extract of the same plant aerial part
leaves. Thirty-seven components were characterized with 86 of oxygenated
terpenes. The composition in percentage was dominated by 11.45 Fokienol, 8.8
thymol, 8.21 Alpha Terpineol, 7.24 Chrysanthemumic acid, 7.13 Terpinen4-ol and
6.59 Borneol with a high degree of polymorphism in the occurrence of these
compounds as compared with the different species of centaurea.. Free radical
scavenging capacity of the C. damascna methanol extract was calculated by DPPH
and FRAP test. DPPH radicals were scavenged with an IC50 value of 17.08
microgram per ml. Antioxidant capacities obtained by the FRAP was 51.9 and
expressed in mg Trolox gram per Liter dry weight. The total phenolic compounds
of the methanol extracts of aerial parts, as estimated by Folin Ciocalteu
reagent method, was about 460 milligram GAE per gram. The phenolic contents in
the extracts highly correlate with their antioxidant activity, confirming that
the antioxidant activity of this plant extracts is considerably phenolic
contents dependent.

Acute lymphoblastic leukemia is the most common malignancy in childhood.
Successful treatment requires initial high-intensity chemotherapy, followed by
low-intensity oral maintenance therapy with oral 6-mercaptopurine (6MP) and
methotrexate (MTX) until 2-3 years after disease onset. However, intra- and
interindividual variability in the pharmacokinetics (PK) and pharmacodynamics
(PD) of 6MP and MTX make it challenging to balance the desired antileukemic
effects with undesired excessive myelosuppression during maintenance therapy. A
model to simulate the dynamics of different cell types, especially neutrophils,
would be a valuable contribution to improving treatment protocols (6MP and MTX
dosing regimens) and a further step to understanding the heterogeneity in
treatment efficacy and toxicity. We applied and modified a recently developed
semi-mechanistic PK/PD model to neutrophils and analyzed their behavior using a
nonlinear mixed-effects modeling approach and clinical data obtained from 116
patients. The PK model of 6MP influenced the accuracy of absolute neutrophil
count (ANC) predictions, whereas the PD effect of MTX did not. Predictions
based on ANC were more accurate than those based on white blood cell counts.
Using the new cross-validated mathematical model, simulations of different
treatment protocols showed a linear dose-effect relationship and reduced ANC
variability for constant dosages. Advanced modeling allows the identification
of optimized control criteria and the weighting of specific influencing factors
for protocol design and individually adapted therapy to exploit the optimal
effect of maintenance therapy on survival.

Motivation: Combination therapies have been widely used to treat cancers.
However, it is cost- and time-consuming to experimentally screen synergistic
drug pairs due to the enormous number of possible drug combinations. Thus,
computational methods have become an important way to predict and prioritize
synergistic drug pairs.
  Results: We proposed a Deep Tensor Factorization (DTF) model, which
integrated a tensor factorization method and a deep neural network (DNN), to
predict drug synergy. The former extracts latent features from drug synergy
information while the latter constructs a binary classifier to predict the drug
synergy status. Compared to the tensor-based method, the DTF model performed
better in predicting drug synergy. The area under the precision-recall curve
(PR AUC) was 0.57 for DTF and 0.24 for the tensor method. We also compared the
DTF model with DeepSynergy and logistic regression models and found that the
DTF outperformed the logistic regression model and achieved almost the same
performance as DeepSynergy using several typical metrics for the classification
task. Applying the DTF model to predict missing entries in our drug-cell line
tensor, we identified novel synergistic drug combinations for 10 cell lines
from the 5 cancer types. A literature survey showed that some of these
predicted drug synergies have been identified in vivo or in vitro. Thus, the
DTF model could be valuable in silico tool for prioritizing novel synergistic
drug combinations.

Microarray techniques are widely used in Gene expression analysis. These
techniques are based on discovering submatrices of genes that share similar
expression patterns across a set of experimental conditions with coherence
constraint. Actually, these submatrices are called biclusters and the
extraction process is called biclustering. In this paper we present a novel
binary particle swarm optimization model for the gene expression biclustering
problem. Hence, we apply the binary particle swarm optimization algorithm with
a proposed measure, called Discretized Column-based Measure (DCM) as a novel
cost function for evaluating biclusters where biological relevance, MSR and the
size of the bicluster are considered as evaluation metrics for our results.
Results are compared to the existing algorithms and they show the validity of
our proposed approach.

T-cell receptors (TCR) are key proteins of the adaptive immune system,
generated randomly in each individual, whose diversity underlies our ability to
recognize infections and malignancies. Modeling the distribution of TCR
sequences is of key importance for immunology and medical applications. Here,
we compare two inference methods trained on high-throughput sequencing data: a
knowledge-guided approach, which accounts for the details of sequence
generation, supplemented by a physics-inspired model of selection; and a
knowledge-free Variational Auto-Encoder based on deep artificial neural
networks. We show that the knowledge-guided model outperforms the deep network
approach at predicting TCR probabilities, while being more interpretable, at a
lower computational cost.

Background:Diverse tacrolimus population pharmacokinetic models in adult
liver transplant recipients have been established to describe the PK
characteristics of tacrolimus in the last two decades. However, their
extrapolated predictive performance remains unclear.Therefore,in this study,we
aimed to evaluate their external predictability and identify their potential
influencing factors. Methods:The external predictability of each selected popPK
model was evaluated using an independent dataset of 84 patients with 572 trough
concentrations prospectively collected from Huashan Hospital. Prediction and
simulation based diagnostics and Bayesian forecasting were conducted to
evaluate model predictability. Furthermore, the effect of model structure on
the predictive performance was investigated.Results:Sixteen published popPK
models were assessed. In prediction-based diagnostics,the prediction error
within 30% was below 50% in all the published models. The simulation based
normalised prediction distribution error test and visual predictive check
indicated large discrepancies between the observations and simulations in most
of the models. Bayesian forecasting showed improvement in model predictability
with two to three prior observations. Additionally, the predictive performance
of the nonlinear Michaelis Menten model was superior to that of linear
compartment models,indicating the underlying nonlinear kinetics of tacrolimus
in liver transplant recipients.Conclusions:The published models performed
inadequately in prediction and simulation based diagnostics. Bayesian
forecasting may improve the predictive performance of the models. Furthermore,
nonlinear kinetics of tacrolimus may be mainly caused by the properties of the
drug itself, and incorporating nonlinear kinetics may be considered to improve
model predictability.

Background Little is known about the population pharmacokinetics (PPK) of
tacrolimus (TAC) in pediatric primary nephrotic syndrome (PNS). This study
aimed to compare the predictive performance between nonlinear and linear PK
models and investigate the significant factors of TAC PK characteristics in
pediatric PNS. Methods Data were obtained from 71 pediatric patients with PNS,
along with 525 TAC trough concentrations at steady state. The demographic,
medical, and treatment details were collected. Genetic polymorphisms were
analyzed. The PPK models were developed using nonlinear mixed effects model
software. Two modeling strategies, linear compartmental and nonlinear Michaelis
Menten (MM) models, were evaluated and compared. Results Body weight, age,
daily dose of TAC, co-therapy drugs (including azole antifungal agents and
diltiazem), and CYP3A5*3 genotype were important factors in the final linear
model (onecompartment model), whereas only body weight, codrugs, and CYP3A5*3
genotype were the important factors in the nonlinear MM model. Apparent
clearance and volume of distribution in the final linear model were 7.13 L/h
and 142 L, respectively. The maximal dose rate (Vmax) of the nonlinear MM model
was 1.92 mg/day and the average concentration at steady state at half-Vmax (Km)
was 1.98 ng/mL. The nonlinear model described the data better than the linear
model. Dosing regimens were proposed based on the nonlinear PK model.Conclusion
Our findings demonstrate that the nonlinear MM model showed better predictive
performance than the linear compartmental model, providing reliable support for
optimizing TAC dosing and adjustment in children with PNS.

Drug-drug interactions (DDI) can cause severe adverse drug reactions and pose
a major challenge to medication therapy. Recently, informatics-based approaches
are emerging for DDI studies. In this paper, we aim to identify key
pharmacological components in DDI based on large-scale data from DrugBank, a
comprehensive DDI database. With pharmacological components as features,
logistic regression is used to perform DDI classification with a focus on
searching for most predictive features, a process of identifying key
pharmacological components. Using univariate feature selection with chi-squared
statistic as the ranking criteria, our study reveals that top 10% features can
achieve comparable classification performance compared to that using all
features. The top 10% features are identified to be key pharmacological
components. Furthermore, their importance is quantified by feature coefficients
in the classifier, which measures the DDI potential and provides a novel
perspective to evaluate pharmacological components.

A while ago, the ideas of evolutionary biology inspired computer scientists
to develop a thriving nowadays field of evolutionary computation (EC), in
general, and genetic algorithms (GA), in particular. At the same time, the
directed evolution of biological molecules (in vitro evolution) is reasonably
interpreted as an implementation of GA in biochemical experiments. One of the
theoretical foundations of GA, justifying the effectiveness of evolutionary
search, is the concept of building blocks (BB). In EC, it is reasonable to
match these BBs to domains and motifs of macromolecules in evolutionary and
synthetic biology. Computer scientists have shown and carefully studied the
importance of preserving already found BBs for the effectiveness of
evolutionary search. For this purpose, dozens of algorithms have been
developed, including heuristic crossover algorithms. On the other hand, the
experimental procedures defining and preserving domains remain a poorly
developed area in the techniques of evolution in vitro. In this paper, we
demonstrate how several simple algorithms preserving the BBs can increase the
efficiency of in vitro evolution in numerical experiments by almost an order of
magnitude. As test problems, we propose and use such well-known problems of
synthetic biology as the evolutionary search for strong bacterial promoters
(with several motifs) and search for multi-domain RNA devices, as compared to
the classic GA tests (Royal Road functions). The success of these primary tests
with simple algorithms gives us every reason to expect that the implementation
and application of more advanced and modern EC procedures will give an even
greater increase in efficiency. Such an increase in search efficiency will
significantly reduce the cost of in vitro evolution experiments, which will
fully cover the costs of developing new experimental procedures based on these
algorithms.

Chagas disease American trypanosomiasis is caused by a flagellated parasite:
trypanosoma cruzi, transmitted by an insect of the genus Triatoma and also by
blood transfusions. In Latin America the number of infected people is
approximately 6 million, with a population exposed to the risk of infection of
550000. It is our interest to develop a non-invasive, low-cost methodology,
capable of detecting any alteration early on cardiaca produced by T. cruzi. We
analyzed the 24 hour RR records in patients with ECG abnormalities (CH2),
patients without ECG alterations (CH1) who had positive serological findings
for Chagas disease and healthy (Control) matched by sex and age. We found
significant differences between the Control, CH1 and CH2 groups that show
dysautonomy and enervation of the autonomic nervous system.

Motivation. Cancer heterogeneity is observed at multiple biological levels.
To improve our understanding of these differences and their relevance in
medicine, approaches to link organ- and tissue-level information from
diagnostic images and cellular-level information from genomics are needed.
However, these "radiogenomic" studies often use linear, shallow models, depend
on feature selection, or consider one gene at a time to map images to genes.
Moreover, no study has systematically attempted to understand the molecular
basis of imaging traits based on the interpretation of what the neural network
has learned. These current studies are thus limited in their ability to
understand the transcriptomic drivers of imaging traits, which could provide
additional context for determining clinical traits, such as prognosis.
  Results. We present an approach based on neural networks that takes
high-dimensional gene expressions as input and performs nonlinear mapping to an
imaging trait. To interpret the models, we propose gene masking and gene
saliency to extract learned relationships from radiogenomic neural networks. In
glioblastoma patients, our models outperform comparable classifiers (>0.10 AUC)
and our interpretation methods were validated using a similar model to identify
known relationships between genes and molecular subtypes. We found that imaging
traits had specific transcription patterns, e.g., edema and genes related to
cellular invasion, and 15 radiogenomic associations were predictive of
survival. We demonstrate that neural networks can model transcriptomic
heterogeneity to reflect differences in imaging and can be used to derive
radiogenomic associations with clinical value.

Closed loop anesthesia delivery (CLAD) systems can help anesthesiologists
efficiently achieve and maintain desired anesthetic depth over an extended
period of time. A typical CLAD system would use an anesthetic marker,
calculated from physiological signals, as real-time feedback to adjust
anesthetic dosage towards achieving a desired set-point of the marker. Since
control strategies for CLAD vary across the systems reported in recent
literature, a comparative analysis of common control strategies can be useful.
For a nonlinear plant model based on well-established models of compartmental
pharmacokinetics and sigmoid-Emax pharmacodynamics, we numerically analyze the
set-point tracking performance of three output-feedback linear control
strategies: proportional-integral-derivative (PID) control, linear quadratic
Gaussian (LQG) control, and an LQG with integral action (ILQG). Specifically,
we numerically simulate multiple CLAD sessions for the scenario where the plant
model parameters are unavailable for a patient and the controller is designed
based on a nominal model and controller gains are held constant throughout a
session. Based on the numerical analyses performed here, conditioned on our
choice of model and controllers, we infer that in terms of accuracy and bias
PID control performs better than ILQG which in turn performs better than LQG.
In the case of noisy observations, ILQG can be tuned to provide a smoother
infusion rate while achieving comparable steady-state response with respect to
PID. The numerical analyses framework and findings, reported here, can help
CLAD developers in their choice of control strategies. This paper may also
serve as a tutorial paper for teaching control theory for CLAD.

High-throughput sequencing of B- and T-cell receptors makes it possible to
track immune repertoires across time, in different tissues, and in acute and
chronic diseases or in healthy individuals. However, quantitative comparison
between repertoires is confounded by variability in the read count of each
receptor clonotype due to sampling, library preparation, and expression noise.
Here, we present a general Bayesian approach to disentangle repertoire
variations from these stochastic effects. Using replicate experiments, we first
show how to learn the natural variability of read counts by inferring the
distributions of clone sizes as well as an explicit noise model relating true
frequencies of clones to their read count. We then use that null model as a
baseline to infer a model of clonal expansion from two repertoire time points
taken before and after an immune challenge. Applying our approach to yellow
fever vaccination as a model of acute infection in humans, we identify
candidate clones participating in the response.

In this paper, we ask if it is possible to increase the interpretability in
multivariate analysis by aligning and projecting covariates onto comparative
subspaces. We demonstrate our method as well as the interpretative power of PLS
decomposed models and how robust interpretability can lead to quantitative
insights.
  We discuss the statistical properties of the PLS weights, $p$-values
associated with specific axes, as well as their alignment properties.
  The applicability of this approach within life science is also demonstrated
by applying it to three use cases of publically available datasets.
  Further we present hierarchical pathway enrichment results stemming from
aligned $p$-values, which are compared with results derived from enrichment
analysis, as an external validation of our method.
  We find that the method can uncover known results from genomics for all of
the studied use cases, i.e. microarray data from multiple sclerosis and
diabetes patients as well as RNA sequencing data from breast cancer patients.

In a systematic review, we investigate current applications of ultrasound in
locomotion research. Shortcomings in the range of view of ultrasound systems
affect the direct validation of musculoskeletal simulations as inverse
approaches have to be applied. We present currently used methods to estimate
muscle and tendon length in human plantarflexors.

Stochastic simulation algorithms (SSAs) are widely used to numerically
investigate the properties of stochastic, discrete-state models. The Gillespie
Direct Method is the pre-eminent SSA, and is widely used to generate sample
paths of so-called agent-based or individual-based models. However, the
simplicity of the Gillespie Direct Method often renders it impractical where
large-scale models are to be analysed in detail. In this work, we carefully
modify the Gillespie Direct Method so that it uses a customised binary decision
tree to trace out sample paths of the model of interest. We show that a
decision tree can be constructed to exploit the specific features of the chosen
model. Specifically, the events that underpin the model are placed in
carefully-chosen leaves of the decision tree in order to minimise the work
required to keep the tree up-to-date. The computational efficencies that we
realise can provide the apparatus necessary for the investigation of
large-scale, discrete-state models that would otherwise be intractable. Two
case studies are presented to demonstrate the efficiency of the method.

Hooge et al. asked the question: "Is human classification by experienced
untrained observers a gold standard in fixation detection?" They conclude the
answer is no. If they had entitled their paper: "Is human classification by
experienced untrained observers a gold standard in fixation detection when data
quality is very poor, data are error-filled, data presentation was not optimal,
and the analysis was seriously flawed?", I would have no case to make. In the
present report, I will present evidence to support my view that this latter
title is justified. The low quality data assessment is based on using a
relatively imprecise eye-tracker, the absence of head restraint for any
subjects, and the use of infants as the majority of subjects (60 of 70
subjects). Allowing subjects with more than 50% missing data (as much as 95%)
is also evidence of low quality data. The error-filled assessment is based on
evidence that a number of the "fixations" classified by "experts" have obvious
saccades within them, and that, apparently, a number of fixations were
classified on the basis of no signal at all. The evidence for non-optimal data
presentation stems from the fact that, in a number of cases, perfectly good
data was not presented to the coders. The flaws in the analysis are evidenced
by the fact that entire stretches of missing data were considered classified,
and that the measurement of saccade amplitude was based on many cases in which
there was no saccade at all. Without general evidence to the contrary, it is
correct to assume that some human classifiers under some conditions may meet
the criteria for a gold standard, and classifiers under other conditions may
not. This conditionality is not recognized by Hooge et al. A fair assessment
would conclude that whether or not humans can be considered a gold standard is
still very much an open question.

The aim of this study was to investigate the age dependence of the fitness
and body mass index (BMI) in Korean adults and to find an effective exercise to
restore the degradation of fitness due to aging. The age dependence of the
fitness and BMI were calculated using their lump mean values (LMVs) and a
linear regression method. The fitness sensitivity percentage to age (FSPA) and
fitness sensitivity percentage to BMI (FSPB) were introduced as indicators for
the effective improvement of the fitness. The results showed that the
degradation of fitness due to aging, especially the degradation of
cardiorespiratory endurance and muscular endurance, could be improved
effectively by controlling the 20-m multi-stage shuttle run and sit-up scores
for both males and females. The results also showed that the BMIs could be
effectively controlled with enhancing the 10-m shuttle run and standing long
jump scores for both males and females. It is expected that the LMV, FSPA, and
FSPB could be used to improve fitness effectively and to establish personal
exercise aims.

For high-throughput cell culture and associated analytics, droplet-based
cultivation systems open up the opportunities for parallelization and rapid
data generation. In contrast to microfluidics with continuous flow, sessile
droplet approaches enhance the flexibility for fluid manipulation with usually
less operational effort. Generating biologically favorable conditions and
promoting cell growth in a droplet, however, is particularly challenging due to
mass transfer limitations, which has to be solved by implementing an effective
mixing technique. Here, capillary waves induced by vertical oscillation are
used to mix inside a sessile droplet micro-bioreactor (MBR) system avoiding
additional moving parts inside the fluid. Depending on the excitation
frequency, different patterns are formed on the oscillating liquid surface,
which are described by a model of a vibrated sessile droplet. Analyzing mixing
times and oxygen transport into the liquid, a strong dependency of mass
transfer on the oscillation parameters, especially the excitation frequency, is
demonstrated. Oscillations at distinct capillary wave resonant frequencies lead
to rapid homogenization with mixing times of 2 s and volumetric liquid-phase
mass transfer coefficients of more than 340 h-1. This shows that the mass
transfer in a droplet MBR can be specifically controlled via capillary waves,
what is subsequently demonstrated for cultivations of Escherichia coli BL21
cells. Therefore, the presented MBR in combination with vertical oscillation
mixing for intensified mass transfer is a promising tool for highly parallel
cultivation and data generation.

We present the use of single-cell entropy (scEntropy) to measure the order of
the cellular transcriptome profile from single-cell RNA-seq data, which leads
to a method of unsupervised cell type classification through scEntropy followed
by the Gaussian mixture model (scEGMM). scEntropy is straightforward in
defining an intrinsic transcriptional state of a cell. scEGMM is a coherent
method of cell type classification that includes no parameters and no
clustering; however, it is comparable to existing machine learning-based
methods in benchmarking studies and facilitates biological interpretation.

Detection and monitoring of patients with pulmonary hypertension, defined as
mean blood pressure in the main pulmonary artery above 25 mmHg, requires a
combination of imaging and hemodynamic measurements. This study demonstrates
how to combine imaging data from microcomputed tomography (micro-CT) images
with hemodynamic pressure and flow waveforms from control and hypertensive
mice. Specific attention is devoted to developing a tool that processes CT
images, generating subject-specific arterial networks in which 1D fluid
dynamics modeling is used to predict blood pressure and flow. Each arterial
network is modeled as a directed graph representing vessels along the principal
pathway to ensure perfusion of all lobes. The 1D model couples these networks
with structured tree boundary conditions informed by the image data. Fluid
dynamics equations are solved in this network and compared to measurements of
pressure in the main pulmonary artery. Analysis of micro-CT images reveals that
the branching ratio is the same in the control and hypertensive animals, but
that the vessel length to radius ratio is significantly lower in the
hypertensive animals. Fluid dynamics predictions show that in addition to
changed network geometry, vessel stiffness is higher in the hypertensive animal
models than in the control models.

Animals such as insects have provided a rich source of inspiration for
designing robots. For example, animals navigate to goals via efficient
coordination of individual motor actions, and demonstrate natural solutions to
problems also faced by engineers. Recording individual body part positions
during large scale movement would therefore be useful. Such multi-scale
observations, however, are challenging. With video, for example, there is
typically a trade-off between the volume over which an animal can be recorded
and spatial resolution within the volume. Even with high pixel-count cameras,
motion blur can be a challenge when using available light. Here we present a
new approach for tracking animals, such as insects, with an optical system that
bypasses this tradeoff by actively pointing a telephoto video camera at the
animal. This system is based around high-speed pan-tilt mirrors which steer an
optical path shared by a quadrant photodiode and a high-resolution, high-speed
telephoto video recording system. The mirror is directed to lock on to the
image of a 25-milligram retroreflector worn by the animal. This system allows
high-magnification videography with reduced motion blur over a large tracking
volume. With our prototype, we obtained millisecond order closed-loop latency
and recorded videos of flying insects in a tracking volume extending to an
axial distance of 3 meters and horizontally and vertically by 40 degrees. The
system offers increased capabilities compared to other video recording
solutions and may be useful for the study of animal behavior and the design of
bio-inspired robots.

Drug-induced liver injury (DILI) is the most common cause of acute liver
failure and a frequent reason for withdrawal of candidate drugs during
preclinical and clinical testing. An important type of DILI is cholestatic
liver injury, caused by buildup of bile salts within hepatocytes; it is
frequently associated with inhibition of bile salt transporters, such as the
bile salt export pump (BSEP). Reliable in silico models to predict BSEP
inhibition directly from chemical structures would significantly reduce costs
during drug discovery and could help avoid injury to patients. Unfortunately,
models published to date have been insufficiently accurate to encourage wide
adoption. We report our development of classification and regression models for
BSEP inhibition with substantially improved performance over previously
published models. Our model development leveraged the ATOM Modeling PipeLine
(AMPL) developed by the ATOM Consortium, which enabled us to train and evaluate
thousands of candidate models. In the course of model development, we assessed
a variety of schemes for chemical featurization, dataset partitioning and class
labeling, and identified those producing models that generalized best to novel
chemical entities. Our best performing classification model was a neural
network with ROC AUC = 0.88 on our internal test dataset and 0.89 on an
independent external compound set. Our best regression model, the first ever
reported for predicting BSEP IC50s, yielded a test set $R^2 = 0.56$ and mean
absolute error 0.37, corresponding to a mean 2.3-fold error in predicted IC50s,
comparable to experimental variation. These models will thus be useful as
inputs to mechanistic predictions of DILI and as part of computational
pipelines for drug discovery.

The ARF-AID (Auxin Response Factor-Auxin Inducible Degron) system is a
re-engineered auxin-inducible protein degradation system. Inducible degron
systems are widely used to specifically and rapidly deplete proteins of
interest in cell lines and organisms. An advantage of inducible degradation is
that the biological system under study remains intact and functional until
perturbation. This feature necessitates that the endogenous levels of the
protein are maintained. However, endogenous tagging of genes with AID can
result in chronic, auxin-independent proteasome-mediated degradation. The
additional expression of the ARF-PB1 domain in the re-engineered ARF-AID system
prevents chronic degradation of AID-tagged proteins while preserving rapid
degradation of tagged proteins. Here we describe the protocol for engineering
human cell lines to implement the ARF-AID system for specific and inducible
protein degradation. These methods are adaptable and can be extended from cell
lines to organisms.

In barley plants, water shortage causes many changes on the morphological,
physiological and biochemical levels resulting in the reduction of grain yield.
In the present study the results of various experiments on the response of the
same barley recombinant inbred lines to water shortage, including phenotypic,
proteomic and metabolomic traits were integrated. Obtained results suggest that
by a multi-omic approach it is possible to indicate proteomic and metabolomic
traits important for reaction of barley plants to reduced water availability.
Analysis of regression of drought effect (DE) for grain weight per plant on DE
of proteomic and metabolomic traits allowed us to suggest ideotype of barley
plants tolerant to water shortage. It was shown that grain weight under drought
was determined significantly by six proteins in leaves and five in roots, the
function of which were connected with defence mechanisms, ion/electron
transport, carbon (in leaves) and nitrogen (in roots) metabolism, and in leaves
additionally by two proteins of unknown function. Out of numerous metabolites
detected in roots only Aspartic and Glutamic acids and one metabolite of
unknown function, were found to have significant influence on grain weight per
plant. The role of these traits as biomarkers, and especially as suggested
targets of ideotype breeding, has to be further studied. One of the direction
to be followed is genetic co-localization of proteomic, metabolomic and
phenotypic traits in the genetic and physical maps of barley genome that can
describe putative functional associations between traits; this is the next step
of our analysis that is in progress.

Deficient myelination of the brain is associated with neurodevelopmental
delays, particularly in high-risk infants, such as those born small in relation
to their gestational age (SGA). New methods are needed to further study this
condition. Here, we employ Color Spatial Light Interference Microscopy (cSLIM),
which uses a brightfield objective and RGB camera to generate pathlength-maps
with nanoscale sensitivity in conjunction with a regular brightfield image.
Using tissue sections stained with Luxol Fast Blue, the myelin structures were
segmented from a brightfield image. Using a binary mask, those portions were
quantitatively analyzed in the corresponding phase maps. We first used the
CLARITY method to remove tissue lipids and validate the sensitivity of cSLIM to
lipid content. We then applied cSLIM to brain histology slices. These specimens
are from a previous MRI study, which demonstrated that appropriate for
gestational age (AGA) piglets have increased internal capsule myelination (ICM)
compared to small for gestational age (SGA) piglets and that a hydrolyzed fat
diet improved ICM in both. The identity of samples was blinded until after
statistical analyses.

Dynamic flux balance analysis uses a quasi-steady state assumption to
calculate an organism's metabolic activity at each time-step of a dynamic
simulation, using the well-known technique of flux balance analysis. For
microbial communities, this calculation is especially costly and involves
solving a linear constrained optimization problem for each member of the
community at each time step. However, this is unnecessary and inefficient, as
prior solutions can be used to inform future time steps. Here, we show that a
basis for the space of internal fluxes can be chosen for each microbe in a
community and this basis can be used to simulate forward by solving a
relatively inexpensive system of linear equations at most time steps. We can
use this solution as long as the resulting metabolic activity remains within
the optimization problem's constraints (i.e. the solution to the linear system
of equations remains a feasible to the linear program). As the solution becomes
infeasible, it first becomes a feasible but degenerate solution to the
optimization problem, and we can solve a different but related optimization
problem to choose an appropriate basis to continue forward simulation. We
demonstrate the efficiency and robustness of our method by comparing with
currently used methods on a four species community, and show that our method
requires at least $91\%$ fewer optimizations to be solved. For reproducibility,
we prototyped the method using Python. Source code is available at
\verb|https://github.com/jdbrunner/surfin_fba|.

Chemotaxis enables cells to systematically approach distant targets that emit
a diffusible guiding substance. However, the visual observation of an encounter
between a cell and a target does not necessarily indicate the presence of a
chemotactic approach mechanism, as even a blindly migrating cell can come
across a target by chance. To distinguish between the chemotactic approach and
blind migration, we present an objective method that is based on the analysis
of time-lapse recorded cell migration trajectories. First, we validate our
method with simulated data, demonstrating that it reliably detects the presence
or absence of remote cell-cell interactions. In a second step, we apply the
method to data from three-dimensional collagen gels, interspersed with highly
migratory natural killer (NK) cells that were derived from two different human
donors. We find for one of the donors an attractive interaction between the NK
cells, pointing to a cooperative behavior of these immune cells. When adding
nearly stationary K562 tumor cells to the system, we find a repulsive
interaction between K562 and NK cells for one of the donors. By contrast, we
find attractive interactions between NK cells and an IL-15-secreting variant of
K562 tumor cells. We therefore speculate that NK cells find wild-type tumor
cells only by chance, but are programmed to leave a target quickly after a
close encounter. We provide a freely available Python implementation of our
p-value method that can serve as a general tool for detecting long-range
interactions in collective systems of self-driven agents.

Microbial fuel cells implemented in constructed wetlands (CW-MFCs), albeit a
relatively new technology still under study, have shown to improve treatment
efficiency of urban wastewater. So far the vast majority of CW-MFC systems
investigated were designed as lab-scale systems working under rather
unrealistic hydraulic conditions using synthetic wastewater. The main objective
of this work was to quantify CW-MFCs performance operated under different
conditions in a more realistic setup using meso-scale systems with horizontal
flow fed with real urban wastewater. Operational conditions tested were organic
loading rate (4.9+-1.6, 6.7+-1.4 and 13.6+-3.2 g COD/m2.day) and hydraulic
regime (continuous vs intermittent feeding) as well as different electrical
connections: CW control (conventional CW without electrodes), open-circuit
CW-MFC (external circuit between anode and cathode not connected) and
closed-circuit CW-MFC (external circuit connected). Eight horizontal subsurface
flow CWs were operated for about four months. Each wetland consisted of a PVC
reservoir of 0.193 m2 filled with 4/8 mm granitic riverine gravel. All wetlands
had intermediate sampling points for gravel and interstitial liquid sampling.
The CW-MFCs were designed as three MFCs incorporated one after the other along
the flow path of the CWs. Results showed no significant differences between
tested organic loading rates, hydraulic regimes or electrical connections,
however, on average, systems operated in closed-circuit CW-MFC mode under
continuous flow outperformed the other experimental conditions. Closed-circuit
CW-MFC compared to conventional CW control systems showed around 5% and 22%
higher COD and ammonium removal, respectively. Correspondingly, overall
bacteria activity, as measured by the fluorescein diacetate technique, was
higher (4% to 34%) in closed-circuit systems when compared to CW control
systems.

In fields such as ecology, microbiology, and genomics, non-Euclidean
distances are widely applied to describe pairwise dissimilarity between
samples. Given these pairwise distances, principal coordinates analysis (PCoA)
is commonly used to construct a visualization of the data. However, confounding
covariates can make patterns related to the scientific question of interest
difficult to observe. We provide aPCoA as an easy-to-use tool, available as
both an R package and a Shiny app, to improve data visualization in this
context, enabling enhanced presentation of the effects of interest.

As synchronized activity is associated with basic brain functions and
pathological states, spike train synchrony has become an important measure to
analyze experimental neuronal data. Many different measures of spike train
synchrony have been proposed, but there is no gold standard allowing for
comparison of results between different experiments. This work aims to provide
guidance on which synchrony measure is best suitable to quantify the effect of
epileptiform inducing substances (e.g. bicuculline (BIC)) in in vitro neuronal
spike train data. Spike train data from recordings are likely to suffer from
erroneous spike detection, such as missed spikes (false negative) or noise
(false positive). Therefore, different time-scale dependent (cross-correlation,
mutual information, spike time tiling coefficient) and time-scale independent
(Spike-contrast, phase synchronization, A-SPIKE-synchronization,
A-ISI-distance, ARI-SPIKE-distance) synchrony measures were compared in terms
of their robustness to erroneous spike trains. As a result of the in silico
manipulated data, Spike-contrast was the only measure being robust to false
negative as well as false-positive spikes. Analyzing the experimental data set
revealed that all measures were able to capture the effect of BIC in a
statistically significant way, with Spike-contrast showing the highest
statistical significance even at low spike detection thresholds. In summary, we
suggest the usage of Spike-contrast to complement established synchrony
measures, as it is time scale independent and robust to erroneous spike trains.

Models of radiative transfer (RT) are important tools for remote sensing of
vegetation, as they facilitate forward simulations of remotely sensed data as
well as inverse estimation of biophysical and biochemical properties from
vegetation optical properties. The remote sensing estimation of foliar protein
content is a key to monitoring the nitrogen cycle in terrestrial ecosystems in
particular to better understand photosynthetic capacity of plants and improve
nitrogen management in agriculture. However, no physically based leaf RT model
currently allows for proper decomposition of leaf dry matter into
nitrogen-based proteins and carbon-based constituents (CBC), estimated from
optical properties of fresh or dry foliage. We developed a new version of the
PROSPECT model, named PROSPECT-PRO, which separates nitrogen-based constituents
(proteins) from CBC (including cellulose, lignin, hemicellulose and starch).
PROSPECT-PRO was calibrated and validated on subsets of the LOPEX dataset,
accounting for both fresh and dry broadleaf and grass samples. We applied an
iterative model inversion optimization algorithm to identify optimal spectral
subdomains for retrieval of leaf protein and CBC contents, with 2125-2174 nm
optimal for proteins and 2025-2349 nm optimal for CBCs. PROSPECT-PRO inversions
revealed a better performance in estimating proteins from optical properties of
fresh than dry leaves. We further tested the ability of PROSPECT-PRO to
estimate leaf mass per area (LMA) as the sum of proteins and CBC using
independent datasets acquired for numerous plant species. Results showed that
PROSPECT-PRO is fully compatible and comparable with its predecessor PROSPECT-D
in indirect estimation of LMA. We can conclude from findings of this study that
PROSPECT-PRO has a high potential in establishing the carbon-to-nitrogen ratio
based on the retrieved CBC-to-proteins ratio.

Cells exhibit a wide variety of different shapes. This diversity poses a
challenge for computational approaches that attempt to shed light on the role
cell geometry plays in regulating cell physiology and behavior. The simulation
platform Simmune is capable of embedding the computational representation of
signaling pathways into realistic models of cellular morphology. However,
Simmune's current approach to account for the cell geometry is limited to
deterministic models of reaction-diffusion processes, thus providing a
coarse-grained description that ignores stochastic local fluctuations. Here we
present an extension of Simmune that removes these limitations by employing an
alternative computational representation of cellular geometry that is smooth
and grid-free. These features make it possible to incorporate a fully
stochastic, spatially resolved description of the cellular biochemistry. The
alternative computational representation is compatible with Simmune's current
approach for specifying molecular interactions. This means that a modeler using
the approach needs to create a model of cellular biochemistry and morphology
only once to be able to use it for both, deterministic and stochastic
simulations.

In the context of natural-based wastewater treatment technologies (such as
constructed wetlands - CW) the use of a low-cost, continuous-like biosensor
tool for the assessment of operational conditions is of key importance for
plant management optimization. The objective of the present study was to assess
the potential use of constructed wetland microbial fuel cells (CW-MFC) as a
domestic wastewater COD assessment tool. For the purpose of this work four
lab-scale CW-MFCs were set up and fed with pre-settled domestic wastewater at
different COD concentrations. Under laboratory conditions two different anodic
materials were tested (graphite rods and gravel). Furthermore, a pilot-plant
based experiment was also conducted to confirm the findings previously recorded
for lab-scale experiments. Results showed that in spite of the low coulombic
efficiencies recorded, either gravel or graphite-based anodes were suitable for
the purposes of domestic wastewater COD assessment. Significant linear
relationships could be established between inlet COD concentrations and CW-MFC
Ecell whenever contact time was above 10 hours. Results also showed that the
accuracy of the CW-MFC was greatly compromised after several weeks of
operation. Pilot experiments showed that CW-MFC presents a good bio-indication
response between week 3 and 7 of operation (equivalent to an accumulated
organic loading between 100 and 200 g COD/m2, respectively). Main conclusion of
this work is that of CW-MFC could be used as an "alarm-tool" for qualitative
continuous influent water quality assessment rather than a precise COD
assessment tool due to a loss of precision after several weeks of operation.

Reproducibility and reusability of the results of data-based modeling studies
are essential. Yet, there has been -- so far -- no broadly supported format for
the specification of parameter estimation problems in systems biology. Here, we
introduce PEtab, a format which facilitates the specification of parameter
estimation problems using Systems Biology Markup Language (SBML) models and a
set of tab-separated value files describing the observation model and
experimental data as well as parameters to be estimated. We already implemented
PEtab support into eight well-established model simulation and parameter
estimation toolboxes with hundreds of users in total. We provide a Python
library for validation and modification of a PEtab problem and currently 20
example parameter estimation problems based on recent studies. Specifications
of PEtab, the PEtab Python library, as well as links to examples, and all
supporting software tools are available at https://github.com/PEtab-dev/PEtab,
a snapshot is available at https://doi.org/10.5281/zenodo.3732958. All original
content is available under permissive licenses.

In the current study, the authors demonstrate the method aimed at analyzing
the distribution of acute coronary syndrome (ACS) cases in Saint Petersburg
using the synthetic population approach and a statistical model for arterial
hypertension prevalence. The cumulative number of emergency services calls in a
separate geographical area (a grid cell of a map) associated with ACS is
matched with the assessed number of dwellers and individuals with arterial
hypertension, which makes it possible to find locations with excessive ACS
incidence. The proposed method is implemented in Python programming language,
the visualization results are shown using QGIS open software. Three categories
of locations are proposed based on the analysis results. The demonstrated
method might be applied for using the statistical assessments of hidden health
conditions in the population to categorize spatial distributions of their
visible consequences.

In this paper, we examine cross-country differences, in terms of the age
distribution of symptomatic cases, hospitalizations, intensive care unit (ICU)
cases, and fatalities due to the novel COVID-19. By calculating conditional
probabilities, we bridge country-level incidence data gathered from different
countries and attribute the variability in data to country demographics. We
then provide case, hospitalization, ICU, and fatality estimates for a
comprehensive list of countries using the existing data from a variety of
countries.

Accurate characterization of extracellular vesicles (EVs) is critical to
explore their diagnostic and therapeutic applications. As the EV research field
has developed, so too have the techniques used to characterize them. The
development of reference materials is required for the standardization of these
techniques. This work, initiated from the ISEV 2017 Biomarker Workshop in
Birmingham, UK, and with further discussion during the ISEV 2019
Standardization Workshop in Ghent, Belgium, sets out to elucidate which
reference materials are required and which are currently available to
standardize commonly used analysis platforms for characterizing EV size,
concentration, refractive index, and epitope expression. Due to their
predominant use, a particular focus is placed on the optical methods
nanoparticle tracking analysis and flow cytometry.

Hepatitis type B is one of the most common infectious disease worldwide that
can pose severe threats to human health up to the point that may contribute to
severe liver damage or cancer. Over the past two decades a large number of
dynamic models have been presented based on experimental data to predict the
HBV infection behavior. Besides, several kinds of controllers have been
employed to obtain effective solutions from the HBV treatment. In this essay we
consider the nonlinear HBV dynamic model which subjected to both parametric and
non-parametric uncertainties without using any linearization. In previous
control methods three HBV dynamic states should be measured virus safe, and
infected cells. However in most of the biological systems, the amount of virus
is experimentally measured. Accordingly, the necessity to seek a method that
can estimate the amount of required drug by receiving the virus data emerges.
An ANFIS method is developed in this work to provide an intelligent controller
for the drug dosage based on the number of viruses together with an estimator
for the amount of infected and uninfected cells. This controller is trained
first using the data provided from a previous adaptive control strategy. After
that to improve the closed-loop system capabilities two unmeasured state
variables of fundamental dynamics are estimated through the training phase of
the ANFIS observer. The results of simulations demonstrated that the accuracy
of the proposed intelligent controller is high in the tracking of the desired
descending virus population.

Background Brain tumours represent the highest cause of mortality in the
paediatric oncological population. Diagnosis is commonly performed with
magnetic resonance imaging and spectroscopy. Survival biomarkers are
challenging to identify due to the relatively low numbers of individual tumour
types, especially for rare tumour types such as atypical rhabdoid tumours.
  Methods 69 children with biopsy-confirmed brain tumours were recruited into
this study. All participants had both perfusion and diffusion weighted imaging
performed at diagnosis. Data were processed using conventional methods, and a
Bayesian survival analysis performed. Unsupervised and supervised machine
learning were performed with the survival features, to determine novel
sub-groups related to survival. Sub-group analysis was undertaken to understand
differences in imaging features, which pertain to survival.
  Findings Survival analysis showed that a combination of diffusion and
perfusion imaging were able to determine two novel sub-groups of brain tumours
with different survival characteristics (p <0.01), which were subsequently
classified with high accuracy (98%) by a neural network. Further analysis of
high-grade tumours showed a marked difference in survival (p=0.029) between the
two clusters with high risk and low risk imaging features.
  Interpretation This study has developed a novel model of survival for
paediatric brain tumours, with an implementation ready for integration into
clinical practice. Results show that tumour perfusion plays a key role in
determining survival in brain tumours and should be considered as a high
priority for future imaging protocols.

Molecular dynamics (MD) simulations have been widely applied to study
macromolecules including proteins. However, high-dimensionality of the datasets
produced by simulations makes it difficult for thorough analysis, and further
hinders a deeper understanding of biomacromolecules. To gain more insights into
the protein structure-function relations, appropriate dimensionality reduction
methods are needed to project simulations onto low-dimensional spaces. Linear
dimensionality reduction methods, such as principal component analysis (PCA)
and time-structure based independent component analysis (t-ICA), could not
preserve sufficient structural information. Though better than linear methods,
nonlinear methods, such as t-distributed stochastic neighbor embedding (t-SNE),
still suffer from the limitations in avoiding system noise and keeping
inter-cluster relations. ivis is a novel deep learning-based dimensionality
reduction method originally developed for single-cell datasets. Here we applied
this framework for the study of light, oxygen and voltage (LOV) domain of
diatom Phaeodactylum tricornutum aureochrome 1a (PtAu1a). Compared with other
methods, ivis is shown to be superior in constructing Markov state model (MSM),
preserving information of both local and global distances and maintaining
similarity between high dimension and low dimension with the least information
loss. Moreover, ivis framework is capable of providing new prospective for
deciphering residue-level protein allostery through the feature weights in the
neural network. Overall, ivis is a promising member in the analysis toolbox for
proteins.

Background Small for gestational age (SGA) birthweight, a risk factor of
infant mortality and delayed child development, is associated with maternal
educational attainment. Maternal tobacco smoking during pregnancy could
contribute to this association. We aimed to quantify the contribution of
maternal smoking during pregnancy to social inequalities in child birthweight
for gestational age (GA). Methods Data come from the French nation-wide ELFE
cohort study, which included 17,155 singletons. Birthweights for GA were
calculated using z-scores. Associations between maternal educational
attainment, tobacco smoking during pregnancy and child birthweight for GA were
ascertained using mediation analysis. Mediation analyses were also stratified
by maternal pre-pregnancy body mass index.Results Low maternal educational
attainment was associated with an increased odd of tobacco smoking during
pregnancy (adjusted OR (ORa)=2.58 [95% CI 2.34, 2.84]) as well as a decrease in
child birthweight for GA (RRa=0.94 [95% 0.91, 0.98]). Tobacco smoking during
pregnancy was associated with a decrease in offspring birthweight for GA
(RRa=0.73 [95% CI 0.70, 0.76]). Mediation analysis suggests that 39% of the
effect of low maternal educational attainment on offspring birthweight for GA
was mediated by smoking during pregnancy. A more important direct effect of
maternal educational attainment on child birthweight for GA was observed among
underweight women (RRa=0.82 [95%CI 0.72, 0.93]).Conclusions The relationship
between maternal educational attainment and child birthweight for GA is
strongly mediated by smoking during pregnancy. Reducing maternal smoking could
lessen the occurrence of infant SGA and decrease socioeconomic inequalities in
birthweight for GA.Keywords Birthweight, educational attainment, tobacco
smoking, pregnancy, mediation analysis, health inequalities

Background An advantaged socioeconomic position (SEP) and satisfying social
support during pregnancy (SSP) have been found to be protective factors of
maternal postpartum depression (PDD). An advantaged SEP is also associated with
satisfying SSP, making SSP a potential mediator of social inequalities in PPD.
SEP, SSP and PPD are associated with migrant status. The aim of this study was
to quantify the mediating role of SSP in social inequalities in PPD regarding
mother's migrant status. Methods A sub-sample of 15,000 mothers from the French
nationally-representative ELFE cohort study was used for the present analyses.
SEP was constructed as a latent variable measured with educational attainment,
occupational grade, employment, financial difficulties and household income.
SSP was characterized as perceived support from partner (good relation,
satisfying support and paternal leave) and actual support from midwives
(psychosocial risk factors assessment and antenatal education). Mediation
analyses with multiple mediators, stratified by migrant status were conducted.
Results Study population included 76% of non-migrant women, 12% of second and
12% of first generation migrant. SEP was positively associated with support
from partner, regardless of migrant status. Satisfying partner support was
associated with a 8 (non-migrant women) to 11% (first generation migrant women)
reduction in PPD score. Limitations History of depression was not
reported.Conclusions Partner support could reduce social inequalities in PPD.
This work supports the need of interventions, longitudinal and qualitative
studies including fathers and adapted to women at risk of PPD to better
understand the role of SSP in social inequalities in PPD. Keywords social
support, postpartum depression, epidemiology, social inequalities, pregnancy,
mediation analysis

Rapid testing of appropriate specimens from patients suspected for a disease
during an epidemic, such as the current Coronavirus outbreak, is of a great
importance for the disease management and control. We propose a method to
enhance processing large amounts of collected samples. The method is based on
mixing samples in testing tubes in a specific configuration, as opposed to
testing single samples in each tube, and accounting for natural virus amounts
in infected patients from variation of positiveness in test tubes. To
illustrate the efficiency of the suggested method we carry out numerical tests
for actual scenarios under various tests. Applying the proposed method enhances
the number of tests by order of magnitudes, where all positives are identified
with no false negatives, and the effective testing time can be reduced
drastically even when the uncertainty in the test is relatively high.

Head motion induced by impacts has been deemed as one of the most important
measures in brain injury prediction, given that the majority of brain injury
metrics use head kinematics as input. Recently, researchers have focused on
using fast approaches, such as machine learning, to approximate brain
deformation in real-time for early brain injury diagnosis. However, those
requires large number of kinematic measurements, and therefore data
augmentation is required given the limited on-field measured data available. In
this study we present a principal component analysis-based method that emulates
an empirical low-rank substitution for head impact kinematics, while requiring
low computational cost. In characterizing our existing data set of 537 head
impacts, consisting of 6 degrees of freedom measurements, we found that only a
few modes, e.g. 15 in the case of angular velocity, is sufficient for accurate
reconstruction of the entire data set. Furthermore, these modes are
predominantly low frequency since over 70% to 90% of the angular velocity
response can be captured by modes that have frequencies under 40Hz. We compared
our proposed method against existing impact parametrization methods and showed
significantly better performance in injury prediction using a range of
kinematic-based metrics -- such as head injury criterion and rotational injury
criterion (RIC) -- and brain tissue deformation-metrics -- such as brain angle
metric, maximum principal strain (MPS) and axonal fiber strains (FS). In all
cases, our approach reproduced injury metrics similar to the ground truth
measurements with no significant difference, whereas the existing methods
obtained significantly different (p<0.01) values as well as poor injury
classification sensitivity and specificity. This emulator will enable us to
provide the necessary data augmentation to build a head impact kinematic data
set of any size.

Motivation: Computational protein structure prediction has taken over the
structural community in past few decades, mostly focusing on the development of
Template-Free modelling (TFM) or ab initio modelling protocols. Fragment-based
assembly (FBA), falls under this category and is by far the most popular
approach to solve the spatial arrangements of proteins. FBA approaches usually
rely on sequence based profile comparison to generate fragments from a
representative structural database. Here we report the use of Protein Blocks
(PBs), a structural alphabet (SA) to perform such sequence comparison and to
build customised fragment libraries for TFM. Results: We demonstrate that
predicted PB sequences for a query protein can be used to search for high
quality fragments that overall cover above 90% of the query. The fragments
generated are of minimum length of 11 residues, and fragments that cover more
than 30% of the query length were often obtained. Our work shows that PBs can
serve as a good way to extract structurally similar fragments from a database
of representatives of non-homologous structures and of the proteins that
contain less ordered regions.

Recently, Antimicrobial peptides (AMPs) have been an area of interest in the
researches, as the first line of defense against the bacteria. They are raising
attention as an efficient way of fighting multidrug resistance. Discovering and
identification of AMPs in the wet labs are challenging, expensive, and
time-consuming. Therefore, using computational methods for AMP predictions have
grown attention as they are more efficient approaches. In this paper, we
developed a promising ensemble learning algorithm that integrates well-known
learning models to predict AMPs. First, we extracted the optimal features from
the physicochemical, evolutionary, and secondary structure properties of the
peptide sequences. Our ensemble algorithm then trains the data using
conventional algorithms. Finally, the proposed ensemble algorithm has improved
the performance of the prediction by about 10% comparing to the traditional
learning algorithms

We present a new technique for rapid modeling and construction of
scientifically accurate mesoscale biological models. Resulting 3D models are
based on few 2D microscopy scans and the latest knowledge about the biological
entity represented as a set of geometric relationships. Our new technique is
based on statistical and rule-based modeling approaches that are rapid to
author, fast to construct, and easy to revise. From a few 2D microscopy scans,
we learn statistical properties of various structural aspects, such as the
outer membrane shape, spatial properties and distribution characteristics of
the macromolecular elements on the membrane. This information is utilized in 3D
model construction. Once all imaging evidence is incorporated in the model,
additional information can be incorporated by interactively defining rules that
spatially characterize the rest of the biological entity, such as mutual
interactions among macromolecules, their distances and orientations to other
structures. These rules are defined through an intuitive 3D interactive
visualization and modeling feedback loop. We demonstrate the utility of our
approach on a use case of the modeling procedure of the SARS-CoV-2 virus
particle ultrastructure. Its first complete atomistic model, which we present
here, can steer biological research to new promising directions in fighting
spread of the virus.

In Mediterranean aquaculture little research has examined the interaction
between rearing density and dietary composition on main key performance
indicators, physiological processes and gut bacterial community. A study was
undertaken, therefore to assess growth response, digestive enzyme activity,
humoral immunity on skin mucus, plasma biochemistry and gut microbiota of
gilthead sea bream (Sparus aurata, L. 1758) reared at high (HD) and low (LD)
final stocking densities and fed high (FM30FO15, 30 % fishmeal FM, 15 % fish
oil, FO) and low (FM10FO3; 10 % FM and 3 % FO) FM and FO levels. Isonitrogenous
and isolipidic extruded diets were fed to triplicate fish groups (initial
weight: 96.2 g) to overfeeding over 98 days. The densities tested had no major
effects on overall growth and feed efficiency of sea bream reared at high or
low FM and FO dietary level. Results of digestive enzyme activity indicated a
comparable digestive efficiency among rearing densities and within each dietary
treatment. Plasma parameters related to nutritional and physiological
conditions were not affected by rearing densities under both nutritional
conditions a similar observation was also achieved through the study of
lysozyme, protease, antiprotease and total protein determination in skin mucus,
For the first time on this species, the effect of rearing density on gut
bacterial community was studied. Different response in relation to dietary
treatment under HD and LD were detected. Low FM-FO diet maintained steady the
biodiversity of the gut bacterial community between LD and HD conditions while
fish fed high FM-FO level showed a reduced biodiversity at HD. According to the
results, it seems feasible to rear gilthead sea bream at the on-growing phase
at a density up to 36-44 kg m3 with low or high FM-FO diet without negatively
affecting growth, feed efficiency, welfare condition and gut bacterial
community.

Background: Scale equating is a statistical technique used to establish
equivalence relations between different scales. Its use is quite popular in
educational evaluation, however, unusual in the health area, where scales of
measures are tools that integrate clinical practice. With the use of different
scales, there is a difficulty in comparing scientific results, such as NESSCA
and SARA scales, tools for assessing the commitment to Machado-Joseph disease
(SCA3/MJD). Objective: Explore the method of scale equating and demonstrate its
application through NESSCA and SARA scales, using the Item Response Theory
(IRT) approach in assessing SCA3/MJD commitment. Methods: Data came from 227
patients from the Hospital de Cl\'inicas de Porto Alegre with SCA3/MJD who have
complete measures for NESSCA and/or SARA scales. The equating design used is
that of non-equivalent groups with common items, with separate calibration. The
IRT model used in the estimation of the parameters was the generalized partial
credit, for NESSCA and SARA. The linear transformation was performed using the
Mean/Mean, Mean/Sigma, Haebara and StokingLord methods and the equation of the
true score was applied to obtain an estimated relationship between the scores
of the scales. Results: Difference between NESSCA score estimated by SARA and
observed NESSCA score has shown median of 0.82 points, by Mean/Sigma method.
This was the best method of linear transformation among the tested.
Conclusions: This study extended the use of scale equating under IRT approach
to health outcomes and established an equivalence relationship between NESSCA
and SARA scores, making the comparison between patients and scientific results
feasible.

We present a family of alchemical perturbation potentials that enable the
calculation of hydration free energies of small to medium-sized molecules in a
concerted single alchemical coupling step instead of the commonly used sequence
of two distinct coupling steps for Lennard-Jones and electrostatic
interactions. The perturbation potentials are based on the softplus function of
the solute-solvent interaction energy designed to focus sampling near entropic
bottlenecks along the alchemical pathway. We present a general framework to
optimize the parameters of alchemical perturbation potentials of this kind. The
optimization procedure is based on the $\lambda$-function formalism and the
maximum-likelihood parameter estimation procedure we developed earlier to avoid
the occurrence of multi-modal distributions of the coupling energy along the
alchemical path. A novel soft-core function applied to the overall
solute-solvent interaction energy rather than individual interatomic pair
potentials critical for this result is also presented. Because it does not
require modifications of core force and energy routines, the soft-core
formulation can be easily deployed in molecular dynamics simulation codes. We
illustrate the method by applying it to the estimation of the hydration free
energy in water droplets of compounds of varying size and complexity. In each
case, we show that convergence of the hydration free energy is achieved
rapidly. This work paves the way for the ongoing development of more
streamlined algorithms to estimate free energies of molecular binding with
explicit solvation.

We propose `Tapestry', a novel approach to pooled testing with application to
COVID-19 testing with quantitative Reverse Transcription Polymerase Chain
Reaction (RT-PCR) that can result in shorter testing time and conservation of
reagents and testing kits. Tapestry combines ideas from compressed sensing and
combinatorial group testing with a novel noise model for RT-PCR used for
generation of synthetic data. Unlike Boolean group testing algorithms, the
input is a quantitative readout from each test and the output is a list of
viral loads for each sample relative to the pool with the highest viral load.
While other pooling techniques require a second confirmatory assay, Tapestry
obtains individual sample-level results in a single round of testing, at
clinically acceptable false positive or false negative rates. We also propose
designs for pooling matrices that facilitate good prediction of the infected
samples while remaining practically viable. When testing $n$ samples out of
which $k \ll n$ are infected, our method needs only $O(k \log n)$ tests when
using random binary pooling matrices, with high probability. However, we also
use deterministic binary pooling matrices based on combinatorial design ideas
of Kirkman Triple Systems to balance between good reconstruction properties and
matrix sparsity for ease of pooling. In practice, we have observed the need for
fewer tests with such matrices than with random pooling matrices. This makes
Tapestry capable of very large savings at low prevalence rates, while
simultaneously remaining viable even at prevalence rates as high as 9.5\%.
Empirically we find that single-round Tapestry pooling improves over two-round
Dorfman pooling by almost a factor of 2 in the number of tests required. We
validate Tapestry in simulations and wet lab experiments with oligomers in
quantitative RT-PCR assays. Lastly, we describe use-case scenarios for
deployment.

Mechano-electric coupling (MEC) in atrial tissue has received sparse
investigation to date, despite the well-known association between chronic
atrial dilation and atrial fibrillation (AF). Of note, no fewer than six
different mechanisms pertaining to stretch-activated channels, cellular
capacitance and geometric effects have been identified in the literature as
potential players. In this mini review, we briefly survey each of these
pathways to MEC. We then perform computational simulations using single cell
and tissue models in presence of various stretch regimes and MEC pathways. This
allows us to assess the relative significance of each pathway in determining
action potential duration, conduction velocity and rotor stability. For chronic
atrial stretch, we find that stretch-induced alterations in membrane
capacitance decrease conduction velocity and increase action potential
duration, in agreement with experimental findings. In the presence of
time-dependent passive atrial stretch, stretch-activated channels play the
largest role, leading to after-depolarizations and rotor hypermeandering. These
findings suggest that physiological atrial stretches, such as passive stretch
during the atrial reservoir phase, may play an important part in the mechanisms
of atrial arrhythmogenesis.

Equation learning methods present a promising tool to aid scientists in the
modeling process for biological data. Previous equation learning studies have
demonstrated that these methods can infer models from rich datasets, however,
the performance of these methods in the presence of common challenges from
biological data has not been thoroughly explored. We present an equation
learning methodology comprised of data denoising, equation learning, model
selection and post-processing steps that infers a dynamical systems model from
noisy spatiotemporal data. The performance of this methodology is thoroughly
investigated in the face of several common challenges presented by biological
data, namely, sparse data sampling, large noise levels, and heterogeneity
between datasets. We find that this methodology can accurately infer the
correct underlying equation and predict unobserved system dynamics from a small
number of time samples when the data is sampled over a time interval exhibiting
both linear and nonlinear dynamics. Our findings suggest that equation learning
methods can be used for model discovery and selection in many areas of biology
when an informative dataset is used. We focus on glioblastoma multiforme
modeling as a case study in this work to highlight how these results are
informative for data-driven modeling-based tumor invasion predictions.

Factors that mediate ethanol preference in Drosophila melanogaster are not
well understood. A major confound has been the use of diverse methods to
estimate ethanol consumption. We measured fly consumptive ethanol preference on
base diets varying in nutrients, taste, and ethanol concentration. Both sexes
showed ethanol preference that was abolished on high nutrient concentration
diets. Additionally, manipulating total food intake without altering the
nutritive value of the base diet or the ethanol concentration was sufficient to
evoke or eliminate ethanol preference. Absolute ethanol intake and food volume
consumed were stronger predictors of ethanol preference than caloric intake or
the dietary caloric content. Our findings suggest that the effect of the base
diet on ethanol preference is largely mediated by total consumption associated
with the delivery medium, which ultimately determines the level of ethanol
intake. We speculate that a physiologically relevant threshold for ethanol
intake is essential for preferential ethanol consumption.

Research papers in the biomedical field come with large and complex data sets
that are shared with the scientific community as unstructured data files via
public data repositories. Examples are sequencing, microarray, and mass
spectroscopy data. The papers discuss and visualize only a small part of the
data, the part that is in its research focus. For labs with similar but not
identical research interests different parts of the data might be important.
They can thus download the full data, preprocess it, integrate it with data
from other publications and browse those parts that they are most interested
in. This requires substantial work as well as programming and analysis
expertise that only few biological labs have on board. In contrast, providing
access to published data over web browsers makes all data visible, allows for
easy interaction with it, and lowers the barrier to working with data from
others.

The g-ratio, quantifying the comparative thickness of the myelin sheath
encasing an axon, is a geometrical invariant that has high functional relevance
because of its importance in determining neuronal conduction velocity. Advances
in MRI data acquisition and signal modelling have put in vivo mapping of the
g-ratio, across the entire white matter, within our reach. This capacity would
greatly increase our knowledge of the nervous system: how it functions, and how
it is impacted by disease. This is the second review on the topic of g-ratio
mapping using MRI. As such, it summarizes the most recent developments in the
field, while also providing methodological background pertinent to aggregate
g-ratio weighted mapping, and discussing pitfalls associated with these
approaches. Using simulations based on recently published data, this review
demonstrates the relevance of the calibration step for three myelin-markers
(macromolecular tissue volume, myelin water fraction, and bound pool fraction).
It highlights the need to estimate both the slope and offset of the
relationship between these MRI-based markers and the true myelin volume
fraction if we are really to achieve the goal of precise, high sensitivity
g-ratio mapping in vivo. Other challenges discussed in this review further
evidence the need for gold standard measurements of human brain tissue from ex
vivo histology. We conclude that the quest to find the most appropriate MRI
biomarkers to enable in vivo g-ratio mapping is ongoing, with the potential of
many novel techniques yet to be investigated.

The COVID-19 pandemic has led to an unprecedented response in terms of
clinical research activity. An important part of this research has been focused
on randomized controlled clinical trials to evaluate potential therapies for
COVID-19. The results from this research need to be obtained as rapidly as
possible. This presents a number of challenges associated with considerable
uncertainty over the natural history of the disease and the number and
characteristics of patients affected, and the emergence of new potential
therapies. These challenges make adaptive designs for clinical trials a
particularly attractive option. Such designs allow a trial to be modified on
the basis of interim analysis data or stopped as soon as sufficiently strong
evidence has been observed to answer the research question, without
compromising the trial's scientific validity or integrity. In this paper we
describe some of the adaptive design approaches that are available and discuss
particular issues and challenges associated with their use in the pandemic
setting. Our discussion is illustrated by details of four ongoing COVID-19
trials that have used adaptive designs.

Mobility restrictions imposed to suppress coronavirus transmission can alter
physical activity (PA) and sleep patterns. Characterization of response
heterogeneity and their underlying reasons may assist in tailoring customized
interventions. We obtained wearable data covering baseline, incremental
movement restriction and lockdown periods from 1824 city-dwelling, working
adults aged 21 to 40 years, incorporating 206,381 nights of sleep and 334,038
days of PA. Four distinct rest activity rhythms (RARs) were identified using
k-means clustering of participants' temporally distributed step counts.
Hierarchical clustering of the proportion of time spent in each of these RAR
revealed 4 groups who expressed different mixtures of RAR profiles before and
during the lockdown. Substantial but asymmetric delays in bedtime and waketime
resulted in a 24 min increase in weekday sleep duration with no loss in sleep
efficiency. Resting heart rate declined 2 bpm. PA dropped an average of 38%. 4
groups with different compositions of RAR profiles were found. Three were
better able to maintain PA and weekday/weekend differentiation during lockdown.
The least active group comprising 51 percent of the sample, were younger and
predominantly singles. Habitually less active already, this group showed the
greatest reduction in PA during lockdown with little weekday/weekend
differences. Among different mobility restrictions, removal of habitual social
cues by lockdown had the largest effect on PA and sleep. Sleep and resting
heart rate unexpectedly improved. RAR evaluation uncovered heterogeneity of
responses to lockdown and can identify characteristics of persons at risk of
decline in health and wellbeing.

Flooded rice production is crucial to global food security, but there are
associated environmental concerns. In particular, it is a significant source of
methane and nitrous oxide emissions and a large consumer of water resources,
while arsenic, cadmium and lead levels in the grain are a serious health
concern. There is also a tendency to use more organic fertilisers to close
nutrient cycles, posing a threat of even higher greenhouse gas emissions and
grain arsenic levels. It has been shown that alternate wetting and drying (AWD)
water management reduces both water use and greenhouse gas emissions, but
success at maintaining yields varies. This study tested the effect of early AWD
(e-AWD) versus continuous flooding (CF) water management practices on grain
yields, greenhouse gas emissions and grain arsenic, cadmium and lead levels in
a split plot field experiment with organic fertilisers under organic
management. The e-AWD water regime showed no difference in yield for the
organic treatments. Yields significantly increased by 5 to 16 percent in the
combination treatments. Root biomass and root length increased in the e-AWD
treatments up to 72 and 41 percent, respectively. The e-AWD water regime
reduced seasonal methane emissions by 71 to 85 percent for organic treatments
and by 51 to 76 percent for combination treatments; this was linked to a 15 to
47 percent reduction in dissolved organic carbon. Nitrous oxide emissions
increased by 23 to 305 percent but accounted for less than 20 percent of global
warming potential. Area and yield scaled global warming potentials were reduced
by 67 to 83 percent. The e-AWD regime altered soil redox potentials, resulting
in a reduction in grain arsenic and lead concentrations of up to 66 % and 73 %
respectively. Grain cadmium levels were also reduced up to 33 % in organic
treatments.

Background: Worldwide demand for SARS-CoV-2 RT-PCR testing is increasing as
more countries are impacted by COVID-19 and as testing remains central to
contain the spread of the disease, both in countries where the disease is
emerging and in countries that are past the first wave but exposed to
re-emergence. Group testing has been proposed as a solution to expand testing
capabilities but sensitivity concerns have limited its impact on the management
of the pandemic. Digital PCR (RT-dPCR) has been shown to be more sensitive than
RT-PCR and could help in this context.
  Methods: We implemented RT-dPCR based COVID-19 group testing on commercially
available system and assay (Naica System from Stilla Technologies) and
investigated the sensitivity of the method in real life conditions of a
university hospital in Paris, France, in May 2020. We tested the protocol in a
direct comparison with reference RT-PCR testing on 448 samples split into
groups of 3 sizes for RT-dPCR analysis: 56 groups of 8 samples, 28 groups of 16
samples and 14 groups of 32 samples.
  Results: Individual RT-PCR testing identified 25 positive samples. Using
groups of 8, testing by RT-dPCR identified 23 groups as positive, corresponding
to 26 true positive samples including 2 samples not initially detected by
individual RT-PCR but confirmed positive by further RT-PCR and RT-dPCR
investigation. For groups of 16, 15 groups tested positive, corresponding to 25
true positive samples identified. 100% concordance is found for groups of 32
but with limited data points.

The Annulated Treeboa (Corallus annulatus) is one of nine currently
recognized species in the boid genus Corallus. Its disjunct range extends from
eastern Guatemala into northern Honduras, southeastern Nicaragua, northeastern
Costa Rica, and southwestern Panama to northern Colombia west of the Andes. It
is the only species of Corallus found on the Caribbean versant of Costa Rica,
where it occurs at elevations to at least 650m and perhaps as high as 1,000m.
Corallus annulatus occurs mostly in primary and secondary lowland tropical wet
and moist rainforest and it appears to be genuinely rare. Besides C. cropanii
and C. blombergi (the latter closely related to C. annulatus), it is the rarest
member of the genus. Aside from information on habitat and activity, little is
known regarding its natural history.

{\mu}Manager, an open-source microscopy acquisition software, has been an
essential tool for many microscopy experiments over the past 15 years, but is
not easy to use for experiments in which image acquisition and analysis are
closely coupled. This is because {\mu}Manager libraries are written in C++ and
Java, whereas image processing is increasingly carried out with data science
and machine learning tools most easily accessible through the Python
programming language. We present Pycro-Manager, a tool that enables rapid
development of such experiments, while also providing access to the wealth of
existing tools within {\mu}Manager through Python.

Recent research has shown there can be detrimental neurological effects of
short- and long-term exposure to contact sports. In the present study,
metabolomic profiling was combined with inflammatory miRNA quantification,
computational behavior with virtual reality (VR) testing of motor control, and
head collision event monitoring to explore trans-omic and collision effects on
human behavior across a season of players on a collegiate American football
team. We integrated permutation-based statistics with mediation analyses to
test complex, directional relationships between miRNAs, metabolites, and VR
task performance. Fourteen significant mediations (metabolite = mediator; miRNA
= independent variable; VR score = dependent variable) were discovered at
preseason (N=6) and across season (N=8) with Sobel p-values less than or equal
to 0.05 and with total effects at or exceeding 50%. The majority of mediation
findings involved long to medium chain fatty acids (2-HG, 8-HOA, UND, sebacate,
suberate, and heptanoate). In parallel, TCA metabolites were found to be
significantly decreased at postseason relative to preseason. HAEs were
associated with metabolomic measures and miRNA levels across-season. Together,
these observations suggest a state of chronic HAE-induced neuroinflammation (as
evidence by elevated miRNAs) and mitochondrial dysfunction (as observed by
abnormal FAs and TCA metabolites) that together produce subtle changes in
neurological function (as observed by impaired motor control behavior). These
findings point to a shift in mitochondrial metabolism, away from mitochondria
function, consistent with other illnesses classified as mitochondrial
disorders, suggesting a plausible mechanism underlying HAEs in contact sports
and potential avenue for treatment intervention.

Collision sports athletes experience many head acceleration events (HAEs) per
season. The effects of these subconcussive events are largely understudied
since HAEs may produce no overt symptoms, and are likely to diffusely manifest
across multiple scales of study (e.g., molecular, cellular network, and
behavior). This study integrated resting-state fMRI with metabolome,
transcriptome and computational virtual reality (VR) behavior measures to
assess the effects of exposure to HAEs on players in a collegiate American
football team. Permutation-based mediation and moderation analysis was used to
investigate relationships between network fingerprint, changes in omic measures
and VR metrics over the season. Change in an energy cycle fatty acid,
tridecenedioate, moderated the relationship between 1) miR-505 and DMN
fingerprint and 2) the relationship between DMN fingerprint and worsening VR
Balance measures (all p less than or equal to 0.05). In addition, the
similarity in DMN over the season was negatively related to cumulative number
of HAEs above 80G, and DMN fingerprint was less similar across the season in
athletes relative to age-matched non-athletes. miR-505 was also positively
related to average number of HAEs above 25G per session. It is important to
note that tridecenedioate has a double bond making it a candidate for ROS
scavenging. These findings between a candidate ROS-related metabolite,
inflammatory miRNA, altered brain imaging and diminished behavioral performance
suggests that impact athletes may experience chronic neuroinflammation. The
rigorous permutation-based mediation/moderation may provide a methodology for
investigating complex multi-scale biological data within humans alone and thus
assist study of other functional brain problems.

We proposed a dual-template multi-cycled DNA nanomachine driven by polymerase
nicking enzyme with high efficiency. The reaction system simply consists of two
templates (T1, T2) and two enzymes (KF polymerase, Nb.BbvCI). The two templates
are similar in structure (X-X-Y, Y-Y-C): primer recognition region, primer
analogue generation region, output region (3 to 5), and there is a nicking site
between each two regions. Output of T1 is the primer of T2 and G-rich fragment
(G3) is designed as the final products. In the presence of HIV-1, numerous of
G3 were generated owing to the multi-cycled amplification strategy and formed
into G-triplex ThT complex after the addition of thioflavin T (ThT), which
greatly enhanced the fluorescence intensity as signal reporter in the
label-free sensing strategy. A dynamic response range of 50 fM-2 nM for HIV-1
gene detection can be achieved through this multi-cycled G-triplex machine, and
benefit from the high efficiency amplification strategy, enzymatic reaction can
be completed within 45 minutes followed by fluorescence measurement. In
addition, analysis of other targets can be achieved by replacing the template
sequence. Thus there is a certain application potential for trace biomarker
analysis in this strategy.

With the advancement of high-throughput biotechnologies, we increasingly
accumulate biomedical data about diseases, especially cancer. There is a need
for computational models and methods to sift through, integrate, and extract
new knowledge from the diverse available data to improve the mechanistic
understanding of diseases and patient care. To uncover molecular mechanisms and
drug indications for specific cancer types, we develop an integrative framework
able to harness a wide range of diverse molecular and pan-cancer data. We show
that our approach outperforms competing methods and can identify new
associations. Furthermore, through the joint integration of data sources, our
framework can also uncover links between cancer types and molecular entities
for which no prior knowledge is available. Our new framework is flexible and
can be easily reformulated to study any biomedical problems.

Aggregates of misfolded tau proteins (or just 'tau' for brevity) play a
crucial role in the progression of Alzheimer's disease (AD) as they correlate
with cell death and accelerated tissue atrophy. Longitudinal positron emission
tomography (PET) scans can be used quantify the extend of abnormal tau spread.
Such PET-based image biomarkers are a promising technology for AD diagnosis and
prognosis. Here, we propose to calibrate an organ-scale biophysical
mathematical model using longitudinal PET scans to extract characteristic
growth patterns and spreading of tau. The biophysical model is a
reaction-advection-diffusion partial differential equation (PDE) with only two
scalar unknown parameters, one representing the spreading (the diffusion part
of the PDE) and the other one the growth of tau (the reaction part of the PDE).
The advection term captures tissue atrophy and is obtained from diffeomorphic
registration of longitudinal magnetic resonance imaging (MRI) scans. We
describe the method, present a numerical scheme for the calibration of the
growth and spreading parameters, perform a sensitivity study using synthetic
data, and we perform a preliminary evaluation on clinical scans from the ADNI
dataset. We study whether such model calibration is possible and investigate
the sensitivity of such calibration to the time between consecutive scans and
the presence of atrophy. Our findings show that despite using only two
calibration parameters, the model can reconstruct clinical scans quite
accurately. We discovered that small time intervals between scans and the
presence of background noise create difficulties. Our reconstructed model fits
the data well, yet the study on clinical data also reveals shortcomings of the
simplistic model. Interestingly, the parameters show significant variability
across patients, an indication that these parameters could be useful
biomarkers.

COVID-19 has fast-paced drug re-positioning for its treatment. This work
builds computational models for the same. The aim is to assist clinicians with
a tool for selecting prospective antiviral treatments. Since the virus is known
to mutate fast, the tool is likely to help clinicians in selecting the right
set of antivirals for the mutated isolate.
  The main contribution of this work is a manually curated database publicly
shared, comprising of existing associations between viruses and their
corresponding antivirals. The database gathers similarity information using the
chemical structure of drugs and the genomic structure of viruses. Along with
this database, we make available a set of state-of-the-art computational drug
re-positioning tools based on matrix completion. The tools are first analysed
on a standard set of experimental protocols for drug target interactions. The
best performing ones are applied for the task of re-positioning antivirals for
COVID-19. These tools select six drugs out of which four are currently under
various stages of trial, namely Remdesivir (as a cure), Ribavarin (in
combination with others for cure), Umifenovir (as a prophylactic and cure) and
Sofosbuvir (as a cure). Another unanimous prediction is Tenofovir alafenamide,
which is a novel tenofovir prodrug developed in order to improve renal safety
when compared to the counterpart tenofovir disoproxil. Both are under trail,
the former as a cure and the latter as a prophylactic. These results establish
that the computational methods are in sync with the state-of-practice. We also
demonstrate how the selected drugs change as the SARS-Cov-2 mutates over time,
suggesting the importance of such a tool in drug prediction.
  The dataset and software is available publicly at
https://github.com/aanchalMongia/DVA and the prediction tool with a
user-friendly interface is available at http://dva.salsa.iiitd.edu.in.

BioStatFlow is a free web application, useful to facilitate the performance
of statistical analyses of "omics", including metabolomics, data using R
packages. It is a fast and easy on-line tool for biologists who are not experts
in univariate and multivariate statistics, do not have time to learn R
language, and only have basic notions in biostatistics. It guides the biologist
through the different steps of a statistical workflow, from data normalization
and imputation of missing data to univariate and multivariate analyses. It also
includes tools to reconstruct and visualize networks based on correlations. All
outputs are easily saved in a session or downloaded. New analytical modules can
be easily included upon request. BioStatFlow is available online:
http://biostatflow.org

Limitations in the design of the experiment of Boulware et al[1] are
considered in Cohen[2]. They are not subject to correction but they are
reported for readers' consideration. However, they made an analysis for the
incidence based on Fisher's hypothesis test for means while they published
detailed time dependent data which were not analyzed, disregarding an important
information. Here we make the analyses with this time dependent data adopting a
simple regression analysis.
  We conclude their randomized, double-blind, placebo-controlled trial presents
statistical evidence, at 99% confidence level, that the treatment of Covid-19
patients with hydroxychloroquine is effective in reducing the appearance of
symptoms if used before or right after exposure to the virus. For 0 to 2 days
after exposure to virus, the estimated relative reduction in symptomatic
outcomes is 72% after 0 days, 48.9% after 1 day and 29.3% after 2 days. For 3
days after exposure, the estimated relative reduction is 15.7% but results are
not statistically conclusive and for 4 or more days after exposure there is no
statistical evidence that hydroxychloroquine is effective in reducing the
appearance of symptoms.
  Our results show that the time elapsed between infection and the beginning of
treatment is crucial for the efficacy of hydroxychloroquine as a treatment to
Covid-19.

The genes that encode the targets of most therapies do not have rare variants
with large-effect or common variants with moderate effects on the biomarker
reflecting the pharmacologic action of the corresponding therapy. Therefore,
providing genetic target validation for most therapies is challenging. Novel
methods are being developed to combine multiple variants in the gene encoding
the target of a therapy that are weakly associated with the biomarker
reflecting the pharmacologic action of that therapy into a genetic score that
can be used as an adequate instrumental variable. We describe one approach to
solve this important problem.

Many systems on our planet are known to shift abruptly and irreversibly from
one state to another when they are forced across a "tipping point," such as
mass extinctions in ecological networks, cascading failures in infrastructure
systems, and social convention changes in human and animal networks. Such a
regime shift demonstrates a system's resilience that characterizes the ability
of a system to adjust its activity to retain its basic functionality in the
face of internal disturbances or external environmental changes. In the past 50
years, attention was almost exclusively given to low dimensional systems and
calibration of their resilience functions and indicators of early warning
signals without considerations for the interactions between the components.
Only in recent years, taking advantages of the network theory and lavish real
data sets, network scientists have directed their interest to the real-world
complex networked multidimensional systems and their resilience function and
early warning indicators. This report is devoted to a comprehensive review of
resilience function and regime shift of complex systems in different domains,
such as ecology, biology, social systems and infrastructure. We cover the
related research about empirical observations, experimental studies,
mathematical modeling, and theoretical analysis. We also discuss some ambiguous
definitions, such as robustness, resilience, and stability.

The disease trajectory for clinical sepsis, in terms of temporal cytokine and
phenotypic dynamics, can be interpreted as a random dynamical system. The
ability to make accurate predictions about patient state from clinical
measurements has eluded the biomedical community, primarily due to the paucity
of relevant and high-resolution data. We have utilized two distinct neural
network architectures, Long Short-Term Memory and Multi-Layer Perceptron, to
take a time sequence of five measurements of eleven simulated serum cytokine
concentrations as input and to return both the future cytokine trajectories as
well as an aggregate metric representing the patient's state of health. The
neural networks converged within 50 epochs for cytokine trajectory predictions
and health-metric regressions, with the expected amount of error (due to
stochasticity in the simulation). The mapping from a specific cytokine profile
to a state-of-health is not unique, and increased levels of inflammation result
in less accurate predictions. Due to the propagation of machine learning error
combined with computational model stochasticity over time, the network should
be re-grounded in reality daily as predictions can diverge from the true model
trajectory as the system evolves towards a probabilistic basin of attraction.
This work serves as a proof-of-concept for the use of artificial neural
networks to predict disease progression in sepsis. This work is not intended to
replace a trained clinician, rather the goal is to augment intuition with
quantifiable statistical information to help them make the best decisions. We
note that this relies on a valid computational model of the system in question
as there does not exist sufficient data to inform a machine-learning trained,
artificially intelligent, controller.

In this study, we investigated the inhibition of SARS-CoV-2 spike
glycoprotein with HIV drugs and their combinations. This glycoprotein is
essential for the reproduction of the SARS-COV-2 virus, so its inhibition opens
new avenues for the treatment of patients with COVID-19 disease. In doing so,
we used the VINI in silico model of cancer, whose high accuracy in finding
effective drugs and their combinations was confirmed in vitro by comparison
with existing results from NCI-60 bases, and in vivo by comparison with
existing clinical trial results. In the first step, the VINI model calculated
the inhibition efficiency of SARS-CoV-2 spike glycoprotein with 44 FDA-approved
antiviral drugs. Of these drugs, HIV drugs have been shown to be effective,
while others mainly have shown weak or no efficiency. Subsequently, the VINI
model calculated the inhibition efficiency of all possible double and triple
HIV drug combinations, and among them identified ten with the highest
inhibition efficiency. These ten combinations were analyzed by Medscape
drug-drug interaction software and LexiComp Drug Interactions. All combinations
except the combination of cobicistat_abacavir_rilpivirine appear to have
serious interactions (risk rating category D) when dosage
adjustments/reductions are required for possible toxicity. Finally, the VINI
model compared the inhibition efficiency of cobicistat_abacivir_rilpivirine
combination with cocktails and individual drugs already used or planned to be
tested against SARS-CoV-2. Combination cobicistat_abacivir_rilpivirine
demonstrated the highest inhibition of SARS-CoV-2 spike glycoprotein over
others. Thus, this combination seems to be a promising candidate for the
further in vitro testing and clinical trials.

Single-cell RNA sequencing provides tremendous insights to understand
biological systems. However, the noise from dropout can corrupt the downstream
biological analysis. Hence, it is desirable to impute the dropouts accurately.
In this work, we propose a simple and powerful dropout imputation method
(scGNN) by applying a bottlenecked Graph Convolutional Neural Network on an
induced hierarchical cell similarity graph. We show scGNN has competitive
performance against state-of-the-art baselines across three datasets and can
improve downstream analysis.

Biochemical processes in cells are governed by complex networks of many
chemical species interacting stochastically in diverse ways and on different
time scales. Constructing microscopically accurate models of such networks is
often infeasible. Instead, here we propose a systematic framework for building
phenomenological models of such networks from experimental data, focusing on
accurately approximating the time it takes to complete the process, the First
Passage (FP) time. Our phenomenological models are mixtures of Gamma
distributions, which have a natural biophysical interpretation. The complexity
of the models is adapted automatically to account for the amount of available
data and its temporal resolution. The framework can be used for predicting the
behavior of various FP systems under varying external conditions. To
demonstrate the utility of the approach, we build models for the distribution
of inter-spike intervals of a morphologically complex neuron, a Purkinje cell,
from experimental and simulated data. We demonstrate that the developed models
can not only fit the data but also make nontrivial predictions. We demonstrate
that our coarse-grained models provide constraints on more mechanistically
accurate models of the involved phenomena.

Mathematical modelling allows us to concisely describe fundamental principles
in biology. Analysis of models can help to both explain known phenomena, and
predict the existence of new, unseen behaviours. Model analysis is often a
complex task, such that we have little choice but to approach the problem with
computational methods. Numerical continuation is a computational method for
analysing the dynamics of nonlinear models by algorithmically detecting
bifurcations. Here we aim to promote the use of numerical continuation tools by
providing an introduction to nonlinear dynamics and numerical bifurcation
analysis. Many numerical continuation packages are available, covering a wide
range of system classes; a review of these packages is provided, to help both
new and experienced practitioners in choosing the appropriate software tools
for their needs.

Background: Human gait exhibits complex fractal fluctuations among
consecutive strides. The time series of gait parameters are long-range
correlated (statistical persistence). In contrast, when gait is synchronized
with external rhythmic cues, the fluctuation regime is modified to stochastic
oscillations around the target frequency (statistical anti-persistence). To
highlight these two fluctuation modes, the prevalent methodology is the
detrended fluctuation analysis (DFA). The DFA outcome is the scaling exponent,
which lies between 0.5 and 1 if the time series exhibit long-range
correlations, and below 0.5 if the time series is anti-correlated. A
fundamental assumption for applying DFA is that the analyzed time series
results from a time-invariant generating process. However, a gait time series
may be constituted by an ensemble of sub-segments with distinct fluctuation
regimes (e.g., correlated and anti-correlated). Methods: Several proportions of
correlated and anti-correlated time series were mixed together and then
analyzed through DFA. The original (before mixing) time series were generated
via autoregressive fractionally integrated moving average (ARFIMA) modelling or
actual gait data. Results: Results evidenced a nonlinear sensitivity of DFA to
the mix of correlated and anti-correlated series. Notably, adding a small
proportion of correlated segments into an anti-correlated time series had
stronger effects than the reverse. Significance: In case of changes in gait
control during a walking trial, the resulting time series may be a patchy
ensemble of several fluctuation regimes. When applying DFA, the scaling
exponent may be misinterpreted. Cued walking studies may be most at risk of
suffering this issue in cases of sporadic synchronization with external cues.

Background: IGEDEPP (Interaction of Gene and Environment of Depression during
PostPartum) is a prospective multicenter cohort study of 3,310 Caucasian women
who gave birth between 2011 and 2016, with follow-up until one year postpartum.
The aim of the current study is to describe the cohort and estimate the
prevalence and cumulative incidence of early and late postpartum depression
(PPD). Methods: Socio-demographic data, personal and family psychiatric
history, as well as stressful life events during childhood and pregnancy were
evaluated at baseline. Early and late PPD were assessed at 8 weeks and 1 year
postpartum respectively, using DSM-5 criteria. Results: The prevalence of early
PPD was 8.3% (95%CI 7.3-9.3), and late PPD 12.9% (95%CI 11.5-14.2), resulting
in an 8-week cumulative incidence of 8.5% (95%CI 7.4-9.6) and a one-year
cumulative incidence of PPD of 18.1% (95%CI: 17.1-19.2). Nearly half of the
cohort (N=1571, 47.5%) had a history of at least one psychiatric or addictive
disorder, primarily depressive disorder (35%). Almost 300 women in the cohort
(9.0%) reported childhood trauma. During pregnancy, 47.7% women experienced a
stressful event, 30.2% in the first 8 weeks and 43.9% between 8 weeks and one
year postpartum. Nearly one in five women reported at least one stressful
postpartum event at 8 weeks. Conclusion: Incident depressive episodes affected
nearly one in five women during the first year postpartum. Most women had
stressful perinatal events. Further IGEDEPP studies will aim to disentangle the
impact of childhood and pregnancy-related stressful events on postpartum mental
disorders.

Understanding and modelling the complexity of the immune system is a
challenge that is shared by the ImmunoComplexiT$^1$ thematic network from the
RNSC. The immune system is a complex biological, adaptive, highly diversified,
self-organized and degenerative cognitive network of entities, allowing for a
robust and resilient system with emergent properties such as anamnestic
responses and regulation. The adaptive immune system has evolved into a complex
system of billions of highly diversified lymphocytes all interacting as a
connective dynamic, multi-scale organised and distributed system, in order to
collectively insure body and species preservation. The immune system is
characterized by complexity at different levels: network organisation through
fluid cell populations with inter-and intra-cell signalling, lymphocyte
receptor diversity, cell clonotype selection and competition at cell level,
migration and interaction inside the immunological tissues and fluid
dissemination through the organism, homeostatic regulation while rapid
adaptation to a changing environment.

This technical note introduces parametric dynamic causal modelling, a method
for inferring slow changes in biophysical parameters that control fluctuations
of fast neuronal states. The application domain we have in mind is inferring
slow changes in variables (e.g., extracellular ion concentrations or synaptic
efficacy) that underlie phase transitions in brain activity (e.g., paroxysmal
seizure activity). The scheme is efficient and yet retains a biophysical
interpretation, in virtue of being based on established neural mass models that
are equipped with a slow dynamic on the parameters (such as synaptic rate
constants or effective connectivity). In brief, we use an adiabatic
approximation to summarise fast fluctuations in hidden neuronal states (and
their expression in sensors) in terms of their second order statistics; namely,
their complex cross spectra. This allows one to specify and compare models of
slowly changing parameters (using Bayesian model reduction) that generate a
sequence of empirical cross spectra of electrophysiological recordings.
Crucially, we use the slow fluctuations in the spectral power of neuronal
activity as empirical priors on changes in synaptic parameters. This introduces
a circular causality, in which synaptic parameters underwrite fast neuronal
activity that, in turn, induces activity-dependent plasticity in synaptic
parameters. In this foundational paper, we describe the underlying model,
establish its face validity using simulations and provide an illustrative
application to a chemoconvulsant animal model of seizure activity.

Volumetric measurements are known to provide more information when it comes
to segmenting tumors, in comparison to one- and two-dimensional measurements,
and thus can lead to better informed therapy. In this work, we review the free
and easily accessible computer platforms available for conducting these 3D
measurements, such as Horos and 3D Slicer and compare the segmentations to
commercial Visage software. We compare the time for 3D segmentation of tumors
and demonstrate how to use a novel plugin that we developed in 3D slicer for
the efficient and accurate segmentation of the cystic component of a tumor.

The use of multiple drugs accounts for almost 30% of all hospital admission
and is the 5th leading cause of death in America. Since over 30% of all adverse
drug events (ADEs) are thought to be caused by drug-drug interactions (DDI),
better identification and prediction of administration of known DDIs in primary
and secondary care could reduce the number of patients seeking urgent care in
hospitals, resulting in substantial savings for health systems worldwide along
with better public health. However, current DDI prediction models are prone to
confounding biases along with either inaccurate or a lack of access to
longitudinal data from Electronic Health Records (EHR) and other drug
information such as FDA Adverse Event Reporting System (FAERS) which continue
to be the main barriers in measuring the prevalence of DDI and characterizing
the phenomenon in medical care. In this review, analytical models including
Label Propagation using drug side effect data and Supervised Learning DDI
Prediction model using Drug-Gene interactions (DGIs) data are discussed.
Improved identification of DDIs in both of these models compared to previous
versions are highlighted while limitations that include bias, inaccuracy, and
insufficient data are also assessed. A case study of Psoriasis DDI prediction
by DGI data using Random Forest Classifier was studied. Transfer Matrix
Recurrent Neural Networks (TM-RNN) that address the above limitations are
discussed in future works.

The prediction of protein interactions (CPIs) is crucial for the in-silico
screening step in drug discovery. Recently, many end-to-end representation
learning methods using deep neural networks have achieved significantly better
performance than traditional machine learning algorithms. Much effort has
focused on the compound representation or the information extraction from the
compound-protein interaction to improve the model capability by taking the
advantage of the neural attention mechanism. However, previous studies have
paid little attention to representing the protein sequences, in which the
long-range interactions of residue pairs are essential for characterizing the
structural properties arising from the protein folding. We incorporate the
self-attention mechanism into the protein representation module for CPI
modeling, which aims at capturing the long-range interaction information within
proteins. The proposed module concerning protein representation, called Protein
Transformer, with an integration with an existing CPI model, has shown a
significant improvement in the prediction performance when compared with
several existing CPI models.

Genome-scale stoichiometric modeling of metabolism has become a standard
systems biology tool for modeling cellular physiology and growth. Extensions of
this approach are also emerging as a valuable avenue for predicting,
understanding and designing microbial communities. COMETS (Computation Of
Microbial Ecosystems in Time and Space) was initially developed as an extension
of dynamic flux balance analysis, which incorporates cellular and molecular
diffusion, enabling simulations of multiple microbial species in spatially
structured environments. Here we describe how to best use and apply the most
recent version of this platform, COMETS 2, which incorporates a more accurate
biophysical model of microbial biomass expansion upon growth, as well as
several new biological simulation modules, including evolutionary dynamics and
extracellular enzyme activity. COMETS 2 provides user-friendly Python and
MATLAB interfaces compatible with the well-established COBRA models and
methods, and comprehensive documentation and tutorials, facilitating the use of
COMETS for researchers at all levels of expertise with metabolic simulations.
This protocol provides a detailed guideline for installing, testing and
applying COMETS 2 to different scenarios, with broad applicability to microbial
communities across biomes and scales.

Cox-nnet is a neural-network based prognosis prediction method, originally
applied to genomics data. Here we propose the version 2 of Cox-nnet, with
significant improvement on efficiency and interpretability, making it suitable
to predict prognosis based on large-scale electronic medical records (EMR)
datasets. We also add permutation-based feature importance scores and the
direction of feature coefficients. Applying on an EMR dataset of OPTN kidney
transplantation, Cox-nnet v2.0 reduces the training time of Cox-nnet up to 32
folds (n=10,000) and achieves better prediction accuracy than Cox-PH (p<0.05).
Availability and implementation: Cox-nnet v2.0 is freely available to the
public at https://github.com/lanagarmire/Cox-nnet-v2.0

We present experimental results demonstrating that, relative to continuous
illumination, an increase of a factor of 3-10 in the photon efficiency of algal
photo-synthesis is attainable via the judicious application of pulsed light for
light intensities of practical interest (e.g., average-to-peak solar photon
flux). We also propose a simple model that can account for all the
measurements. The model (1) reflects the essential rate-limiting elements in
bio-productivity, (2) incorporates the impact of photon arrival-time statistics
and (3) accounts for how the enhancement in photon efficiency depends on the
timescales of light pulsing and photon flux density. The key is avoiding
clogging of the photosynthetic pathway by properly timing the light-dark cycles
experienced by algal cells. We show how this can be realized with pulsed light
sources, or by producing pulsed-light effects from continuous illumination via
turbulent mixing in dense algal cultures in thin photo-bioreactors.

Motivation: Accurate estimation of false discovery rate (FDR) of spectral
identification is a central problem in mass spectrometry-based proteomics. Over
the past two decades, target decoy approaches (TDAs) and decoy-free approaches
(DFAs), have been widely used to estimate FDR. TDAs use a database of decoy
species to faithfully model score distributions of incorrect peptide-spectrum
matches (PSMs). DFAs, on the other hand, fit two-component mixture models to
learn the parameters of correct and incorrect PSM score distributions. While
conceptually straightforward, both approaches lead to problems in practice,
particularly in experiments that push instrumentation to the limit and generate
low fragmentation-efficiency and low signal-to-noise-ratio spectra. Results: We
introduce a new decoy-free framework for FDR estimation that generalizes
present DFAs while exploiting more search data in a manner similar to TDAs. Our
approach relies on multi-component mixtures, in which score distributions
corresponding to the correct PSMs, best incorrect PSMs, and second-best
incorrect PSMs are modeled by the skew normal family. We derive EM algorithms
to estimate parameters of these distributions from the scores of best and
second-best PSMs associated with each experimental spectrum. We evaluate our
models on multiple proteomics datasets and a HeLa cell digest case study
consisting of more than a million spectra in total. We provide evidence of
improved performance over existing DFAs and improved stability and speed over
TDAs without any performance degradation. We propose that the new strategy has
the potential to extend beyond peptide identification and reduce the need for
TDA on all analytical platforms.

Investigating molecular heterogeneity provides insights about tumor origin
and metabolomics. The increasing amount of data gathered makes manual analyses
infeasible - therefore, automated unsupervised learning approaches are utilized
for discovering heterogeneity. However, automated unsupervised analyses require
a lot of experience with setting their hyperparameters and usually an upfront
knowledge about the number of expected substructures. Moreover, numerous
measured molecules require an additional step of feature engineering to provide
valuable results. In this work, we propose DiviK: a scalable stepwise algorithm
with local data-driven feature space adaptation for the segmentation of
high-dimensional datasets. The combination of three quality indices: Dice
Index, Rand Index and EXIMS score are used to assess the quality of
unsupervised analyses in 3D space. DiviK was validated on two separate
high-throughput datasets acquired by Mass Spectrometry Imaging in 2D and 3D.
DiviK could be one of the default choices to consider during the initial
exploration of Mass Spectrometry Imaging data. It provides a trade-off between
absolute heterogeneity detection and focus on biologically plausible
structures, and does not require specifying the number of expected structures
before the analysis. With its unique local feature space adaptation, it is
robust against dominating global patterns when focusing on the detail. Finally,
due to its simplicity, DiviK is easily generalizable to an even more flexible
framework, useful for other '-omics' data, or tabular data in general
(including medical images after appropriate embedding). A generic
implementation is freely available under Apache 2.0 license at
https://github.com/gmrukwa/divik.

Sepsis accounts for more than 50% of hospital deaths, and the associated cost
ranks the highest among hospital admissions in the US. Improved understanding
of disease states, severity, and clinical markers has the potential to
significantly improve patient outcomes and reduce cost. We develop a
computational framework that identifies disease states in sepsis using clinical
variables and samples in the MIMIC-III database. We identify six distinct
patient states in sepsis, each associated with different manifestations of
organ dysfunction. We find that patients in different sepsis states are
statistically significantly composed of distinct populations with disparate
demographic and comorbidity profiles. Collectively, our framework provides a
holistic view of sepsis, and our findings provide the basis for future
development of clinical trials and therapeutic strategies for sepsis.

Metaproteomics are becoming widely used in microbiome research for gaining
insights into the functional state of the microbial community. Current
metaproteomics studies are generally based on high-throughput tandem mass
spectrometry (MS/MS) coupled with liquid chromatography. The identification of
peptides and proteins from MS data involves the computational procedure of
searching MS/MS spectra against a predefined protein sequence database and
assigning top-scored peptides to spectra. Existing computational tools are
still far from being able to extract all the information out of large MS/MS
datasets acquired from metaproteome samples. In this paper, we proposed a
deep-learning-based algorithm, called DeepFilter, for improving the rate of
confident peptide identifications from a collection of tandem mass spectra.
Compared with other post-processing tools, including Percolator, Q-ranker,
PeptideProphet, and Iprophet, DeepFilter identified 20% and 10% more
peptide-spectrum-matches and proteins, respectively, on marine microbial and
soil microbial metaproteome samples with false discovery rate at 1%.

Choosing appropriate hyperparameters for unsupervised clustering algorithms
in an optimal way depending on the problem under study is a long standing
challenge, which we tackle while adapting clustering algorithms for immune
disorder diagnoses. We compare the potential ability of unsupervised clustering
algorithms to detect disease flares and remission periods through analysis of
laboratory data from systemic lupus erythematosus patients records with
different hyperparameter choices. To determine which clustering strategy is the
best one we resort to a Bayesian analysis based on the Plackett-Luce model
applied to rankings. This analysis quantifies the uncertainty in the choice of
clustering methods for a given problem

Motivation: Accurate data analysis and quality control is critical for
metagenomic studies. Though many tools exist to analyze metagenomic data there
is no consistent framework to integrate and run these tools across projects.
Currently, computational analysis of metagenomes is time consuming, often
misses potentially interesting results, and is difficult to reproduce. Further,
comparison between metagenomic studies is hampered by inconsistencies in tools
and databases.
  Results: We present the MetaSUB Core Analysis Pipeline (CAP) a comprehensive
tool to analyze metagenomes and summarize the results of a project. The CAP is
designed in a bottom up fashion to perform QC, preprocessing, analysis and even
to build relevant databases and install necessary tools.
  Availability and Implementation: The CAP is available under an MIT License on
GitHub at https://github.com/MetaSUB/CAP2 and on the Python Package Index.
Documentation and examples are available on GitHub.

Drug-target interaction (DTI) prediction has become a foundational task in
drug repositioning, polypharmacology, drug discovery, as well as drug
resistance and side-effect prediction. DTI identification using machine
learning is gaining popularity in these research areas. Through the years,
numerous deep learning methods have been proposed for DTI prediction.
Nevertheless, prediction accuracy and efficiency remain key challenges.
Pharmaco-electroencephalogram (pharmaco-EEG) is considered valuable in the
development of central nervous system-active drugs. Quantitative EEG analysis
demonstrates high reliability in studying the effects of drugs on the brain.
Earlier preclinical pharmaco-EEG studies showed that different types of drugs
can be classified according to their mechanism of action on neural activity.
Here, we propose a convolutional neural network for EEG-mediated DTI
prediction. This new approach can explain the mechanisms underlying complicated
drug actions, as it allows the identification of similarities in the mechanisms
of action and effects of psychotropic drugs.

Background: After a decade of a treatment as prevention (TasP) strategy based
on progressive HIV testing scale-up and earlier treatment, a reduction in the
estimated number of new infections in men-who-have-sex-with-men (MSM) in
England had yet to be identified by 2010. To achieve internationally agreed
targets for HIV control and elimination, test-and-treat prevention efforts have
been dramatically intensified over the period 2010-2015, and, from 2016,
further strengthened by pre-exposure prophylaxis (PrEP).
  Methods: Application of a novel age-stratified back-calculation approach to
data on new HIV diagnoses and CD4 count-at-diagnosis, enabled age-specific
estimation of HIV incidence, undiagnosed infections and mean time-to-diagnosis
across both the 2010-2015 and 2016-2018 periods. Estimated incidence trends
were then extrapolated, to quantify the likelihood of achieving HIV elimination
by 2030.
  Findings: A fall in HIV incidence in MSM is estimated to have started in
2012/3, eighteen months before the observed fall in new diagnoses. A steep
decrease from 2,770 annual infections (95% credible interval 2.490-3,040) in
2013 to 1,740 (1,500-2,010) in 2015 is estimated, followed by steady decline
from 2016, reaching 854 (441-1,540) infections in 2018. A decline is
consistently estimated in all age groups, with a fall particularly marked in
the 24-35 age group, and slowest in the 45+ group. Comparable declines are
estimated in the number of undiagnosed infections.
  Interpretation: The peak and subsequent sharp decline in HIV incidence
occurred prior to the phase-in of PrEP. Definining elimination as a public
health threat to be < 50 new infections (1.1 infections per 10,000 at risk),
40% of incidence projections hit this threshold by 2030. In practice, targeted
policies will be required, particularly among the 45+y where STIs are
increasing most rapidly.

Genome-wide epistasis analysis is a powerful tool to infer gene interactions,
which can guide drug and vaccine development and lead to a deeper understanding
of microbial pathogenesis. We have considered all complete SARS-CoV-2 genomes
deposited in the GISAID repository until \textbf{four} different cut-off dates,
and used Direct Coupling Analysis together with an assumption of Quasi-Linkage
Equilibrium to infer epistatic contributions to fitness from polymorphic loci.
We find \textbf{eight} interactions, of which three between pairs where one
locus lies in gene ORF3a, both loci holding non-synonymous mutations. We also
find interactions between two loci in gene nsp13, both holding non-synonymous
mutations, and four interactions involving one locus holding a synonymous
mutation. Altogether we infer interactions between loci in viral genes ORF3a
and nsp2, nsp12 and nsp6, between ORF8 and nsp4, and between loci in genes
nsp2, nsp13 and nsp14. The paper opens the prospect to use prominent
epistatically linked pairs as a starting point to search for combinatorial
weaknesses of recombinant viral pathogens.

Background: Limited research exists on the association between changes in
physical activity levels and injury in children. Objective: To assess how well
different variations of the acute:chronic workload ratio (ACWR), a measure of
change in activity, predict injury in children. Methods: We conducted a
prospective cohort study using data from 1670 Danish schoolchildren measured
over 5.5 years (2008 to 2014). Coupled 4-week, uncoupled 4-week, and uncoupled
5-week ACWRs were calculated using activity frequency in the past week as the
acute load (numerator), and average weekly activity frequency in the past 4 or
5 weeks as the chronic load (denominator). We modelled the relationship between
different ACWR variations and injury using generalized linear and generalized
additive models, with and without accounting for repeated measures. Results:
The prognostic relationship between the ACWR and injury risk was best
represented using a generalized additive mixed model for the uncoupled 5-week
ACWR. It predicted an injury risk of ~3% for ACWRs between 0.8 (activity level
decreased by 20%) and 1.5 (activity level increased by 50%). When activity
decreased by more than 20% (ACWR< 0.8), injury risk was lower (minimum of 1.5%
at ACWR=0). When activity increased by more than 50% (ACWR > 1.5), injury risk
was higher (maximum of 6% at ACWR = 5). Girls were at significantly higher risk
of injury than boys. Conclusion: Increases in physical activity in children are
associated with much lower injury risks compared to previous results in adults.

Phenotypic heterogeneity is a most fascinating property of a population of
cells, which shows the differences among individuals even with the same genetic
background and extracellular environmental conditions. However, the lack of
high-throughput analysis of phenotypic diversity has limited our research
progress. To deal with it, we constructed a novel parameter estimation method
in FACS-seq, a commonly used experimental framework, to achieve simultaneous
characterization of thousands of variants in a library. We further demonstrated
the model's ability in estimating the expression properties of each variant,
which we believe can help to decipher the mechanisms of phenotypic
heterogeneity.

Proteins are the active working horses in our body. These biomolecules
perform all vital cellular functions from DNA replication and general
biosynthesis to metabolic signaling and environmental sensing. While static 3D
structures are now readily available, observing the functional cycle of
proteins - involving conformational changes and interactions - remains very
challenging, e.g., due to ensemble averaging. However, time-resolved
information is crucial to gain a mechanistic understanding of protein function.
Single-molecule techniques such as FRET and force spectroscopies provide
answers but can be limited by the required labelling, a narrow time bandwidth,
and more. Here, we describe electrical nanopore detection as a tool for probing
protein dynamics. With a time bandwidth ranging from microseconds to hours, it
covers an exceptionally wide range of timescales that is very relevant for
protein function. First, we discuss the working principle of label-free
nanopore experiments, various pore designs, instrumentation, and the
characteristics of nanopore signals. In the second part, we review a few
nanopore experiments that solved research questions in protein science, and we
compare nanopores to other single-molecule techniques. We hope to make
electrical nanopore sensing more accessible to the biochemical community, and
to inspire new creative solutions to resolve a variety of protein dynamics -
one molecule at a time.

1.A goal of many research programs in biology is to extract meaningful
insights from large, complex data sets. Researchers in Ecology, Evolution and
Behavior (EEB) often grapple with long-term, observational data sets from which
they construct models to address fundamental questions about biology.
Similarly, epidemiologists analyze large, complex observational data sets to
understand the distribution and determinants of human health and disease. A key
difference in the analytical workflows for these two distinct areas of biology
is delineation of data analysis tasks and explicit use of causal inference
methods, widely adopted by epidemiologists. 2.Here, we review the most recent
causal inference literature and describe an analytical workflow that has direct
applications for EEB researchers. 3.The first half of this commentary defines
four distinct analytical tasks (description, prediction, association, and
causal inference), and the corresponding approaches to data analysis and model
selection. The latter half is dedicated to walking the reader through the steps
of casual inference, focusing on examples from EEB. 4.Given increasing interest
in causal inference and common misperceptions regarding the task of causal
inference, we aim to facilitate an exchange of ideas between disciplinary silos
and provide a framework for analyses of all data, though particularly relevant
for observational data.

Infrared thermography (IRT) is a valuable diagnostic tool in equine
veterinary medicine however, little is known about its application in donkeys.
The aim was to find patterns in thermal images of donkeys and horses, and
determine if these patterns share similarities. The study was carried out on 18
donkeys and 16 horses. All equids underwent thermal imaging with an infrared
camera and measuring the skin thickness and hair coat length. On the class maps
of each thermal image, 15 regions of interest (ROIs) were annotated and then
combined into 10 groups of ROIs (GORs). The existence of statistically
significant differences between surface temperatures in GORs was tested both
`globally' for all animals of a given species and `locally' for each animal.
Two special cases of animals that differ from the rest were also discussed. Our
results indicated that the majority of thermal patterns are similar for both
species however, average surface temperatures in horses are higher than in
donkeys. It may be related to differences in the skin and hair coat. We
concluded, the patterns of both species are associated with GORs, rather than
an individual ROI, with higher uniformity of donkeys patterns.

The aim of this study is to evaluate the integration of microbial
electrochemical technologies (MET) with anaerobic digestion (AD) to overcome AD
limitations caused by propionate accumulation. The study focuses on
understanding to what extent the inoculum impacts on the behaviour of the
integrated systems (AD-MET) from the perspective of propionate degradation,
methane production and microbial population dynamics. Three different inocula
were used: two from environmental sources (anaerobic sludge and river sediment)
and another one from a pre-enriched electroactive consortium adapted to
propionate degradation. Contrary to expectations, the reactor inoculated with
the pre-enriched consortium was not able to maintain its initial good
performance in the long run, and the bioelectrochemical activity collapsed
after three months of operation. In contrast, the reactor inoculated with
anaerobic sludge, although it required a relatively longer time to produce any
observable current, was able to maintain the electrogenic activity operation
(0.8 A.m-2) as well as the positive contribution of AD-MET integration to
tackle propionate accumulation and to enhance methane yield (338 mL.gCOD-1).
However, it must also be highlighted that from a purely energetic point of view
the AD-MET was not favorable.

The acute respiratory distress syndrome (ARDS) is characterized by the acute
development of diffuse alveolar damage (DAD) resulting in increased vascular
permeability and decreased alveolar gas exchange. Mechanical ventilation is a
potentially lifesaving intervention to improve oxygen exchange but has the
potential to cause ventilator-induced lung injury (VILI). A general strategy to
reduce VILI is to use low tidal volume and low-pressure ventilation, but
optimal ventilator settings for an individual patient are difficult for the
bedside physician to determine and mortality from ARDS remains unacceptably
high. Motivated by the need to minimize VILI, scientists have developed models
of varying complexity to understand diseased pulmonary physiology. However,
simple models often fail to capture real-world injury while complex models tend
to not be estimable with clinical data, limiting the clinical utility of
existing models. To address this gap, we present a physiologically anchored
data-driven model to better model lung injury. Our approach relies on using
clinically relevant features in the ventilator waveform data that contain
information about pulmonary physiology, patients-ventilator interaction and
ventilator settings. Our lung model can reproduce essential physiology and
pathophysiology dynamics of differently damaged lungs for both controlled mouse
model data and uncontrolled human ICU data. The estimated parameters values
that are correlated with a known measure of lung physiology agree with the
observed lung damage. In future endeavors, this model could be used to
phenotype ventilator waveforms and serve as a basis for predicting the course
of ARDS and improving patient care.

When addressing spatial biological questions using mathematical models,
symmetries within the system are often exploited to simplify the problem by
reducing its physical dimension. In a reduced-dimension model molecular
movement is restricted to the reduced dimension, changing the nature of
molecular movement. This change in molecular movement can lead to
quantitatively and even qualitatively different results in the full and reduced
systems. Within this manuscript we discuss the condition under which restricted
molecular movement in reduced-dimension models accurately approximates
molecular movement in the full system. For those systems which do not satisfy
the condition, we present a general method for approximating unrestricted
molecular movement in reduced-dimension models. We will derive a mathematically
robust, finite difference method for solving the 2D diffusion equation within a
1D reduced-dimension model. The methods described here can be used to improve
the accuracy of many reduced-dimension models while retaining benefits of
system simplification.

We present a novel approach for imaging the beating embryonic heart, based on
combining two independent imaging channels to capture the full spatio-temporal
information of the moving 3D structure. High-resolution, optically-sectioned
image recording is accompanied by simultaneous acquisition of low-resolution,
whole-heart recording, allowing the latter to be used in post-acquisition
processing to determine the macroscopic spatio-temporal phase of the heart
beating cycle. Once determined, or 'stamped', the phase information common to
both imaging channels is used to reconstruct the 3D beating heart. We
demonstrated our approach in imaging the beating heart of the zebrafish embryo,
capturing the entire heart over its full beating cycle, and characterizing
cellular dynamic behavior with sub-cellular resolution.

Neural methods of molecule property prediction require efficient encoding of
structure and property relationship to be accurate. Recent work using graph
algorithms shows limited generalization in the latent molecule encoding space.
We build a Transformer-based molecule encoder and property predictor network
with novel input featurization that performs significantly better than existing
methods. We adapt our model to semi-supervised learning to further perform well
on the limited experimental data usually available in practice.

Selection of appropriate tree species is an important forest management
decision that may affect sequestration of carbon (C) in soil. However,
information about tree species effects on soil C stocks at the global scale
remains unclear. Here, we quantitatively synthesized 850 observations from
field studies that were conducted in a common garden or monoculture plantations
to assess how tree species type (broadleaf vs. conifer), mycorrhizal
association (arbuscular mycorrhizal (AM) vs. ectomycorrhizal (ECM)), and
N-fixing ability (N-fixing vs. non-N-fixing), directly and indirectly, affect
topsoil (with a median depth of 10 cm) C concentration and stock, and how such
effects were influenced by environmental factors such as geographical location
and climate. We found that (1) tree species type, mycorrhizal association, and
N-fixing ability were all important factors affecting soil C, with lower forest
floor C stocks under broadleaved (44%), AM (39%), or N-fixing (28%) trees
respectively, but higher mineral soil C concentration (11%, 22%, and 156%) and
stock (9%, 10%, and 6%) under broadleaved, AM, and N-fixing trees respectively;
(2) tree species type, mycorrhizal association, and N-fixing ability affected
forest floor C stock and mineral soil C concentration and stock directly or
indirectly through impacting soil properties such as microbial biomass C and
nitrogen; (3) tree species effects on mineral soil C concentration and stock
were mediated by latitude, MAT, MAP, and forest stand age. These results reveal
how tree species and their specific traits influence forest floor C stock and
mineral soil C concentration and stock at a global scale. Insights into the
underlying mechanisms of tree species effects found in our study would be
useful to inform tree species selection in forest management or afforestation
aiming to sequester more atmospheric C in soil for mitigation of climate
change.

Cancer immunotherapy provides durable clinical benefit in only a small
fraction of patients, particularly due to a lack of reliable biomarkers for
accurate prediction of treatment outcomes and evaluation of response. Here, we
demonstrate the first application of label-free Raman spectroscopy for
elucidating biochemical changes induced by immunotherapy in the tumor
microenvironment. We used CT26 murine colorectal cancer cells to grow tumor
xenografts and subjected them to treatment with anti-CTLA-4 and anti-PD-L1
antibodies. Multivariate curve resolution - alternating least squares (MCR-ALS)
decomposition of Raman spectral dataset obtained from the treated and control
tumors revealed subtle differences in lipid, nucleic acid, and collagen content
due to therapy. Our supervised classification analysis using support vector
machines and random forests provided excellent prediction accuracies for both
immune checkpoint inhibitors and delineated important spectral markers specific
to each therapy, consistent with their differential mechanisms of action. Our
findings pave the way for in vivo studies of response to immunotherapy in
clinical patients using label-free Raman spectroscopy and machine learning.

Background: Depression in people with bipolar disorder is a major cause of
long-term disability, possibly leading to early mortality and currently,
limited safe and effective therapies exist. A double-blinded randomized
placebo-controlled trial (CEQUEL study) was conducted to evaluate the efficacy
of Lamotrigine plus Quetiapine versus Quetiapine monotherapy in patients with
bipolar type I or type II disorders.
  Objective: The objective of our study was to reanalyze CEQUEL data and
determine an unbiased classification accuracy for active lamotrigine versus
placebo. We also wanted to establish the time it took for the drug to provide
statistically significant outcomes.
  Methods: Between October 21, 2008 and April 27, 2012, 202 participants from
27 sites in United Kingdom were randomly assigned to two treatments; 101:
lamotrigine, 101: placebo. The primary variable used for estimating depressive
symptoms was based on the Quick Inventory of Depressive Symptomatology-self
report version 16 (QIDS-SR16). We analyze the data using feature engineering
and simple classifiers.
  Results: From weeks 10 to 14, the mean difference in QIDS-SR16 ratings
between the groups was -1.6317 (P=.09; sample size=81, 77; 95% CI -0.2403 to
3.5036). From weeks 48 to 52, the mean difference was -2.0032 (P=.09; sample
size=54, 48; 95% CI -0.3433 to 4.3497). The coefficient of variation and
detrended fluctuation analysis (DFA) exponent alpha had the greatest
explanatory power. The out-of-sample classification accuracy for the 138
participants who reported more than 10 times after week 12 was 62%. A
consistent classification accuracy higher than the no-information benchmark was
obtained in week 44.
  Conclusions: Lamotrigine plus Quetiapine treatment decreased depressive
symptoms in patients, but with substantial temporal instability. A trial of at
least 44 weeks was required to achieve consistent results.

Pressure Ulcer is one of the most problems in patients with bed rest.
Reposition and skin care are deterrent against the incidence of pressure ulcer.
Objective: This study aimed to analyze the effectiveness of sesame oil for the
prevention of pressure ulcer in patients with bed rest undergoing
hospitalization. Method: This study used a randomized controlled trial design.
Forty samples were divided groups: control and intervention groups. This study
was analysed using Chi Square. Results: The results showed that there was a
significant difference between two group (p=0,04). Conclusions: Skin care with
sesame oil can prevention of pressure ulcers. These results recommended that
sesame oil can be used for nursing intervention for the prevention of pressure
ulcers.

There has been a continuing demand for traditional and complementary medicine
worldwide. A fundamental and important topic in Traditional Chinese Medicine
(TCM) is to optimize the prescription and to detect herb regularities from TCM
data. In this paper, we propose a novel clustering model to solve this general
problem of herb categorization, a pivotal task of prescription optimization and
herb regularities. The model utilizes Random Walks method, Bayesian rules and
Expectation Maximization(EM) models to complete a clustering analysis
effectively on a heterogeneous information network. We performed extensive
experiments on the real-world datasets and compared our method with other
algorithms and experts. Experimental results have demonstrated the
effectiveness of the proposed model for discovering useful categorization of
herbs and its potential clinical manifestations.

The present study evaluated the capacity of a semi-closed, tubular horizontal
photobioreactor (PBR) to remove pesticides from agricultural run-off. The study
was carried out in July to study its efficiency under the best conditions
(highest solar irradiation). A total of 51 pesticides, including 10
transformation products, were selected and investigated based on their
consumption rate and environmental relevance. Sixteen of them were detected in
the agricultural run-off, and the estimated removal efficiencies ranged from
negative values, obtained for 3 compounds, namely terbutryn, diuron, and
imidacloprid, to 100%, achieved for 10 compounds. The acidic herbicide MCPA was
removed by 88% on average, and the insecticides 2,4-D and diazinon showed
variable removals, between 100% and negative values. The environmental risk
associated with the compounds still present in the effluent of the PBR was
evaluated using hazard quotients (HQs), calculated using the average and
highest measured concentrations of the compounds. HQ values > 10 (meaning high
risk) were obtained for imidacloprid (21), between 1 and 10 (meaning moderate
risk) for 2,4-D (2.8), diazinon (4.6) and terbutryn (1.5), and < 1 (meaning low
risk) for the remaining compounds diuron, linuron, and MCPA. The PBR treatment
yielded variable removals depending on the compound, similar to conventional
wastewater treatment plants. This study provides new data on the capacity of
icroalgae-based treatment systems to eliminate a wide range of priority
pesticides under real/environmental conditions.

The occurrence of the extensively used herbicide diuron in the environment
poses a severe threat to the ecosystem and human health. Four different
ligninolytic fungi were studied as biodegradation candidates for the removal of
diuron. Among them, T. versicolor was the most effective species, degrading
rapidly not only diuron (83%) but also the major metabolite 3,4-dichloroaniline
(100%), after 7-day incubation. During diuron degradation, five transformation
products (TPs) were found to be formed and the structures for three of them are
tentatively proposed. According to the identified TPs, a hydroxylated
intermediate 3-(3,4-dichlorophenyl)-1-hydroxymethyl-1-methylurea (DCPHMU) was
further metabolized into the N-dealkylated compounds
3-(3,4-dichlorophenyl)-1-methylurea (DCPMU) and 3,4-dichlorophenylurea (DCPU).
The discovery of DCPHMU suggests a relevant role of hydroxylation for
subsequent N-demethylation, helping to better understand the main reaction
mechanisms of diuron detoxification. Experiments also evidenced that
degradation reactions may occur intracellularly and be catalyzed by the
cytochrome P450 system. A response surface method, established by central
composite design, assisted in evaluating the effect of operational variables in
a trickle-bed bioreactor immobilized with T. versicolor on diuron removal. The
best performance was obtained at low recycling ratios and influent flow rates.
Furthermore, results indicate that the contact time between the contaminant and
immobilized fungi plays a crucial role in diuron removal. This study represents
a pioneering step forward amid techniques for bioremediation of
pesticides-contaminated waters using fungal reactors at a real scale.

Brain injuries are a major reason for mortality and morbidity following
trauma in sports, work and traffic. Apart from the trauma at the site of impact
(coup injury), other regions of the brain remote from the impact locations
(non-coup) are commonly affected. We show that a screw theory-based method can
be used to account for the combined effect of head rotational and linear
accelerations in causing brain injuries. A scalar measure obtained from the
inner product of the motion screw and the impact screw is shown to be a
predictor of the severity and the location of non-coup brain injuries under an
impact. The predictions are consistent with head impact experiments conducted
with non-human primates. The methodology is proved using finite element
simulations and already published experimental results

Genes/Proteins do not work alone within our body, rather as a group they
perform certain activities indicated as pathways. Signalling transduction
pathways (STPs) are some of the important pathways that transmit biological
signals from protein-to-protein controlling several cellular activities.
However, many diseases such as cancer target some of these signalling pathways
for their growth and malignance, but demystifying their underlying mechanisms
are a very complicated tasks. In this study, we use a fully Bayesian approach
to develop methodologies in discovering novel driver bio-markers in aberrant
STPs given two-conditional high-throughput gene expression data. This project,
namely PathTurbEr (Pathway Perturbation Driver), is applied on a global gene
expression dataset derived from the lapatinib (an EGFR/HER dual inhibitor)
sensitive and resistant samples from breast cancer cell lines (SKBR3).
Differential expression analysis revealed 512 differentially expressed genes
(DEGs) and their signalling pathway enrichment analysis revealed 22 singalling
pathways as aberrated including PI3K-AKT, Hippo, Chemokine, and TGF-beta
singalling pathway as highly dysregulated in lapatinib resistance. Next, we
model the aberrant activities in TGF-beta STP as a causal Bayesian network (BN)
from given observational datasets using three Markov Chain Monte Carlo (MCMC)
sampling methods, i.e. Neighbourhood sampler (NS) and Hit-and-Run (HAR)
sampler, which has already proven to have more robust inference with lower
chances of getting stuck at local optima and faster convergence compared to
other state-of-art methods. Next, we examined the structural features of the
optimal BN as a statistical process that generates the global structure using,
$p_1$-model, a special class of Exponential Random Graph Models (ERGMs) and
MCMC methods for their hyper-parameter sampling....

An unsolved challenge in the development of antigen specific immunotherapies
is determining the optimal antigens to target. Comprehension of antigen-MHC
binding is paramount towards achieving this goal. Here, we present CASTELO, a
combined machine learning-molecular dynamics (ML-MD) approach to design novel
antigens of increased MHC binding affinity for a Type 1 diabetes
(T1D)-implicated system. We build upon a small molecule lead optimization
algorithm by training a convolutional variational autoencoder (CVAE) on MD
trajectories of 48 different systems across 4 antigens and 4 HLA serotypes. We
develop several new machine learning metrics including a structure-based anchor
residue classification model as well as cluster comparison scores. ML-MD
predictions agree well with experimental binding results and free energy
perturbation-predicted binding affinities. Moreover, ML-MD metrics are
independent of traditional MD stability metrics such as contact area and RMSF,
which do not reflect binding affinity data. Our work supports the role of
structure-based deep learning techniques in antigen specific immunotherapy
design.

Mitigation strategies that remove infectious individuals from the greater
population have to balance their efficacy with the economic effects associated
with quarantine and have to contend with the limited resources available to the
public health authorities. Prior strategies have relied on testing and contact
tracing to find individuals before they become infectious and in order to limit
their interactions with others until after their infectious period has passed.
Manual contact tracing is a public health intervention where individuals
testing positive are interviewed to identify other members of the community who
they may have come into contact with. These interviews can take a significant
amount of time that has to be tallied in the overall accounting of the outbreak
cost. The concept of contact tracing has been expanded recently into Automated
Exposure Notification whereby cellphones can be used as sensor platforms to log
close contacts and notify the owner in the event that one of their close
contacts tests positive. The intention is that this notification will prompt
the person to be tested and then restrict their interactions with others until
their status is determined. In this paper we describe our efforts to
investigate the effectiveness of contact tracing interventions on controlling
an outbreak. This is accomplished by creating a model of disease spread and
then observing the impact that simulated tracing and testing have on the number
of infected individuals. Model parameters are explored to identify critical
transition points where interventions become effective. We estimate the
benefits as well as costs in order to offer insight to public health officials
as they select courses of action.

In this paper, we address technical difficulties that arise when applying
Markov chain Monte Carlo (MCMC) to hierarchical models designed to perform
clustering in the space of latent parameters of subject-wise generative models.
Specifically, we focus on the case where the subject-wise generative model is a
dynamic causal model (DCM) for fMRI and clusters are defined in terms of
effective brain connectivity. While an attractive approach for detecting
mechanistically interpretable subgroups in heterogeneous populations, inverting
such a hierarchical model represents a particularly challenging case, since DCM
is often characterized by high posterior correlations between its parameters.
In this context, standard MCMC schemes exhibit poor performance and extremely
slow convergence. In this paper, we investigate the properties of hierarchical
clustering which lead to the observed failure of standard MCMC schemes and
propose a solution designed to improve convergence but preserve computational
complexity. Specifically, we introduce a class of proposal distributions which
aims to capture the interdependencies between the parameters of the clustering
and subject-wise generative models and helps to reduce random walk behaviour of
the MCMC scheme. Critically, these proposal distributions only introduce a
single hyperparameter that needs to be tuned to achieve good performance. For
validation, we apply our proposed solution to synthetic and real-world datasets
and also compare it, in terms of computational complexity and performance, to
Hamiltonian Monte Carlo (HMC), a state-of-the-art Monte Carlo. Our results
indicate that, for the specific application domain considered here, our
proposed solution shows good convergence performance and superior runtime
compared to HMC.

Pancreatic cancer (PC) is the fourth leading cause of cancer death in the
United States due to its five-year survival rate of 10%. Late diagnosis,
affiliated with the asymptomatic nature in early stages and the location of the
cancer with respect to the pancreas, makes current widely-accepted screening
methods unavailable. Prior studies have achieved low (70-75%) diagnostic
accuracy, possibly because 80% of PC cases are associated with diabetes,
leading to misdiagnosis. To address the problems of frequent late diagnosis and
misdiagnosis, we developed an accessible, accurate and affordable diagnostic
tool for PC, by analyzing the expression of nineteen genes in PC and diabetes.
First, machine learning algorithms were trained on four groups of subjects,
depending on the occurrence of PC and Diabetes. The models were analyzed with
400 PC subjects at varying stages to ensure validity. Naive Bayes, Neural
Network and K-Nearest Neighbors models achieved the highest testing accuracy of
around 92.6%. Second, the biological implication of the nineteen genes was
investigated using bioinformatics tools. It was found that these genes were
significantly involved in regulating the cytoplasm, cytoskeleton and nuclear
receptor activity in the pancreas, specifically in acinar and ductal cells. Our
novel tool is the first in the literature that achieves a PC diagnostic
accuracy of above 90%, having the potential to significantly improve the
detection of PC in the background of diabetes and increase the five-year
survival rate.

Ordinary differential equation models facilitate the understanding of
cellular signal transduction and other biological processes. However, for large
and comprehensive models, the computational cost of simulating or calibrating
can be limiting. AMICI is a modular toolbox implemented in C++/Python/MATLAB
that provides efficient simulation and sensitivity analysis routines tailored
for scalable, gradient-based parameter estimation and uncertainty
quantification.
  AMICI is published under the permissive BSD-3-Clause license with source code
publicly available on https://github.com/AMICI-dev/AMICI. Citeable releases are
archived on Zenodo.

Artificial intelligence is nowadays used for cell detection and
classification in optical microscopy, during post-acquisition analysis. The
microscopes are now fully automated and next expected to be smart, to make
acquisition decisions based on the images. It calls for analysing them on the
fly. Biology further imposes training on a reduced dataset due to cost and time
to prepare the samples and have the datasets annotated by experts. We propose
here a real-time image processing, compliant with these specifications by
balancing accurate detection and execution performance. We characterised the
images using a generic, high-dimensional feature extractor. We then classified
the images using machine learning for the sake of understanding the
contribution of each feature in decision and execution time. We found that the
non-linear-classifier random forests outperformed Fisher's linear discriminant.
More importantly, the most discriminant and time-consuming features could be
excluded without any significant loss in accuracy, offering a substantial gain
in execution time. It suggests a feature-group redundancy likely related to the
biology of the observed cells. We offer a method to select fast and
discriminant features. In our assay, a 79.6 $\pm$ 2.4 % accurate classification
of a cell took 68.7 $\pm$ 3.5 ms (mean $\pm$ SD, 5-fold cross-validation nested
in 10 bootstrap repeats), corresponding to 14 cells per second, dispatched into
8 phases of the cell cycle using 12 feature-groups and operating a consumer
market ARM-based embedded system. Interestingly, a simple neural network
offered similar performances paving the way to faster training and
classification, using parallel execution on a general-purpose graphic
processing unit. Finally, this strategy is also usable for deep neural networks
paving the way to optimising these algorithms for smart microscopy.

Reaction--diffusion mechanism are a robust paradigm that can be used to
represent many biological and physical phenomena over multiple spatial scales.
Applications include intracellular dynamics, the migration of cells and the
patterns formed by vegetation in semi-arid landscapes. Moreover, domain growth
is an important process for embryonic growth and wound healing. There are many
numerical modelling frameworks capable of simulating such systems on growing
domains, however each of these may be well suited to different spatial scales
and particle numbers. Recently, spatially extended hybrid methods on static
domains have been produced in order to bridge the gap between these different
modelling paradigms in order to represent multiscale phenomena. However, such
methods have not been developed with domain growth in mind. In this paper, we
develop three hybrid methods on growing domains, extending three of the
prominent static domain hybrid methods. We also provide detailed algorithms to
allow others to employ them. We demonstrate that the methods are able to
accurately model three representative reaction-diffusion systems accurately and
without bias.

Coronavirus disease 2019 (COVID-19) caused by severe acute respiratory
syndrome coronavirus 2 (SARS-CoV-2) has a worldwide devastating effect. The
understanding of evolution and transmission of SARS-CoV-2 is of paramount
importance for the COVID-19 control, combating, and prevention. Due to the
rapid growth of both the number of SARS-CoV-2 genome sequences and the number
of unique mutations, the phylogenetic analysis of SARS-CoV-2 genome isolates
faces an emergent large-data challenge. We introduce a dimension-reduced
$k$-means clustering strategy to tackle this challenge. We examine the
performance and effectiveness of three dimension-reduction algorithms:
principal component analysis (PCA), t-distributed stochastic neighbor embedding
(t-SNE), and uniform manifold approximation and projection (UMAP). By using
four benchmark datasets, we found that UMAP is the best-suited technique due to
its stable, reliable, and efficient performance, its ability to improve
clustering accuracy, especially for large Jaccard distanced-based datasets, and
its superior clustering visualization. The UMAP-assisted $k$-means clustering
enables us to shed light on increasingly large datasets from SARS-CoV-2 genome
isolates.

We present an indirect signal processing-based measurement method for
biological quantities in humans that cannot be directly measured. We develop
the method by focusing on estimating hepatic enzyme and drug transporter
activity through breath-biopsy samples clinically obtained via the erythromycin
breath test (EBT): a small dose of radio-labeled drug is injected and the
subsequent content of radio-labeled CO$_2$ is measured repeatedly in exhaled
breath; the resulting time series is analyzed. To model EBT we developed a
14-variable non-linear reduced order dynamical model that describes the
behavior of the drug and its metabolites in the human body well enough to
capture all biological phenomena of interest. Based on this system of coupled
non-linear ordinary differential equations (ODEs) we treat the measurement
problem as inverse problem: we estimate the ODE parameters of individual
patients from the measured EBT time series. These estimates then provide a
measurement of the liver activity of interest. The parameters are hard to
estimate as the ODEs are stiff and the problem needs to be regularized to
ensure stable convergence. We develop a formal operator framework to capture
and treat the specific non-linearities present, and perform perturbation
analysis to establish properties of the estimation procedure and its solution.
Development of the method required 150,000 CPU hours at a supercomputing
center, and a single production run takes CPU 24 hours. We introduce and
analyze the method in the context of future precision dosing of drugs for
vulnerable patients (e.g., oncology, nephrology, or pediatrics) to eventually
ensure efficacy and avoid toxicity.

Angiogenesis is the process by which blood vessels form from pre-existing
vessels. It plays a key role in many biological processes, including embryonic
development and wound healing, and contributes to many diseases including
cancer and rheumatoid arthritis. The structure of the resulting vessel networks
determines their ability to deliver nutrients and remove waste products from
biological tissues. Here we simulate the Anderson-Chaplain model of
angiogenesis at different parameter values and quantify the vessel
architectures of the resulting synthetic data. Specifically, we propose a
topological data analysis (TDA) pipeline for systematic analysis of the model.
TDA is a vibrant and relatively new field of computational mathematics for
studying the shape of data. We compute topological and standard descriptors of
model simulations generated by different parameter values. We show that TDA of
model simulation data stratifies parameter space into regions with similar
vessel morphology. The methodologies proposed here are widely applicable to
other synthetic and experimental data including wound healing, development, and
plant biology.

A new system, Bee Cluster 3D, allowing the study of the time evolution of the
3D temperature distribution in a bee hive is presented. This system can be used
to evaluate the cluster size and the location of the queen during winter. In
summer, the device can be used to quantify the size of the brood nest and the
breeding activity of the queen. The system does not disturb the activity of the
colony and can be used on any hive. This electronic system was developed to be
non-intrusive, miniaturized, and energy autonomous.

This paper presents a cloud-connected indoor air quality sensor system that
can be deployed to patients' homes to study personal microenvironmental
exposure for asthma research and management. The system consists of multiple
compact sensor units that can measure residential NO2, ozone, humidity, and
temperature at one minute resolution and a cloud based informatic system that
acquires, stores, and visualizes the microenvironmental data in real time. The
sensor hardware can measure NO2 as low as 10 ppb and ozone at 15 ppb. The cloud
informatic system is implemented using open-source software on Amazon Web
Service for easy deployment and scalability. This system was successfully
deployed to pediatric asthma patients' homes in a pilot study. In this study,
we discovered that some families can have short term NO2 exposure higher than
EPA's one hour exposure limit (100 ppb), and NO2 micropollution episodes often
arise from natural gas appliance usage such as gas stove burning during
cooking. By combining the personalized air pollutant exposure measurements with
the physiological responses from a patient diary and medical record, this
system can enable novel asthma research and personalized asthma management.

1. Spatial memory plays a role in the way animals perceive their
environments, resulting in memory-informed movement patterns that are
observable to ecologists. Developing mathematical techniques to understand how
animals use memory in their environments allows for an increased understanding
of animal cognition. 2. Here we describe a model that accounts for the memory
of seasonal or ephemeral qualities of an animal's environment. The model
captures multiple behaviors at once by allowing for resource selection in the
present time as well as long-distance navigations to previously visited
locations within an animal's home range. 3. We performed a set of analyses on
simulated data to test our model, determining that it can provide informative
results from as little as one year of discrete-time location data. We also show
that the accuracy of model selection and parameter estimation increases with
more location data. 4. This model has potential to identify a specific
mechanism in which animals use memory to optimize their foraging, by revisiting
temporally and predictably variable resources at consistent time lags.

Chimeric Antigen Receptor (CAR) T-cell therapy is an immunotherapy that has
recently become highly instrumental in the fight against life-threatening
diseases. A variety of modeling and computational simulation efforts have
addressed different aspects of CAR T therapy, including T-cell activation, T-
and malignant cell population dynamics, therapeutic cost-effectiveness
strategies, and patient survival analyses. In this article, we present a
systematic review of those efforts, including mathematical, statistical, and
stochastic models employing a wide range of algorithms, from differential
equations to machine learning. To the best of our knowledge, this is the first
review of all such models studying CAR T therapy. In this review, we provide a
detailed summary of the strengths, limitations, methodology, data used, and
data lacking in current published models. This information may help in
designing and building better models for enhanced prediction and assessment of
the benefit-risk balance associated with novel CAR T therapies, as well as with
the data collection essential for building such models.

The removal of organic micropollutants (OMPs) has been investigated in
constructed wetlands (CWs) operated as bioelectrochemical systems (BES). The
operation of CWs as BES (CW-BES), either in the form of microbial fuel cells
(MFC) or microbial electrolysis cells (MEC), has only been investigated in
recent years. The presented experiment used CW meso-scale systems applying a
realistic horizontal flow regime and continuous feeding of real urban
wastewater spiked with four OMPs (pharmaceuticals), namely carbamazepine (CBZ),
diclofenac (DCF), ibuprofen (IBU) and naproxen (NPX). The study evaluated the
removal efficiency of conventional CW systems (CW-control) as well as CW
systems operated as closed-circuit MFCs (CW-MFCs) and MECs (CW-MECs). Although
a few positive trends were identified for the CW-BES compared to the CW-control
(higher average CBZ, DCF and NPX removal by 10-17% in CW-MEC and 5% in CW-MFC),
these proved to be not statistically significantly different. Mesoscale
experiments with real wastewater could thus not confirm earlier positive
effects of CW-BES found under strictly controlled laboratory conditions with
synthetic wastewaters.

Pulmonary hypertension (PH), defined by a mean pulmonary arterial pressure
(mPAP) $>$ 20 mmHg, is characterized by increased pulmonary vascular resistance
and decreased pulmonary arterial compliance. There are few measurable
biomarkers of PH progression, but a conclusive diagnosis of the disease
requires invasive right heart catheterization (RHC). Patient-specific
computational models of the cardiovascular system are a potential noninvasive
tool for determining additional indicators of disease severity. Using
computational modeling, this study quantifies physiological parameters
indicative of disease severity in nine PH patients. The model includes all four
heart chambers and the pulmonary and systemic circulations. We consider two
sets of calibration data: static (systolic \& diastolic values) RHC data and a
combination of static and continuous, time-series waveform data. We determine a
subset of identifiable parameters for model calibration using sensitivity
analyses and multistart inference, and carry out uncertainty quantification
post-inference. Results show that additional waveform data enables accurate
calibration of the right atrial reservoir and pump function across the PH
cohort. Model outcomes, including stroke work and pulmonary
resistance-compliance relations, reflect typical right heart dynamics in PH
phenotypes. Lastly, we show that estimated parameters agree with previous,
non-modeling studies, supporting this type of analysis in translational PH
research.

The extraction of $k$-mers is a fundamental component in many complex
analyses of large next-generation sequencing datasets, including reads
classification in genomics and the characterization of RNA-seq datasets. The
extraction of all $k$-mers and their frequencies is extremely demanding in
terms of running time and memory, owing to the size of the data and to the
exponential number of $k$-mers to be considered. However, in several
applications, only frequent $k$-mers, which are $k$-mers appearing in a
relatively high proportion of the data, are required by the analysis. In this
work we present SPRISS, a new efficient algorithm to approximate frequent
$k$-mers and their frequencies in next-generation sequencing data. SPRISS
employs a simple yet powerful reads sampling scheme, which allows to extract a
representative subset of the dataset that can be used, in combination with any
$k$-mer counting algorithm, to perform downstream analyses in a fraction of the
time required by the analysis of the whole data, while obtaining comparable
answers. Our extensive experimental evaluation demonstrates the efficiency and
accuracy of SPRISS in approximating frequent $k$-mers, and shows that it can be
used in various scenarios, such as the comparison of metagenomic datasets and
the identification of discriminative $k$-mers, to extract insights in a
fraction of the time required by the analysis of the whole dataset.

INTRODUCTION: Heterogeneity in the progression of Alzheimer's disease makes
it challenging to predict the rate of cognitive and functional decline for
individual patients. Tools for short-term prediction could help enrich clinical
trial designs and focus prevention strategies on the most at-risk patients.
METHOD: We built a prognostic model using baseline cognitive scores and
MRI-based features to determine which subjects with mild cognitive impairment
remained stable and which functionally declined (measured by a two-point
increase in CDR-SB) over 2 and 3-year follow-up periods, periods typical of the
length of clinical trials. RESULTS: Combining both sets of features yields 77%
accuracy (81% sensitivity and 75% specificity) to predict cognitive decline at
2 years (74% accuracy at 3 years with 75% sensitivity and 73% specificity).
Using this tool to select trial participants yields a 3.8-fold decrease in the
required sample size for a 2-year study (2.8-fold decrease for a 3-year study)
for a hypothesized 25% treatment effect to reduce cognitive decline.
DISCUSSION: This cohort enrichment tool could accelerate treatment development
by increasing power in clinical trials.

The infectivity of a virus sample is measured by the infections it causes,
via a plaque or focus forming assay (PFU or FFU) or an endpoint dilution (ED)
assay (TCID$_{50}$, CCID$_{50}$, EID$_{50}$, etc., hereafter collectively
ID$_{50}$). The counting of plaques or foci at a given dilution intuitively and
directly provides the concentration of infectious doses in the undiluted
sample. However, it has many technical and experimental limitations. For
example, it relies on one's judgement in distinguishing between two merged
plaques and a larger one, or between small plaques and staining artifacts. In
this regard, ED assays are more robust because one need only determine whether
infection occurred. The output of the ED assay, the 50% infectious dose
(ID$_{50}$), is calculated using either the Spearman-Karber (SK, 1908,1931) or
Reed-Muench (RM, 1938) mathematical approximations. However, these are often
miscalculated and their ID$_{50}$ approximation is biased. We propose that the
PFU and FFU assays be abandoned, and that the measured output of the ED assay,
the ID$_{50}$, be replaced by a more useful measure we coined Specific
INfections (SIN). We introduce a free, open-source web-application, midSIN,
that computes the SIN concentration in a virus sample from a standard ED assay,
requiring no changes to current experimental protocols. We demonstrate that the
SIN/mL of a sample reliably corresponds to the number of infections the sample
will cause per unit volume, and directly relates to the multiplicity of
infection. midSIN estimates are shown to be more accurate and robust than those
using the RM and SK approximations. The impact of ED plate design choices
(dilution factor, replicates per dilution) on measurement accuracy is also
explored. The simplicity of SIN as a measure and the greater accuracy of midSIN
make them an easy, superior replacement for the PFU, FFU, and ID$_{50}$
measures.

Recent efforts have focused on providing a systematic analysis of syntrophic
microbial growth yields. These biokinetic parameters are key to developing an
accurate mathematical description of the anaerobic digestion process. The
agreement between experimentally determined growth yields and those obtained
from bioenergetic estimations is therefore of great interest. Considering five
important syntrophic groups, including acetoclastic and hydrogenotrophic
methanogens, as well as propionate, butyrate and lactate oxidizers, previous
findings suggest that measured and estimated growth yields were consistent only
for acetoclastic methanogens. A re-analysis revealed that data are also
consistent for lactate oxidizers and hydrogenotrophic methanogens, whereas the
limited data available for propionate and butyrate oxidizers are unsupportive
of firm conclusions. These results highlight pertinent challenges in the
analysis of microbial syntrophy and encourage more accurate measurements of
syntrophic microbial growth yields in the future.

The goal of this paper is to review and critically assess different methods
to monitor key process variables for ethanol production from lignocellulosic
biomass. Because cellulose-based biofuels cannot yet compete with
non-cellulosic biofuels, process control and optimization are of importance to
lower the production costs. This study reviews different monitoring schemes, to
indicate what the added value of real-time monitoring is for process control.
Furthermore, a comparison is made on different monitoring techniques to measure
the off-gas, the concentrations of dissolved components in the inlet to the
process, the concentrations of dissolved components in the reactor, and the
biomass concentration. Finally, soft sensor techniques and available models are
discussed, to give an overview of modeling techniques that analyze data, with
the aim of coupling the soft sensor predictions to the control and optimization
of cellulose to ethanol fermentation. The paper ends with a discussion of
future needs and developments.

The novel coronavirus SARS-CoV-2, which emerged in late 2019, has since
spread around the world and infected hundreds of millions of people with
coronavirus disease 2019 (COVID-19). While this viral species was unknown prior
to January 2020, its similarity to other coronaviruses that infect humans has
allowed for rapid insight into the mechanisms that it uses to infect human
hosts, as well as the ways in which the human immune system can respond. Here,
we contextualize SARS-CoV-2 among other coronaviruses and identify what is
known and what can be inferred about its behavior once inside a human host.
Because the genomic content of coronaviruses, which specifies the virus's
structure, is highly conserved, early genomic analysis provided a significant
head start in predicting viral pathogenesis and in understanding potential
differences among variants. The pathogenesis of the virus offers insights into
symptomatology, transmission, and individual susceptibility. Additionally,
prior research into interactions between the human immune system and
coronaviruses has identified how these viruses can evade the immune system's
protective mechanisms. We also explore systems-level research into the
regulatory and proteomic effects of SARS-CoV-2 infection and the immune
response. Understanding the structure and behavior of the virus serves to
contextualize the many facets of the COVID-19 pandemic and can influence
efforts to control the virus and treat the disease.

The aim of this paper is to investigate the cardiorespiratory synchronization
in athletes subjected to extreme physical stress combined with a cognitive
stress tasks. ECG and respiration were measured in 14 athletes before and after
the Ironmen competition. Stroop test was applied between the measurements
before and after the Ironmen competition to induce cognitive stress.
Synchrogram and empirical mode decomposition analysis were used for the first
time to investigate the effects of physical stress, induced by the Ironmen
competition, on the phase synchronization of the cardiac and respiratory
systems of Ironmen athletes before and after the competition. A cognitive
stress task (Stroop test) was performed both pre- and post-Ironman event in
order to prevent the athletes from cognitively controlling their breathing
rates. Our analysis showed that cardiorespiratory synchronization increased
post-Ironman race compared to pre-Ironman. The results suggest that the amount
of stress the athletes are recovering from post-competition is greater than the
effects of the Stroop test. This indicates that the recovery phase after the
competition is more important for restoring and maintaining homeostasis, which
could be another reason for stronger synchronization.

Coronavirus disease 2019 (COVID-19) has caused global disruption and a
significant loss of life. Existing treatments that can be repurposed as
prophylactic and therapeutic agents could reduce the pandemic's devastation.
Emerging evidence of potential applications in other therapeutic contexts has
led to the investigation of dietary supplements and nutraceuticals for
COVID-19. Such products include vitamin C, vitamin D, omega 3 polyunsaturated
fatty acids, probiotics, and zinc, all of which are currently under clinical
investigation. In this review, we critically appraise the evidence surrounding
dietary supplements and nutraceuticals for the prophylaxis and treatment of
COVID-19. Overall, further study is required before evidence-based
recommendations can be formulated, but nutritional status plays a significant
role in patient outcomes, and these products could help alleviate deficiencies.
For example, evidence indicates that vitamin D deficiency may be associated
with greater incidence of infection and severity of COVID-19, suggesting that
vitamin D supplementation may hold prophylactic or therapeutic value. A growing
number of scientific organizations are now considering recommending vitamin D
supplementation to those at high risk of COVID-19. Because research in vitamin
D and other nutraceuticals and supplements is preliminary, here we evaluate the
extent to which these nutraceutical and dietary supplements hold potential in
the COVID-19 crisis.

State-dependent Na+ channel blockers are often prescribed to treat cardiac
arrhythmias, but many Na+ channel blockers are known to have pro-arrhythmic
side effects. While the anti and proarrhythmic potential of a Na+ channel
blocker is thought to depend on the characteristics of its rate-dependent
block, the mechanisms linking these two attributes are unclear. Furthermore,
how specific properties of rate-dependent block arise from the binding kinetics
of a particular drug is poorly understood. Here, we examine the rate-dependent
effects of the Na+ channel blocker lidocaine by constructing and analyzing a
novel drug-channel interaction model. First, we identify the predominant mode
of lidocaine binding in a 24 variable Markov model for lidocaine-Na+ channel
interaction by Moreno et al. We then develop a novel 3-variable lidocaine-Na+
channel interaction model that incorporates only the predominant mode of drug
binding. Our low-dimensional model replicates the extensive voltage-clamp data
used to parameterize the Moreno et al. model. Furthermore, the effects of
lidocaine on action potential upstroke velocity and conduction velocity in our
model are similar to those predicted by the Moreno et al. model. By exploiting
the low-dimensionality of our model, we derive an algebraic expression for
level of rate-dependent block as a function of pacing frequency, restitution
properties, diastolic and plateau potentials, and drug binding rate constants.
Our model predicts that the level of rate-dependent block is sensitive to
alterations in restitution properties and increases in diastolic potential, but
it is insensitive to variations in the shape of the action potential waveform
and lidocaine binding rates.

A better fundamental understanding of human induced pluripotent stem
cell-derived cardiomyocytes (hiPSC-CMs) has the potential to advance
applications ranging from drug discovery to cardiac repair. Automated
quantitative analysis of beating hiPSC-CMs is an important and fast developing
component of the hiPSC-CM research pipeline. Here we introduce "Sarc-Graph," a
computational framework to segment, track, and analyze sarcomeres in
fluorescently tagged hiPSC-CMs. Our framework includes functions to segment
z-discs and sarcomeres, track z-discs and sarcomeres in beating cells, and
perform automated spatiotemporal analysis and data visualization. In addition
to reporting good performance for sarcomere segmentation and tracking with
little to no parameter tuning and a short runtime, we introduce two novel
analysis approaches. First, we construct spatial graphs where z-discs
correspond to nodes and sarcomeres correspond to edges. This makes measuring
the network distance between each sarcomere (i.e., the number of connecting
sarcomeres separating each sarcomere pair) straightforward. Second, we treat
tracked and segmented components as fiducial markers and use them to compute
the approximate deformation gradient of the entire tracked population. This
represents a new quantitative descriptor of hiPSC-CM function. We showcase and
validate our approach with both synthetic and experimental movies of beating
hiPSC-CMs. By publishing Sarc-Graph, we aim to make automated quantitative
analysis of hiPSC-CM behavior more accessible to the broader research
community.

The glutamatergic modulator ketamine has been shown to rapidly reduce
depressive symptoms in patients with treatment-resistant major depressive
disorder (TRD). Although its mechanisms of action are not fully understood,
changes in cortical excitation/inhibition (E/I) following ketamine
administration are well documented in animal models and could represent a
potential biomarker of treatment response. Here, we analyse neuromagnetic
virtual electrode timeseries collected from the primary somatosensory cortex in
18 unmedicated patients with TRD and in an equal number of age-matched healthy
controls during a somatosensory 'airpuff' stimulation task. These two groups
were scanned as part of a clinical trial of ketamine efficacy under three
conditions: a) baseline; b) 6-9 hours following subanesthetic ketamine
infusion; and c) 6-9 hours following placebo-saline infusion. We obtained
estimates of E/I interaction strengths by using Dynamic Causal Modelling (DCM)
on the timeseries, thereby allowing us to pinpoint, under each scanning
condition, where each subject's dynamics lie within the Poincar\'e diagram - as
defined in dynamical systems theory. We demonstrate that the Poincar\'e diagram
offers classification capability for TRD patients, in that the further the
patients' coordinates were shifted (by virtue of ketamine) toward the stable
(top-left) quadrant of the Poincar\'e diagram, the more their depressive
symptoms improved. The same relationship was not observed by virtue of a
placebo effect - thereby verifying the drug-specific nature of the results. We
show that the shift in neural dynamics required for symptom improvement
necessitates an increase in both excitatory and inhibitory coupling. We present
accompanying MATLAB code made available in a public repository, thereby
allowing for future studies to assess individually-tailored treatments of TRD.

Time-resolved analysis of periodically excited luminescence decays by the
phasor method in the presence of time-gating or binning is revisited.
Analytical expressions for discrete configurations of square gates are derived
and the locus of the phasors of such modified periodic single-exponential
decays is compared to the canonical universal semicircle. The effects of IRF
offset, decay truncation and gate shape are also discussed. Finally, modified
expressions for the phase and modulus lifetimes are provided for some simple
cases. A discussion of a modified phasor calibration approach is presented.

Phytochemical composition of ethanol leaf extract of Diodella sarmentosa was
profiled with GC-MS and the inhibitory property of the extract against total
microbial dehydrogenases were assessed. The major constituents of the extract
were squalene (29.50%), Phytol (24.68%), phenol, 3-pentadecyl- (18.58%),
1-Butanol, 3-methyl- (9.09%) and n-Hexadecanoic acid (7.78%). The minimum
inhibitory concentration (MIC) of the extract against broad spectrum of
microbial population was assessed. Bacillus subtilis, Candidas spp, and
Penicillium spp were more sensitive to the treatment and thus; were further
investigated using Dehydrogenase activity assay method. Total dehydrogenase
activities of Bacillus subtilis, Candidas spp, and Penicillium spp at the
extract concentration range of 0 to 2000mg/ml were progressively inhibited at
increasing extract concentrations. The threshold inhibitory concentrations
(IC50) of the extracts against Candidas spp, Penicillium spp and Bacillus
subtilis were 275micro. g/ml, 322 micro. g/ml and 411 micro. g/ml respectively.
Our findings suggested the extract as a useful source of antimicrobial
phytochemicals for pharmaceutical use.

Face masks have been widely used as a protective measure against COVID-19.
However, pre-pandemic empirical studies have produced mixed statistical results
on the effectiveness of masks against respiratory viruses. The implications of
the studies' recognized limitations have not been quantitatively and
statistically analyzed, leading to confusion regarding the effectiveness of
masks. Such confusion may have contributed to organizations such as the WHO and
CDC initially not recommending that the general public wear masks. Here we show
that when the adherence to mask-usage guidelines is taken into account, the
empirical evidence indicates that masks prevent disease transmission: all
studies we analyzed that did not find surgical masks to be effective were
under-powered to such an extent that even if masks were 100% effective, the
studies in question would still have been unlikely to find a statistically
significant effect. We also provide a framework for understanding the effect of
masks on the probability of infection for single and repeated exposures. The
framework demonstrates that more frequently wearing a mask provides
super-linearly compounding protection, as does both the susceptible and
infected individual wearing a mask. This work shows (1) that both theoretical
and empirical evidence is consistent with masks protecting against respiratory
infections and (2) that nonlinear effects and statistical considerations
regarding the percentage of exposures for which masks are worn must be taken
into account when designing empirical studies and interpreting their results.

One of the major limitations of nanomedicine is the scarce penetration of
nanoparticles in tumoral tissues. These constrains have been tried to be solved
by different strategies, such as the employ of polyethyleneglycol (PEG) to
avoid the opsonization or reducing the extracellular matrix (ECM) density. Our
research group has developed some strategies to overcome these limitations such
as the employ of pH-sensitive collagenase nanocapsules for the digestion of the
collagen-rich extracellular matrix present in most of tumoral tissues. However,
a deeper understanding of physicochemical kinetics involved in the nanocapsules
degradation process is needed to understand the nanocapsule framework
degradation process produced during the penetration in the tissue. For this, in
this work it has been employed a double-fluorescent labelling strategy of the
polymeric enzyme nanocapsule as a crucial chemical tool which allowed the
analysis of nanocapsules and free collagenase during the diffusion process
throughout a tumour-like collagen matrix. This extrinsic label strategy
provides far greater advantages for observing biological processes. For the
detection of enzyme, collagenase has been labelled with fluorescein
Isothiocyanate (FITC), whereas the nanocapsule surface was labelled with
rhodamine Isothiocyanate (RITC). Thus, it has been possible to monitor the
hydrolysis of nanocapsules and their diffusion throughout a thick 3D Collagen
gel during the time, obtaining a detailed temporal evaluation of the
pH-sensitive collagenase nanocapsule behaviour. These collagenase nanocapsules
displayed a high enzymatic activity in low concentrations at acidic pH, and
their efficiency to penetrate into tissue models pave the way to a wide range
of possible nanomedical applications, especially in cancer therapy.

Sleep has a profound influence on the physiology of body systems and
biological processes. Molecular studies have shown circadian-regulated shifts
in protein expression patterns across human tissues, further emphasizing the
unique functional, behavioral and pharmacokinetic landscape of sleep. Thus,
many pathological processes are also expected to exhibit sleep-specific
manifestations. Nevertheless, sleep is seldom utilized for the study, detection
and treatment of non-sleep-specific pathologies. Modern advances in biosensor
technologies have enabled remote, non-invasive recording of a growing number of
physiologic parameters and biomarkers. Sleep is an ideal time frame for the
collection of long and clean physiological time series data which can then be
analyzed using data-driven algorithms such as deep learning. In this
perspective paper, we aim to highlight the potential of sleep as an auspicious
time for diagnosis, management and therapy of nonsleep-specific pathologies. We
introduce key clinical studies in selected medical fields, which leveraged
novel technologies and the advantageous period of sleep to diagnose, monitor
and treat pathologies. We then discuss possible opportunities to further
harness this new paradigm and modern technologies to explore human health and
disease during sleep and to advance the development of novel clinical
applications: From sleep medicine to medicine during sleep.

Computational and mathematical models rely heavily on estimated parameter
values for model development. Identifiability analysis determines how well the
parameters of a model can be estimated from experimental data. Identifiability
analysis is crucial for interpreting and determining confidence in model
parameter values and to provide biologically relevant predictions. Structural
identifiability analysis, in which one assumes data to be noiseless and
arbitrarily fine-grained, has been extensively studied in the context of
ordinary differential equation (ODE) models, but has not yet been widely
explored for age-structured partial differential equation (PDE) models. These
models present additional difficulties due to increased number of variables and
partial derivatives as well as the presence of boundary conditions. In this
work, we establish a pipeline for structural identifiability analysis of
age-structured PDE models using a differential algebra framework and derive
identifiability results for specific age-structured models. We use epidemic
models to demonstrate this framework because of their wide-spread use in many
different diseases and for the corresponding parallel work previously done for
ODEs. In our application of the identifiability analysis pipeline, we focus on
a Susceptible-Exposed-Infected model for which we compare identifiability
results for a PDE and corresponding ODE system and explore effects of
age-dependent parameters on identifiability. We also show how practical
identifiability analysis can be applied in this example.

Optimal use and distribution of Covid-19 vaccines involves adjustments of
dosing. Due to the rapidly-evolving pandemic, such adjustments often need to be
introduced before full efficacy data are available. As demonstrated in other
areas of drug development, quantitative systems pharmacology (QSP) is well
placed to guide such extrapolation in a rational and timely manner. Here we
propose for the first time how QSP can be applied real time in the context of
COVID-19 vaccine development.

The Influenza type A virus can be considered as one of the most severe
viruses that can infect multiple species with often fatal consequences to the
hosts. The Haemagglutinin (HA) gene of the virus has the potential to be a
target for antiviral drug development realised through accurate identification
of its sub-types and possible the targeted hosts. In this paper, to accurately
predict if an Influenza type A virus has the capability to infect human hosts,
by using only the HA gene, is therefore developed and tested. The predictive
model follows three main steps; (i) decoding the protein sequences into
numerical signals using EIIP amino acid scale, (ii) analysing these sequences
by using Discrete Fourier Transform (DFT) and extracting DFT-based features,
(iii) using a predictive model, based on Artificial Neural Networks and using
the features generated by DFT. In this analysis, from the Influenza Research
Database, 30724, 18236 and 8157 HA protein sequences were collected for Human,
Avian and Swine, respectively. Given this set of the proteins, the proposed
method yielded 97.36% (+- 0.04%), 97.26% (+- 0.26%), 0.978 (+- 0.004), 0.963
(+- 0.005) and 0.945 (+- 0.005) for the training accuracy validation accuracy,
precision, recall and Mathews Correlation Coefficient (MCC) respectively, based
on a 10-fold cross-validation. The classification model generated by using one
of the largest dataset, if not the largest, yields promising results that could
lead to early detection of such species and help develop precautionary
measurements for possible human infections.

Mahi-mahi (Coryphaena hippurus) are a highly migratory pelagic fish, but
little is known about what environmental factors drive their broad
distribution. This study examined how temperature influences aerobic scope and
swimming performance in mahi. Mahi were acclimated to four temperatures
spanning their natural range (20, 24, 28, and 32{\deg}C; 5-27 days) and
critical swimming speed (Ucrit), metabolic rates, aerobic scope, and optimal
swim speed were measured. Aerobic scope and Ucrit were highest in
28{\deg}C-acclimated fish. 20{\deg}C-acclimated mahi experienced significantly
decreased aerobic scope and Ucrit relative to 28{\deg}C-acclimated fish (57 and
28% declines, respectively). 32{\deg}C-acclimated mahi experienced increased
mortality and a significant 23% decline in Ucrit, and a trend for a 26% decline
in factorial aerobic scope relative to 28{\deg}C-acclimated fish. Absolute
aerobic scope showed a similar pattern to factorial aerobic scope. Our results
are generally in agreement with previously observed distribution patterns for
wild fish. Although thermal performance can vary across life stages, the
highest tested swim performance and aerobic scope found in the present study
(28{\deg}C), aligns with recently observed habitat utilization patterns for
wild mahi and could be relevant for climate change predictions.

Nowadays, the screening methods for the early detection of lung cancer
struggle with several limitations such as many false positive results and low
sensitivity. The detection of specific biomarkers is of high interest to
complement these conventional screening methods. The objective of this study is
to prove the power of 1H-NMR in metabolomics for the detection of smoking
behavior, which is the leading risk factor for lung cancer, and as such gain
more insights in the metabolic alterations that are caused by smoking. In this
research, 1H-NMR spectra of human blood plasma samples were divided in 110
integration regions, from which the integration values were used to train an
OPLS-DA classification model that underwent further data reduction. Results
show that a classification model could discriminate between individuals based
on their smoking status with a sensitivity of 96 percent and a specificity of
94 percent. This study also demonstrates that by performing a pathway-specific
variable reduction of almost 50 percent, the sensitivity and specificity of the
model almost remains the same. To conclude, 1H-NMR analyses show that the
bloods metabolic profile of a smoker is altered compared to that of a
non-smoker. Also, pathway-specific variable reduction shows great potential to
perform overall data reduction. This workflow could be interesting to apply in
the identification of lung cancer, to potentially detect specific biomarkers.

The area under the curve (AUC) of the receiver operating characteristics
curve (ROC) evaluates the separation between patients and nonpatients or
discrimination. For risk prediction models these risk distributions can be
derived from the population risk distribution so are not independent as in
diagnosis. A ROC curve AUC formula based on the underlying population risk
distribution clarifies how discrimination is defined mathematically and that
generation of the equivalent c-statistic effects a Monte Carlo integration of
the formula. For a selection of continuous risk distributions, exact analytic
formulas or numerical results for the ROC curve AUC and overlap measure are
presented and demonstrate a linear or near-linear dependence on their standard
deviation. The ROC curve AUC is also shown to be highly dependent on the mean
population risk, a distinction from the independence from disease prevalence
for diagnostic tests. The converse of discrimination, overlap, has been
quantified by the overlap measure, which appears to provide equivalent
information. As achieving wider population risk distributions is the goal of
risk prediction modeling for clinical risk stratification, interpreting the ROC
curve AUC as a measure of dispersion, rather than discrimination, when
comparing risk prediction models may be more relevant.

Equation learning aims to infer differential equation models from data. While
a number of studies have shown that differential equation models can be
successfully identified when the data are sufficiently detailed and corrupted
with relatively small amounts of noise, the relationship between observation
noise and uncertainty in the learned differential equation models remains
unexplored. We demonstrate that for noisy data sets there exists great
variation in both the structure of the learned differential equation models as
well as the parameter values. We explore how to combine data sets to quantify
uncertainty in the learned models, and at the same time draw mechanistic
conclusions about the target differential equations. We generate noisy data
using a stochastic agent-based model and combine equation learning methods with
approximate Bayesian computation (ABC) to show that the correct differential
equation model can be successfully learned from data, while a quantification of
uncertainty is given by a posterior distribution in parameter space.

We combine infectious disease transmission and the non-pharmaceutical
intervention (NPI) response to disease incidence into one closed model
consisting of two coupled delay differential equations for the incidence rate
and the time-dependent reproduction number. The model contains three free
parameters, the initial reproduction number, the intervention strength, and the
response delay relative to the time of infection. The NPI response is modeled
by assuming that the rate of change of the reproduction number is proportional
to the negative deviation of the incidence rate from an intervention threshold.
This delay dynamical system exhibits damped oscillations in one part of the
parameter space, and growing oscillations in another, and these are separated
by a surface where the solution is a strictly periodic nonlinear oscillation.
For parameters relevant for the COVID-19 pandemic, the tipping transition from
damped to growing oscillations occurs for response delays of the order of one
week, and suggests that effective control and mitigation of successive epidemic
waves cannot be achieved unless NPIs are implemented in a precautionary manner,
rather than merely as a response to the present incidence rate.

Agent-Based Models are a powerful class of computational models widely used
to simulate complex phenomena in many different application areas. However, one
of the most critical aspects, poorly investigated in the literature, regards an
important step of the model credibility assessment: solution verification. This
study overcomes this limitation by proposing a general verification framework
for Agent-Based Models that aims at evaluating the numerical errors associated
with the model. A step-by-step procedure, which consists of two main
verification studies (deterministic and stochastic model verification), is
described in detail and applied to a specific mission critical scenario: the
quantification of the numerical approximation error for UISS-TB, an ABM of the
human immune system developed to predict the progression of pulmonary
tuberculosis. Results provide indications on the possibility to use the
proposed model verification workflow to systematically identify and quantify
numerical approximation errors associated with UISS-TB and, in general, with
any other ABMs.

Real-time PCR, or Real-time Quantitative PCR (qPCR) is an effective approach
to quantify nucleic acid samples. Given the complicated reaction system along
with thermal cycles, there has been long-term confusion on accurately
calculating the initial nucleic acid amounts from the fluorescence signals.
Although many improved algorithms had been proposed, the classical threshold
method is still the primary choice in the routine application. In this study,
we will first illustrate the origin of the linear relationship between the
threshold value and logarithm of the initial nucleic acid amount by
reconstructing the PCR reaction process with stochastic simulations. We then
develop a new method for the absolute quantification of nucleic acid samples
with qPCR. By monitoring the fluorescence signal changes in every stage of the
thermal cycle, we are able to calculate a representation of the step-wise
efficiency change. This is the first work calculated PCR efficiency change
directly from the fluorescence signal, without fitting or sophisticated
analysis. Our results revealed that the efficiency change during the PCR
process is complicated and can not be modeled simply by monotone function
model. Based on the calculated efficiency, we illustrate a new absolute qPCR
analysis method for accurately determining nucleic acid amount. The efficiency
problem is completely avoided in this new method.

Betti curves of symmetric matrices were introduced in (Giusti et. al., 2015)
as a new class of matrix invariants that depend only on the relative ordering
of matrix entries. These invariants are computed using persistent homology, and
can be used to detect underlying structure in biological data that may
otherwise be obscured by monotone nonlinearities. Here we prove three theorems
that fully characterize the Betti curves of rank 1 symmetric matrices. We then
illustrate how these Betti curve signatures arise in natural data obtained from
calcium imaging of neural activity in zebrafish.

A beat-to-beat Tele-fetal Monitoring and comparison with clinical data are
studied with a wavelet transformation approach. Tele-fetal monitoring is a big
progress toward a wearable medical device for a pregnant woman capable of
obtaining prenatal care at home. We apply a wavelet transformation algorithm
for fetal cardiac monitoring using a portable fetal Doppler medical device.
Choosing an appropriate mother wavelet, 85 different mother wavelets are
investigated. The efficiency of the proposed method is evaluated using two data
sets including public and clinical. From publicly available data on PhysioBank,
and simultaneous clinical measurement, we prove that the comparison between
obtained fetal heart rate by the algorithm and the baselines yields a promising
accuracy beyond 95%. Finally, we conclude that the proposed algorithm would be
a robust technique for any similar tele-fetal monitoring approach.

After emerging in China in late 2019, the novel Severe acute respiratory
syndrome-like coronavirus 2 (SARS-CoV-2) spread worldwide and as of early 2021,
continues to significantly impact most countries. Only a small number of
coronaviruses are known to infect humans, and only two are associated with the
severe outcomes associated with SARS-CoV-2: Severe acute respiratory
syndrome-related coronavirus, a closely related species of SARS-CoV-2 that
emerged in 2002, and Middle East respiratory syndrome-related coronavirus,
which emerged in 2012. Both of these previous epidemics were controlled fairly
rapidly through public health measures, and no vaccines or robust therapeutic
interventions were identified. However, previous insights into the immune
response to coronaviruses gained during the outbreaks of severe acute
respiratory syndrome (SARS) and Middle East respiratory syndrome (MERS) have
proved beneficial to identifying approaches to the treatment and prophylaxis of
novel coronavirus disease 2019 (COVID-19). A number of potential therapeutics
against SARS-CoV-2 and the resultant COVID-19 illness were rapidly identified,
leading to a large number of clinical trials investigating a variety of
possible therapeutic approaches being initiated early on in the pandemic. As a
result, a small number of therapeutics have already been authorized by
regulatory agencies such as the Food and Drug Administration (FDA) in the
United States, and many other therapeutics remain under investigation. Here, we
describe a range of approaches for the treatment of COVID-19, along with their
proposed mechanisms of action and the current status of clinical investigation
into each candidate. The status of these investigations will continue to
evolve, and this review will be updated as progress is made.

Thermostability is an important prerequisite for enzymes employed for
industrial applications. Several machine learning based models have thus been
formulated for protein classification based on this particular trait. These
models have employed features derived from sequences, structures or both
resulting in a >93% accuracy based on a 10-fold cross-validation. Besides using
various proteins from a wide range of organisms, such studies also rely on
hundreds of features. In the present study, an enzyme specific classification
model was created using significantly less number of features that provides a
similar accuracy of classification for thermophilic and non-thermophilic enzyme
serine proteases. For building the classifier, 219 thermophilic and 200
mesophilic bacterial genomes were mined for their respective serine protease
sequences. Features were extracted for 800 sequences followed by feature
selection. We deployed a random forest based classifier that identified
thermophilic and non-thermophilic serine proteases with an accuracy of 95.71%.
Knowledge of thermostability along with amino acid positional shifts can be
vital for downstream protein engineering techniques. Thus, to emphasize the
real time application of the enzyme specific classification model, a web
platform has been designed. Combining the sequence data and the classification
model, this prototype can allow users to align their query serine protease
sequence against the custom database and identify its thermophilic nature.

Neoadjuvant chemotherapy has been used for breast cancer aiming at
downgrading before surgery. In this article we propose a new quantitative
analysis of the effects of the neoadjuvant therapy to obtain numerical,
personalized, predictions on the shrinkage of the tumor size after the drug
doses, by data assimilation of the individual patient. The algorithm has been
validated by a sample of 37 patients with histological diagnosis of locally
advanced primary breast carcinoma. The biopsy specimen, the initial tumor size
and its reduction after each treatment were known for all patients. We find
that: a) the measure of tumor size at the diagnosis and after the first dose
permits to predict the size reduction for the follow up; b) the results are in
agreement with our data sample, within 10-20 %, for about 90% of the patients.
The quantitative indications suggest the best time for surgery. The analysis is
patient oriented, weakly model dependent and can be applied to other cancer
phenotypes.

Sleep disorders can be a negative factor both for learning as for the mental
and physical development of adolescents. It has been shown that, in many
populations, adolescents tend to have a poor sleep quality, and a very late
chronotype. Furthermore, these features peak at adolescence, in the sense that
adults tend to sleep better and have an earlier chronotype. But what happens
when we consider adolescents in a population where already adults have poor
sleep quality and a very late chronotype? We have conducted two non-clinical
studies in the city of Bariloche, Argentina aimed at measuring sleep quality,
chronotype, and social jet lag, using the Pittsburgh and Munich questionnaires.
These were administered individually to groups of high school students, as well
as to smaller samples of adults and preadolescents, in order to study
differences between adolescents and these groups. The results show that in this
population sleep quality is much poorer than in most other healthy populations
recorded elsewhere. Furthermore, sleep quality is consistently worse for
adolescents than for the other groups. The difference with adults seems to be
due mainly to increased daytime sleepiness and sleep latency, whereas the
difference with preadolescents seems to be due mainly to shorter sleep
duration. We also found that the chronotypes of all the groups are very late,
with a peak at an age between 18 and 24 ys. Social jet lag and sleep onset
latency are also large, and they peak at adolescence, which suggests that they
might be closely related to the large prevalence of poor sleep quality that we
find in adolescents.

Mesoporous bioactive glasses, MBG, are gaining increasing interest in the
design of new biomaterials for bone defects treatment. An important research
trend to enhance their biological behavior is the inclusion of moderate amounts
of oxides with therapeutical action such as CuO. In this paper, MBG with
composition were synthesized, investigating the influence of the CuO content
and some synthesis parameters in their properties. Two batch were developed;
first one using HCl as catalyst and chlorides as CaO and CuO precursors, second
one, using HNO3 and nitrates. MBG of chlorides batch exhibited calcium/copper
phosphate nanoparticles, between 10 and 20 nm. Nevertheless, CuO- containing
MBG of nitrates batch showed nuclei of metallic copper nanoparticles larger
than 50 nm and quicker in vitro bioactive responses. Thus, they were coated by
an apatite-like layer after 24 h soaked in simulated body fluid, a remarkably
short period for MBG containing up to 5 % of CuO. A model, focused in the
copper location in the glass network, was proposed to relate nanostructure and
in vitro behaviour. Moreover, after 24 h in MEM or THB culture media, all the
MBG released therapeutic amounts of Ca2+ and Cu2+ ions. Because the quick
bioactive response in SBF, the capacity to host biomolecules in their pores and
to release therapeutic concentrations of Ca2+ and Cu2+ ions, MBG of the nitrate
batch are considered as excellent biomaterials for bone regeneration.

One key task in virtual screening is to accurately predict the binding
affinity ($\triangle$$G$) of protein-ligand complexes. Recently, deep learning
(DL) has significantly increased the predicting accuracy of scoring functions
due to the extraordinary ability of DL to extract useful features from raw
data. Nevertheless, more efforts still need to be paid in many aspects, for the
aim of increasing prediction accuracy and decreasing computational cost. In
this study, we proposed a simple scoring function (called OnionNet-2) based on
convolutional neural network to predict $\triangle$$G$. The protein-ligand
interactions are characterized by the number of contacts between protein
residues and ligand atoms in multiple distance shells. Compared to published
models, the efficacy of OnionNet-2 is demonstrated to be the best for two
widely used datasets CASF-2016 and CASF-2013 benchmarks. The OnionNet-2 model
was further verified by non-experimental decoy structures from docking program
and the CSAR NRC-HiQ data set (a high-quality data set provided by CSAR), which
showed great success. Thus, our study provides a simple but efficient scoring
function for predicting protein-ligand binding free energy.

Bone regeneration is a clinical challenge that requires multiple approaches.
Sometimes, it also includes the development of new osteogenic and antibacterial
biomaterials to treat the occurrence of possible infection processes derived
from surgery. This study evaluates the antibacterial properties of
meso-macroporous scaffolds coated with gelatin and based on a bioactive glass
and after being doped with 4% ZnO (4ZN-GE) and loaded with saturated and
minimally inhibitory concentrations of one of the antibiotics levofloxacin
(LEVO), vancomycin (VANCO), rifampicin (RIFAM) or gentamicin (GENTA). After the
physicochemical characterization of the materials, inorganic ion and antibiotic
release studies were performed from the scaffolds. In addition, molecular
modeling allowed the determination of electrostatic potential density maps and
hydrogen bonds of the antibiotics and the glass matrix. In vitro antibacterial
studies (in plankton, inhibition halos and biofilm destruction) with S. aureus
and E. coli as model bacteria showed a synergistic effect of zinc ions and
antibiotics. The effect was especially noticeable in planktonic cultures of S.
aureus with 4ZN-GE scaffolds loaded with VANCO, LEVO or RIFAM and in cultures
of E. coli with LEVO or GENTA. Furthermore, S. aureus biofilms were completely
destroyed by 4ZN-GE scaffolds loaded with VANCO, LEVO or RIFAM and total
destruction of E. coli biofilm was achieved with 4ZN-GE scaffolds loaded with
GENTA or LEVO. This approach could be an important step in the fight against
microbial resistance and provide much needed options for the treatment of bone
infection.

Previously we have developed the concept of the dynamic pathosome, which
suggests that individual patterns of phenotype development, i.e., phenotypic
trajectories, contain more information than is commonly appreciated and that a
phenotype's past trajectory predicts its future development. In this article,
we present a pathosome-inspired approach to analyzing longitudinal data by
functional linear models. We demonstrate how to use this approach and compare
it with classical linear models on data from the Czech section of the European
Longitudinal Study of Pregnancy and Childhood (ELSPAC). Our results show that
functional linear models explain more observed variance in age at menarche from
height and weight data than the commonly used approaches. Furthermore, we
demonstrate that functional linear models can be used to identify crucial time
points that can be used to create linear models achieving almost the same
performance as functional linear models. In addition, we use data from the
Berkeley growth study (BGS) to demonstrate that growth trajectories from birth
to 15 years can be used to explain 97% of observed variance of height at 18
years, thus supporting the notion that a phenotype's past trajectory affects
its future course. Overall, this article presents experimental support for the
concept of the dynamic pathosome and presents a method that can be used as a
powerful tool for analyzing quantitative longitudinal data.

Network inference is a major field of interest for the ecological community,
especially in light of the high cost and difficulty of manual observation, and
easy availability of remote, long term monitoring data. In addition, comparing
across similar network structures, especially with spatial, environmental, or
temporal variability and, simulating processes on networks to create toy models
and hypotheses - are topics of considerable interest to the researchers. A
large number of methods are being developed in the network science community to
achieve these objectives but either don't have their code available or an
implementation in R, the language preferred by ecologists and other biologists.
We provide a suite of three packages which will provide a central suite of
standardized network inference methods from time-series data (constructnet),
distance metrics (disgraph) and (process) simulation models (dynet) to the
growing R network analysis environment and would help ecologists and biologists
to perform and compare methods under one roof. These packages are implemented
in a coherent, consistent framework - making comparisons across methods and
metrics easier. We hope that these tools in R will help increase the
accessibility of network tools to ecologists and other biologists, who the
language for most of their analysis.

Advances in imagery at atomic and near-atomic resolution, such as cryogenic
electron microscopy (cryo-EM), have led to an influx of high resolution images
of proteins and other macromolecular structures to data banks worldwide.
Producing a protein structure from the discrete voxel grid data of cryo-EM maps
involves interpolation into the continuous spatial domain. We present a novel
data format called the neural cryo-EM map, which is formed from a set of neural
networks that accurately parameterize cryo-EM maps and provide native,
spatially continuous data for density and gradient. As a case study of this
data format, we create graph-based interpretations of high resolution
experimental cryo-EM maps. Normalized cryo-EM map values interpolated using the
non-linear neural cryo-EM format are more accurate, consistently scoring less
than 0.01 mean absolute error, than a conventional tri-linear interpolation,
which scores up to 0.12 mean absolute error. Our graph-based interpretations of
115 experimental cryo-EM maps from 1.15 to 4.0 Angstrom resolution provide high
coverage of the underlying amino acid residue locations, while accuracy of
nodes is correlated with resolution. The nodes of graphs created from atomic
resolution maps (higher than 1.6 Angstroms) provide greater than 99% residue
coverage as well as 85% full atomic coverage with a mean of than 0.19 Angstrom
root mean squared deviation (RMSD). Other graphs have a mean 84% residue
coverage with less specificity of the nodes due to experimental noise and
differences of density context at lower resolutions. This work may be
generalized for transforming any 3D grid-based data format into non-linear,
continuous, and differentiable format for the downstream geometric deep
learning applications.

Cryo-Electron Tomography (cryo-ET) is a new 3D imaging technique with
unprecedented potential for resolving submicron structural detail. Existing
volume visualization methods, however, cannot cope with its very low
signal-to-noise ratio. In order to design more powerful transfer functions, we
propose to leverage soft segmentation as an explicit component of visualization
for noisy volumes. Our technical realization is based on semi-supervised
learning where we combine the advantages of two segmentation algorithms. A
first weak segmentation algorithm provides good results for propagating sparse
user provided labels to other voxels in the same volume. This weak segmentation
algorithm is used to generate dense pseudo labels. A second powerful
deep-learning based segmentation algorithm can learn from these pseudo labels
to generalize the segmentation to other unseen volumes, a task that the weak
segmentation algorithm fails at completely. The proposed volume visualization
uses the deep-learning based segmentation as a component for segmentation-aware
transfer function design. Appropriate ramp parameters can be suggested
automatically through histogram analysis. Finally, our visualization uses
gradient-free ambient occlusion shading to further suppress visual presence of
noise, and to give structural detail desired prominence. The cryo-ET data
studied throughout our technical experiments is based on the highest-quality
tilted series of intact SARS-CoV-2 virions. Our technique shows the high impact
in target sciences for visual data analysis of very noisy volumes that cannot
be visualized with existing techniques.

Tuberculosis remains today a major public health issue with a total of 9
million new cases and 2 million deaths annually. The lack of an effective
vaccine and the increasing emergence of new strains of Mycobacterium
tuberculosis (Mtb) highly resistant to antibiotics, anticipate a complicated
scenario in the near future. The use of nanoparticles features as an
alternative to antibiotics in tackling this problem due to their potential
effectiveness in resistant bacterial strains. In this context, silver
nanoparticles have demonstrated high bactericidal efficacy, although their use
is limited by their relatively high toxicity, which calls for the design of
nanocarriers that allow silver based nanoparticles to be safely delivered to
the target cells or tissues. In this work mesoporous silica nanoparticles are
used as carriers of silver based nanoparticles as antimycobacterial agent
against Mtb. Two different synthetic approaches have been used to afford, on
the one hand, a 2D hexagonal mesoporous silica nanosystem which contains silver
bromide nanoparticles distributed all through the silica network and, on the
other hand, a core@shell nanosystem with metallic silver nanoparticles as core
and mesoporous silica shell in a radial mesoporous rearrangement. Both
materials have demonstrated good antimycobacterial capacity in in vitro test
using Mtb, being lower the minimum inhibitory concentration for the nanosystem
which contains silver bromide. Therefore, the interaction of this material with
the mycobacterial cell has been studied by cryo-electron microscopy,
establishing a direct connection between the antimycobactericidal effect
observed and the damage induced in the cell envelope.

Abortion is one of the biggest causes of maternal deaths, accounting for 15%
of maternal deaths in Southeast Asia. The increase in and effectiveness of
using contraception are still considered to be the effective method to reduce
abortion rate. Data pertaining to abortion incidence and effective efforts to
reduce abortion rate in Indonesia is limited and difficult to access. Meanwhile
such supporting information is necessary to enable the planning and evaluation
of abortion control programs. This paper exemplifies the use of a mathematical
model to explain an abortion decline scenario. The model employs determinants
proposed by Bongaarts, which include average reproductive period, contraceptive
prevalence and effectiveness, total fertility rate (TFR), and intended total
fertility rate (ITFR), as well as birth and abortion intervals. The data used
is from the 1991-2007 Indonesian Demography and Health Survey (Survei Demografi
dan Kesehatan Indonesia/SDKI), and the unit of analysis is women who had been
married and aged 15-49 years old. Based on the current contraceptive prevalence
level in Indonesia at 59-61%, the estimated total abortion rate is 1.9-2.2.
Based on the plot of this total abortion rate, an abortion decline scenario can
be estimated. At the current TFR level of 2.6, the required contraceptive
prevalence is 69% (9% increase) for a decrease of one abortion case per woman.
With a delay of one year in the age of the first marriage and a birth interval
of three years, it is estimated that the abortion rate will decline from 3.05
to 0.69 case per woman throughout her reproductive period. Based on the
assumption of contraceptive prevalence growth at 1-1.4%, it can be estimated
that abortion rate will reach nearly 0 between 2018 and 2022.

Due to delay in reporting, the daily national and statewide COVID-19
incidence counts are often unreliable and need to be estimated from recent
data. This process is known in economics as nowcasting. We describe in this
paper a simple random forest statistical model for nowcasting the COVID - 19
daily new infection counts based on historic data along with a set of simple
covariates, such as the currently reported infection counts, day of the week,
and time since first reporting. We apply the model to adjust the daily
infection counts in Ohio, and show that the predictions from this simple
data-driven method compare favorably both in quality and computational burden
to those obtained from the state-of-the-art hierarchical Bayesian model
employing a complex statistical algorithm.

Dynamic Causal Modeling (DCM) is a Bayesian framework for inferring on hidden
(latent) neuronal states, based on measurements of brain activity. Since its
introduction in 2003 for functional magnetic resonance imaging data, DCM has
been extended to electrophysiological data, and several variants have been
developed. Their biophysically motivated formulations make these models
promising candidates for providing a mechanistic understanding of human brain
dynamics, both in health and disease. However, due to their complexity and
reliance on concepts from several fields, fully understanding the mathematical
and conceptual basis behind certain variants of DCM can be challenging. At the
same time, a solid theoretical knowledge of the models is crucial to avoid
pitfalls in the application of these models and interpretation of their
results. In this paper, we focus on one of the most advanced formulations of
DCM, i.e. conductance-based DCM for cross-spectral densities, whose components
are described across multiple technical papers. The aim of the present article
is to provide an accessible exposition of the mathematical background, together
with an illustration of the model's behavior. To this end, we include
step-by-step derivations of the model equations, point to important aspects in
the software implementation of those models, and use simulations to provide an
intuitive understanding of the type of responses that can be generated and the
role that specific parameters play in the model. Furthermore, all code utilized
for our simulations is made publicly available alongside the manuscript to
allow readers an easy hands-on experience with conductance-based DCM.

Modern sleep monitoring development is shifting towards the use of
unobtrusive sensors combined with algorithms for automatic sleep scoring. Many
different combinations of wet and dry electrodes, ear-centered,
forehead-mounted or headband-inspired designs have been proposed, alongside an
ever growing variety of machine learning algorithms for automatic sleep
scoring. In this paper, we compare 13 different, realistic sensor setups
derived from the same data set and analysed with the same pipeline. We find
that all setups which include both a lateral and an EOG derivation show
similar, state-of-the-art performance, with average Cohen's kappa values of at
least 0.80. This indicates that electrode distance, rather than position, is
important for accurate sleep scoring. Finally, based on the results presented,
we argue that with the current competitive performance of automated staging
approaches, there is an urgent need for establishing an improved benchmark
beyond current single human rater scoring.

Light detection and ranging (LiDAR) provides information on the vertical
structure of forest stands enabling detailed and extensive ecosystem study. The
vertical structure is often summarized by scalar features and data-reduction
techniques that limit the interpretation of results. Instead, we quantified the
influence of three variables, species, crown cover, and age, on the vertical
distribution of airborne LiDAR returns from forest stands. We studied 5,428
regular, even-aged stands in Quebec (Canada) with five dominant species: balsam
fir (Abies balsamea (L.) Mill.), paper birch (Betula papyrifera Marsh), black
spruce (Picea mariana (Mill.) BSP), white spruce (Picea glauca Moench) and
aspen (Populus tremuloides Michx.). We modeled the vertical distribution
against the three variables using a functional general linear model and a novel
nonparametric graphical test of significance. Results indicate that LiDAR
returns from aspen stands had the most uniform vertical distribution. Balsam
fir and white birch distributions were similar and centered at around 50% of
the stand height, and black spruce and white spruce distributions were skewed
to below 30% of stand height (p<0.001). Increased crown cover concentrated the
distributions around 50% of stand height. Increasing age gradually shifted the
distributions higher in the stand for stands younger than 70-years, before
plateauing and slowly declining at 90-120 years. Results suggest that the
vertical distributions of LiDAR returns depend on the three variables studied.

This study aimed to optimize the extraction conditions in order to maximize
the recovery yields of quercetin and total flavonoids from red onion skin
wastes using sequential microwave-ultrasound-assisted extraction. Five
effective factors on quercetin extraction yield were investigated using
response surface methodology. The method was successfully performed under
optimal conditions of 60 s microwave irradiation followed by 15 min sonication
at 70 {\deg}C, 70% ethanol with solvent to solid ratio of 30 mL/g. Based on the
optimization results, ultrasound temperature was found to be a highly
significant and influential factor for the recovery. The maximum recovery
yields of quercetin and total flavonoids from red onion skin were estimated to
be 10.32 and 12.52%, respectively. The predicted values for quercetin (10.05%)
and total flavonoids (12.72%) were very close to the experimental results. The
recovery yields obtained from different extraction methods were as follows:
ultrasound-microwave-assisted extraction (7.66% quercetin and 10.18% total
flavonoids), ultrasound-assisted extraction (5.36% quercetin and 8.34% total
flavonoids), and microwave-assisted extraction (5.03% quercetin and 7.91 %
total flavonoids). The validity of the projected model was examined by the
obtained experimental data; in which, the validated model was suitable for the
recovery of the valuable products from onion skin wastes for further scale-up
in the food processes.

Computational optimal feedback control (OFC) models in the sensorimotor
control literature span a vast range of different implementations. Among the
popular algorithms, finite-horizon, receding-horizon or infinite-horizon
linear-quadratic regulators (LQR) have been broadly used to model human
reaching movements. While these different implementations have their unique
merits, all three have limitations in simulating the temporal evolution of
visuomotor feedback responses. Here we propose a novel approach - a
mixed-horizon OFC - by combining the strengths of the traditional
finite-horizon and the infinite-horizon controllers to address their individual
limitations. Specifically, we use the infinite-horizon OFC to generate
durations of the movements, which are then fed into the finite-horizon
controller to generate control gains. We then demonstrate the stability of our
model by performing extensive sensitivity analysis of both re-optimisation and
different cost functions. Finally, we use our model to provide a fresh look to
previously published studies by reinforcing the previous results, providing
alternative explanations to previous studies, or generating new predictive
results for prior experiments.

The origins of herbal medicines are important for their treatment effect,
which could be potentially distinguished by electronic nose system. As the odor
fingerprint of herbal medicines from different origins can be tiny, the
discrimination of origins can be much harder than that of different categories.
Better feature extraction methods are significant for this task to be more
accurately done, but there lacks systematic studies on different feature
extraction methods. In this study, we classified different origins of three
categories of herbal medicines with different feature extraction methods:
manual feature extraction, mathematical transformation, deep learning
algorithms. With 50 repetitive experiments with bootstrapping, we compared the
effectiveness of the extractions with a two-layer neural network w/o
dimensionality reduction methods (principal component analysis, linear
discriminant analysis) as the three base classifiers. Compared with the
conventional aggregated features, the Fast Fourier Transform method and our
novel approach (longitudinal-information-in-a-line) showed an significant
accuracy improvement(p < 0.05) on all 3 base classifiers and all three herbal
medicine categories. Two of the deep learning algorithm we applied also showed
partially significant improvement: one-dimensional convolution neural
network(1D-CNN) and a novel graph pooling based framework - multivariate time
pooling(MTPool).

Quality traits are some of the most important and time-consuming phenotypes
to evaluate in plant breeding programs. These traits are often evaluated late
in the breeding pipeline due to their cost, resulting in the potential
advancement of many lines that are not suitable for release. Near-infrared
spectroscopy (NIRS) is a non-destructive tool that can rapidly increase the
speed at which quality traits are evaluated. However, most spectrometers are
non-portable or prohibitively expensive. Recent advancements have led to the
development of consumer-targeted, inexpensive spectrometers with demonstrated
potential for breeding applications. Unfortunately, the mobile applications for
these spectrometers are not designed to rapidly collect organized samples at
the scale necessary for breeding programs. To that end, we developed
Prospector, a mobile application that connects with LinkSquare portable NIR
spectrometers and allows breeders to efficiently capture NIR data. In this
report, we outline the core functionality of the app and how it can easily be
integrated into breeding workflows as well as the opportunities for further
development. Prospector and other high throughput phenotyping tools and
technologies are required for plant breeders to develop the next generation of
improved varieties necessary to feed a growing global population.

To enable personalized cancer treatment, machine learning models have been
developed to predict drug response as a function of tumor and drug features.
However, most algorithm development efforts have relied on cross validation
within a single study to assess model accuracy. While an essential first step,
cross validation within a biological data set typically provides an overly
optimistic estimate of the prediction performance on independent test sets. To
provide a more rigorous assessment of model generalizability between different
studies, we use machine learning to analyze five publicly available cell
line-based data sets: NCI60, CTRP, GDSC, CCLE and gCSI. Based on observed
experimental variability across studies, we explore estimates of prediction
upper bounds. We report performance results of a variety of machine learning
models, with a multitasking deep neural network achieving the best cross-study
generalizability. By multiple measures, models trained on CTRP yield the most
accurate predictions on the remaining testing data, and gCSI is the most
predictable among the cell line data sets included in this study. With these
experiments and further simulations on partial data, two lessons emerge: (1)
differences in viability assays can limit model generalizability across
studies, and (2) drug diversity, more than tumor diversity, is crucial for
raising model generalizability in preclinical screening.

Diffusion magnetic resonance imaging (dMRI) tractography is an advanced
imaging technique that enables in vivo mapping of the brain's white matter
connections at macro scale. Over the last two decades, the study of brain
connectivity using dMRI tractography has played a prominent role in the
neuroimaging research landscape. In this paper, we provide a high-level
overview of how tractography is used to enable quantitative analysis of the
brain's structural connectivity in health and disease. We first provide a
review of methodology involved in three main processing steps that are common
across most approaches for quantitative analysis of tractography, including
methods for tractography correction, segmentation and quantification. For each
step, we aim to describe methodological choices, their popularity, and
potential pros and cons. We then review studies that have used quantitative
tractography approaches to study the brain's white matter, focusing on
applications in neurodevelopment, aging, neurological disorders, mental
disorders, and neurosurgery. We conclude that, while there have been
considerable advancements in methodological technologies and breadth of
applications, there nevertheless remains no consensus about the "best"
methodology in quantitative analysis of tractography, and researchers should
remain cautious when interpreting results in research and clinical
applications.

Mathematical modeling and simulation is a promising approach to personalized
cancer medicine. Yet, the complexity, heterogeneity and multi-scale nature of
cancer pose significant computational challenges. Coupling discrete cell-based
models with continuous models using hybrid cellular automata is a powerful
approach for mimicking biological complexity and describing the dynamical
exchange of information across different scales. However, when clinically
relevant cancer portions are taken into account, such models become
computationally very expensive. While efficient parallelization techniques for
continuous models exist, their coupling with discrete models, particularly
cellular automata, necessitates more elaborate solutions. Building upon FEniCS,
a popular and powerful scientific computing platform for solving partial
differential equations, we developed parallel algorithms to link stochastic
cellular automata with differential equations (
https://bitbucket.org/HTasken/cansim ). The algorithms minimize the
communication between processes that share cellular automata neighborhood
values while also allowing for reproducibility during stochastic updates. We
demonstrated the potential of our solution on a complex hybrid cellular
automaton model of breast cancer treated with combination chemotherapy. On a
single-core processor, we obtained nearly linear scaling with an increasing
problem size, whereas weak parallel scaling showed moderate growth in solving
time relative to increase in problem size. Finally we applied the algorithm to
a problem that is 500 times larger than previous work, allowing us to run
personalized therapy simulations based on heterogeneous cell density and tumor
perfusion conditions estimated from magnetic resonance imaging data on an
unprecedented scale.

Microbiota profiles measure the structure of microbial communities in a
defined environment (known as microbiomes). In the past decade, microbiome
research has focused on health applications as a result of which the gut
microbiome has been implicated in the development of a broad range of diseases
such as obesity, inflammatory bowel disease, and major depressive disorder. A
key goal of many microbiome experiments is to characterise or describe the
microbial community. High-throughput sequencing is used to generate microbiota
profiles, but data gathered via this method are extremely challenging to
analyse, as the data violate multiple strong assumptions of standard models.
Rough Set Theory (RST) has weak assumptions that are less likely to be
violated, and offers a range of attractive tools for extracting knowledge from
complex data. In this paper we present the first application of RST for
characterising microbiomes. We begin with a demonstrative benchmark microbiota
profile and extend the approach to gut microbiomes gathered from depressed
subjects to enable knowledge discovery. We find that RST is capable of
excellent characterisation of the gut microbiomes in depressed subjects and
identifying previously undescribed alterations to the microbiome-gut-brain
axis. An important aspect of the application of RST is that it provides a
possible solution to an open research question regarding the search for an
optimal normalisation approach for microbiome census data, as one does not
currently exist.

A TL-LUE model modified with a radiation scalar (RTL-LUE) is developed in
this paper. The same maximum LUE is used for both sunlit and shaded leaves, and
the difference in LUE between sunlit and shaded leaf groups is determined by
the same radiation scalar. The RTL-LUE model was calibrated and validated at
global 169 FLUXNET eddy covariance (EC) sites. Results indicate that although
GPP simulations from the TL-LUE model match well with the EC GPP, the RTL-LUE
model can further improve the simulation, for half-hour, 8-day, and yearly time
scales. The TL-LUE model tends to overestimate GPP under conditions of high
incoming photosynthetically active radiation (PAR), because the
radiation-independent LUE values for both sunlit and shaded leaves are only
suitable for low-medium (e.g. average) incoming PAR conditions. The errors in
the RTL-LUE model show lower sensitivity to PAR, and its GPP simulations can
better track the diurnal and seasonal variations of EC GPP by alleviating the
overestimation at noon and growing seasons associated with the TL-LUE model.
This study demonstrates the necessity of considering a radiation scalar in GPP
simulation in LUE models even if the first-order effect of radiation is already
considered through differentiating sunlit and shaded leaves. The simple RTL-LUE
developed in this study would be a useful alternative to complex process-based
models for global carbon cycle research.

As metabolomics datasets are becoming larger and more complex, there is an
increasing need for model-based data integration and analysis to optimally
leverage these data. Dynamical models of metabolism allow for the integration
of heterogeneous data and the analysis of dynamical phenotypes. Here, we review
recent efforts in using dynamical metabolic models for data integration,
focusing on approaches that are not restricted to steady-state measurements or
that require flux distributions as inputs. Furthermore, we discuss recent
advances and current challenges. We conclude that much progress has been made
in various areas, such as the development of scalable simulation tools, and
that, although challenges remain, dynamical modeling is a powerful tool for
metabolomics data analysis that is not yet living up to its full potential.

Despite substantial potential to transform bioscience, medicine, and
bioengineering, whole-cell models remain elusive. One of the biggest challenges
to whole-cell models is assembling the large and diverse array of data needed
to model an entire cell. Thanks to rapid advances in experimentation, much of
the necessary data is becoming available. Furthermore, investigators are
increasingly sharing their data due to increased emphasis on reproducibility.
However, the scattered organization of this data continues to hamper modeling.
Toward more predictive models, we highlight the challenges to assembling the
data needed for whole-cell modeling and outline how we can overcome these
challenges by working together to build a central data warehouse.

Ordinary differential equation models are nowadays widely used for the
mechanistic description of biological processes and their temporal evolution.
These models typically have many unknown and non-measurable parameters, which
have to be determined by fitting the model to experimental data. In order to
perform this task, known as parameter estimation or model calibration, the
modeller faces challenges such as poor parameter identifiability, lack of
sufficiently informative experimental data, and the existence of local minima
in the objective function landscape. These issues tend to worsen with larger
model sizes, increasing the computational complexity and the number of unknown
parameters. An incorrectly calibrated model is problematic because it may
result in inaccurate predictions and misleading conclusions. For non-expert
users, there are a large number of potential pitfalls. Here, we provide a
protocol that guides the user through all the steps involved in the calibration
of dynamic models. We illustrate the methodology with two models, and provide
all the code required to reproduce the results and perform the same analysis on
new models. Our protocol provides practitioners and researchers in biological
modelling with a one-stop guide that is at the same time compact and
sufficiently comprehensive to cover all aspects of the problem.

The mainstay of canine rabies control is fixed point mass dog vaccination
campaigns (MDVC). However, in some regions, ideal vaccination coverage in dogs
is not obtained due to low participation in the MDVC. Travel distance to the
vaccination sites has been identified as an important barrier to participation.
We aim to increase MDVC participation by optimally placing fixed point
vaccination locations to minimize walking distance to the nearest vaccination
location. We quantified participation probability based on walking distance to
the nearest vaccination point using a Poisson regression model. The regression
was fit with survey data collected from 2016-2019. We then used a computational
recursive interchange technique to solve the facility location problem to find
a set of optimal placements of fixed point vaccination locations. Finally, we
compared predicted participation of optimally placed vaccination sites to
historical participation data from surveys collected from 2016-2019. We
identified the p-median algorithm to solve the facility location problem as
ideal for fixed point vaccination placement. We found a predicted increase in
MDVC participation if vaccination locations are placed optimally. We also found
a more even vaccination coverage with optimized vaccination sites; however, the
workload in some optimized locations increased significantly. We developed a
data-driven computational algorithm to combat an ongoing rabies epidemic by
optimally using limited resources to maximize vaccination coverage. The main
positive effects we expect if this algorithm is to be implemented would be
increased overall vaccination coverage and increased spatial evenness of
coverage. A potential negative effect could be the presence of long waiting
lines as participation increases.

What is the influence of chronic maternal prenatal stress (PS) on fetal iron
homeostasis? In a prospective case-control study in 164 pregnant women, we show
that cord blood transferrin saturation is lower in male stressed neonates. The
total effect of PS exposure on fetal ferritin revealed a decrease of 15.4%
compared with controls. Electrocardiogram-based Fetal Stress Index (FSI)
identified affected fetuses non-invasively during the third trimester of
gestation. FSI-based timely detection of fetuses affected by PS can support
early individualized iron supplementation and neurodevelopmental follow-up to
prevent long-term sequelae due to PS-exacerbated impairment of the iron
homeostasis.

At most 1-2% of the global virome has been sampled to date. Recent work has
shown that predicting which host-virus interactions are possible but
undiscovered or unrealized is, fundamentally, a network science problem. Here,
we develop a novel method that combines a coarse recommender system (Linear
Filtering; LF) with an imputation algorithm based on low-rank graph embedding
(Singular Value Decomposition; SVD) to infer host-virus associations. This
combination of techniques results in informed initial guesses based on directly
measurable network properties (density, degree distribution) that are refined
through SVD (which is able to leverage emerging features). Using this method,
we recovered highly plausible undiscovered interactions with a strong signal of
viral coevolutionary history, and revealed a global hotspot of unusually unique
but unsampled (or unrealized) host-virus interactions in the Amazon rainforest.
We develop several tests for quantifying the bias and realism of these
predictions, and show that the LF-SVD method is robust in each aspect. We
finally show that graph embedding of the imputed network can be used to improve
predictions of human infection from viral genome features, showing that the
global structure of the mammal-virus network provides additional insights into
human disease emergence.

In the past few decades, the development of fluorescent technologies and
microscopic techniques has greatly improved scientists' ability to observe
real-time single-cell activities. In this paper, we consider the filtering
problem associate with these advanced technologies, i.e., how to estimate
latent dynamic states of an intracellular multiscale stochastic reaction
network from time-course measurements of fluorescent reporters. A good solution
to this problem can further improve scientists' ability to extract information
about intracellular systems from time-course experiments.
  A straightforward approach to this filtering problem is to use a particle
filter where particles are generated by simulation of the full model and
weighted according to observations. However, the exact simulation of the full
dynamic model usually takes an impractical amount of computational time and
prevents this type of particle filters from being used for real-time
applications, such as transcription regulation networks. Inspired by the recent
development of hybrid approximations to multiscale chemical reaction networks,
we approach the filtering problem in an alternative way. We first prove that
accurate solutions to the filtering problem can be constructed by solving the
filtering problem for a reduced model that represents the dynamics as a hybrid
process. The model reduction is based on exploiting the time-scale separations
in the original network and, therefore, can greatly reduce the computational
effort required to simulate the dynamics. As a result, we are able to develop
efficient particle filters to solve the filtering problem for the original
model by applying particle filters to the reduced model. We illustrate the
accuracy and the computational efficiency of our approach using several
numerical examples.

This study aims to determine the demographic, epidemiologic, and clinical
characteristics of COVID-19 cases that are highly susceptible to COVID-19
infection, with longer hospitalization and at higher risk of mortality and to
provide insights that may be useful to assess the vaccination priority program
and allocate hospital resources. Methods that were used include descriptive
statistics, nonparametric analysis, and survival analysis. Results of the study
reveal that women are more susceptible to infection while men are at risk of
longer hospitalization and higher mortality. Significant risk factors to
COVID-19 mortality are older age, male sex, difficulty breathing, and
comorbidities like hypertension and diabetes. Patients with these combined
symptoms should be considered for admission to the COVID-19 facility for proper
management and care. Also, there is a significant delay in the testing and
diagnosis of those who died, implying that timeliness in the testing and
diagnosis of patients is crucial in patient survival.

Effective biosecurity practices in swine production are key in preventing the
introduction and dissemination of infectious pathogens. Ideally, biosecurity
practices should be chosen by their impact on bio-containment and
bio-exclusion, however quantitative supporting evidence is often unavailable.
Therefore, the development of methodologies capable of quantifying and ranking
biosecurity practices according to their efficacy in reducing risk have the
potential to facilitate better informed choices. Using survey data on
biosecurity practices, farm demographics, and previous outbreaks from 139
herds, a set of machine learning algorithms were trained to classify farms by
porcine reproductive and respiratory syndrome virus status, depending on their
biosecurity practices, to produce a predicted outbreak risk. A novel
interpretable machine learning toolkit, MrIML-biosecurity, was developed to
benchmark farms and production systems by predicted risk, and quantify the
impact of biosecurity practices on disease risk at individual farms.
Quantifying the variable impact on predicted risk 50% of 42 variables were
associated with fomite spread while 31% were associated with local
transmission. Results from machine learning interpretations identified similar
results, finding substantial contribution to predicted outbreak risk from
biosecurity practices relating to: the turnover and number of employees; the
surrounding density of swine premises and pigs; the sharing of trailers;
distance from the public road; and production type. In addition, the
development of individualized biosecurity assessments provides the opportunity
to guide biosecurity implementation on a case-by-case basis. Finally, the
flexibility of the MrIML-biosecurity toolkit gives it potential to be applied
to wider areas of biosecurity benchmarking, to address weaknesses in other
livestock systems and industry relevant diseases.

Automation is becoming ubiquitous in all laboratory activities, leading
towards precisely defined and codified laboratory protocols. However, the
integration between laboratory protocols and mathematical models is still
lacking. Models describe physical processes, while protocols define the steps
carried out during an experiment: neither cover the domain of the other,
although they both attempt to characterize the same phenomena. We should
ideally start from an integrated description of both the model and the steps
carried out to test it, to concurrently analyze uncertainties in model
parameters, equipment tolerances, and data collection. To this end, we present
a language to model and optimize experimental biochemical protocols that
facilitates such an integrated description, and that can be combined with
experimental data. We provide a probabilistic semantics for our language based
on a Bayesian interpretation that formally characterizes the uncertainties in
both the data collection, the underlying model, and the protocol operations. On
a set of case studies we illustrate how the resulting framework allows for
automated analysis and optimization of experimental protocols, including Gibson
assembly protocols.

Animal behavior and neural recordings show that the brain is able to measure
both the intensity of an odor and the timing of odor encounters. However,
whether intensity or timing of odor detections is more informative for
olfactory-driven behavior is not understood. To tackle this question, we
consider the problem of locating a target using the odor it releases. We ask
whether the position of a target is best predicted by measures of timing
intensity of its odor, sampled for a short period of time. To answer this
question, we feed data from accurate numerical simulations of odor transport to
machine learning algorithms that learn how to connect odor to target location.
We find that both intensity and timing can separately predict target location
even from a distance of several meters; however their efficacy varies with the
dilution of the odor in space. Thus organisms that use olfaction from different
ranges may have to switch among different modalities. This has implications on
how the brain should represent odors as the target is approached. We
demonstrate simple strategies to improve accuracy and robustness of the
prediction by modifying odor sampling and appropriately combining distinct
measures together. To test the predictions, animal behavior and odor
representation should be monitored as the animal moves relative to the target,
or in virtual conditions that mimic concentrated dilute environments.

This study exploits previously demonstrated properties such as sensitivity to
the spatial extent and the intensity of local image contrast of the
quantization error in the output of a Self Organizing Map (SOM QE). Here, the
SOM QE is applied to double color staining based cell viability data in 96
image simulations. The results show that the SOM QE consistently and in only a
few seconds detects fine regular spatial increases in relative amounts of RED
or GREEN pixel staining across the test images, reflecting small, systematic
increases or decreases in the percentage of theoretical cell viability below
the critical threshold. Such small changes may carry clinical significance, but
are almost impossible to detect by human vision. Moreover, we demonstrate a
clear sensitivity of the SOM QE to differences in the relative physical
luminance (Y) of the colors, which here translates into a RED GREEN color
selectivity. Across differences in relative luminance, the SOM QE exhibits
consistently greater sensitivity to the smallest spatial increases in RED image
pixels compared with smallest increases of identical spatial extents in GREEN
image pixels. Further selective color contrast studies on simulations of
biological imaging data will allow generating increasingly larger benchmark
datasets and, ultimately, unravel the full potential of fast, economic, and
unprecedentedly precise biological data analysis using the SOM QE.

Ecological networks such as plant-pollinator systems and food webs vary in
space and time. This variability includes fluctuations in global network
properties such as total number and intensity of interactions but also in the
local properties of individual nodes such as the number and intensity of
species-level interactions. Fluctuations of species properties can
significantly affect higher-order network features, e.g. robustness and
nestedness. Local fluctuations should therefore be controlled for in
applications that rely on null models, especially pattern and perturbation
detection. By contrast, most randomization methods for null models used by
ecologists treat node-level local properties as hard constraints that cannot
fluctuate. Here, we synthesise a set of methods that resolves the limit of hard
constraints and is based on statistical mechanics. We illustrate the methods
with some practical examples making available open source computer codes. We
clarify how this approach can be used by experimental ecologists to detect
non-random network patterns with null models that not only rewire but also
redistribute interaction strengths by allowing fluctuations in the null model
constraints (soft constraints). Null modelling of species heterogeneity through
local fluctuations around typical topological and quantitative constraints
offers a statistically robust and expanded (e.g. quantitative null models) set
of tools to understand the assembly and resilience of ecological networks.

Freely and openly shared low-cost electronic applications, known as open
electronics, have sparked a new open-source movement, with much un-tapped
potential to advance scientific research. Initially designed to appeal to
electronic hobbyists, open electronics have formed a global community of
"makers" and inventors and are increasingly used in science and industry. Here,
we review the current benefits of open electronics for scientific research and
guide academics to enter this emerging field. We discuss how electronic
applications, from the experimental to the theoretical sciences, can help (I)
individual researchers by increasing the customization, efficiency, and
scalability of experiments, while improving data quantity and quality; (II)
scientific institutions by improving access and maintenance of high-end
technologies, visibility and interdisciplinary collaboration potential; and
(III) the scientific community by improving transparency and reproducibility,
helping decouple research capacity from funding, increasing innovation, and
improving collaboration potential among researchers and the public. Open
electronics are powerful tools to increase creativity, democratization, and
reproducibility of research and thus offer practical solutions to overcome
significant barriers in science.

The Turing completeness result for continuous chemical reaction networks
(CRN) shows that any computable function over the real numbers can be computed
by a CRN over a finite set of formal molecular species using at most
bimolecular reactions with mass action law kinetics. The proof uses a previous
result of Turing completeness for functions defined by polynomial ordinary
differential equations (PODE), the dualrail encoding of real variables by the
difference of concentration between two molecular species, and a back-end
quadratization transformation to restrict to elementary reactions with at most
two reactants. In this paper, we present a polynomialization algorithm of
quadratic time complexity to transform a system of elementary differential
equations in PODE. This algorithm is used as a front-end transformation to
compile any elementary mathematical function, either of time or of some input
species, into a finite CRN. We illustrate the performance of our compiler on a
benchmark of elementary functions relevant to CRN design problems in synthetic
biology specified by mathematical functions. In particular, the abstract CRN
obtained by compilation of the Hill function of order 5 is compared to the
natural CRN structure of MAPK signalling networks.

Mathematical models of thrombosis are currently used to study clinical
scenarios of pathological thrombus formation. Most of these models involve
inherent uncertainties that must be assessed to increase the confidence in
model predictions and identify avenues of improvement for both thrombosis
modeling and anti-platelet therapies. In this work, an uncertainty
quantification analysis of a multi-constituent thrombosis model is performed
considering a common assay for platelet function (PFA-100). The analysis is
performed using a polynomial chaos expansion as a parametric surrogate for the
thrombosis model. The polynomial approximation is validated and used to perform
a global sensitivity analysis via computation of Sobol' coefficients. Six out
of fifteen parameters were found to be influential in the simulation
variability considering only individual effects. Nonetheless, parameter
interactions are highlighted when considering the total Sobol' indices. In
addition to the sensitivity analysis, the surrogate model was used to compute
the PFA-100 closure times of 300,000 virtual cases that align well with
clinical data. The current methodology could be used including common
anti-platelet therapies to identify scenarios that preserve the hematological
balance.

BACKGROUND: Meniere's Disease (MD) is a condition of the inner ear with
symptoms affecting both vestibular and hearing functions. Some patients with MD
experience vestibular drop attacks (VDAs), which are violent falls caused by
spurious vestibular signals from the utricle and/or saccule. Recent surgical
work has shown that patients who experience VDAs also show distrupted utricular
otolithic membranes. OBJECTIVE: The objective of this study is to determine if
otolithic membrane damage alone is sufficient to induce spurious vestibular
signals, thus potentially eliciting VDAs and the vestibular dysfunction seen in
patients with MD. METHODS: We use a previously developed numerical model to
describe the nonlinear dynamics of an array of active, elastically coupled hair
cells. We then reduce the coupling strength of a selected region of the
membrane to model the effects of tissue damage. RESULTS: As we reduce the
coupling strength, we observe large and abrupt spikes in hair bundle position.
As bundle displacements from the equilibrium position have been shown to lead
to depolarization of the hair-cell soma and hence trigger neural activity, this
spontaneous activity could elicit false detection of a vestibular signal.
CONCLUSIONS: The results of this numerical model suggest that otolithic
membrane damage alone may be sufficient to induce VDAs and the vestibular
dysfunction seen in patients with MD. Future experimental work is needed to
confirm these results in vitro.

The dynamic architecture of the microtubule cytoskeleton is crucial for cell
division, motility and morphogenesis. The dynamic properties of microtubules -
growth, shrinkage, nucleation and severing - are regulated by an arsenal of
microtubule-associated proteins (MAPs). The activities of many of these MAPs
have been reconstituted in vitro using microscope assays. As an alternative to
fluorescence microscopy, interference-reflection microscopy (IRM) has been
introduced as an easy-to-use, wide-field imaging technique that allows
label-free visualization of microtubules with high contrast and speed. IRM
circumvents several problems associated with fluorescence microscopy including
the high concentrations of tubulin required for fluorescent labeling, the
potential perturbation of function caused by the fluorophores, and the risks of
photodamage. IRM can be implemented on a standard epifluorescence microscope at
low cost and can be combined with fluorescence techniques like
total-internal-reflection-fluorescence (TIRF) microscopy. Here we describe the
experimental procedure to image microtubule dynamics and severing using IRM,
providing practical tips and guidelines to resolve possible experimental
hurdles.

Background: Many studies have analyzed the relationship between average
particulate matter less than 2.5{\mu}m in diameter (PM2.5) exposure and the
human health, but few studies have focused on the effect of the freqnecy of
PM2.5 concentration. The main purpose of this study is to analyze the
relationship between the first occurrence of hospitalization for cardiovascular
disease (CVD) and PM2.5 exposure, considering average PM2.5 concentration and
the frequency of high PM2.5 concentration simultaneously. Methods: We used
large-scale cohort data from the National Health Insurance Service of Korea
from 2002 to 2018, which includes a study population of 3,147,595. We estimated
hazard ratios (HR) and 95% confidence intervals (CI) using the Cox
proportional-hazards model with time-dependent covariates, including annual
average PM2.5 and the annual hours of PM2.5 concentration exceeding 55.5
{\mu}g/m3 (FH55), considering the individual residential moving. Results: We
found that the risk was elevated by 6.7% (95% CI, 6.2-7.3) for all CVD, by 7.1%
(95% CI, 5.9-8.4) for ischemic heart disease (IHD), and by 7.5% (95% CI,
6.0-9.1) for stroke per 1{\mu}g/m3 increase of average PM2.5. Interestingly,
the 10-hour increase of FH55 decreased the risk of all CVD, IHD, and stroke by
7.0% (95% CI: 7.0-7.1), 7.3% (95% CI: 7.1-7.4), and 7.4% (95% CI: 7.2-7.6),
respectively. This is due to people checking real-time or daily PM2.5
concentration information and voluntarily avoiding outdoor activities and
exposure to high PM2.5 concentrations. Conclusions: Based on the results, we
conclude that accurate forecasts, information disclosure, and timely warning of
high concentrations of PM2.5 at the national level have the potential to reduce
the risk of CVD occurrence.

Plant breeding is fundamentally comprised of three cyclic activities: 1)
intermating lines to generate novel allelic combinations, 2) evaluation of new
plant cultivars in distinct environments, and 3) selection of superior
individuals to be used as parents in the next breeding cycle. While digital
technologies and tools are commonly utilized for the latter two stages, many
plant research programs still rely on manual annotation and paper tags to track
the crosses that constitute the basis of a plant breeding program. This
presence of analog data is a crack in the foundation of a digital breeding
ecosystem and a significant occasion for errors to be introduced that will
propagate through the entire breeding program. However, implementing digital
cross tracking into breeding programs is difficult due to the non-standardized
workflows that different breeders have adopted. Intercross, an open-source
Android app, aims to provide scientists with a robust and simple solution for
planning, tracking, and managing the crosses being made each season and aims to
serve as the primary tool to digitize crossing data for breeding programs. The
simplicity and flexibility of Intercross allows rapid and broad adoption by
diverse breeding programs and will solidify the concepts of a digital breeding
ecosystem.

Blood feeding represents a critical event in the life cycle of female
mosquitoes. In addition to providing nutrients to the mosquito, blood feeding
facilitates the transmission of parasites and viruses to hosts, potentially
having devastating health consequences. Despite this, our understanding of
these short, yet important bouts of behaviour is incomplete. How and where a
mosquito decides to feed and the success of feeding can influence the
transmission of pathogens, while a more thorough understanding may allow
interventions to reduce or prevent infections. Recent advances in machine
vision and automated tracking presents the opportunity to observe and
understand blood feeding behaviour of mosquitoes at unprecedented spatial and
temporal resolution. Here, we combine these technologies with novel designs for
behavioural arenas and controllable artificial host cues to enable detailed
observations of biting behaviour using relatively inexpensive and readily
available materials. Combined with a workflow for quantitative image analysis,
we are able to describe nuanced, high resolution biting behaviour under tightly
controlled conditions.

We introduce a new method for analyzing the carbon budget using box models
and a mass balance approach. The method describes the net flow of carbon
between the atmosphere and other reservoirs. The method assumes that the data
can be explained by having the carbon move in and out of the atmosphere through
two reservoirs, one consisting of isotopically light carbon (biotic) and the
other consisting of isotopically heavy carbon (abiotic). The systems are
underdetermined from the data, so the Occam's razor approach is to assume that
the least amount of carbon moves between the atmosphere and the reservoirs. As
data from known sources and sinks are added to the model, one can determine the
constraints on the unknown sources and sinks. To illustrate the method, we
analyze data from the Mauna Loa Observatory and data from Antarctic ice cores.

When evaluating the clinical performance of a medical imaging device, a
multi-reader multi-case (MRMC) analysis is usually applied to account for both
case and reader variability. For a clinical task that equates to a quantitative
measurement, an agreement analysis such as a limits of agreement (LOA) method
can be used to compare different measurement methods. In this work, we
introduce four types of comparisons; these types differ depending on whether
the measurements are within or between readers and within or between
modalities. A three-way mixed effect ANOVA model is applied to estimate the
variances of individual differences, which is an essential step for estimating
LOA. To verify the estimates of LOA, we propose a hierarchical model to
simulate quantitative MRMC data. Two simulation studies were conducted to
validate both the simulation and the LOA variance estimates. From the
simulation results, we can conclude that our estimate of variance is unbiased,
and the uncertainty of the estimation drops as the number of readers and cases
increases and rises as the value of true variance increases.

As a novel biomarker from the Fanconi anemia complementation group (FANC)
family, FANCA is antigens to Leukemia cancer. The overexpression of FANCA has
predicted the second most common cancer in the world that is responsible for
cancer-related deaths. Non-synonymous SNPs are an essential group of SNPs that
lead to alterations in encoded polypeptides. Changes in the amino acid
sequences of gene products lead to Leukemia. First, we study individual SNPs in
the coding region of FANCA and computational tools like PROVEAN, PolyPhen2,
MuPro, and PANTHER to compute deleterious mutation scores. The
three-dimensional structural and functional prediction conducted using
I-TASSER. Further, the predicted structure refined using the GlaxyWeb tool. In
the study, the proteomic data has been retrieved from the UniProtKB. The coding
region of the dataset contains 100 non-synonymous single nucleotide
polymorphisms (nsSNPs), and 24 missense SNPs have been determined as
deleterious by all analyses. In this work, six well-known computational tools
were employed to study Leukemia-associated nsSNPs. It is inferred that these
nsSNPs could play their role in the up-regulation of FANCA, which further leads
to provoke leukemia advancement. The current research would benefit researchers
and practitioners in handling cancer-associated diseases related to FANCA. The
proposed study would also help to develop precision medicine in the field of
drug discovery.

Infectious diseases in livestock are well-known to infect multiple hosts and
persist through the combination of within- and between-host transmission
pathways. Uncertainty remains about the epidemic consequences of the disease
being introduced on farms with more than one susceptible host. Here we describe
multi-host contact networks to elucidate the potential of disease spread among
farms with multiple species. Four years of between-farm animal movement data of
bovine, swine, small ruminants, and multi-host, were described through both
static and time-series networks; the in-going and out-going contact chains were
also calculated. We use the proposed stochastic multilevel model to simulate
scenarios in which infection was seeded into a single host and multi-hosts
farms, to estimate epidemic trajectories and simulate network-based control
actions to assess the reduction of secondarily infected farms. Our analysis
showed that the swine network was more connected than cattle and small
ruminants in the temporal network view. The small ruminants network was shown
disconnected, however, allowing the interaction among networks with different
hosts enabling the spread of disease throughout the network. Independently from
the initial infected host, secondary infections were observed crossing overall
species. We showed that targeting the top 3.25% of the farms ranked by degree
could reduce the total number of infected farms below 70% at the end of the
simulation period. In conclusion, we demonstrated the potential of the
multi-host network in disease propagation, therefore, it becomes important to
consider the observed multi-host movement dynamics while designing surveillance
and preparedness control strategies.

We present Clinica (www.clinica.run), an open-source software platform
designed to make clinical neuroscience studies easier and more reproducible.
Clinica aims for researchers to i) spend less time on data management and
processing, ii) perform reproducible evaluations of their methods, and iii)
easily share data and results within their institution and with external
collaborators. The core of Clinica is a set of automatic pipelines for
processing and analysis of multimodal neuroimaging data (currently, T1-weighted
MRI, diffusion MRI and PET data), as well as tools for statistics, machine
learning and deep learning. It relies on the brain imaging data structure
(BIDS) for the organization of raw neuroimaging datasets and on established
tools written by the community to build its pipelines. It also provides
converters of public neuroimaging datasets to BIDS (currently ADNI, AIBL, OASIS
and NIFD). Processed data include image-valued scalar fields (e.g. tissue
probability maps), meshes, surface-based scalar fields (e.g. cortical thickness
maps) or scalar outputs (e.g. regional averages). These data follow the ClinicA
Processed Structure (CAPS) format which shares the same philosophy as BIDS.
Consistent organization of raw and processed neuroimaging files facilitates the
execution of single pipelines and of sequences of pipelines, as well as the
integration of processed data into statistics or machine learning frameworks.
The target audience of Clinica is neuroscientists or clinicians conducting
clinical neuroscience studies involving multimodal imaging, and researchers
developing advanced machine learning algorithms applied to neuroimaging data.

1. Movement is the primary means by which animals obtain resources and avoid
hazards. Most movement exhibits directional bias that is related to
environmental features (taxis), such as the location of food patches,
predators, ocean currents, or wind. Numerous behaviours with directional bias
can be characterized by maintaining orientation at an angle relative to the
environmental stimuli (menotaxis), including navigation relative to sunlight or
magnetic fields and energy-conserving flight across wind. However, no
statistical methods exist to flexibly classify and characterise such
directional bias. 2. We propose a biased correlated random walk model that can
identify menotactic behaviours by predicting turning angle as a trade-off
between directional persistence and directional bias relative to environmental
stimuli without making a priori assumptions about the angle of bias. We apply
the model within the framework of a multi-state hidden Markov model (HMM) and
describe methods to remedy information loss associated with coarse
environmental data to improve the classification and parameterization of
directional bias. 3. Using simulation studies, we illustrate how our method
more accurately classifies behavioural states compared to conventional
correlated random walk HMMs that do not incorporate directional bias. We
illustrate the application of these methods by identifying cross wind olfactory
foraging and drifting behaviour mediated by wind-driven sea ice drift in polar
bears (Ursus maritimus) from movement data collected by satellite telemetry. 4.
The extensions we propose can be readily applied to movement data to identify
and characterize behaviours with directional bias toward any angle, and open up
new avenues to investigate more mechanistic relationships between animal
movement and the environment.

Experiments that compare rhythmic properties across different genetic
alterations and entrainment conditions underlie some of the most important
breakthroughs in circadian biology. A robust estimation of the rhythmic
properties of the circadian signals goes hand in hand with these discoveries.
Widely applied traditional signal analysis methods such as fitting cosine
functions or Fourier transformations rely on the assumption that oscillation
periods do not change over time. However, novel high-resolution recording
techniques have shown that, most commonly, circadian signals exhibit
time-dependent changes of periods and amplitudes which cannot be captured with
the traditional approaches. In this chapter we introduce a method to determine
time-dependent properties of oscillatory signals, using the novel open-source
Python-based Biological Oscillations Analysis Toolkit (pyBOAT). We show with
examples how to detect rhythms, compute and interpret high-resolution
time-dependent spectral results, analyze the main oscillatory component and to
subsequently determine these main components time-dependent instantaneous
period, amplitude and phase. We introduce step-by-step how such an analysis can
be done by means of the easy-to-use point-and-click graphical user interface
(GUI) provided by pyBOAT or executed within a Python programming environment.
Concepts are explained using simulated signals as well as experimentally
obtained time series.

Carotid artery stenosis is the narrowing of carotid arteries, which supplies
blood to the neck and head. In this work, we train a model to predict the
severity of the stenosis blockage based on SRUC criteria variables and other
patient information. We implement classic machine learning methods, decision
trees and random forests, used in a previous experiment. In addition, we
improve the accuracy through the use of the state-of-art Augmented Neural ODE
deep learning method. Through systematic and theory-rooted analysis, we examine
different parameters to achieve an accuracy of about 77%. These results show
the strong potential in applying recently developing deep learning methods,
while simultaneously suggesting that the current data provided by the SRUC
criteria may be insufficient to predict stenosis severity at a high performance
level.

In machine learning applications, the reliability of predictions is
significant for assisted decision and risk control. As an effective framework
to quantify the prediction reliability, conformal prediction (CP) was developed
with the CPKNN (CP with kNN). However, the conventional CPKNN suffers from high
variance and bias and long computational time as the feature dimensionality
increases. To address these limitations, a new CP framework-conformal
prediction with shrunken centroids (CPSC) is proposed. It regularizes the class
centroids to attenuate the irrelevant features and shrink the sample space for
predictions and reliability quantification. To compare CPKNN and CPSC, we
employed them in the classification of 12 categories of alternative herbal
medicine with electronic nose as a case and assessed them in two tasks: 1)
offline prediction: the training set was fixed and the accuracy on the testing
set was evaluated; 2) online prediction with data augmentation: they filtered
unlabeled data to augment the training data based on the prediction reliability
and the final accuracy of testing set was compared. The result shows that CPSC
significantly outperformed CPKNN in both two tasks: 1) CPSC reached a
significantly higher accuracy with lower computation cost, and with the same
credibility output, CPSC generally achieves a higher accuracy; 2) the data
augmentation process with CPSC robustly manifested a statistically significant
improvement in prediction accuracy with different reliability thresholds, and
the augmented data were more balanced in classes. This novel CPSC provides
higher prediction accuracy and better reliability quantification, which can be
a reliable assistance in decision support.

Defective interfering particles arise spontaneously during a viral infection
as mutants lacking essential parts of the viral genome. Their ability to
replicate in the presence of the wild-type (WT) virus (at the expense of viable
viral particles) is mimicked and exploited by therapeutic interfering
particles. We propose a strategy for the design of therapeutic interfering RNAs
(tiRNAs) against positive-sense single-stranded RNA viruses that assemble via
packaging signal-mediated assembly. These tiRNAs contain both an optimised
version of the virus assembly manual that is encoded by multiple dispersed RNA
packaging signals and a replication signal for viral polymerase, but lack any
protein coding information. We use an intracellular model for hepatitis C viral
(HCV) infection that captures key aspects of the competition dynamics between
tiRNAs and viral genomes for virally produced capsid protein and polymerase. We
show that only a small increase in the assembly and replication efficiency of
the tiRNAs compared with WT virus is required in order to achieve a treatment
efficacy greater than 99%. This demonstrates that the proposed tiRNA design
could be a promising treatment option for RNA viral infections.

Hepatitis B virus is a global health threat, and its elimination by 2030 has
been prioritised by the World Health Organisation. Here we present an
age-structured model for the immune response to an HBV infection, which takes
into account contributions from both cell-mediated and humoral immunity. The
model has been validated using published patient data recorded during acute
infection. It has been adapted to the scenarios of chronic infection, clearance
of infection, and flare-ups via variation of the immune response parameters.
The impacts of immune response exhaustion and non-infectious subviral particles
on the immune response dynamics are analysed. A comparison of different
treatment options in the context of this model reveals that drugs targeting
aspects of the viral life cycle are more effective than exhaustion therapy, a
form of therapy mitigating immune response exhaustion. Our results suggest that
antiviral treatment is best started when viral load is declining rather than in
a flare-up. The model suggests that a fast antibody production rate always lead
to viral clearance, highlighting the promise of antibody therapies currently in
clinical trials.

Image-based experiments can yield many thousands of individual measurements
describing each object of interest, such as cells in microscopy screens.
CellProfiler Analyst is a free, open-source software package designed for the
exploration of quantitative image-derived data and the training of machine
learning classifiers with an intuitive user interface. We have now released
CellProfiler Analyst 3.0, which in addition to enhanced performance adds
support for neural network classifiers, identifying rare object subsets, and
direct transfer of objects of interest from visualisation tools into the
Classifier tool for use as training data. This release also increases
interoperability with the recently released CellProfiler 4, making it easier
for users to detect and measure particular classes of objects in their
analyses.

Background: Trypanosoma brucei is the causative agent of human African
sleeping sickness and Nagana in cattle. In addition to being an important
pathogen T. brucei has developed into a model system in cell biology. Results:
Using Stable Isotope Labelling of Amino acids in Cell culture (SILAC) in
combination with mass spectrometry we determined the abundance of >1600
proteins in the long slender (LS), short stumpy (SS) mammalian bloodstream form
stages relative to the procyclic (PC) insect-form stage. In total we identified
2645 proteins, corresponding to ~30% of the total proteome and for the first
time present a comprehensive overview of relative protein levels in three life
stages of the parasite. Conclusions: We can show the extent of pre-adaptation
in the SS cells, especially at the level of the mitochondrial proteome. The
comparison to a previously published report on monomorphic in vitro grown
bloodstream and procyclic T. brucei indicates a loss of stringent regulation
particularly of mitochondrial proteins in these cells when compared to the
pleomorphic in vivo situation. In order to better understand the different
levels of gene expression regulation in this organism we compared mRNA steady
state abundance with the relative protein abundance-changes and detected
moderate but significant correlation indicating that trypanosomes possess a
significant repertoire of translational and posttranslational mechanisms to
regulate protein abundance.

Despite popular media interest in uncovering the creative habits of
successful people, there is a surprising paucity of empirical research on the
diversity of tendencies and preferences people have when engaged in creative
work. We developed a simple survey that characterized 42 creative habits along
21 independent dimensions. Data from 9,633 respondents revealed seven 'Creative
Species', or clusters of people with combinations of creative habits that tend
to co-occur more than expected by chance. These emergent clusters where
relatively stable to random subsampling of the population and to variation in
model parameters. The seven Creative Species self-sorted along a primary
gradient from those characterized by more 'deliberate' creative habits (e.g.,
Monotasker, Risk Averse, Routine Seeker, Tenacious, Make it Happen) to those
characterized by more 'open' creative habits (e.g. Multitasker, Risk Friendly,
Novelty Seeker, Reframer, Let it Happen). A weaker second gradient was defined
by outward and rational vs inward and intuitive creators. For the subset of
respondents with data about their broad professional discipline (Art, Science,
and Business) and gender, some groups were more (or less) common in some
Creative Species than expected by chance, but the absolute magnitude of these
differences were generally small; and knowing the discipline or gender of a
person was not a good single predictor of their creative preferences or
tendencies. Together these results suggest that independent of discipline or
gender, people vary widely in the habits, behaviors, and contexts in which they
feel most creative. Understanding creative diversity is critical for improving
the creative performance of both individuals and collaborative teams.

Understanding primate behavior is a mission-critical goal of both biology and
biomedicine. Despite the importance of behavior, our ability to rigorously
quantify it has heretofore been limited to low-information measures like
preference, looking time, and reaction time, or to non-scaleable measures like
ethograms. However, recent technological advances have led to a major
revolution in behavioral measurement. Specifically, digital video cameras and
automated pose tracking software can provide detailed measures of full body
position (i.e., pose) of multiple primates over time (i.e., behavior) with high
spatial and temporal resolution. Pose-tracking technology in turn can be used
to detect behavioral states, such as eating, sleeping, and mating. The
availability of such data has in turn spurred developments in data analysis
techniques. Together, these changes are poised to lead to major advances in
scientific fields that rely on behavioral as a dependent variable. In this
review, we situate the tracking revolution in the history of the study of
behavior, argue for investment in and development of analytical and research
techniques that can profit from the advent of the era of big behavior, and
propose that zoos will have a central role to play in this era.

In this paper, a new framework for obtaining personalized optimal treatment
strategies in colon cancer-induced angiogenesis is presented. The dynamics of
colon cancer is given by a It\'o stochastic process, which helps in modeling
the randomness present in the system. The stochastic dynamics is then
represented by the Fokker-Planck (FP) partial differential equation (PDE) that
governs the evolution of the associated probability density function. The
optimal therapies are obtained using a three step procedure. First, a finite
dimensional FP-constrained optimization problem is formulated that takes input
individual noisy patient data, and is solved to obtain the unknown parameters
corresponding to the individual tumor characteristics. Next, a sensitivity
analysis of the optimal parameter set is used to determine the parameters to be
controlled, thus, helping in assessing the types of treatment therapies.
Finally, a feedback FP control problem is solved to determine the optimal
combination therapies. Numerical results with the combination drug, comprising
of Bevacizumab and Capecitabine, demonstrate the efficiency of the proposed
framework.

PCR-based analysis of skeletonized human remains is a common aspect in both
forensic human identification as well as Ancient DNA research. In this, both
areas not merely utilize very similar methodology, but also share the same
problems regarding quantity and quality of recovered DNA and presence of
inhibitory substances in samples from excavated remains. To enable
amplification based analysis of the remains, development of optimized DNA
extraction procedures is thus a critical factor in both areas. The study here
presents an optimized protocol for DNA extraction from ancient skeletonized
remains using Chelex-100, which proved to be effective in yielding amplifiable
extracts from sample material excavated after centuries in a soil environment,
which consequently have high inhibitor content and overall limited DNA
preservation. Success of the optimization strategies utilized is shown in
significantly improved amplification outcomes compared to the predecessor
method.

Discrete dynamical systems in which model components take on categorical
values have been successfully applied to biological networks to study their
global dynamic behavior. Boolean models in particular have been used
extensively. However, multi-state models have also emerged as effective
computational tools for the analysis of complex mechanisms underlying
biological networks. Models in which variables assume more than two discrete
states provide greater resolution, but this scheme introduces discontinuities.
In particular, variables can increase or decrease by more than one unit in one
time step. This can be corrected, without changing fixed points of the system,
by applying an additional rule to each local activation function. On the other
hand, if one is interested in cyclic attractors of their system, then this rule
can potentially introduce new cyclic attractors that were not observed
previously. This article makes some advancements in understanding the state
space dynamics of multi-state network models with a synchronous update schedule
and establishes conditions under which no new cyclic attractors are added to
networks when the additional rule is applied. Our analytical results have the
potential to be incorporated into modeling software and aid researchers in
their analyses of biological multi-state networks.

Decision-support systems benefit from hidden patterns extracted from digital
information. In the specific domain of gastropod characterization,
morphometrical measurements support biologists in the identification of land
snail specimens. Although snails can be easily identified by their excretory
and reproductive systems, the after-death mollusk body is commonly inaccessible
because of either soft material deterioration or fossilization. This study aims
at characterizing Brazilian land snails by morphometrical data features
manually taken from the shells. In particular, we examined a dataset of shells
by using different learning models that labeled snail specimens with a
precision up to 97.5% (F1-Score = .975, CKC = .967 and ROC Area = .998). The
extracted patterns describe similarities and trends among land snail species
and indicates possible outliers physiologies due to climate traits and
breeding. Finally, we show some morphometrical characteristics dominate others
according to different feature selection biases. Those data-based patterns can
be applied to fast land snail identification whenever their bodies are
unavailable, as in the recurrent cases of lost shells in nature or private and
museum collections.

Intercellular heterogeneity is a major obstacle to successful precision
medicine. Single-cell RNA sequencing (scRNA-seq) technology has enabled
in-depth analysis of intercellular heterogeneity in various diseases. However,
its full potentials for precision medicine are yet to be reached. Towards this,
we propose A Single-cell Guided pipeline to Aid Repurposing of Drugs (ASGARD).
To precisely address the intercellular heterogeneity within each patient,
ASGARD defines a novel drug score predicting drugs for multiple cell clusters.
We tested ASGARD on three independent datasets, including advanced metastatic
breast cancer, acute lymphoblastic leukemia, and coronavirus disease 2019
(COVID-19). On single-drug therapy, ASGARD shows significantly better average
accuracy (AUC=0.95) compared to two other single-cell pipelines (AUC 0.69 and
0.57) and two other bulk-cell-based drug repurposing methods (AUC 0.80 and
0.75). The top-ranked drugs, such as fulvestrant and neratinib for breast
cancer, tretinoin and vorinostat for leukemia, and enalapril for severe
COVID19, are either approved by FDA or in clinical trials treating
corresponding diseases. In conclusion, ASGARD is a promising pipeline guided by
single-cell RNA-seq data, for repurposing drugs towards precision medicine.
ASGARD is free for academic use at https://github.com/lanagarmire/ASGARD.

Cellular heterogeneity is an immanent property of biological systems that
covers very different aspects of life ranging from genetic diversity to
cell-to-cell variability driven by stochastic molecular interactions, and noise
induced cell differentiation. Here, we review recent developments in
characterizing cellular heterogeneity by distributions and argue that
understanding multicellular life requires the analysis of heterogeneity
dynamics at single cell resolution by integrative approaches that combine
methods from non-equilibrium statistical physics, information theory and omics
biology.

Due to the redundancy of our motor system, movements can be performed in many
ways. While multiple motor control strategies can all lead to the desired
behavior, they result in different joint and muscle forces. This creates
opportunities to explore this redundancy, e.g., for pain avoidance or reducing
the risk of further injury. To assess the effect of different motor control
optimization strategies, a direct measurement of muscle and joint forces is
desirable, but problematic for medical and ethical reasons. Computational
modeling might provide a solution by calculating approximations of these
forces. In this study, we used a full-body computational musculoskeletal model
to (1) predict forces measured in knee prostheses during walking and squatting
and (2) to study the effect of different motor control strategies (i.e.,
minimizing joint force vs. muscle activation) on the joint load and prediction
error. We found that musculoskeletal models can accurately predict knee joint
forces with an RMSE of <0.5 BW in the superior direction and about 0.1 BW in
the medial and anterior directions. Generally, minimization of joint forces
produced the best predictions. Furthermore, minimizing muscle activation
resulted in maximum knee forces of about 4 BW for walking and 2.5 BW for
squatting. Minimizing joint forces resulted in maximum knee forces of 2.25 BW
and 2.12 BW, i.e., a reduction of 44% and 15%, respectively. Thus, changing the
muscular coordination strategy can strongly affect knee joint forces. Patients
with a knee prosthesis may adapt their neuromuscular activation to reduce joint
forces during locomotion.

Increasing emphasis on data and quantitative methods in the biomedical
sciences is making biological research more computational. Collecting,
curating, processing, and analysing large genomic and imaging data sets poses
major computational challenges, as does simulating larger and more realistic
models in systems biology. Here we discuss how a relative newcomer among
computer programming languages -- Julia -- is poised to meet the current and
emerging demands in the computational biosciences, and beyond. Speed,
flexibility, a thriving package ecosystem, and readability are major factors
that make high-performance computing and data analysis available to an
unprecedented degree to "gifted amateurs". We highlight how Julia's design is
already enabling new ways of analysing biological data and systems, and we
provide a, necessarily incomplete, list of resources that can facilitate the
transition into the Julian way of computing.

The present study aims to describe an innovative approach that enables the
system to achieve high yielding for biohydrogen (bio-H$_2$) production using
xylose as a by-product of lignocellulosic biomass processing. A hybrid
optimization technique, structural modelling, desirability analysis, and
genetic algorithm could determine the optimum input factors to maximize useful
biogas parameters, especially bio-H$_2$ and CH$_4$. As found, the input factors
(pretreatment, digestion time and biogas relative pressure) and volatile fatty
acids (acetic acid, propionic acid and butyric acid) had indirectly and
significantly impacted the bio-H$_2$ and desirability score. The pretreatment
factor had the most effect on bio-H$_2$ and CH$_4$ production among the
factors, and after that, were propionic acid and digestion time. The
optimization method showed that the best pretreatment was acidic pretreatment,
digestion time > 20 h, relative pressure in a range of 300-800 mbar, acetic
acid in a range of 90-200 mg/L, propionic acid in a range of 20-150 mg/L, and
butyric acid in a range of 250-420 mg/L. These values caused to produce H$_2$ >
10.2 mmol/L, CH$_4$ > 3.9 mmol/L, N$_2$ < 15.3 mmol/L, CO$_2$ < 19.5 mmol/L,
total biogas > 0.31 L, produced biogas > 0.10 L, and accumulated biogas > 0.41
L.

Ivermectin is an antiparasitic drug that some have claimed is an effective
treatment for reducing Covid-19 deaths. To test this claim, two recent peer
reviewed papers both conducted a meta-analysis on a similar set of randomized
controlled trials data, applying the same classical statistical approach.
Although the statistical results were similar, one of the papers (Bryant et al,
2021) concluded that ivermectin was effective for reducing Covid-19 deaths,
while the other (Roman et al, 2021) concluded that there was insufficient
quality of evidence to support the conclusion Ivermectin was effective. This
paper applies a Bayesian approach, to a subset of the same trial data, to test
several causal hypotheses linking Covid-19 severity and ivermectin to mortality
and produce an alternative analysis to the classical approach. Applying diverse
alternative analysis methods which reach the same conclusions should increase
overall confidence in the result. We show that there is strong evidence to
support a causal link between ivermectin, Covid-19 severity and mortality, and:
i) for severe Covid-19 there is a 90.7% probability the risk ratio favours
ivermectin; ii) for mild/moderate Covid-19 there is an 84.1% probability the
risk ratio favours ivermectin. To address concerns expressed about the veracity
of some of the studies we evaluate the sensitivity of the conclusions to any
single study by removing one study at a time. In the worst case, where
(Elgazzar 2020) is removed, the results remain robust, for both severe and mild
to moderate Covid-19. The paper also highlights advantages of using Bayesian
methods over classical statistical methods for meta-analysis. All studies
included in the analysis were prior to data on the delta variant.

Bayesian inference in biological modeling commonly relies on Markov chain
Monte Carlo (MCMC) sampling of a multidimensional and non-Gaussian posterior
distribution that is not analytically tractable. Here, we present the
implementation of a practical MCMC method in the open-source software package
PyBioNetFit (PyBNF), which is designed to support parameterization of
mathematical models for biological systems. The new MCMC method, am,
incorporates an adaptive move proposal distribution. For warm starts, sampling
can be initiated at a specified location in parameter space and with a
multivariate Gaussian proposal distribution defined initially by a specified
covariance matrix. Multiple chains can be generated in parallel using a
computer cluster. We demonstrate that am can be used to successfully solve
real-world Bayesian inference problems, including forecasting of new
Coronavirus Disease 2019 case detection with Bayesian quantification of
forecast uncertainty. PyBNF version 1.1.9, the first stable release with am, is
available at PyPI and can be installed using the pip package-management system
on platforms that have a working installation of Python 3. PyBNF relies on
libRoadRunner and BioNetGen for simulations (e.g., numerical integration of
ordinary differential equations defined in SBML or BNGL files) and
Dask.Distributed for task scheduling on Linux computer clusters.

Recent advances in machine learning have enabled generative models for both
optimization and de novo generation of drug candidates with desired properties.
Previous generative models have focused on producing SMILES strings or 2D
molecular graphs, while attempts at producing molecules in 3D have focused on
reinforcement learning (RL), distance matrices, and pure atom density grids.
Here we present MOLUCINATE (MOLecUlar ConvolutIoNal generATive modEl), a novel
architecture that simultaneously generates topological and 3D atom position
information. We demonstrate the utility of this method by using it to optimize
molecules for desired radius of gyration. In the future, this model can be used
for more useful optimization such as binding affinity for a protein target.

Inferring which pathways are affected by a brain lesion is key for both pre
and post-treatment planning. However, many disruptive lesions cause changes in
the tissue that interrupt tractography algorithms. In such cases, aggregating
information from healthy subjects can provide a solution to inferring the
affected pathways. In this paper, we introduce a novel label fusion technique
that leverages diffusion information to locate brain pathways. Through
simulations and experiments in publicly available data we show that our method
is able to correctly reconstruct brain pathways, even if they are affected by a
focal lesion.

Traditional drug discovery methods are costly and time-consuming. Drug
repositioning (DR) is a common strategy to overcome these issues. Recently,
machine learning methods have been used extensively in DR problem. The
performance of these methods depends on the features, representations and
training dataset. In this problem, feature sets include many redundant
features, which have a negative effect on the performance of methods. Moreover,
selecting an appropriate training set is influential in the rise of machine
learning method accuracy. However, in this problem, we face two obstacles to
find the proper training set. First, most methods employ known and unknown
drug-disease pairs as positive and negative sets, respectively. While the
number of known pairs is much less than unknowns, it leads to machine learning
performance error because of biasing to the majority group. Second, the absence
of a drug-disease association means this association has not been approved
experimentally and may be changed. In this paper, DRP-VEM framework is proposed
to overcome the challenges. We assess DRP-VEM based on different parameters:
disease and drug feature representations, classification methods, and voting
ensemble training approaches. DRP-VEM is evaluated using heterogenous
evaluation criteria. Moreover, we compare DRP-VEM using the best combination of
parameters with DisDrugPred.

In the context of the EU Horizon 2020 GRACIOUS project, we proposed a
quantitative Weight of Evidence (WoE) approach for hazard classification of
nanomaterials (NMs). This approach is based on the requirements of the European
Regulation on Classification, Labelling and Packaging of Substances and
Mixtures (the CLP Regulation), which implements the United Nations' Globally
Harmonized System of Classification and Labelling of Chemicals (UN GHS) in the
European Union. The goal of this WoE methodology is to facilitate
classification of NMs according to CLP criteria, following the decision trees
defined in ECHA's CLP regulatory guidance. The proposed methodology involves
the following stages: (1) collection of data for different NMs related to the
endpoint of interest: each study related to each NM is referred as a Line of
Evidence (LoE); (2) computation of weighted scores for each LoE: each LoE is
weighted by a score calculated based on agreed data quality and completeness
criteria defined in the GRACIOUS project; (3) comparison and integration of the
weighed LoEs for each NM: A Monte Carlo resampling approach is adopted to
quantitatively and probabilistically integrate the weighted evidence; and (4)
assignment of each NM to a hazard class: according to the results, each NM is
assigned to one of the classes defined by the CLP regulation. Furthermore, to
facilitate the integration and the classification of the weighted LoEs, an R
tool was developed. Finally, the approach was tested against an endpoint
relevant to CLP (Aquatic Toxicity) using data retrieved from the eNanoMapper
database, results obtained were consistent to results in ECHA registration
dossiers and in recent literature

In the context of the EU GRACIOUS project, we propose a novel procedure for
similarity assessment and grouping of nanomaterials. This methodology is based
on the (1) Arsinh transformation function for scalar properties, (2) full curve
shape comparison by application of a modified Kolmogorov-Smirnov metric for
bivariate properties, (3) Ordered Weighted Average (OWA) aggregation-based
grouping distance, and (4) hierarchical clustering. The approach allows for
grouping of nanomaterials that is not affected by the dataset, so that group
membership will not change when new candidates are included in the set of
assessed materials. To facilitate the application of the proposed methodology,
a software script was developed by using the R programming language which is
currently under migration to a web tool. The presented approach was tested
against a dataset, derived from literature review, related to immobilisation of
Daphnia magna and reporting information on several nanomaterials.

Terms such as leader, follower, and oppressed sound equally well in the
description of a pack of wolves, a street protest crowd, or a business team and
have very similar meanings. This indicates the presence of some general law or
structure that governs collective behaviour. To reveal this, we selected the
most common parameter for all levels of the organization -- motion. A causality
analysis of distance correlations was performed to obtain follow-up networks
that show who follows whom and how strongly. These networks characterize an
observed system in general and work as an automation bridge between the
biological experiment and the broad field of network analysis. The proposed
method was tested on a school of aquarium fish. The patterns observed in the
network can be easily interpreted and agree with expected behaviour.

Purpose: Magnitude-based fitting of chemical shift-encoded data enables
proton density fat fraction (PDFF) and R2* estimation where complex-based
methods fail or when phase data is inaccessible or unreliable, such as in
multi-centre studies. However, traditional magnitude-based fitting algorithms
suffer from Rician noise-related bias and fat-water swaps. To address these
issues, we propose an algorithm for Magnitude-Only PDFF and R2* estimation with
Rician Noise modelling (MAGORINO).
  Methods: Simulations of multi-echo gradient echo signal intensities are used
to investigate the performance and behavior of MAGORINO over the space of
clinically plausible PDFF, R2* and SNR values. Fitting performance is assessed
in terms of parameter bias, precision and fitting error. To gain deeper
insights into algorithm behavior, the paths on the likelihood functions are
visualized and statistics describing correct optimization are generated.
MAGORINO is compared against Gaussian noise-based magnitude fitting and complex
fitting.
  Results: Simulations show that MAGORINO reduces bias in both PDFF and R2*
measurements compared to Gaussian fitting, through two main mechanisms: (i) a
greater chance of selecting the true (non-swapped) optimum, and (ii) a shift in
the position of the optima such that the estimates are closer to ground truth
solutions, as a result of the correct noise model.
  Conclusion: MAGORINO reduces fat-water swaps and Rician noise-related bias in
PDFF and R2* estimation, thus addressing key limitations of traditional
Gaussian noise-based magnitude-only fitting.

Linear matrix factorizations (LMFs) such as independent component analysis
(ICA), principal component analysis (PCA), and their extensions, have been
widely used for finding relevant spatial maps in brain imaging data. The last
step of an LMF before interpretation is usually to extract the activated brain
regions from the map by thresholding. However, it is difficult to determine an
appropriate threshold level. Thresholding can remove the underlying properties
of spatial maps and their features imposed by the model. In this study, we
propose a threshold-free activated region extraction method which involves
simplifying a brain spatial map to a dendrogram through Morse filtration. Since
a dendrogram is related to the change of clustering structure in Rips
filtration, we first show the relationship between the Rips filtration of a
graph and the Morse filtration of a function. Then, we dendrogramize a spatial
map in order to visualize the activated brain regions and the range of their
importance in a spatial map. The proposed method can be applied to any spatial
maps that a user wants to threshold and interpret. In experiments, we applied
the proposed method to independent component maps (ICMs) obtained from
resting-state fMRI data, and the dominant subnetworks obtained by the PCA of a
correlation-based functional connectivity of FDG PET Alzheimer's disease
neuroimaging initiative (ADNI) data. We found that dendrogramization can help
to understand a brain spatial map without thresholding.

Bacterial biofilms are among the oldest and most prevalent multicellular life
forms on Earth and are increasingly relevant in research areas related to
industrial fouling, medicine and biotechnology. The main hurdles to obtaining
definitive experimental results include time-varying biofilm properties,
structural and chemical heterogeneity, and especially their strong sensitivity
to environmental cues. Therefore, in addition to judicious choice of
measurement tools, a well-designed biofilm study requires strict control over
experimental conditions, more so than most chemical studies. Due to excellent
control over a host of physiochemical parameters, microfluidic flow cells have
become indispensable in microbiological studies. Not surprisingly, the number
of lab-on-chip studies focusing on biofilms and other microbiological systems
with expanded analytical capabilities has expanded rapidly in the past decade.
In this paper, we comprehensively review the current state of microfluidic
bioanalytical research applied to bacterial biofilms and offer a perspective on
new approaches that are expected to drive continued advances in this field.

Given a population of interconnected input-output agents repeatedly exposed
to independent random inputs, we talk of correlated variability when agents'
outputs are variable (i.e., they change randomly at each input repetition) but
correlated (i.e., they do not vary independently across input repetitions).
Correlated variability appears at multiple levels in neuronal systems, from the
molecular level of protein expression to the electrical level of neuronal
excitability, but its functions and origins are still debated. Motivated by
advancing our understanding of correlated variability, we introduce the
(linear) "correlated variability control problem" as the problem of controlling
steady-state correlations in a linear dynamical network in which agents receive
independent random inputs. Although simple, the chosen setting reveals
important connections between network structure, in particular, the existence
and the dimension of dominant (i.e., slow) dynamics in the network, and the
emergence of correlated variability.

In sequence-based predictions, conventionally an input sequence is
represented by a multiple sequence alignment (MSA) or a representation derived
from MSA, such as a position-specific scoring matrix. Recently, inspired by the
development in natural language processing, several applications of sequence
embedding have been observed. Here, we review different approaches of protein
sequence embeddings and their applications including protein contact
prediction, secondary structure, prediction, and function prediction.

Filtering for stochastic reaction networks (SRNs) is an important problem in
systems/synthetic biology aiming to estimate the state of unobserved chemical
species. A good solution to it can provide scientists valuable information
about the hidden dynamic state and enable optimal feedback control. Usually,
the model parameters need to be inferred simultaneously with state variables,
and a conventional particle filter can fail to solve this problem accurately
due to sample degeneracy. In this case, the regularized particle filter (RPF)
is preferred to the conventional ones, as the RPF can mitigate sample
degeneracy by perturbing particles with artificial noise. However, the
artificial noise introduces an additional bias to the estimate, and, thus, it
is questionable whether the RPF can provide reliable results for SRNs. In this
paper, we aim to identify conditions under which the RPF converges to the exact
filter in the filtering problem determined by a bimolecular network. First, we
establish computationally efficient RPFs for SRNs on different scales using
different dynamical models, including the continuous-time Markov process,
tau-leaping model, and piecewise deterministic process. Then, by parameter
sensitivity analyses, we show that the established RPFs converge to the exact
filters if all reactions leading to an increase of the molecular population
have linearly growing propensities and some other mild conditions are satisfied
simultaneously. This ensures the performance of the RPF for a large class of
SRNs, and several numerical examples are presented to illustrate our results.

In certain biological contexts, such as the plumage patterns of birds and
stripes on certain species of fishes, pattern formation takes place behind a
so-called "wave of competency". Currently, the effects of a wave of competency
on the patterning outcome is not well-understood. In this study, we use
Turing's diffusion-driven instability model to study pattern formation behind a
wave of competency, under a range of wave speeds. Numerical simulations show
that in one spatial dimension a slower wave speed drives a sequence of peak
splittings in the pattern, whereas a higher wave speed leads to peak
insertions. In two spatial dimensions, we observe stripes that are either
perpendicular or parallel to the moving boundary under slow or fast wave
speeds, respectively. We argue that there is a correspondence between the one-
and two-dimensional phenomena, and that pattern formation behind a wave of
competency can account for the pattern organization observed in many biological
systems.

Beyond unicellular and multicellular organisms, there is a third type of
structural complexity in living animals: that of the mechanical self-assembly
of groups of distinct multicellular organisms into dynamical, functional
structures. One of the most striking examples of such structures is the army
ant bivouac, a nest which self-assembles solely from the interconnected bodies
of hundreds of thousands of individuals. These bivouacs are difficult to study
because they rapidly disassemble when disturbed, and hence little is known
about the structure and rules that individuals follow during their formation.
Here we use a custom-built Computed Tomography scanner to investigate the
details of the internal structure and growth process of army ant bivouacs. We
show that bivouacs are heterogeneous structures, which throughout their growth
maintain a thick shell surrounding a less dense interior that contains empty
spaces akin to nest chambers. We find that ants within the bivouac do not carry
more than approximately eight times their weight regardless of the size of the
structure or their position within it. This observation suggests that bivouac
size is not limited by physical constraints of the ants' morphology. This study
brings us closer to understanding the rules used by individuals to govern the
formation of these exceptional superorganismal structures, and provides insight
into how to create engineered self-assembling systems with, for instance,
swarms of robots or active matter.

Docking-based virtual screening (VS process) selects ligands with potential
pharmacological activities from millions of molecules using computational
docking methods, which greatly could reduce the number of compounds for
experimental screening, shorten the research period and save the research cost.
Howerver, a majority of compouds with low docking scores could waste most of
the computational resources. Herein, we report a novel and practical
docking-based machine learning method called MLDDM (Machince Learning
Docking-by-Docking Models). It is composed of a regression model and a
classification model that simulates a classical docking by docking protocol
ususally applied in many virtual screening projects. MLDDM could quickly
eliminate compounds with low docking scores and the retained compounds with
potential high docking scores would be examined for further real docking
program. We demonstrated that MLDDM has a good ability to identify active
compounds in the case studies for 10 specific protein targets. Compared to pure
docking by docking based VS protocol, the VS process with MLDDM can achieve an
over 120 times speed increment on average and the consistency rate with
corresponding docking by docking VS protocol is above 0.8. Therefore, it would
be promising to be used for examing ultra-large compound libraries in the
current big data era.

Bernstein fits implemented into R allow another route for Kruskal-Wallis
power-study tool development. Monte-Carlo Kruskal-Wallis power studies were
compared with measured power, with Monte-Carlo ANOVA equivalent and with an
analytical method, with or without normalization, using four simulated runs
each with 60-100 populations (each population with N=30000 from a set of
Pearson-type ranges): random selection gave 6300 samples analysed for
predictive power. Three medical-study datasets (Dialysis/systolic blood
pressure; Diabetes/sleep-hours; Marital-status/high-density-lipoprotein
cholesterol) were also analysed. In three from four simulated runs (run_one,
run_one_relaxed, and run_three) with Pearson types pooled, Monte-Carlo
Kruskal-Wallis gave predicted sample sizes significantly slightly lower than
measured but more accurate than with ANOVA methods; the latter gave high
sample-size predictions. Populations (run_one_relaxed) with ANOVA assumptions
invalid gave Kruskal-Wallis predictions similar to those measured. In two from
three medical studies, Kruskal-Wallis predictions (Dialysis: similar
predictions; Marital: higher than measured) were more accurate than ANOVA (both
higher than measured) but in one (Diabetes) the reverse was found
(Kruskal-Wallis: lower; Monte-Carlo ANOVA: similar to measured). These
preliminary studies appear to show that Monte-Carlo Kruskal-Wallis power
studies based on Bernstein fits might perform better than ANOVA equivalents in
many settings (and provide reasonable results when ANOVA cannot be used); and
both Monte-Carlo methods appeared considerably more accurate than the analysed
analytical version.

Introduction: Cervical cancer is the third most prevalent cancer among women
in Brunei Darussalam. This study aims to report the overall survival rates and
associated factors of patients diagnosed with malignant cervical cancer in
Brunei Darussalam. Methods: A retrospective study of patients diagnosed with
cervical cancer from 2007 to 2017 in Brunei Darussalam. The data were obtained
from the population-based cancer registry in Brunei Darussalam. Kaplan- Meier
survival analysis was used to estimate the overall survival rates at 1-, 3- and
5-year intervals while the log-rank test was used to assess differences in
survival between groups. Cox Proportional Hazard (PH) regression analysis was
used to examine the association of demographic and clinical factors on the
survival of cervical cancer patients. Results: A total of 329 registered
malignant cervical cancer cases were analyzed. The mean age at diagnosis of
patients with cervical cancer was 46.7 years. There were 28.6% deaths and the
overall survival rates at 1, 3 and 5 years were 85.4%, 72.6% and 68.6%
respectively. Age at diagnosis, cancer stage and histology types were
significant predictive factors for overall survival of the patients diagnosed
with cervical cancers when analysed on both log rank tests and Cox PH model.
Conclusion: Age at diagnosis, cancer stage and histology types were
significantly associated with the overall survival rates of cervical cancer
patients in Brunei Darussalam. Early detection and management of cervical
cancer at early stages should be prioritized to improve the survival rate and
quality of cancer care.

In drug discovery, structure-based virtual high-throughput screening (vHTS)
campaigns aim to identify bioactive ligands or "hits" for therapeutic protein
targets from docked poses at specific binding sites. However, while generally
successful at this task, many deep learning methods are known to be insensitive
to protein-ligand interactions, decreasing the reliability of hit detection and
hindering discovery at novel binding sites. Here, we overcome this limitation
by introducing a class of models with two key features: 1) we condition
bioactivity on pose quality score, and 2) we present poor poses of true binders
to the model as negative examples. The conditioning forces the model to learn
details of physical interactions. We evaluate these models on a new benchmark
designed to detect pose-sensitivity.

Reduced fractals, with different levels of self-similarity and magnification,
are defined and used to introduce discrete spectra of reduced fractals. To
justify the concept of reduced fractals and their spectra, some specific
applications to biological systems, such as green algae, are performed and it
is demonstrated how these spectra can be used to classify biological systems.
The ranges of these spectra for the considered systems are determined and their
extension to other natural science systems is suggested.

The complexity of biological systems, and the increasingly large amount of
associated experimental data, necessitates that we develop mathematical models
to further our understanding of these systems. As biological systems are
generally not well understood, most mathematical models of these systems are
based on experimental data, resulting in a seemingly heterogeneous collection
of models that ostensibly represent the same system. To understand the system
we therefore need to know how the different models are related, with a view to
obtaining a unified mathematical description. This goal is complicated by the
fact that distinct mathematical formalisms may be used to represent the same
system, making direct comparison of the models very difficult. In previous work
we developed an appropriate framework for model comparison where we represent
models as labelled simplicial complexes and compare them with two general
methodologies: comparison by distance or equivalence. In this article we
continue the development of our model comparison methodology in two directions.
First, we present a rigorous and automatable methodology for the core process
of comparison by equivalence, namely determining the vertices in a simplicial
representation, corresponding to model components, that are conceptually
related and the identification of these vertices via simplicial operations. Our
methodology is based on considerations of vertex symmetry in the simplicial
representation, for which we develop the required mathematical theory of group
actions on simplicial complexes. This methodology greatly simplifies and
expedites the process of determining model equivalence. Second, we provide an
alternative mathematical framework for our model-comparison methodology by
representing models as groups, which allows for the direct application of
group-theoretic techniques within our model-comparison methodology.

The Implicit Association Test (IAT) is a common behavioral paradigm to assess
implicit attitudes in various research contexts. In recent years, researchers
have sought to collect IAT data remotely using online applications. Compared to
laboratory-based assessments, online IAT experiments have several advantages,
including widespread administration outside of artificial (i.e., laboratory)
environments. Use of survey-software platforms (e.g., Qualtrics) represents an
innovative and cost-effective approach that allows researchers to prepare
online IAT experiments without any programming expertise. However, there are
some drawbacks with the existing survey-software as well as other online IAT
preparation tools, such as limited mobile device compatibility and lack of
helper functionalities for easy adaptation. To address these issues, we
developed an open-source web app (GitHub page:
https://github.com/ycui1-mda/qualtrics_iat) for creating mobile-compatible
Qualtrics-based IAT experiments and scoring the collected responses. The
present study demonstrates the key functionalities of this web app and
describes feasibility data that were collected and scored using the app to show
the tool's validity. We show that the web app provides a complete and
easy-to-adapt toolset for researchers to construct Qualtrics-based IAT
experiments and process the derived IAT data.

Recent advances in microscopy enable three-dimensional live imaging at a high
resolution. Long-term live imaging of a multicellular organism requires
immobilization of the organism under stable physiological conditions. Despite
proper immobilization, challenges remain within live imaging data analysis due
to other intrinsic and extrinsic dynamics, which can result in misalignments of
an image series over time. 4Dia, an ImageJ/Fiji macro script, aligns 3D
timelapse images through Z-stacks as well as over time using any user selected
channel. 4Dia can be used for essentially any tissue sample with no limit on
the size of Z-stack or the number of timepoints.

A key issue to Alzheimer's disease clinical trial failures is poor
participant selection. Participants have heterogeneous cognitive trajectories
and many do not decline during trials, which reduces a study's power to detect
treatment effects. Trials need enrichment strategies to enroll individuals who
will decline. We developed machine learning models to predict cognitive
trajectories in participants with early Alzheimer's disease (n=1342) and
presymptomatic individuals (n=756) over 24 and 48 months respectively. Baseline
magnetic resonance imaging, cognitive tests, demographics, and APOE genotype
were used to classify decliners, measured by an increase in CDR-Sum of Boxes,
and non-decliners with up to 79% area under the curve (cross-validated and
out-of-sample). Using these prognostic models to recruit enriched cohorts of
decliners can reduce required sample sizes by as much as 51%, while maintaining
the same detection power, and thus may improve trial quality, derisk endpoint
failures, and accelerate therapeutic development in Alzheimer's disease.

This paper presents bone adaptation as a geometric flow. The proposed method
is based on two assumptions: first, that the bone surface is smooth (not
fractal) permitting the definition of a tangent plane and, second, that the
interface between marrow and bone tissue phases is orientable. This permits the
analysis of bone adaptation using the well-developed mathematics of geometric
flows and the numerical techniques of the level set method. Most importantly,
topological changes such as holes forming in plates and rods disconnecting can
be treated formally and simulated naturally. First, the relationship between
biological theories of bone adaptation and the mathematical object describing
geometric flow is described. This is termed the adaptation function, $F$, and
is the multi-scale link described by Frost's Utah paradigm between cellular
dynamics and bone structure. Second, a model of age-related bone loss termed
curvature-based bone adaptation is presented. Using previous literature, it is
shown that curvature-based bone adaptation is the limiting continuous equation
of simulated bone atrophy, a discrete model of bone aging. Interestingly, the
parameters of the model can be defined in such a way that the flow is
volume-preserving. This implies that bone health can in principle change in
ways that fundamentally cannot be measured by areal or volumetric bone mineral
density, requiring structure-level imaging. Third, a numerical method is
described and two in silico experiments are performed demonstrating the
non-volume-preserving and volume-preserving cases. Taken together, recognition
of bone adaptation as a geometric flow permits the recruitment of mathematical
and numerical developments over the last 50 years to understanding and
describing the complex surface of bone.

Radiomics is a promising technology that focuses on improvements of image
analysis, using an automated high-throughput extraction of quantitative
features. However, the character of lesion is affected by the surrounding
tissue. A lesion on medical image should be characterized from the
inter-relation between lesion and surrounding tissue as well as property of the
lesion itself. The aim of this study is to introduce a new radiomics feature
which quantitatively analyze the inter-relation between lesion and surrounding
tissue focusing on the value change of rows and columns in a medical image.

The major histocompatibility complex (MHC) class-I pathway supports the
detection of cancer and viruses by the immune system. It presents parts of
proteins (peptides) from inside a cell on its membrane surface enabling
visiting immune cells that detect non-self peptides to terminate the cell. The
ability to predict whether a peptide will get presented on MHC Class I
molecules helps in designing vaccines so they can activate the immune system to
destroy the invading disease protein. We designed a prediction model using a
BERT-based architecture (ImmunoBERT) that takes as input a peptide and its
surrounding regions (N and C-terminals) along with a set of MHC class I (MHC-I)
molecules. We present a novel application of well known interpretability
techniques, SHAP and LIME, to this domain and we use these results along with
3D structure visualizations and amino acid frequencies to understand and
identify the most influential parts of the input amino acid sequences
contributing to the output. In particular, we find that amino acids close to
the peptides' N- and C-terminals are highly relevant. Additionally, some
positions within the MHC proteins (in particular in the A, B and F pockets) are
often assigned a high importance ranking - which confirms biological studies
and the distances in the structure visualizations.

Motivation: The importance of clinical data in understanding the
pathophysiology of complex disorders has prompted the launch of multiple
initiatives designed to generate patient-level data from various modalities.
While these studies can reveal important findings relevant to the disease, each
study captures different yet complementary aspects and modalities which, when
combined, generate a more comprehensive picture of disease aetiology. However,
achieving this requires a global integration of data across studies, which
proves to be challenging given the lack of interoperability of cohort datasets.
Results: Here, we present the Data Steward Tool (DST), an application that
allows for semi-automatic semantic integration of clinical data into ontologies
and global data models and data standards. We demonstrate the applicability of
the tool in the field of dementia research by establishing a Clinical Data
Model (CDM) in this domain. The CDM currently consists of 277 common variables
covering demographics (e.g. age and gender), diagnostics, neuropsychological
tests, and biomarker measurements. The DST combined with this disease-specific
data model shows how interoperability between multiple, heterogeneous dementia
datasets can be achieved.

Microbiology culture reports contain critical information for important
clinical and public health applications. However, microbiology reports often
have complex, semi-structured, free-text data that present a barrier for
secondary use. Here we present the development and validation of an open-source
package designed to ingest free-text microbiology reports, determine whether
the culture is positive, and return a list of SNOMED-CT mapped bacteria. Our
rule-based natural language processing algorithm was developed using
microbiology reports from two different electronic health record systems in a
large healthcare organization, and then externally validated on the reports of
two other institutions with manually-extracted results as a benchmark. Our
algorithm achieved F-1 scores >0.95 on all classification tasks across both
validation sets. Our concept extraction Python package, MicrobEx, is designed
to be reused and adapted to individual institutions as an upstream process for
other clinical applications, such as machine learning studies, clinical
decision support, and disease surveillance systems.

Due to its implication in cancer treatment, the Warburg Effect has received
extensive in silico investigation. Flux Balance Analysis (FBA), based on
constrained optimization, was successfully applied in the Warburg Effect
modelling. Yet, the assumption that cell types have one invariant cellular
objective severely limits the applicability of the previous FBA models.
Meanwhile, we note that cell types with different objectives show different
extents of the Warburg Effect. To extend the applicability of the previous
model and model the disparate cellular pathway preferences in different cell
types, we built a Nonlinear Multi-Objective FBA (NLMOFBA) model by including
three key objective terms (ATP production rate, lactate generation rate and ATP
yield) into one objective function through linear scalarization. By
constructing a cellular objective map and iteratively varying the objective
weights, we showed disparate cellular pathway preferences manifested by
different cell types driven by their unique cellular objectives, and we gained
insights about the causal relationship between cellular objectives and the
Warburg Effect. In addition, we obtained other biologically consistent results
by using our NLMOFBA model. For example, augmented with the constraint
associated with inefficient mitochondria function or limited substrate, NLMOFBA
predicts cellular pathways supported by the biology literature. Collectively,
NLMOFBA can help build a complete understanding towards the Warburg Effect in
different cell types.

The rapid development of the DNA nanotechnology field has been facilitated by
advances in CAD software. However, as more complex concepts arose, the lag
between the needs and software capabilities appeared. Further derailed by
manual installation and software incompatibility across different platforms and
often tedious library management issues, the software has become hard-to-use
for many.
  Here we present NanoFrame, a web-based DNA wireframe design tool for making
3D nanostructures from a single scaffold. Within this software, we devised
algorithms for DNA routing, staple breaking, and wireframe cage opening which,
while modeled for cuboid structure, can be generalized to a variety of platonic
and Archimedean shapes. In addition, NanoFrame provides a platform for editing
auto-generated staple sequences and saving work online.

H-bonds are known to play an important role in the folding of proteins into
three-dimensional structures, which in turn determine their diverse functions.
The conformations around H-bonds are important, in that they can be non-local
along the backbone and are therefore not captured by the methods such as
Ramachandran plots. We study the relationship between the geometry of H-bonds
in proteins, expressed as a spatial rotation between the two bonded peptide
units, and their topology, expressed as a subgraph of the protein fatgraph. We
describe two experiments to predict H-bond rotations from their corresponding
subgraphs. The first method is based on sequence alignment between sequences of
the signed lengths of H-bonds measured along the backbone. The second method is
based on finding an exact match between the descriptions of subgraphs around
H-bonds. We find that 88.14% of the predictions lie inside the ball, centred
around the true rotation, occupying just 1% of the volume of the rotation space
SO(3).

DNA constructs and their annotated sequence maps have been rapidly
accumulating with the advancement of DNA cloning, synthesis, and assembly
methods. Such a resource has the potential to be optimally utilized in an
autonomous DNA building platform. However, most DNA design processes today
remain manually operated with the assistance of graphical user interface (GUI)
software. Furthermore, as seen commonly in the life sciences, reproducibility
of DNA construction process descriptions is usually not guaranteed, and
utilization of previously developed materials and protocols is not
appropriately credited. Here, we developed an open-source process description
and resource sharing framework QUEEN (a framework to generate quinable and
efficiently editable nucleotide sequence resources) to resolve these issues in
building DNA. QUEEN enables the flexible design of new DNA by using existing
DNA resource files and recoding the construction process in an output file
(GenBank file format). The GenBank files generated by QUEEN are able to
regenerate the process codes that perfectly clone themselves and bequeath the
design history to successive DNA constructs that recycle their partial
resources. QUEEN-generated GenBank files are compatible with the existing DNA
repository services and software. We propose QUEEN as a solution to start
significantly advancing our material and protocol sharing of DNA resources.

The latest severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)
variant Omicron (B.1.1.529) has ushered panic responses around the world due to
its contagious and vaccine escape mutations. The essential infectivity and
antibody resistance of the SARS-CoV-2 variant are determined by its mutations
on the spike (S) protein receptor-binding domain (RBD). However, a complete
experimental evaluation of Omicron might take weeks or even months. Here, we
present a comprehensive quantitative analysis of Omicron's infectivity,
vaccine-breakthrough, and antibody resistance. An artificial intelligence (AI)
model, which has been trained with tens of thousands of experimental data
points and extensively validated by experimental data on SARS-CoV-2, reveals
that Omicron may be over ten times more contagious than the original virus or
about twice as infectious as the Delta variant. Based on 132 three-dimensional
(3D) structures of antibody-RBD complexes, we unveil that Omicron may be twice
more likely to escape current vaccines than the Delta variant. The Food and
Drug Administration (FDA)-approved monoclonal antibodies (mAbs) from Eli Lilly
may be seriously compromised. Omicron may also diminish the efficacy of mAbs
from Celltrion and Rockefeller University. However, its impact on Regeneron mAb
cocktail appears to be mild.

The current study explores an artificial intelligence framework for measuring
the structural features from microscopy images of the bacterial biofilms.
Desulfovibrio alaskensis G20 (DA-G20) grown on mild steel surfaces is used as a
model for sulfate reducing bacteria that are implicated in microbiologically
influenced corrosion problems. Our goal is to automate the process of
extracting the geometrical properties of the DA-G20 cells from the scanning
electron microscopy (SEM) images, which is otherwise a laborious and costly
process. These geometric properties are a biofilm phenotype that allow us to
understand how the biofilm structurally adapts to the surface properties of the
underlying metals, which can lead to better corrosion prevention solutions. We
adapt two deep learning models: (a) a deep convolutional neural network (DCNN)
model to achieve semantic segmentation of the cells, (d) a mask
region-convolutional neural network (Mask R-CNN) model to achieve instance
segmentation of the cells. These models are then integrated with moment
invariants approach to measure the geometric characteristics of the segmented
cells. Our numerical studies confirm that the Mask-RCNN and DCNN methods are
227x and 70x faster respectively, compared to the traditional method of manual
identification and measurement of the cell geometric properties by the domain
experts.

Invasions of aquatic invasive species have imposed significant economic and
ecological damage to global aquatic ecosystems. Once an invasive population has
established in a new habitat, eradication can be financially and logistically
impossible, motivating management strategies to rely heavily upon prevention
measures aimed at reducing the introduction and spread. To be productive,
on-the-ground management of aquatic invasive species requires effective
decision-making surrounding the allocation of limited resources. Watercraft
inspections play an important role in managing aquatic invasive species by
preventing the overland transport of invasive species between waterbodies and
providing education to boaters. In this study, we developed and tested an
interactive web-based decision-support tool, AIS Explorer: Prioritization for
Watercraft Inspections, to guide AIS managers in developing efficient
watercraft inspection plans. The decision-support tool is informed by a novel
network model that maximized the number of inspected watercraft that move from
AIS-infested to uninfested waterbodies, within and outside of counties in
Minnesota, USA. It was iteratively built with stakeholder feedback, including
consultations with county managers, beta-testing of the web-based application,
and workshops to educate and train end-users. The co-development and
implementation of data-driven decision support tools demonstrate how
interdisciplinary methods can be used to connect science and management to
support decision-making. The AIS Explorer: Prioritization for Watercraft
Inspections application makes optimized research outputs accessible in multiple
dynamic forms that maintain pace with the identification of new infestations
and local needs. In addition, the decision support tool has supported improved
and closer communication between AIS managers and researchers on this topic.

The advent of high dimensional single cell data in the biomedical sciences
has necessitated the development of dimensionality-reduction tools. t-SNE and
UMAP are the two most frequently used approaches, allowing clear visualisation
of highly complex single cell datasets. Despite the ubiquity of these
approaches and the clear need for quantitative comparison of single cell
datasets, t-SNE and UMAP have largely remained data visualisation tools due to
the lack of robust statistical approaches available. Here, we have derived a
statistical test for evaluating the difference between dimensionality-reduced
datasets, using the Kolmogorov-Smirnov test on the distributions of cross
entropy of single cells within each dataset. As the approach uses the
interrelationship of single cells for comparison, the resulting statistic is
robust and capable of distinguishing between true biological variation and
rotational symmetry generation during dimensionality reduction. Further, the
test provides a valid distance between single cell datasets, allowing the
organisation of multiple samples into a dendrogram for quantitative comparison
of complex datasets. These results demonstrate the largely untapped potential
of dimensionality-reduction tools for biomedical data analysis beyond
visualisation.

Overweight and obese individuals tend to have increased brain age, reflecting
poorer brain health likely due to grey and white matter atrophy related to
obesity. However, it is unclear if older brain age associated with obesity can
be reversed following weight loss and cardiometabolic health improvement. The
aim of this study was to assess the impact of weight loss and cardiometabolic
improvement following bariatric surgery on brain health, as measured by change
in brain age estimated based on voxel-based morphometry (VBM). We used three
datasets: 1) CamCAN to train the brain age prediction model, 2) Human
Connectome Project to investigate whether individuals with obesity have greater
brain age than individuals with normal weight, and 3) pre-surgery, as well as
4, 12, and 24 month post-surgery data from participants (n=87) who underwent a
bariatric surgery to investigate whether weight loss and cardiometabolic
improvement as a result of bariatric surgery lowers the brain age. Our results
from the HCP dataset showed a higher brain age for individuals with obesity
compared to individuals with normal weight (p<0.0001). We also found
significant improvement in brain health, indicated by a decrease of 2.9 and 5.6
years in adjusted delta age at 12 and 24 months following bariatric surgery
compared to baseline (p<0.0005). While the overall effect seemed to be driven
by a global change across all brain regions and not from a specific region, our
exploratory analysis showed lower delta age in certain brain regions at 24
months. This reduced age was also associated with post-surgery improvements in
BMI, systolic/diastolic blood pressure, and HOMA-IR (p<0.05). In conclusion,
these results suggest that obesity-related brain health abnormalities might be
reversed by bariatric surgery-induced weight loss and widespread improvements
in cardiometabolic alterations.

Optimal experimental design for parameter precision attempts to maximize the
information content in experimental data for a most effective identification of
parametric model. With the recent developments in miniaturization and
parallelization of cultivation platforms for high-throughput screening of
optimal growth conditions massive amounts of informative data can be generated
with few experiments. Increasing the quantity of the data means to increase the
number of parameters and experimental design variables which might deteriorate
the identifiability and hamper the online computation of optimal inputs. To
reduce the problem complexity, in this work, we introduce an auxiliary
controller at a lower level that tracks the optimal feeding strategy computed
by a high-level optimizer in an online fashion. The hierarchical framework is
especially interesting for the operation under constraints. The key aspect of
this method are discussed together with an in silico study considering parallel
glucose limited bacterial fed batch cultivations.

pyscreener is a Python library that seeks to alleviate the challenges of
large-scale structure-based design using computational docking. It provides a
simple and uniform interface that is agnostic to the backend docking engine
with which to calculate the docking score of a given molecule in a specified
active site. Additionally, pyscreener features first-class support for task
distribution, allowing users to seamlessly scale their code from a local,
multi-core setup to a large, heterogeneous resource allocation.

Living organisms are primarily made of cells. Identifying them and
characterizing their geometry and spatial distribution is a first step towards
building multi-scale models of these biomaterials. We propose a method to count
cells using nuclei in an X-ray microtomographic scan of a zebrafish. To account
for scanning artifacts and partial volume effect, the method is adaptively
calibrated using parameters approximated from the manifold of manually selected
and optimized special cases. The methodology is tested on nuclei in the eyes of
zebrafish larvae of different ages.

Antigen test kits have been used extensively as a screening tool during the
worldwide pandemic of coronavirus (SARS-CoV-2). While it is generally expected
that taking samples for analysis with PCR testing gives more reliable results
than using antigen test kits, the overall sensitivity and specificity of the
two protocols in the field have not yet been estimated without assuming that
the PCR test constitutes a gold standard. We use latent class models to
estimate the in situ performance of both PCR and antigen testing, using data
from the Danish national registries. The results are based on 240,000 paired
tests results sub-selected from the 55 million test results that were obtained
in Denmark during the period from February 2021 until June 2021.
  We found that the specificity of both tests is very high in our data sample
(>99.7%), while the sensitivity of PCR sampling was estimated to be 95.7% (95%
CI: 92.8-98.4%) and that of the antigen test kits used in Denmark over the
study period was estimated at 53.8% (95% CI: 49.8-57.9%). Our findings can be
used as supplementary information for consideration when implementing serial
testing strategies that employ a confirmatory PCR sample following a positive
result from an antigen test kit, such as the policy used in Denmark. We note
that while this strategy reduces the number of false positives associated with
antigen test screening, it also increases the false negatives. We demonstrate
that the balance of trading false positives for false negatives only favours
the use of serial testing when the expected true prevalence is low. Our results
contain substantial uncertainty in the estimates for sensitivity due to the
relatively small number of positive test results over this period: validation
of our findings in a population with higher prevalence would therefore be
highly relevant for future work.

Multiple sclerosis (MS) is a progressive inflammatory and neurodegenerative
disease of the central nervous system affecting over 2.5 million people
globally. In-clinic six-minute walk test (6MWT) is a widely used objective
measure to evaluate the progression of MS. Yet, it has limitations such as the
need for a clinical visit and a proper walkway. The widespread use of wearable
devices capable of depicting patients activity profiles has the potential to
assess the level of MS-induced disability in free-living conditions. In this
work, we extracted 96 activity features in different temporal granularities
(from minute-level to day-level) and explored their utility in estimating 6MWT
scores in a European (Italy, Spain, and Denmark) MS cohort of 337 participants
over an average of 10-month duration. We combined these features with
participant demographics using three regression models including elastic net,
gradient boosted trees and random forest. In addition, we quantified the
individual feature contribution using feature importance in these regression
models, linear mixed-effects models, generalized estimating equations, and
correlation-based feature selection (CFS). The results showed promising
estimation performance with R2 of 0.30, which was derived using random forest
after CFS. This model was able to distinguish the participants with low
disability from those with high disability. Furthermore, we observed that the
minute-level (no longer than 8 minutes) step count, particularly those
capturing the upper end of the step count distribution, had a stronger
association with 6MWT. The use of a walking aid was indicative of ambulatory
function measured through 6MWT. This study provides a basis for future
investigation into the clinical relevance and utility of wearables in assessing
MS progression in free-living conditions.

Synergy theories for multi-component agent mixtures use 1-agent dose-effect
relations, assumed known from analyzing previous 1-agent experiments, to
calculate baseline Neither-Synergy-Nor-Antagonism mixture dose-effect
relations. The most commonly used synergy theory, Simple Effect Additivity, is
not self-consistent mathematically. Many nonlinear alternatives have been
suggested, almost all of which require an assumption that effects increase
monotonically as dose increases. We here emphasize the recently introduced
Incremental Effect Additivity synergy theory and briefly discuss Loewe
Additivity. By utilizing the fact that, when dose increments approach zero,
dose-effect relations approach linearity, Incremental Effect Additivity theory
to some extent circumvents the non-linearity of dose-effect relations that
plague Simple Effect Additivity calculations. We study mathematical properties
of Incremental Effect Additivity that are relevant to practical implementation
of this synergy theory and hold whatever particular area of biology, medicine,
toxicology or pharmacology is involved. However, as yet Incremental Effect
Additivity synergy theory has only been applied to mixture experiments
simulating the toxic galactic cosmic ray mixture encountered during voyages in
interplanetary space. Our main results are theorems, propositions, examples and
counterexamples revealing various properties of Incremental Effect Additivity
synergy theory including whether or not Neither-Synergy-Nor-Antagonism
dose-effect relations lie between 1-agent dose-effect relations. These results
are amply illustrated with figures.

In this article we are describing a new algorithm for detecting and
validating partial horizontal gene transfers (HGT). The presented algorithm is
based on a sliding window procedure which analyzes fragments of the given
multiple sequence alignment. A bootstrap procedure incorporated in our method
can be used to estimate the support of each inferred partial HGT. The new
algorithm can be also applied to confirm or discard complete (i.e.,
traditional) horizontal gene transfers detected by any HGT inferring algorithm.
While working on a full-genome scale, the introduced algorithm can be used to
assess the level of mosaicism of the whole species genomes as well as the rates
of complete and partial HGT underlying the evolution of the considered set of
species.

In this article, we review existing probabilistic models for modeling
abundance of fixed-length strings (k-mers) in DNA sequencing data. These models
capture dependence of the abundance on various phenomena, such as the size and
repeat content of the genome, heterozygosity levels, and sequencing error rate.
This in turn allows to estimate these properties from k-mer abundance
histograms observed in real data. We also briefly discuss the issue of
comparing k-mer abundance between related sequencing samples and meaningfully
summarizing the results.

Background and Objective: Several studies confirm that the age of hens has a
tremendous impact on external and internal egg quality characteristics. Egg
production could be at serious risk if egg quality characteristics and age of
hens are not seriously considered. This study was conducted to analyze the
phenotypic correlations between some internal and external egg quality
characteristics in old laying hens. Materials and Methods: A total of 288 eggs
of 85-week-old Hy-Line Brown laying hens were collected during 3 weeks and
their internal and external egg characteristics were evaluated. Results:
Phenotypic correlations between egg quality characteristics in old laying hens
indicate a negative impact on shell and albumen quality but not affected yolk
quality characteristics. Conclusion: This study helps to understand that
raising laying hens above 80 weeks would have a negative impact on egg quality
characteristics.

Environmental temperature (ET) often changes the nutrient intake/output for
layers. Changing feed formulations based on ET may need to be utilized to
obtain optimum performance, shell quality and bone status. This study was
conducted to investigate the effects of temperature, Ca intake, non-phytate P
(NPP) intake and in vitro limestone solubility (LS) on egg-shell quality and
bone status in commercial White Leghorn hens. Egg mass and shell weight per
unit surface area (SWUSA) decreased with increasing ET (p lower than 0.05),
especially when ET was 29.7 C (cycling mean ET)or a constant ET was 32.2 C.
Feeding layers a low soluble larger particle size limestone instead of a highly
soluble limestone produced beneficial effects for SWUSA at the thermoneutral ET
(21.1 C) but the beneficial effect was less or disappeared when ET was higher
than 26.6 C in EXP 1 and 2. Feeding layers 245 and 353 mg NPP/h/d supported
satisfactory bone status at 21.1 C, however layers housed at higher than 30 C
needed an additional intake of 50 mg NPP/h/d to support bone integrity. Results
of EXP 1 and 2 indicates that 48 week old layers housed in thermoneutral or
warmer ET require a minimum of 4.2 g Ca/h/d for maintaining optimum shell
quality and bone integrity. Feeding low LS (34.1% in vitro solubility) improved
egg shell quality only for hens housed in thermoneutral ET (21.1 C) and did not
improve egg shell quality at higher ET (constant or cycling). Daily NPP intake
of 245 and 353 mg/h/d supported optimum egg production and bone status at 21.1
C, respectively. A higher NPP and Ca intake may be required for bone status
compared to egg production, especially in older hens.

Insect flight is a strongly nonlinear and actuated dynamical system. As such,
strategies for understanding its control have typically relied on either
model-based methods or linearizations thereof. Here we develop a framework that
combines model predictive control on an established flight dynamics model and
deep neural networks (DNN) to create an efficient method for solving the
inverse problem of flight control. We turn to natural systems for inspiration
since they inherently demonstrate network pruning with the consequence of
yielding more efficient networks for a specific set of tasks. This bio-inspired
approach allows us to leverage network pruning to optimally sparsify a DNN
architecture in order to perform flight tasks with as few neural connections as
possible, however, there are limits to sparsification. Specifically, as the
number of connections falls below a critical threshold, flight performance
drops considerably. We develop sparsification paradigms and explore their
limits for control tasks. Monte Carlo simulations also quantify the statistical
distribution of network weights during pruning given initial random weights of
the DNNs. We demonstrate that on average, the network can be pruned to retain
approximately 7% of the original network weights, with statistical
distributions quantified at each layer of the network. Overall, this work shows
that sparsely connected DNNs are capable of predicting the forces required to
follow flight trajectories. Additionally, sparsification has sharp performance
limits.

A model is presented relating the evolution of genomic GC content over time
to AT$\rightarrow$GC and GC$\rightarrow$AT mutation rates. By employing It\^o
calculus it is shown that if mutation rates in asexually reproducing organisms
are subject to random perturbations that can vary over time several
implications follow. For instance, an extra Brownian motion term appears
influencing nucleotide variability; the greater the variability of the random
perturbations on the mutation rates the stronger the impact of the Brownian
motion term. Reducing the influence of the random perturbations, to limit
fitness decreasing and deleterious mutations, will likely imply divesting
resources to genomic repair systems. The stable mutation rates seen in many
organisms could thus be an evolved strategy to reduce the influence of the
Brownian motion term. Furthermore, if change to genomic GC content, i.e. the GC
content of variable sites or single nucleotide polymorphisms (SNPs), is just as
likely to increase as to decrease, something that resembles knockout of repair
enzymes and removal of selective pressures seen in evolutionary laboratory
experiments, the species genome will likely decay unless infinite resources are
available. These implications are solely a consequence of allowing random
perturbations affect AT- and GC mutation rates and not obtainable using
standard non-stochastic methodology. Finally, a connection between the model
for genomic GC content evolution and the classical Luria-Delbr\"uck mutation
model is presented in a stochastic setting.

Analysis of longitudinal Electronic Health Record (EHR) data is an important
goal for precision medicine. Difficulty in applying Machine Learning (ML)
methods, either predictive or unsupervised, stems in part from the
heterogeneity and irregular sampling of EHR data. We present an unsupervised
probabilistic model that captures nonlinear relationships between variables
over continuous-time. This method works with arbitrary sampling patterns and
captures the joint probability distribution between variable measurements and
the time intervals between them. Inference algorithms are derived that can be
used to evaluate the likelihood of future using under a trained model. As an
example, we consider data from the United States Veterans Health Administration
(VHA) in the areas of diabetes and depression. Likelihood ratio maps are
produced showing the likelihood of risk for moderate-severe vs minimal
depression as measured by the Patient Health Questionnaire-9 (PHQ-9).

In particle-based stochastic reaction-diffusion models, reaction rate and
placement kernels are used to decide the probability per time a reaction can
occur between reactant particles, and to decide where product particles should
be placed. When choosing kernels to use in reversible reactions, a key
constraint is to ensure that detailed balance of spatial reaction-fluxes holds
at all points at equilibrium. In this work we formulate a general
partial-integral differential equation model that encompasses several of the
commonly used contact reactivity (e.g. Smoluchowski-Collins-Kimball) and volume
reactivity (e.g. Doi) particle models. From these equations we derive a
detailed balance condition for the reversible $\textrm{A} + \textrm{B}
\leftrightarrows \textrm{C}$ reaction. In bounded domains with no-flux boundary
conditions, when choosing unbinding kernels consistent with several commonly
used binding kernels, we show that preserving detailed balance of spatial
reaction-fluxes at all points requires spatially varying unbinding rate
functions near the domain boundary. Brownian Dynamics simulation algorithms can
realize such varying rates through ignoring domain boundaries during unbinding
and rejecting unbinding events that result in product particles being placed
outside the domain.

Biofilms are spatially organized microorganism colonies embedded in a
self-produced matrix, conferring to the microbial community resistance to
environmental stresses. Motile bacteria have been observed swimming in the
matrix of pathogenic exogeneous host biofilms. This observation opened new
promising routes for deleterious biofilms biocontrol: these bacterial swimmers
enhance biofilm vascularization for chemical treatment or could deliver
biocontrol agent by microbial hitchhiking or local synthesis.
%\cite{muok2021microbial,yu2020hitchhiking,samad2017swimming}. Hence,
characterizing swimmer trajectories in the biofilm matrix is of particular
interest to understand and optimize its biocontrol.In this study, a new
methodology is developed to analyze time-lapse confocal laser scanning images
to describe and compare the swimming trajectories of bacterial swimmers
populations and their adaptations to the biofilm structure. The method is based
on the inference of a kinetic model of swimmer population including mechanistic
interactions with the host biofilm. After validation on synthetic data, the
methodology is implemented on images of three different motile {Bacillus
species swimming in a Staphylococcus aureus biofilm. The fitted model allows to
stratify the swimmer populations by their swimming behavior and provides
insights into the mechanisms deployed by the micro-swimmers to adapt their
swimming traits to the biofilm matrix.

Baker's yeast (Saccharomyces cerevisiae) is a model organism for studying the
morphology that emerges at the scale of multi-cell colonies. To look at how
morphology develops, we collect a dataset of time-lapse photographs of the
growth of different strains of S. cerevisiae. We discuss the general
statistical challenges that arise when using time-lapse photographs to extract
time-dependent features. In particular, we show how texture-based feature
engineering and representative clustering can be successfully applied to
categorize the development of yeast colony morphology using our dataset. The
local binary pattern (LBP) from image processing is used to score the surface
texture of colonies. This texture score develops along a smooth trajectory
during growth. The path taken depends on how the morphology emerges. A
hierarchical clustering of the colonies is performed according to their texture
development trajectories. The clustering method is designed for practical
interpretability; it obtains the best representative colony image for any
hierarchical sub-cluster.

Serological tests are important for understanding the physiopathology and
following the evolution of the Covid-19 pandemic. Assays based on flow
cytometry (FACS) of tissue culture cells expressing the spike (S) protein of
SARS-CoV-2 have repeatedly proven to perform slightly better than the
plate-based assays ELISA and CLIA (chemiluminescent immuno-assay), and markedly
better than lateral flow immuno-assays (LFIA). Here, we describe an optimized
and very simple FACS assay based on staining a mix of two Jurkat cell lines,
expressing either high levels of the S protein (Jurkat-S) or a fluorescent
protein (Jurkat-R expressing m-Cherry, or Jurkat-G, expressing GFP, which serve
as an internal negative control). We show that the Jurkat-S\&R-flow test has a
much broader dynamic range than a commercial ELISA test and performs at least
as well in terms of sensitivity and specificity. Also, it is more sensitive and
quantitative than the hemagglutination-based test HAT, which we described
recently. The Jurkat-flow test requires only a few microliters of blood; thus,
it can be used to quantify various Ig isotypes in capillary blood collected
from a finger prick. It can be used also to evaluate serological responses in
mice, hamsters, cats and dogs. FACS tests offer a very attractive solution for
laboratories with access to tissue culture and flow cytometry who want to
monitor serological responses in humans or in animals, and how these relate to
susceptibility to infection, or re-infection, by the virus, and to protection
against Covid-19.

MicroRNAs play an indispensable role in numerous biological processes ranging
from organismic development to tumor progression.In oncology,these microRNAs
constitute a fundamental regulation role in the pathology of cancer that
provides the basis for probing into the influences on clinical features through
transcriptome data. Previous work focused on machine learning (ML) for
searching biomarkers in different cancer databases, but the functions of these
biomarkers are fully not clear. Taking lung cancer as a prototype case of
study. Through integrating clinical information into the transcripts expression
data, we systematically analyzed the effect of microRNA on diagnostic and
prognostic factors at deteriorative lung adenocarcinoma (LUAD). After dimension
reduction, unsupervised hierarchical clustering was used to find the diagnostic
factors which represent the unique expression patterns of microRNA at various
patient's stages. In addition, we developed a classification framework, Light
Gradient Boosting Machine (LightGBM) and SHAPley Additive explanation (SHAP)
algorithm, to screen out the prognostic factors. Enrichment analyses show that
the diagnostic and prognostic factors are not only enriched in cancer-related
athways, but also involved in many vital cellular signaling transduction and
immune responses. These key microRNAs also impact the survival risk of LUAD
patients at all (or a specific) stage(s) and some of them target some important
Transcription Factors (TF).The key finding is that five microRNAs
(hsa-mir-196b, hsa-mir-31, hsa-mir-891a, hsa-mir-34c, and hsa-mir-653) can then
serve as not only potential diagnostic factors but also prognostic tools in the
monitoring of lung cancer.

Accurate information about protein content in the organism is instrumental
for a better understanding of human biology and disease mechanisms. While the
presence of certain types of proteins can be life-threatening, the abundance of
others is an essential condition for an individual's overall well-being.
Protein microarray is a technology that enables the quantification of thousands
of proteins in hundreds of human samples in a parallel manner. In a series of
studies involving protein microarrays, we have explored and implemented various
data science methods for all-around analysing of these data. This analysis has
enabled the identification and characterisation of proteins targeted by the
autoimmune reaction in patients with the APS1 condition. We have also assessed
the utility of applying machine learning methods alongside statistical tests in
a study based on protein expression data to evaluate potential biomarkers for
endometriosis. The keystone of this work is a web-tool PAWER. PAWER implements
relevant computational methods, and provides a semi-automatic way to run the
analysis of protein microarray data online in a drag-and-drop and
click-and-play style. The source code of the tool is publicly available. The
work that laid the foundation of this thesis has been instrumental for a number
of subsequent studies of human disease and also inspired a contribution to
refining standards for validation of machine learning methods in biology.

We studied the effects of spatial configuration on collective dynamics in a
nearest-neighbour and diffusively coupled lattice of heterogeneous nodes. The
networks contained nodes from two populations, which differed in their
intrinsic excitability. Initially, these populations were uniformly and
randomly distributed throughout the lattice. We then developed an iterative
algorithm for perturbing the arrangement of the network such that nodes from
the same population were increasingly likely to be adjacent to one another. We
found that the global input strength, or network drive, necessary to transition
the network from a state of quiescence to a state of synchronised and
oscillatory activity was decreased as network sortedness was increased.
Moreover, for weak coupling, we found that regimes of partial synchronisation
exist (i.e., 2:1 resonance in the activity of the two populations), which were
dependent both on network drive (sometimes in a non-monotonic fashion) and
network sortedness.

Bayesian Active Learning (BAL) is an efficient framework for learning the
parameters of a model, in which input stimuli are selected to maximize the
mutual information between the observations and the unknown parameters.
However, the applicability of BAL to experiments is limited as it requires
performing high-dimensional integrations and optimizations in real time:
current methods are either too time consuming, or only applicable to specific
models. Here, we propose an Efficient Sampling-Based Bayesian Active Learning
(ESB-BAL) framework, which is efficient enough to be used in real-time
biological experiments. We apply our method to the problem of estimating the
parameters of a chemical synapse from the postsynaptic responses to evoked
presynaptic action potentials. Using synthetic data and synaptic whole-cell
patch-clamp recordings, we show that our method can improve the precision of
model-based inferences, thereby paving the way towards more systematic and
efficient experimental designs in physiology.

Evaluation the prediction of Efficiency index by DVH parameter for SRS
treatment plans using Supervised Machine learning and the performance of
predictive model algorithms of RapidMiner GO in the parameter prediction are
investigated. Dose volume histogram (DVH) based Efficiency index was calculated
for 100 clinical SRS plans generated by Leksell Gamma plan, and the results
were compared to predicted values produced by machine learning toolbox of
RapidMiner Go, algorithms are namely, Generalized linear model (GLR), Decision
Tree Model, Support Vector Machine (SVM), Gradient Boosted Trees (GBT), Random
Forest (RF) and Deep learning Model (DL). Root mean square error (RMSE),
Average absolute error, Absolute relative error, squared correlation and model
building time were determined to evaluate the performance of each algorithm.
The GLR algorithm model had square correlation of 0.974 with the smallest RMSE
of 0.01, relatively high prediction speed, and fast model building time with
2.812 s, according to the results. The RMSE values for all models were between
0.01 upto 0.021, all algorithms performed well. The RMSE of the Gradient
Boosted Tree, Random Forest, and Decision Tree regression algorithms was found
to be greater than 0.01, suggesting that they are not appropriate for
predicting EI in this analysis. RapidMiner GO machine learning models can be
used to predict DVH parameters like EI in SRS treatment planning QA. To
effectively evaluate the parameter, it is necessary to choose a suitable
machine learning algorithm.

Effective biological utilization of wood biomass is necessary worldwide.
Since several insect larvae can use wood biomass as a nutrient source, studies
on their digestive mechanism are expected to speculate a novel rule in wood
biomass processing. Here, the relationships of inhabitant bacteria involved in
carbon and nitrogen metabolism in the intestine of beetle larvae, an insect
model, are investigated. Bacterial analysis of larval feces showed enrichment
of members of which could include candidates for plant growth promotion,
nitrogen cycle modulation, and/or environmental protection. The abundances of
these bacteria were not necessarily positively correlated with the abundance in
the habitat, suggesting that they might be selectively enriched in the
intestines of larvae. Further association analysis predicted that carbon and
nitrogen metabolism in the intestine was affected by the presence of the other
common bacteria, the populations of which were not remarkably altered in the
habitat and feces. Based on hypotheses targeting these selected bacterial
groups, structural estimation modeling analyses statistically suggested that
their metabolism of carbon and nitrogen and their stable isotopes, {\delta}13C
and {\delta}15N, may be associated with fecal enriched bacteria and other
common bacteria. In addition, other causal inference analyses, such as causal
mediation analysis, linear non-Gaussian acyclic model (LiNGAM), and
BayesLiNGAM, did not necessarily affirm the existence of prominent bacteria
involved in metabolism, implying its importance as the bacterial groups for
metabolism rather than a remarkable bacterium. Thus, these observations
highlight a multifaceted view of symbiotic bacterial groups utilizing carbon
and nitrogen from wood biomass in insect larvae as a cultivator of potentially
environmentally beneficial bacteria.

Accelerometers are widely used to measure physical activity behaviour,
including in children. The traditional method for processing acceleration data
uses cut points to define physical activity intensity, relying on calibration
studies that relate the magnitude of acceleration to energy expenditure.
However, these relationships do not generalise across diverse populations and
hence they must be parametrised for each subpopulation (e.g., age groups) which
is costly and makes studies across diverse populations and over time difficult.
A data driven approach that allows physical activity intensity states to emerge
from the data, without relying on parameters derived from external populations,
and offers a new perspective on this problem and potentially improved results.
We applied an unsupervised machine learning approach, namely a hidden semi
Markov model, to segment and cluster the accelerometer data recorded from 279
children (9 to 38 months old) with a diverse range of physical and
social-cognitive abilities (measured using the Paediatric Evaluation of
Disability Inventory). We benchmarked this analysis with the cut points
approach calculated using the best available thresholds for the population.
Time spent active as measured by this unsupervised approach correlated more
strongly with measures of the childs mobility, social-cognitive capacity,
independence, daily activity, and age than that measured using the cut points
approach. Unsupervised machine learning offers the potential to provide a more
sensitive, appropriate, and cost-effective approach to quantifying physical
activity behaviour in diverse populations, compared to the current cut points
approach. This, in turn, supports research that is more inclusive of diverse or
rapidly changing populations.

Natural decomposition of organic matter is essential in food systems, and
compost is used worldwide as an organic fermented fertilizer. However, as a
feature of the ecosystem, its effects on the animals are poorly understood.
Here we show that oral administration of compost and/or its derived
thermophilic Bacillaceae, i.e., Caldibacillus hisashii and Weizmannia
coagulans, can modulate the prophylactic activities of various industrial
animals. The fecal omics analyses in the modulatory process showed an improving
trend dependent upon animal species, environmental conditions, and
administration. However, structural equation modeling (SEM) estimated the
grouping candidates of bacteria and metabolites as standard key components
beyond the animal species. In particular, the SEM model implied a strong
relationship among partly digesting fecal amino acids, increasing genus
Lactobacillus as inhabitant beneficial bacteria and 2-aminoisobutyric acid
involved in lantibiotics. These results highlight the potential role of compost
for sustainable protective control in agriculture, fishery, and livestock
industries.

Reproduction of pre-clinical results has a high failure rate. The fundamental
methodology including replication ("protocol") for hypothesis
testing/validation to a state allowing inference, varies within medical and
plant sciences with little justification. Here, five protocols are
distinguished which deal differently with systematic/random errors and vary
considerably in result veracity. Aim: to compare prevalence of protocols
(defined in text). Medical/plant science articles from 2017/2019 were surveyed:
713 random articles assessed for eligibility for counts: first (with p-values):
1) non-replicated; 2) global; 3) triple-result protocols; second: 4)
replication-error protocol; 5) meta-analyses. Inclusion criteria:
human/plant/fungal studies with categorical groups. Exclusion criteria: phased
clinical trials, pilot studies, cases, reviews, technology, rare subjects,
-omic studies. Abbreviated PICOS question: which protocol was evident for a
main result with categorically distinct group difference(s) ? Electronic
sources: Journal Citation Reports 2017/2019, Google. Triplication prevalence
differed dramatically between sciences (both years p<10-16; cluster-adjusted
chi-squared tests): From 320 studies (80/science/year): in 2017, 53 (66%, 95%
confidence interval (C.I.) 56%:77%) and in 2019, 48 (60%, C.I. 49%:71%) plant
studies had triple-result or triplicated global protocols, compared with, in
both years, 4 (5%, C.I. 0.19%:9.8%) medical studies. Plant sciences had a
higher prevalence of protocols more likely to counter generalised systematic
errors (the most likely cause of false positives) and random error than
non-replicated protocols, without suffering from serious flaws found with
random-Institutes protocols. It is suggested that a triple-result
(organised-reproduction) protocol, with Institute consortia, is likely to solve
most problems connected with the replicability crisis.

Protein design is a technique to engineer proteins by modifying their
sequence to obtain novel functionalities. In this method, amino acids in the
sequence are permutated to find the low energy states satisfying the
configuration. However, exploring all possible combinations of amino acids is
generally impossible to achieve on conventional computers due to the
exponential growth of possibilities with the number of designable sites. Thus,
sampling methods are currently used as a conventional approach to address the
protein design problems. Recently, quantum computation methods have shown the
potential to solve similar types of problems. In the present work, we use the
general idea of Grover's algorithm, a pure quantum computation method, to
design circuits at the gate-based level and address the protein design problem.
In our quantum algorithms, we use custom pair-wise energy tables consisting of
eight different amino acids. Also, the distance reciprocals between designable
sites are included in calculating energies in the circuits. Due to the noisy
state of current quantum computers, we mainly use quantum computer simulators
for this study. However, a very simple version of our circuits is implemented
on real quantum devices to examine their capabilities to run these algorithms.
Our results show that using $\mathcal{O}(\sqrt N)$ iterations, the circuits
find the correct results among all $N$ possibilities, providing the expected
quadratic speed up of Grover's algorithm over classical methods.

Gait is an essential manifestation of depression. Laboratory gait
characteristics have been found to be closely associated with depression.
However, the gait characteristics of daily walking in real-world scenarios and
their relationships with depression are yet to be fully explored. This study
aimed to explore associations between depression symptom severity and
daily-life gait characteristics derived from acceleration signals in real-world
settings. In this study, we used two ambulatory datasets: a public dataset with
71 elder adults' 3-day acceleration signals collected by a wearable device, and
a subset of an EU longitudinal depression study with 215 participants and their
phone-collected acceleration signals (average 463 hours per participant). We
detected participants' gait cycles and force from acceleration signals and
extracted 20 statistics-based daily-life gait features to describe the
distribution and variance of gait cadence and force over a long-term period
corresponding to the self-reported depression score. The gait cadence of faster
steps (75th percentile) over a long-term period has a significant negative
association with the depression symptom severity of this period in both
datasets. Daily-life gait features could significantly improve the goodness of
fit of evaluating depression severity relative to laboratory gait patterns and
demographics, which was assessed by likelihood-ratio tests in both datasets.
This study indicated that the significant links between daily-life walking
characteristics and depression symptom severity could be captured by both
wearable devices and mobile phones. The gait cadence of faster steps in
daily-life walking has the potential to be a biomarker for evaluating
depression severity, which may contribute to clinical tools to remotely monitor
mental health in real-world settings.

This study aims to statistically assess the effectiveness of vaccination
against SARS-CoV-2. It is indispensable to investigate the relationship between
Covid-19 deadliness and vaccination in order to study the impact of vaccine in
real-world. We studied rates of infection and death due to Covid-19 in
different countries with respect to their levels of vaccination. People who
received the required dose of vaccination were considered as fully vaccinated
in this study. Based on the percentage of fully vaccinated population,
countries were categorized into several groups. Though a high-level study on
the vaccine effectiveness may not provide much insight for individual level
differences, a global analysis is imperative to infer the influence of
vaccination as a controlling measure of the pandemic.

Purpose: Automatic methods are required for the early detection of hepatic
steatosis to avoid progression to cirrhosis and cancer. Here, we developed a
fully automated deep learning pipeline to quantify hepatic steatosis on
non-contrast enhanced chest computed tomography (CT) scans. Materials and
Methods: We developed and evaluated our pipeline on chest CT images of 1,431
randomly selected National Lung Screening Trial (NLST) participants. A dataset
of 451 CT scans with volumetric liver segmentations of expert readers was used
for training a deep learning model. For testing, in an independent dataset of
980 CT scans hepatic attenuation was manually measured by an expert reader on
three cross-sectional images at different hepatic levels by selecting three
circular regions of interest. Additionally, 100 randomly selected cases of the
test set were volumetrically segmented by expert readers. Hepatic steatosis on
the test set was defined as mean hepatic attenuation of < 40 Hounsfield unit.
Spearman correlation was conducted to analyze liver fat quantification accuracy
and the Cohen's Kappa coefficient was calculated for hepatic steatosis
prediction reliability. Results: Our pipeline demonstrated strong performance
and achieved a mean dice score of 0.970 for the volumetric liver segmentation.
The spearman correlation of the liver fat quantification was 0.954 (P <0.0001)
between the automated and expert reader measurements. The cohen's kappa
coefficient was 0.875 for automatic assessment of hepatic steatosis.
Conclusion: We developed a fully automatic deep learning-based pipeline for the
assessment of hepatic steatosis in chest CT images. With the fast and cheap
screening of hepatic steatosis, our pipeline has the potential to help initiate
preventive measures to avoid progression to cirrhosis and cancer.

Compost is used worldwide as a soil conditioner for crops, but its functions
have still been explored. Here, the omics profiles of carrots were
investigated, as a root vegetable plant model, in a field amended with compost
fermented with thermophilic Bacillaceae for growth and quality indices.
Exposure to compost significantly increased the productivity, antioxidant
activity, red color, and taste of the carrot root and altered the soil
bacterial composition with the levels of characteristic metabolites of the
leaf, root, and soil. Based on the data, structural equation modeling (SEM)
estimated that L-2-aminoadipate, phenylalanine, flavonoids and / or carotenoids
in plants were optimally linked by exposure to compost. The SEM of the soil
estimated that the genus Paenibacillus, L-2-aminoadipate and nicotinamide, and
S-methyl L-cysteine were optimally involved during exposure. These estimates
did not show a contradiction between the whole genomic analysis of
compost-derived Paenibacillus isolates and the bioactivity data, inferring the
presence of a complex cascade of plant growth-promoting effects and modulation
of the nitrogen cycle by compost itself. These observations have provided
information on the qualitative indicators of compost in complex soil-plant
interactions and offer a new perspective for chemically independent sustainable
agriculture through the efficient use of natural nitrogen.

Drug development is an expensive and time-consuming process where thousands
of chemical compounds are being tested in order to find those possessing
drug-like properties while being safe and effective. One of key parts of the
early drug discovery process has become virtual drug screening -- a method used
to narrow down search for potential drugs by running computer simulations of
drug-target interactions. As these methods are known to demand huge amounts of
computational power to get accurate results, prediction models based on machine
learning techniques became a popular solution requiring less computational
power as well as offering the ability to generate novel chemical structures for
further research. Deep learning is to stay in drug discovery but has a long way
to go. Only in the past few years with increases in computing power have
researchers really started to embrace the potential of neural networks in
various stages of the drug discovery process. While prediction methods promise
great perspective in the future development of drug discovery they open new
questions and challenges that still have to be solved.

Background: Most acute stroke (AS) patients in the United States are
initially evaluated at a primary stroke center (PSC) and a significant
proportion requires transfer to a comprehensive stroke center (CSC) for
advanced treatment. A CSC typically accepts patients from multiple PSCs in its
network, leading to capacity limits. This study uses a queueing model to
estimate impacts on CSC capacity due to transfers from PSCs.
  Methods: The model assumes that the number of AS patients arriving at each
PSC, proportion of AS patients transferred, and length of stay in the CSC
Neurologic Intensive Care Unit (Neuro-ICU) by type of AS are random, while the
transfer rates of ischemic and hemorrhagic AS patients are control variables.
The main outcome measure is the "overflow" probability, namely, the probability
of a CSC not having capacity (unavailability of a Neuro-ICU bed) to accept a
transfer. Data simulations of the model, using a base case and an expanded
case, were performed to illustrate the effects of changing key parameters, such
as transfer rates from PSCs and CSC Neuro-ICU capacity on overflow capacity.
  Results: Data simulations of the model using a base case show that an
increase of a PSC's ischemic stroke transfer rate from 15% to 55% raises the
overflow probability from 30.62% to 36.13%. Further simulations of the expanded
case show that to maintain an a priori CSC overflow probability of 30.62% when
adding a PSC with a AS transfer rate of 15% to the network, other PSCs would
need to decrease their transfer rate by 12.5% or the CSC Neuro-ICU would need
to add 2 beds.
  Discussion: A queuing model can be used to estimate the effects of change in
the size of a PSC-CSC network, change in AS transfer rates, or change in number
of CSC Neuro-ICU beds of a CSC on its capacity on the overflow probability in
the CSC.

Objective: Data clustering is a common exploration step in the omics era,
notably in genomics and proteomics where many genes or proteins can
bequantified from one or more experiments. Bayesian clustering is a powerful
algorithm that can classify several thousands of genes or proteins. AutoClass
C, its original implementation, handles missing data, automatically determines
the best number of clusters but is not user-friendly.Results: We developed an
online tool called AutoClassWeb, which provides an easy-to-use web interface
for Bayesian clustering with AutoClass. Input data are entered as TSV files.
Results are provided in formats that ease further analyses with spreadsheet
programs or with programming languages, such as Python or R. AutoClassWeb is
implemented in Python and is published under the 3-Clauses BSD license. The
source code is available athttps://github.com/pierrepo/autoclassweb along with
a detailed documentation.

Motivation: Lung cancer is one of the leading causes for cancer-related
death, with a five-year survival rate of 18%. It is a priority for us to
understand the underlying mechanisms that affect the implementation and
effectiveness of lung cancer therapeutics. In this study, we combine the power
of Bioinformatics and Systems Biology to comprehensively uncover functional and
signaling pathways of drug treatment using bioinformatics inference and
multiscale modeling of both scRNA-seq data and proteomics data. The innovative
and cross-disciplinary approach can be further applied to other computational
studies in tumorigenesis and oncotherapy. Results: A time series of lung
adenocarcinoma-derived A549 cells after DEX treatment were analysed. (1) We
first discovered the differentially expressed genes in those lung cancer cells.
Then through the interrogation of their regulatory network, we identified key
hub genes including TGF-\b{eta}, MYC, and SMAD3 varied underlie DEX treatment.
Further enrichment analysis revealed the TGF-\b{eta} signaling pathway as the
top enriched term. Those genes involved in the TGF-\b{eta} pathway and their
crosstalk with the ERBB pathway presented a strong survival prognosis in
clinical lung cancer samples. (2) Based on biological validation and further
curation, a multiscale model of tumor regulation centered on both
TGF-\b{eta}-induced and ERBB-amplified signaling pathways was developed to
characterize the dynamics effects of DEX therapy on lung cancer cells. Our
simulation results were well matched to available data of SMAD2, FOXO3,
TGF\b{eta}1, and TGF\b{eta}R1 over the time course. Moreover, we provided
predictions of different doses to illustrate the trend and therapeutic
potential of DEX treatment.

We analyse mathematical models in order to understand how microstructural
features of vascular networks may affect blood-flow dynamics, and to identify
particular characteristics that promote the onset of self-sustained
oscillations. By focusing on a simple three-node motif, we predict that network
"redundancy", in the form of a redundant vessel connecting two main
flow-branches, together with differences in haemodynamic resistance in the
branches, can promote the emergence of oscillatory dynamics. We use existing
mathematical descriptions for blood rheology and haematocrit splitting at
vessel branch-points to construct our flow model; we combine numerical
simulations and stability analysis to study the dynamics of the three-node
network and its relation to the system's multiple steady-state solutions.
While, for the case of equal inlet-pressure conditions, a "trivial" equilibrium
solution with no flow in the redundant vessel always exists, we find that it is
not stable when other, stable, steady-state attractors exist. In turn, these
"nontrivial" steady-state solutions may undergo a Hopf bifurcation into an
oscillatory state. We use the branch diameter ratio, together with the inlet
haematocrit rate, to construct a two-parameter stability diagram that
delineates regimes in which such oscillatory dynamics exist. We show that flow
oscillations in this network geometry are only possible when the branch
diameters are sufficiently different to allow for a sufficiently large flow in
the redundant vessel, which acts as the driving force of the oscillations.
These microstructural properties, which were found to promote oscillatory
dynamics, could be used to explore sources of flow instability in biological
microvascular networks.

Visual neuroprostheses are the only FDA-approved technology for the treatment
of retinal degenerative blindness. Although recent work has demonstrated a
systematic relationship between electrode location and the shape of the
elicited visual percept, this knowledge has yet to be incorporated into retinal
prosthesis design, where electrodes are typically arranged on either a
rectangular or hexagonal grid. Here we optimize the intraocular placement of
epiretinal electrodes using dictionary learning. Importantly, the optimization
process is informed by a previously established and psychophysically validated
model of simulated prosthetic vision. We systematically evaluate three
different electrode placement strategies across a wide range of possible
phosphene shapes and recommend electrode arrangements that maximize visual
subfield coverage. In the near future, our work may guide the prototyping of
next-generation neuroprostheses.

To take advantage of recent and ongoing advances in large-scale computational
methods, and to preserve the scientific data created by publicly funded
research projects, data archives must be created as well as standards for
specifying, identifying, and annotating deposited data. The OpenNeuro.org
archive, begun as a repository for magnetic resonance imaging (MRI) data, is
such an archive. We present a gateway to OpenNeuro for human electrophysiology
data (BIDS-formatted EEG and MEG, as well as intracranial data). The NEMAR
gateway allows users to visualize electrophysiological data, including
time-domain and frequency-domain dynamics time locked to sets of experimental
events recorded using BIDS- and HED-formatted data annotation. In addition,
NEMAR allows users to process archived EEG data on the XSEDE high-performance
resources at SDSC in conjunction with the Neuroscience Gateway (nsgportal.org),
a freely available and easy to use portal to leverage high-performance
computing resources for neuroscience research.

Cytometry experiments yield high-dimensional point cloud data that is
difficult to interpret manually. Boolean gating techniques coupled with
comparisons of relative abundances of cellular subsets is the current standard
for cytometry data analysis. However, this approach is unable to capture more
subtle topological features hidden in data, especially if those features are
further masked by data transforms or significant batch effects or
donor-to-donor variations in clinical data. Analysis of publicly available
cytometry data describing non-na\"ive CD8+ T cells in COVID-19 patients and
healthy controls shows that systematic structural differences exist between
single cell protein expressions in COVID-19 patients and healthy controls. We
identify proteins of interest by a decision-tree based classifier, sample
points randomly and compute persistence diagrams from these sampled points. The
resulting persistence diagrams identify regions in cytometry datasets of
varying density and identify protruded structures such as `elbows'. We compute
Wasserstein distances between these persistence diagrams for random pairs of
healthy controls and COVID-19 patients and find that systematic structural
differences exist between COVID-19 patients and healthy controls in the
expression data for T-bet, Eomes, and Ki-67. Further analysis shows that
expression of T-bet and Eomes are significantly downregulated in COVID-19
patient non-na\"ive CD8+ T cells compared to healthy controls. This
counter-intuitive finding may indicate that canonical effector CD8+ T cells are
less prevalent in COVID-19 patients than healthy controls. This method is
applicable to any cytometry dataset for discovering novel insights through
topological data analysis which may be difficult to ascertain otherwise with a
standard gating strategy or existing bioinformatic tools.

The ongoing COVID-19 pandemic continues to affect communities around the
world. To date, almost 6 million people have died as a consequence of COVID-19,
and more than one-quarter of a billion people are estimated to have been
infected worldwide. The design of appropriate and timely mitigation strategies
to curb the effects of this and future disease outbreaks requires close
monitoring of their spatio-temporal trajectories. We present machine learning
methods to anticipate sharp increases in COVID-19 activity in US counties in
real-time. Our methods leverage Internet-based digital traces -- e.g.,
disease-related Internet search activity from the general population and
clinicians, disease-relevant Twitter micro-blogs, and outbreak trajectories
from neighboring locations -- to monitor potential changes in population-level
health trends. Motivated by the need for finer spatial-resolution
epidemiological insights to improve local decision-making, we build upon
previous retrospective research efforts originally conceived at the state level
and in the early months of the pandemic. Our methods -- tested in real-time and
in an out-of-sample manner on a subset of 97 counties distributed across the US
-- frequently anticipated sharp increases in COVID-19 activity 1-6 weeks before
the onset of local outbreaks (defined as the time when the effective
reproduction number $R_t$ becomes larger than 1 consistently). Given the
continued emergence of COVID-19 variants of concern -- such as the most recent
one, Omicron -- and the fact that multiple countries have not had full access
to vaccines, the framework we present, while conceived for the county-level in
the US, could be helpful in countries where similar data sources are available.

Photon-HDF5 is an open-source and open file format for storing
photon-counting data from single molecule microscopy experiments, introduced to
simplify data exchange and increase the reproducibility of data analysis. Part
of the Photon-HDF5 ecosystem, is phconvert, an extensible python library that
allows converting proprietary formats into Photon-HDF5 files. However, its use
requires some proficiency with command line instructions, the python
programming language, and the YAML markup format. This creates a significant
barrier for potential users without that expertise, but who want to benefit
from the advantages of releasing their files in an open format. In this work,
we present a GUI that lowers this barrier, thus simplifying the use of
Photon-HDF5. This tool uses the phconvert python library to convert data files
originally saved in proprietary data formats to Photon-HDF5 files, without
users having to write a single line of code. Because reproducible analyses
depend on essential experimental information, such as laser power or sample
description, the GUI also includes (currently limited) functionality to
associate valid metadata with the converted file, without having to write any
YAML. Finally, the GUI includes several productivity-enhancing features such as
whole-directory batch conversion and the ability to re-run a failed batch, only
converting the files that could not be converted in the previous run.

We present a new approach for improving motif scanning accuracy, based on
analysis of in-between similarity. Given a set of motifs obtained from a
scanning process, we construct an associated weighted graph. We also compute
the expected weight of an edge in such a graph. It turns out that restricting
results to the maximal clique in the graph, computed with respect to the
expected weight, greatly increases precision, hence improves accuracy of the
scan. We tested the method on an ungapped motif-characterized protein family
from five plant proteomes. The method was applied to three iterative motif
scanners - PSI-BLAST, JackHMMer and IGLOSS - with very good results

The lateral diffusion and trapping of neurotransmitter receptors within the
postsynaptic membrane of a neuron plays a key role in determining synaptic
strength and plasticity. Trapping is mediated by the reversible binding of
receptors to scaffolding proteins (slots) within a synapse. In this paper we
introduce a new method for analyzing the transient dynamics of synapses in a
diffusion-trapping model of receptor trafficking. Given a population of
spatially distributed synapses, each of which has a fixed number of slots, we
calculate the rate of relaxation to the steady-state distribution of bound
slots (synaptic weights) in terms of a set of local accumulation times.
Assuming that the rates of exocytosis and endocytosis are sufficiently slow, we
show that the steady-state synaptic weights are independent of each other
(purely local). On the other hand, the local accumulation time of a given
synapse depends on the number of slots and the spatial location of all the
synapses, indicating a form of transient heterosynaptic plasticity. This
suggests that local accumulation time measurements could provide useful
information regarding the distribution of synaptic weights within a dendrite.

A gene expression compendium is a heterogeneous collection of gene expression
experiments assembled from data collected for diverse purposes. The widely
varied experimental conditions and genetic backgrounds across samples creates a
tremendous opportunity for gaining a systems level understanding of the
transcriptional responses that influence phenotypes. Variety in experimental
design is particularly important for studying microbes, where the
transcriptional responses integrate many signals and demonstrate plasticity
across strains including response to what nutrients are available and what
microbes are present. Advances in high-throughput measurement technology have
made it feasible to construct compendia for many microbes. In this review we
discuss how these compendia are constructed and analyzed to reveal
transcriptional patterns.

Mortality displacement is the concept that deaths are moved forward in time
(e.g., a few days, several months, and years) by exposure from when they would
occur without the exposure, which is common in environmental time-series
studies. Using concepts of a frail population and loss of life expectancy, it
is understood that mortality displacement may decrease rate ratio (RR). Such
decreases are thought to be minimal or substantial depending on study
populations. Environmental epidemiologists have interpreted RR considering
mortality displacement. This theoretical paper reveals that mortality
displacement can be formulated as a built-in selection bias of RR in Cox models
due to unmeasured risk factors independent from exposure of interest, and
mortality displacement can also be viewed as an effect modifier by integrating
the concepts of rate and loss of life expectancy. Thus, depending on the
framework through which we view bias, mortality displacement can be categorized
as selection bias in the bias taxonomy of epidemiology, and simultaneously
mortality displacement can be seen as an effect modifier. This dichotomy
provides useful implications regarding policy, effect modification, exposure
time-windows selection, and generalizability, specifically why research in
epidemiology may produce unexpected and heterogeneous RR over different studies
and sub-populations.

A stroke is defined as a neurologic deficit arising from an interruption in
blood supply to the brain. According to the World Health Organization, over 15
million people suffer from strokes annually, of which almost 70% die or are
permanently disabled. Effective treatment must be administered within one hour
to prevent irreversible brain damage. Unfortunately, the current gold standards
for diagnosis, CT and MRI, are time-consuming, expensive, and immobile.
Electroencephalograms reveal biomarkers of strokes while being inexpensive and
available for remote use, but no system exists that utilizes them for this
purpose. To address this issue, we created StrokeSight, a novel, open-source
web application that automatically provides a full diagnosis and visualization
of ischemic and hemorrhagic strokes in under 50 seconds using 60-second
electroencephalograms. We first calculated the averaged power spectral
densities for 132, 60-second electroencephalogram readings, which we then used
to train three deep neural networks that respectively predict a stroke type
(control/ischemic/hemorrhagic), location (left/right hemisphere), and severity
(small/large) with accuracies of 97.5%, 94.4%, and 100%. StrokeSight also
implements a novel process to visualize spectral abnormalities caused by
strokes. Azimuthal equidistant projection and multivariate spline interpolation
are used to reshape 3D electrodes onto a head-shaped 2D plane and then a
contour map of each frequency band power is created, allowing neurologists to
quickly and accurately interpret electroencephalogram data. StrokeSight could
act as a revolutionary solution for stroke care that drastically improves the
speed, cost efficiency, and accessibility of stroke diagnosis while allowing
for personalized treatment and interpretation.

Background: Higher endogenous testosterone levels are associated with reduced
chronic disease risk and mortality. Since the mid-20th century, there have been
significant changes in dietary patterns, and men's testosterone levels have
declined in western countries. Cross-sectional studies show inconsistent
associations between fat intake and testosterone in men.
  Methods: Studies eligible for inclusion were intervention studies, with
minimal confounding variables, comparing the effect of low-fat vs high-fat
diets on men's sex hormones. 9 databases were searched from their inception to
October 2020, yielding 6 eligible studies, with a total of 206 participants.
Random effects meta-analyses were performed using Cochrane's Review Manager
software. Cochrane's risk of bias tool was used for quality assessment.
  Results: There were significant decreases in sex hormones on low-fat vs
high-fat diets. Standardised mean differences with 95% confidence intervals
(CI) for outcomes were: total testosterone [-0.38 (95% CI -0.75 to -0.01) P =
0.04]; free testosterone [-0.37 (95% CI -0.63 to -0.11) P = 0.005]; urinary
testosterone [-0.38 (CI 95% -0.66 to -0.09) P = 0.009], and dihydrotestosterone
[-0.3 (CI 95% -0.56 to -0.03) P = 0.03]. There were no significant differences
for luteinising hormone or sex hormone binding globulin. Subgroup analysis for
total testosterone, European and American men, showed a stronger effect [-0.52
(95% CI -0.75 to -0.3) P < 0.001].
  Conclusions: Low-fat diets appear to decrease testosterone levels in men, but
further randomised controlled trials are needed to confirm this effect. Men
with European ancestry may experience a greater decrease in testosterone, in
response to a low-fat diet.

Non-coding RNA structure and function are essential to understanding various
biological processes, such as cell signaling, gene expression, and
post-transcriptional regulations. These are all among the core problems in the
RNA field. With the rapid growth of sequencing technology, we have accumulated
a massive amount of unannotated RNA sequences. On the other hand, expensive
experimental observatory results in only limited numbers of annotated data and
3D structures. Hence, it is still challenging to design computational methods
for predicting their structures and functions. The lack of annotated data and
systematic study causes inferior performance. To resolve the issue, we propose
a novel RNA foundation model (RNA-FM) to take advantage of all the 23 million
non-coding RNA sequences through self-supervised learning. Within this
approach, we discover that the pre-trained RNA-FM could infer sequential and
evolutionary information of non-coding RNAs without using any labels.
Furthermore, we demonstrate RNA-FM's effectiveness by applying it to the
downstream secondary/3D structure prediction, SARS-CoV-2 genome structure and
evolution prediction, protein-RNA binding preference modeling, and gene
expression regulation modeling. The comprehensive experiments show that the
proposed method improves the RNA structural and functional modelling results
significantly and consistently. Despite only being trained with unlabelled
data, RNA-FM can serve as the foundational model for the field.

COVID-19 vaccines have proven to be effective against SARS-CoV-2 infection.
However, the dynamics of vaccine-induced immunological memory development and
neutralizing antibodies generation are not fully understood, limiting vaccine
development and vaccination regimen determination. Herein, we constructed a
mathematical model to characterize the vaccine-induced immune response based on
fitting the viral infection and vaccination datasets. With the example of
CoronaVac, we revealed the association between vaccine-induced immunological
memory development and neutralizing antibody levels. The establishment of the
intact immunological memory requires more than 6 months after the first and
second doses, after that a booster shot can induce high levels neutralizing
antibodies. By introducing the maximum viral load and recovery time after viral
infection, we quantitatively studied the protective effect of vaccines against
viral infection. Accordingly, we optimized the vaccination regimen, including
dose and vaccination timing, and predicted the effect of the fourth dose. Last,
by combining the viral transmission model, we showed the suppression of virus
transmission by vaccination, which may be instructive for the development of
public health policies.

Understanding the common topological characteristics of the human brain
network across a population is central to understanding brain functions. The
abstraction of human connectome as a graph has been pivotal in gaining insights
on the topological properties of the brain network. The development of
group-level statistical inference procedures in brain graphs while accounting
for the heterogeneity and randomness still remains a difficult task. In this
study, we develop a robust statistical framework based on persistent homology
using the order statistics for analyzing brain networks. The use of order
statistics greatly simplifies the computation of the persistent barcodes. We
validate the proposed methods using comprehensive simulation studies and
subsequently apply to the resting-state functional magnetic resonance images.
We found a statistically significant topological difference between the male
and female brain networks.

Retrosynthesis analysis is a critical task in organic chemistry central to
many important industries. Previously, various machine learning approaches have
achieved promising results on this task by representing output molecules as
strings and autoregressively decoded token-by-token with generative models.
Text generation or machine translation models in natural language processing
were frequently utilized approaches. The token-by-token decoding approach is
not intuitive from a chemistry perspective because some substructures are
relatively stable and remain unchanged during reactions. In this paper, we
propose a substructure-level decoding model, where the substructures are
reaction-aware and can be automatically extracted with a fully data-driven
approach. Our approach achieved improvement over previously reported models,
and we find that the performance can be further boosted if the accuracy of
substructure extraction is improved. The substructures extracted by our
approach can provide users with better insights for decision-making compared to
existing methods. We hope this work will generate interest in this fast growing
and highly interdisciplinary area on retrosynthesis prediction and other
related topics.

Explainable ML for molecular toxicity prediction is a promising approach for
efficient drug development and chemical safety. A predictive ML model of
toxicity can reduce experimental cost and time while mitigating ethical
concerns by significantly reducing animal and clinical testing. Herein, we use
a deep learning framework for simultaneously modeling in vitro, in vivo, and
clinical toxicity data. Two different molecular input representations are used:
Morgan fingerprints and pre-training SMILES embeddings. A multi-task deep
learning model accurately predicts toxicity for all endpoints, including
clinical, as indicated by AUROC and balanced accuracy. In particular, SMILES
embeddings as input to the multi-task model improved clinical toxicity
predictions compared to existing models in MoleculeNet benchmark. Additionally,
our multi-task approach is comprehensive in the sense that it is comparable to
state-of-the-art approaches for specific endpoints in in vitro, in vivo and
clinical platforms. Through both the multi-task model and transfer learning, we
were able to indicate the minimal need of in vivo data for clinical toxicity
predictions. To provide confidence and explain the model's predictions, we
adapt a post-hoc contrastive explanation method that returns pertinent positive
and pertinent negative features, which correspond well to known mutagenic and
reactive toxicophores, such as unsubstituted bonded heteroatoms, aromatic
amines, and Michael receptors. Furthermore, toxicophore recovery by pertinent
feature analysis captures more of the in vitro (53%) and in vivo (56%), rather
than of the clinical (8%), endpoints, and indeed uncovers a preference in known
toxicophore data towards in vitro and in vivo experimental data. To our
knowledge, this is the first contrastive explanation, using both present and
absent substructures, for predictions of clinical and in vivo molecular
toxicity.

Epidemic models are used to analyze the progression or outcome of an epidemic
under different control policies like vaccinations, quarantines, lockdowns, use
of face-masks, pharmaceutical interventions, etc. When these models accurately
represent real-life situations, they may become an important tool in the
decision-making process. Among these models, compartmental models are very
popular and assume individuals move along a series of compartments that
describe their current health status. Nevertheless, these models are mostly
Markovian, that is, the time in each compartment follows an exponential
distribution. In epidemic models, exponential sojourn times are most of the
times unrealistic, for instance, they imply that the probability that a patient
will recover from some disease in the next time unit is independent of the time
the patient has been sick. This is an important restriction that prevents these
models from being widely accepted and trusted by decision-makers. In spite of
the need to incorporate algorithms to tackle the problem, literature on the
topic is scarce. Here, we introduce a novel approach to simulate general
stochastic epidemic models that accepts any distribution for the sojourn times
that is efficient.

Intercellular signaling has an important role in organism development, but
not all communication occurs using the same mechanism. Here, we analyze the
energy efficiency of intercellular signaling by two canonical mechanisms:
diffusion of signaling molecules and direct transport mediated by signaling
cellular protrusions. We show that efficient contact formation for direct
transport can be established by an optimal rate of projecting protrusions,
which depends on the availability of information about the location of the
target cell. The optimal projection rate also depends on how signaling
molecules are transported along the protrusion, in particular the ratio of the
energy cost for contact formation and molecule synthesis. Also, we compare the
efficiency of the two signaling mechanisms, under various model parameters. We
find that the direct transport is favored over the diffusion when transporting
a large amount of signaling molecules. There is a critical number of signaling
molecules at which the efficiency of the two mechanisms are the same. The
critical number is small when the distance between cells is far, which helps
explain why protrusion-based mechanisms are observed in long-range cellular
communications.

Accurately finding proteins and genes that have a certain function is the
prerequisite for a broad range of biomedical applications. Despite the
encouraging progress of existing computational approaches in protein function
prediction, it remains challenging to annotate proteins to a novel function
that is not collected in the Gene Ontology and does not have any annotated
proteins. This limitation, a side effect from the widely-used multi-label
classification problem setting of protein function prediction, hampers the
progress of studying new pathways and biological processes, and further slows
down research in various biomedical areas. Here, we tackle this problem by
annotating proteins to a function only based on its textual description so that
we do not need to know any associated proteins for this function. The key idea
of our method ProTranslator is to redefine protein function prediction as a
machine translation problem, which translates the description word sequence of
a function to the amino acid sequence of a protein. We can then transfer
annotations from functions that have similar textual description to annotate a
novel function. We observed substantial improvement in annotating novel
functions and sparsely annotated functions on CAFA3, SwissProt and GOA
datasets. We further demonstrated how our method accurately predicted gene
members for a given pathway in Reactome, KEGG and MSigDB only based on the
pathway description. Finally, we showed how ProTranslator enabled us to
generate the textual description instead of the function label for a set of
proteins, providing a new scheme for protein function prediction. We envision
ProTranslator will give rise to a protein function "search engine" that returns
a list of proteins based on the free text queried by the user.

Paxlovid is a promising, orally bioavailable novel drug for SARS--CoV--2 with
excellent safety profiles. Our main goal here is to explore the pharmacometric
features of this new antiviral. To provide a detailed assessment of Paxlovid,
we propose a hybrid multiscale mathematical approach. We demonstrate that the
results of the present \textit{in silico} evaluation match the clinical
expectations remarkably well: on the one hand, our computations successfully
replicate the outcome of an actual \textit{in vitro} experiment; on the other
hand we verify both the sufficiency and the necessity of Paxlovid's two main
components (nirmatrelvir and ritonavir) for a simplified \textit{in vivo} case.
Moreover, in the simulated context of our computational framework we visualize
the importance of early interventions, and identify the time window where a
unit--length delay causes the highest level of tissue damage. Finally, the
results' sensitivity to the diffusion coefficient of the virus is explored in
details.

Patient-derived xenografts (PDXs) are an appealing platform for preclinical
drug studies because the in vivo environment of PDXs helps preserve tumor
heterogeneity and usually better mimics drug response of patients with cancer
compared to CCLs. We investigate multimodal neural network (MM-Net) and data
augmentation for drug response prediction in PDXs. The MM-Net learns to predict
response using drug descriptors, gene expressions (GE), and histology
whole-slide images (WSIs) where the multi-modality refers to the tumor
features. We explore whether the integration of WSIs with GE improves
predictions as compared with models that use GE alone. We use two methods to
address the limited number of response values: 1) homogenize drug
representations which allows to combine single-drug and drug-pairs treatments
into a single dataset, 2) augment drug-pair samples by switching the order of
drug features which doubles the sample size of all drug-pair samples. These
methods enable us to combine single-drug and drug-pair treatments, allowing us
to train multimodal and unimodal neural networks (NNs) without changing
architectures or the dataset. Prediction performance of three unimodal NNs
which use GE are compared to assess the contribution of data augmentation
methods. NN that uses the full dataset which includes the original and the
augmented drug-pair treatments as well as single-drug treatments significantly
outperforms NNs that ignore either the augmented drug-pairs or the single-drug
treatments. In assessing the contribution of multimodal learning based on the
MCC metric, MM-Net statistically significantly outperforms all the baselines.
Our results show that data augmentation and integration of histology images
with GE can improve prediction performance of drug response in PDXs.

Summary: The Systems Biology Markup Language (SBML) is an extensible standard
format for exchanging biochemical models. One of the extensions for SBML is the
SBML Layout and Render package. This allows modelers to describe a biochemical
model as a pathway diagram. However, up to now there has been little support to
help users easily add and retrieve such information from SBML. In this
application note, we describe a new Python package called SBMLDiagrams. This
package allows a user to add layout and render information or retrieve it using
a straightforward Python API. The package uses skia-python to support the
rendering of the diagrams, allowing export to commons formats such as PNG or
PDF. Availability: SBMLDiagrams is publicly available and licensed under the
liberal MIT open-source license. The package is available for all major
platforms. The source code has been deposited at GitHub
(github.com/sys-bio/SBMLDiagrams). Users can install the package using the
standard pip installation mechanism: pip install SBMLDiagrams. Contact:
hsauro@uw.edu.

Molecular dynamics (MD) simulation is widely used to study protein
conformations and dynamics. However, conventional simulation suffers from being
trapped in some local energy minima that are hard to escape. Thus, most
computational time is spent sampling in the already visited regions. This leads
to an inefficient sampling process and further hinders the exploration of
protein movements in affordable simulation time. The advancement of deep
learning provides new opportunities for protein sampling. Variational
autoencoders are a class of deep learning models to learn a low-dimensional
representation (referred to as the latent space) that can capture the key
features of the input data. Based on this characteristic, we proposed a new
adaptive sampling method, latent space assisted adaptive sampling for protein
trajectories (LAST), to accelerate the exploration of protein conformational
space. This method comprises cycles of (i) variational autoencoders training,
(ii) seed structure selection on the latent space and (iii) conformational
sampling through additional MD simulations. The proposed approach is validated
through the sampling of four structures of two protein systems: two metastable
states of E. Coli adenosine kinase (ADK) and two native states of Vivid (VVD).
In all four conformations, seed structures were shown to lie on the boundary of
conformation distributions. Moreover, large conformational changes were
observed in a shorter simulation time when compared with conventional MD (cMD)
simulations in both systems. In metastable ADK simulations, LAST explored two
transition paths toward two stable states while cMD became trapped in an energy
basin. In VVD light state simulations, LAST was three times faster than cMD
simulation with a similar conformational space.

Dynamic radiographic measurement of 3D TKA kinematics has provided important
information for implant design and surgical technique for over 30 years.
However, current methods of measuring TKA kinematics are too cumbersome or
time-consuming for practical clinical application. Even state-of-the-art
techniques require human-supervised initialization or human supervision
throughout the entire optimization process. Elimination of human supervision
could potentially bring this technology into clinical practicality. Therefore,
we propose a fully autonomous pipeline for quantifying TKA kinematics from
single-plane imaging. First, a convolutional neural network segments the
femoral and tibial implants from the image. Second, segmented images are
compared to Normalized Fourier Descriptor shape libraries for initial pose
estimates. Lastly, a Lipschitzian optimization routine minimizes the difference
between the segmented image and the projected implant. This technique reliably
reproduces human-supervised kinematics measurements from internal datasets and
external validation studies, with RMS differences of less than 0.7mm and
4{\deg} for internal studies and 0.8mm and 1.7{\deg} for external validation
studies. This performance indicates that it will soon be practical to perform
these measurements in a clinical setting.

We propose a whole-body model of the metabolism in man as well as a
generalized approach for modeling metabolic networks. Using this approach, we
are able to write a large metabolic network in a systematic and compact way. We
demonstrate the approach using a whole-body model of the metabolism of the
three macronutrients, carbohydrates, proteins and lipids. The model contains 7
organs, 16 metabolites and 31 enzymatic reactions. All reaction rates are
described by Michaelis-Menten kinetics with an addition of a hormonal regulator
based on the two hormones insulin and glucagon. We incorporate ingestion of
food in order to simulate metabolite concentrations during the feed-fast cycle.
The model can simulate several days due to the inclusion of storage forms
(glycogen, muscle protein and lipid droplets), that can be depleted if food is
not ingested regularly. A physiological model incorporating complex cellular
metabolism and whole-body mass dynamics can be used in virtual clinical trials.
Such trials can be used to improve the development of medicine, treatment
strategies such as control algorithms, and increase the likelihood of a
successful clinical trial.

Flavor is expressed through interaction of molecules via gustatory and
olfactory mechanisms. Knowing the utility of flavor molecules in food and
fragrances, it is valuable to add a comprehensive repository of flavor
compounds characterizing their flavor profile, chemical properties, regulatory
status, consumption statistics, taste/aroma threshold values, reported uses in
food categories, and synthesis. FlavorDB2
(https://cosylab.iiitd.edu.in/flavordb2/) is an updated database of flavor
molecules with an user-friendly interface. This repository simplifies the
search for flavor molecules, their attributes and offers a range of
applications including food pairing. FlavorDB2 serves as a standard repository
of flavor compounds.

The human gut microbiome is associated with a large number of disease
etiologies. As such, it is a natural candidate for machine learning based
biomarker development for multiple diseases and conditions. The microbiome is
often analyzed using 16S rRNA gene sequencing. However, several properties of
microbial 16S rRNA gene sequencing hinder machine learning, including
non-uniform representation, a small number of samples compared with the
dimension of each sample, and sparsity of the data, with the majority of
bacteria present in a small subset of samples. We suggest two novel methods to
combine information from different bacteria and improve data representation for
machine learning using bacterial taxonomy. iMic and gMic translate the
microbiome to images and graphs respectively, and convolutional neural networks
are then applied to the graph or image. We show that both algorithms improve
performance of static 16S rRNA gene sequence-based machine learning compared to
the best state-of-the-art methods. Furthermore, these methods ease the
interpretation of the classifiers. iMic is then extended to dynamic microbiome
samples, and an iMic explainable AI algorithm is proposed to detect bacteria
relevant to each condition.

Aim: There is increasing interest in the role of chronic inflammation on
pathogenesis of various disease, and one of its markers, high NLR is associated
with various mortality and morbidity risk. Insulin resistance (IR) might be one
potential associate factors, as suggested in preclinical studies. However,
epidemiological studies are scarce which investigated the association between
NLR, and insulin resistance (IR) and they included only diabetes mellitus
patients, not the general population. This study aims to determine if there is
a direct correlation between NLR and IR in the US general population. Methods:
The sample consists of 3,307 from general population, provided by National
Health and Nutrition Examination Survey (NHANES). Homeostasis Model Assessment
of Insulin Resistance (HOMA-IR) value was calculated to evaluate insulin
resistance. We investigated the relationship between their NLR and HOMA-IR
values by bivariate and multivariate linear regression analyses. As insulin use
could results in inaccurate HOMA-IR estimation, we excluded them and ran the
analyses in subgroup analyses. Results: There was a relationship shown when
insulin users were included, having a beta coefficient value of 0.010 (95%
confidence interval [CI] of 0.003-0.017). However, when insulin users were
excluded, the beta value decreased to 0.004 (95% CI of -0.006-0.015). The
statistical significance was not reached when age, sex, and body mass index
were adjusted for in the multivariate analyses. Conclusion: There is no visible
relationship between IR and NLR in the general population. IR might not explain
the variation of NLR value in healthy people, and further studies are needed to
reveal the associated factor of high NLR.

The exchange of large and complex slide microscopy imaging data in biomedical
research and pathology practice is impeded by a lack of data standardization
and interoperability, which is detrimental to the reproducibility of scientific
findings and clinical integration of technological innovations. Slim is an
open-source, web-based slide microscopy viewer that implements the
internationally accepted Digital Imaging and Communications in Medicine (DICOM)
standard to achieve interoperability with a multitude of existing medical
imaging systems. We showcase the capabilities of Slim as the slide microscopy
viewer of the NCI Imaging Data Commons and demonstrate how the viewer enables
interactive visualization of traditional brightfield microscopy and
highly-multiplexed immunofluorescence microscopy images from The Cancer Genome
Atlas and Human Tissue Atlas Network, respectively, using standard DICOMweb
services. We further show how Slim enables the collection of standardized image
annotations for the development or validation of machine learning models and
the visual interpretation of model inference results in the form of
segmentation masks, spatial heat maps, or image-derived measurements.

Living cells need a constant availability of certain resources to have a
sustained gene expression process. Limited availability of cellular resources
for gene expression, like ribosomes, along with a variation of resource
affinity, significantly modifies the system dynamics. Factors like the
variation in rate of binding, or variation in efficiency of the recruited
resource have the potential to affect crucial dynamical phenomena like cell
fate determination. In this paper, we have taken a very important motif, a
bistable genetic toggle switch, and explored the effect of resource imbalance
in this circuit in terms of the bifurcations taking place. We show that initial
asymmetric biasing to resource via resource affinity or gene copy number,
significantly modifies the cell fate transition, both in pitchfork and saddle
node type bifurcation. Our study establishes that in a limited resource
environment, controlled resource allocation can be an important factor for
robust functioning of the synthetic or cellular genetic switches.

Moult is an essential life-history event in mammals and birds, as the
maintenance of fur and feathers is critical for survival. Despite this moult
remains a poorly understood life-history event. This may in part be because the
robust analysis of moult observations requires non-standard statistical
techniques. We present extensions to existing moult phenology models which
accommodate features of real world moult datasets, such as repeated measures
data and misclassified data. We describe the theory behind the extended moult
phenology models, and demonstrate that the extensions can improve inferences
under a wide range of sampling conditions. We implement a Bayesian inference
framework for existing and extended moult phenology models in moultmcmc , an R
package using fast Hamiltonian Monte Carlo sampling. Our package provides an
interface for modelling moult phenology data from real world datasets and
thereby further facilitates the uptake of appropriate statistical methods for
such data.

In a recent opinion article, Muff et al. recapitulate well-known objections
to the Neyman-Pearson Null-Hypothesis Significance Testing (NHST) framework and
call for reforming our practices in statistical reporting. We agree with them
on several important points: the significance threshold P<0.05 is only a
convention, chosen as a compromise between type I and II error rates;
transforming the p-value into a dichotomous statement leads to a loss of
information; and p-values should be interpreted together with other statistical
indicators, in particular effect sizes and their uncertainty. In our view, a
lot of progress in reporting results can already be achieved by keeping these
three points in mind. We were surprised and worried, however, by Muff et al.'s
suggestion to interpret the p-value as a "gradual notion of evidence". Muff et
al. recommend, for example, that a P-value > 0.1 should be reported as "little
or no evidence" and a P-value of 0.001 as "strong evidence" in favor of the
alternative hypothesis H1.

DNA has many valuable characteristics that make it suitable for a long-term
storage medium, in particular its durability and high information density. DNA
can be stored safely for hundreds of years with virtually no degradation, in
contrast to hard disk drives which typically last for about 5 years.
Furthermore, the duration of DNA-Storage can be extended to potentially up to
thousands of years if it is desiccated and cooled in storage.
  Advances in DNA technologies have made it possible to store the entirety of
Wikipedia in a test tube and read that information using a handheld sequencing
device, although imperfections in writing (synthesis) and reading (sequencing)
need to be mitigated for it to be viable as a mainstream storage medium. New
sequencing technologies, such as nanopore sequencing, aim to penetrate the
consumer world, thanks to their affordability and size. However, the error
characteristics of nanopore sequencing are not yet well characterised.
  DNA Storage Error Simulator models errors that can be introduced in all the
phases of DNA storage workflow, including synthesis, storage, PCR for
amplification and finally sequencing. The error characteristics for sequencing
and synthesis can be configured in all necessary detail or can be chosen from a
predefined set of values based on available literature and our own analysis.
  Availability: DNA Storage Error Simulator can be accessed online from:
https://master.dbahb2jho41s4.amplifyapp.com
(https://dnastorage.doc.ic.ac.uk/DNA-error-simulator)

Drug repurposing is an unconventional approach that is used to investigate
new therapeutic aids of existing and shelved drugs. Recent advancement in
technologies and the availability of the data of genomics, proteomics,
transcriptomics, etc., and with the accessibility of large and reliable
database resources, there are abundantly of opportunities to discover drugs by
drug repurposing in an efficient manner. The recent pandemic of SARS-COV-2,
that caused the death of 6,245,750 human beings to date, has tremendously
increase the exceptional usage of bioinformatics tools in interpreting the
molecular characterizations of viral infections. In this paper, we have
employed various bioinformatics tools such as AutoDock-Vina, PyMol etc. We have
found a leading drug candidate Cepharanthine that has shown better results and
effectiveness than recently used antiviral drug candidates such as Favipiravir,
IDX184, Remedesivir, Ribavirin and etc. This paper has analyzed Cepharanthine
potential therapeutic importance as a drug of choice in managing COVID-19
cases. It is anticipated that proposed study would be beneficial for
researchers and medical practitioners in handling SARS-CoV-2 and its variant
related diseases.

Motivation: While the analysis of a single RNA sequencing (RNAseq) dataset
has been well described in the literature, modern research workflows often have
additional complexity in that related RNAseq experiments are performed
sequentially over time. The simplest and most widely used analysis strategy
ignores the temporal aspects and analyses each dataset separately. However,
this can lead to substantial inflation of the overall false discovery rate
(FDR). We propose applying recently developed methodology for online hypothesis
testing to analyse sequential RNAseq experiments in a principled way,
guaranteeing FDR control at all times while never changing past decisions.
Results: We show that standard offline approaches have variable control of FDR
of related RNAseq experiments over time and a naively composed approach may
improperly change historic decisions. We demonstrate that the online FDR
algorithms are a principled way to guarantee control of FDR. Furthermore, in
certain simulation scenarios, we observe empirically that online approaches
have comparable power to offline approaches. Availability and Implementation:
The onlineFDR package is freely available at http:
//www.bioconductor.org/packages/onlineFDR. Additional code used for the
simulation studies can be found at
https://github.com/latlio/onlinefdr_rnaseq_simulation

The study of aerosols and droplets emitted from the oral cavity has become
increasingly important throughout the COVID-19 pandemic. Studies show
particulates emitted while speaking were generally much smaller compared to
coughing or sneezing. However, recent investigations revealed that they are
large enough to carry respiratory contagions. Although studies have shown that
particulate emissions do indeed occur during speech, to date, there is little
information about the relative contribution of different speech sounds in
producing particle emissions. This study compares airborne aerosol generation
in participants producing isolated speech sounds: fricative consonants, plosive
consonants, and vowel sounds. While participants produced isolated speech
tasks, a planar beam of laser light, a high-speed camera, and image software
calculated the number of particulates detected overtime. This study compares
airborne aerosols emitted by human participants at a distance of 2.54 cm
between the laser sheet and the mouth and reveals statistically significant
increases in particulate counts over ambient dust distribution for all speech
sounds. Vowel sounds were statistically greater than consonants, suggesting
that mouth opening, as opposed to place of vocal tract constriction or manner
of sound production, might be the primary influence in the degree to which
particulates become aerosolized during speech. Results of this research will
inform boundary conditions for computation models of aerosolized particulates
during speech.

If model identifiability is not confirmed, inferences from infectious disease
transmission models may not be reliable, so they might lead to misleading
recommendations. Structural identifiability analysis characterizes whether it
is possible to obtain unique solutions for all unknown model parameters, given
the model structure. In this work, we studied the structural identifiability of
some typical deterministic compartmental models for infectious disease
transmission, focusing on the influence of the data type considered as model
output on the identifiability of unknown model parameters, including initial
conditions. We defined 26 model versions, each having a unique combination of
underlying compartmental structure and data type(s) considered as model
output(s). Four compartmental model structures and three common data types in
disease surveillance (incidence, prevalence and detected vector counts) were
studied. The structural identifiability of some parameters varied depending on
the type of model output. In general, models with multiple data types as
outputs had more structurally identifiable parameters, than did models with a
single data type as output. This study highlights the importance of a careful
consideration of data types as an integral part of the inference process with
compartmental infectious disease transmission models.

With the evolution of modern warfare and the increased use of improvised
explosive devices (IEDs), there has been an increase in blast-induced traumatic
brain injuries (bTBI) among military personnel and civilians. The increased
prevalence of bTBI necessitates bTBI models that result in a properly scaled
injury for the model organism being used. The primary laboratory model for bTBI
is the shock tube, wherein a compressed gas ruptures a thin membrane,
generating a shockwave. To generate a shock wave that is properly scaled from
human to rodent subjects the shock wave must have a short duration and high
peak overpressure while fitting a Friedlander waveform, the ideal
representation of a blast wave. A large variety of factors have been
experimentally characterized in attempts to create an ideal waveform, however
we found current research on the gas composition being used to drive shock wave
formation to be lacking. To better understand the effect the driver gas has on
the waveform being produced, we utilized a previously established murine shock
tube bTBI model in conjunction with several distinct driver gasses. In
agreement with previous findings, helium produced a shock wave most closely
fitting the Friedlander waveform in contrast to the plateau-like waveforms
produced by some other gases. The peak pressure at the exit of the shock tube
and 5 cm from the exit have a strong negative correlation with the density of
the gas being used: helium the least dense gas used produces the highest peak
overpressure. Density of the driver gas also exerts a strong positive effect on
the duration of the shock wave, with helium producing the shortest duration
wave. Due to its ability to produce a Friedlander waveform and produce a
waveform following proper injury scaling guidelines, helium is an ideal gas for
use in shock tube models for bTBI.

Based upon apical growth and hyphal branching, the two main processes that
drive the growth pattern of a fungal network, we propose here a two-dimensions
simulation based on a binary-tree modelling allowing us to extract the main
characteristics of a generic thallus growth. In particular, we showed that, in
a homogeneous environment, the fungal growth can be optimized for exploration
and exploitation of its surroundings with a specific angular distribution of
apical branching. Two complementary methods of extracting angle values have
been used to confront the result of the simulation with experimental data
obtained from the thallus growth of the saprophytic filamentous fungus
Podospora anserina. Finally, we propose here a validated model that, while
being computationally low-cost, is powerful enough to test quickly multiple
conditions and constraints. It will allow in future works to deepen the
characterization of the growth dynamic of fungal network, in addition to
laboratory experiments, that could be sometimes expensive, tedious or of
limited scope.

Compartment models of cell culture are widely used in cytology, pharmacology,
toxicology and other fields. Numerical simulation, data modeling and prediction
of compartment models can be realized by traditional differential equation
modeling methods. At the same time, with the development of software and
hardware, Physical Informed Neural Network (PINN) is widely used to solve
differential equation models. This work models, simulates and predicts the cell
culture compartment model based on the machine learning framework PyTorch with
an 16 hidden layers neural network, including 8 linear layers and 8 feedback
active layers. The results showed a loss value of 0.0004853 for three-component
four-parameter quantitative pharmacodynamic model predictions in this way,
which is evaluated by Mean Square Error (MSE). In summary, Physical Informed
Neural Network can serve as an effective tool to deal with cell culture
compartment models and may perform better in dealing with big datasets.

The deformation of cellular membranes regulates trafficking processes, such
as exocytosis and endocytosis. Classically, the Helfrich continuum model is
used to characterize the forces and mechanical parameters that cells tune to
accomplish membrane shape changes. While this classical model effectively
captures curvature generation, one of the core challenges in using it to
approximate a biological process is selecting a set of mechanical parameters
(including bending modulus and membrane tension) from a large set of reasonable
values. We used the Helfrich model to generate a large synthetic dataset from a
random sampling of realistic mechanical parameters and used this dataset to
train machine learning models. These models produced promising results,
accurately classifying model behavior and predicting membrane shape from
mechanical parameters. We also note emerging methods in machine learning that
can leverage the physical insight of the Helfrich model to improve performance
and draw greater insight into how cells control membrane shape change.

Anticipating changes to vehicle interiors with future automated driving
systems, the automobile industry recently has focused attention on crash
response in relaxed postures with increased seatback recline. Prior research
found that this posture may result in greater risk of lumbar spine injury in
the event of a frontal crash. This study developed a lumbar spine injury risk
function that estimated injury risk as a function of simultaneously applied
compression force and flexion moment. Force and moment failure data from 40
compression-flexion tests were utilized in a Weibull survival model, including
appropriate data censoring. A mechanics-based injury metric was formulated,
where lumbar spine compression force and flexion moment were normalized to
specimen geometry. Subject age was incorporated as a covariate to further
improve model fit. A weighting factor was included to adjust the influence of
force and moment, and parameter optimization yielded a value of 0.11. Thus, the
normalized compression force component had a greater effect on injury risk than
the normalized flexion moment component. Additionally, as force was nominally
increased, less moment was required to produce injury for a given age and
specimen geometry. The resulting injury risk function can be utilized to
improve occupant safety in the field.

Machine learning (ML) techniques have gained popularity in the neuroimaging
field due to their potential for classifying neuropsychiatric disorders.
However, the diagnostic predictive power of the existing algorithms has been
limited by small sample sizes, lack of representativeness, data leakage, and/or
overfitting. Here, we overcome these limitations with the largest multi-site
sample size to date (n=5,356) to provide a generalizable ML classification
benchmark of major depressive disorder (MDD). Using brain measures from
standardized ENIGMA analysis pipelines in FreeSurfer, we were able to classify
MDD vs healthy controls (HC) with around 62% balanced accuracy, but when
harmonizing the data using ComBat balanced accuracy dropped to approximately
52%. Similar results were observed in stratified groups according to age of
onset, antidepressant use, number of episodes and sex. Future studies
incorporating higher dimensional brain imaging/phenotype features, and/or using
more advanced machine and deep learning methods may achieve more encouraging
prospects.

PURPOSE: To extend magnitude-based PDFF (Proton Density Fat Fraction) and
$R_2^*$ mapping with resolved water-fat ambiguity to calculate field
inhomogeneity (field map) using the phase images.
  THEORY: The estimation is formulated in matrix form, resolving the field map
in a least-squares sense. PDFF and $R_2^*$ from magnitude fitting may be
updated using the estimated field maps.
  METHODS: The limits of quantification of our voxel-independent implementation
were assessed. Bland-Altman was used to compare PDFF and field maps from our
method against a reference complex-based method on 152 UK Biobank subjects (1.5
T Siemens). A separate acquisition (3 T Siemens) presenting field
inhomogeneities was also used.
  RESULTS: The proposed field mapping was accurate beyond double the
complex-based limit range. High agreement was obtained between the proposed
method and the reference in UK Biobank (PDFF bias = -0.03 %, LoA (limits of
agreement) [-0.1,0.1] %; Field map bias = 0.06 Hz, LoA = [-0.2,0.3] Hz). Robust
field mapping was observed at 3 T, for inhomogeneities over 300 Hz including
rapid variation across edges.
  CONCLUSION: Field mapping following magnitude-based water-fat separation with
resolved water-fat ambiguity was demonstrated in-vivo and showed potential at
high field.

The skin microbiome plays an important role in the maintenance of a healthy
skin. It is an ecosystem, composed of several species, competing for resources
and interacting with the skin cells. Imbalance in the cutaneous microbiome,
also called dysbiosis, has been correlated with several skin conditions,
including acne and atopic dermatitis. Generally, dysbiosis is linked to
colonization of the skin by a population of opportunistic pathogenic bacteria
(for example C. acnes in acne or S. aureus in atopic dermatitis). Treatments
consisting in non-specific elimination of cutaneous microflora have shown
conflicting results. It is therefore necessary to understand the factors
influencing shifts of the skin microbiome composition. In this work, we
introduce a mathematical model based on ordinary differential equations, with 2
types of bacteria populations (skin commensals and opportunistic pathogens) to
study the mechanisms driving the dominance of one population over the other. By
using published experimental data, assumed to correspond to the observation of
stable states in our model, we derive constraints that allow us to reduce the
number of parameters of the model from 13 to 5. Interestingly, a meta-stable
state settled at around 2 days following the introduction of bacteria in the
model, is followed by a reversed stable state after 300 hours. On the time
scale of the experiments, we show that certain changes of the environment, like
the elevation of skin surface pH, create favorable conditions for the emergence
and colonization of the skin by the opportunistic pathogen population. Such
predictions help identifying potential therapeutic targets for the treatment of
skin conditions involving dysbiosis of the microbiome, and question the
importance of meta-stable states in mathematical models of biological
processes.

With the new era of genomics, an increasing number of animal species are
amenable to large-scale data generation. This had led to the emergence of new
multi-species ontologies to annotate and organize these data. While anatomy and
cell types are well covered by these efforts, information regarding development
and life stages is also critical in the annotation of animal data. Its lack can
hamper our ability to answer comparative biology questions and to interpret
functional results. We present here a collection of development and life stage
ontologies for 21 animal species, and their merge into a common multi-species
ontology. This work has allowed the integration and comparison of
transcriptomics data in 52 animal species.

Evapotranspiration (ET) represents the largest water loss flux in drylands,
but ET and its partition into plant transpiration (T) and soil evaporation (E)
are poorly quantified, especially at fine temporal scales. Physically-based
remote sensing models relying on sensible heat flux estimates, like the
two-source energy balance model, could benefit from considering more explicitly
the key effect of stomatal regulation on dryland ET. The objective of this
study is to assess the value of solar-induced chlorophyll fluorescence (SIF), a
proxy for photosynthesis, to constrain the canopy conductance (Gc) of an
optimal stomatal model within a two-source energy balance model in drylands. We
assessed our ET estimation using in situ eddy covariance GPP as a benchmark,
and compared with results from using the Contiguous solar-induced chlorophyll
fluorescence (CSIF) remote sensing product instead of GPP, with and without the
effect of root-zone soil moisture on the Gc. The estimated ET was robust across
four steppes and two tree-grass dryland ecosystem. Comparison of ET simulated
against in situ GPP yielded an average R2 of 0.73 (0.86) and RMSE of 0.031
(0.36) mm at half-hourly (daily) timescale. Including explicitly the soil
moisture effect on Gc, increased the R2 to 0.76 (0.89). For the CSIF model, the
average R2 for ET estimates also improved when including the effect of soil
moisture: from 0.65 (0.79) to 0.71 (0.84), with RMSE ranging between 0.023
(0.22) and 0.043 (0.54) mm depending on the site. Our results demonstrate the
capacity of SIF to estimate subdaily and daily ET fluxes under very low ET
conditions. SIF can provide effective vegetation signals to constrain stomatal
conductance and partition ET into T and E in drylands. This approach could be
extended for regional estimates using remote sensing SIF estimates such as
CSIF, TROPOMI-SIF, or the upcoming FLEX mission, among others.

Synthetic biologists have made great progress over the past decade in
developing methods for modular assembly of genetic sequences and in engineering
biological systems with a wide variety of functions in various contexts and
organisms. However, current paradigms in the field entangle sequence and
functionality in a manner that makes abstraction difficult, reduces engineering
flexibility, and impairs predictability and design reuse. Functional Synthetic
Biology aims to overcome these impediments by focusing the design of biological
systems on function, rather than on sequence. This reorientation will decouple
the engineering of biological devices from the specifics of how those devices
are put to use, requiring both conceptual and organizational change, as well as
supporting software tooling. Realizing this vision of Functional Synthetic
Biology will allow more flexibility in how devices are used, more opportunity
for reuse of devices and data, improvements in predictability, and reductions
in technical risk and cost.

Coral reef science is a fast-growing field propelled by the need to better
understand coral health and resilience to devise strategies to slow reef loss
resulting from environmental stresses. Key to coral resilience are the
symbiotic interactions established within a complex holobiont, i.e. the
multipartite assemblages comprising the host coral organism, endosymbiotic
dinoflagellates, bacteria, archaea, fungi, and viruses. Tara Pacific is an
ambitious project built upon the experience of previous Tara Oceans
expeditions, and leveraging state-of-the-art sequencing technologies and
analyses to dissect the biodiversity and biocomplexity of the coral holobiont
screened across most archipelagos spread throughout the entire Pacific Ocean.
Here we detail the Tara Pacific workflow for multi-omics data generation, from
sample handling to nucleotide sequence data generation and deposition. This
unique multidimensional framework also includes a large amount of concomitant
metadata collected side-by-side that provide new assessments of coral reef
biodiversity including micro-biodiversity and shape future investigations of
coral reef dynamics and their fate in the Anthropocene.

Chronic Myeloid Leukemia (CML) is a biphasic malignant clonal disorder that
progresses, first with a chronic phase, where the cells have enhanced
proliferation only, and then to a blast phase, where the cells have the ability
of self-renewal. It is well-recognized that the Philadelphia chromosome (which
contains the BCR-ABL fusion gene) is the "hallmark of CML". However, empirical
studies have shown that the mere presence of BCR-ABL may not be a sufficient
condition for the development of CML, and further modifications related to
tumor suppressors may be necessary. Accordingly, we develop a three-mutation
stochastic model of CML progression, with the three stages corresponding to the
non-malignant cells with BCR-ABL presence, the malignant cells in the chronic
phase and the malignant cells in the blast phase. We demonstrate that the model
predictions agree with age incidence data from the United States. Finally, we
develop a framework for the retrospective estimation of the time of onset of
malignancy, from the time of detection of the cancer.

Understanding factors affecting social interactions among animals is
important for applied animal behavior research. Thus, there is a need to elicit
statistical models to analyze data collected from pairwise behavioral
interactions. In this study, we propose treating social interaction data as
dyadic observations and propose a statistical model for their analysis. We
performed posterior predictive checks of the model through different validation
strategies: stratified 5-fold random cross-validation, block-by-social-group
cross-validation, and block-by-focal-animals validation. The proposed model was
applied to a pig behavior dataset collected from 797 growing pigs freshly
remixed into 59 social groups that resulted in 10,032 records of directional
dyadic interactions. The response variable was the duration in seconds that
each animal spent delivering attacks on another group mate. Generalized linear
mixed models were fitted. Fixed effects included sex, individual weight, prior
nursery mate experience, and prior littermate experience of the two pigs in the
dyad. Random effects included aggression giver, aggression receiver, dyad, and
social group. A Bayesian framework was utilized for parameter estimation and
posterior predictive model checking. Prior nursery mate experience was the only
significant fixed effect. In addition, a weak but significant correlation
between the random giver effect and the random receiver effect was obtained
when analyzing the attacking duration. The predictive performance of the model
varied depending on the validation strategy, with substantially lower
performance from the block-by-social-group strategy than other validation
strategies. Collectively, this paper demonstrates a statistical model to
analyze interactive animal behaviors, particularly dyadic interactions.

The clinical spectrum of severe acute respiratory syndrome coronavirus 2
(SARS-CoV-2), the strain of coronavirus that caused the COVID-19 pandemic, is
broad, extending from asymptomatic infection to severe immunopulmonary
reactions that, if not categorized properly, may be life-threatening.
Researchers rate COVID-19 patients on a scale from 1 to 8 according to the
severity level of COVID-19, 1 being healthy and 8 being extremely sick, based
on a multitude of factors including number of clinic visits, days since the
first sign of symptoms, and more. However, there are two issues with the
current state of severity level designation. Firstly, there exists variation
among researchers in determining these patient scores, which may lead to
improper treatment. Secondly, researchers use a variety of metrics to determine
patient severity level, including metrics involving plasma collection that
require invasive procedures. This project aims to remedy both issues by
introducing a machine learning framework that unifies severity level
designations based on noninvasive saliva biomarkers. Our results show that we
can successfully use machine learning on salivaomics data to predict the
severity level of COVID-19 patients, indicating the presence of viral load
using saliva biomarkers.

In recent decades, there has been an increase in polypharmacy, the concurrent
administration of multiple drugs per patient. Studies have shown that
polypharmacy is linked to adverse patient outcomes and there is interest in
elucidating the exact causes behind this observation. In this paper, we are
studying the relationship between drug prescriptions, drug-drug interactions
(DDIs) and patient mortality. Our focus is not so much on the number of
prescribed drugs, the typical metric in polypharmacy research, but rather on
the specific combinations of drugs leading to a DDI. To learn the space of
real-world drug combinations, we first assessed the drug prescription landscape
of the UK Biobank, a large patient data registry. We observed distinct drug
constellation patterns driven by the UK Biobank participants' disease status.
We show that these drug prescription clusters matter in terms of the number and
types of expected DDIs, and may possibly explain observed differences in health
outcomes.

Being motivated by recent progress in nanopore sensing, we develop a theory
of the effect of large analytes, or blockers, trapped within the nanopore
confines, on diffusion flow of small solutes. The focus is on the nanopore
diffusion resistance which is the ratio of the solute concentration difference
in the reservoirs connected by the nanopore to the solute flux driven by this
difference. Analytical expressions for the diffusion resistance are derived for
a cylindrically symmetric blocker whose axis coincides with the axis of a
cylindrical nanopore in two limiting cases where the blocker radius changes
either smoothly or abruptly. Comparison of our theoretical predictions with the
results obtained from Brownian dynamics simulations shows good agreement
between the two.

In molecular discovery and drug design, structure-property relationships and
activity landscapes are often qualitatively or quantitatively analyzed to guide
the navigation of chemical space. The roughness (or smoothness) of these
molecular property landscapes is one of their most studied geometric
attributes, as it can characterize the presence of activity cliffs, with
rougher landscapes generally expected to pose tougher optimization challenges.
Here, we introduce a general, quantitative measure for describing the roughness
of molecular property landscapes. The proposed roughness index (ROGI) is
loosely inspired by the concept of fractal dimension and strongly correlates
with the out-of-sample error achieved by machine learning models on numerous
regression tasks.

Immunoaffinity-based liquid biopsies of circulating tumor cells (CTCs) hold
great promise for cancer management, but typically suffer from low throughput,
relative complexity and post-processing limitations. Here we address these
issues simultaneously by decoupling and independently optimizing the nano-,
micro- and macro-scales of an enrichment device that is simple to fabricate and
operate. Unlike other affinity-based devices, our scalable mesh approach
enables optimum capture conditions at any flow rate, as demonstrated with
constant capture efficiencies, above 75% between 50-200 uL/min. The device
achieved 96% sensitivity and 100% specificity when used to detect CTCs in the
blood of 79 cancer patients and 20 healthy controls. We demonstrate its post
processing capacity with the identification of potential responders to immune
checkpoint inhibition therapy and the detection of HER2 positive breast cancer.
The results compare well with other assays, including clinical standards. This
suggests that our approach, which overcomes major limitations associated with
affinity-based liquid biopsies, could help improve cancer management.

PepSIRF is a command-line, module-based open-source software package that
facilitates the analysis of data from highly-multiplexed serology assays (e.g.,
PepSeq or PhIP-Seq). It has nine separate modules in its current release
(v1.5.0): demux, info, subjoin, norm, bin, zscore, enrich, link, and deconv.
These modules can be used together to conduct analyses ranging from
demultiplexing raw high-throughput sequencing data to the identification of
enriched peptides. QIIME 2 is an open-source, community-developed and
plugin-based bioinformatics platform that focuses on data and analytical
transparency. QIIME 2's features include integrated and automatic tracking of
data provenance, a semantic type system, and built-in support for many types of
user interfaces. Here, we describe three new QIIME 2 plugins that allow users
to conduct PepSIRF analyses within the QIIME 2 environment and extend the core
functionality of PepSIRF in two key ways: 1) enabling generation of interactive
visualizations and 2) enabling automation of analysis pipelines that include
multiple PepSIRF modules.

We investigated the negative relationship between mortality and COVID-19
vaccination at ecological level, which has been established through clinical
trials and other investigations at the individual level. We conducted an
exploratory, correlational, country-level analysis of open data centralized by
Our World in Data concerning the cumulative COVID-19 mortality for the winter
wave of the pandemic as function of the vaccination rate in October 2021. In
order to disentangle the protective relationship from confounding processes, we
controlled variables that capture country-level social development and level of
testing. We also deployed three segmentation tactics, distinguishing among
countries based on their level of COVID-19 testing, age structure, and types of
vaccines used. Controlling for confounding factors did not highlight a
statistically significant global relationship between vaccination and
cumulative mortality in the total country sample. As suggested by previous
estimates at country level, a strong, significant, negative relationship
between cumulative mortality and vaccination was highlighted through
segmentation analysis for countries positioned at the higher end of the social
development spectrum. The strongest estimate for vaccine effectiveness at
ecological level was obtained for countries that use Western-only vaccines.
This may partly reflect the higher effectiveness of Western vaccines in
comparison with the average of all vaccines in use; it may also derive from the
lower social heterogeneity of countries included in this segment. COVID-19
testing has a significant and positive relationship with cumulative mortality
for all subsamples. This indicates that testing intensity should be controlled
as a potential confounder in future ecological analyses of COVID-19 mortality.

Purpose: Performance models are important tools for coaches and athletes to
optimise competition outcomes or training schedules. A recently published
hydraulic performance model has been reported to outperform established
work-balance models in predicting recovery during intermittent exercise. The
new hydraulic model was optimised to predict exercise recovery dynamics. In
this work, we hypothesised that the benefits of the model come at the cost of
inaccurate predictions of metabolic responses to exercise such as
$\dot{V}_{\mathrm{O}_2}$.
  Methods: Hydraulic model predictions were compared to breath-by-breath
$\dot{V}_{\mathrm{O}_2}$ data from 25 constant high-intensity exercise tests of
5 participants (age $32\pm7.8$ years, weight $73.6 \pm 5.81$ kg,
$\dot{V}_{\mathrm{O}_2\mathrm{max}} \; 3.59 \pm 0.62$ L/min). Each test was
performed to volitional exhaustion on a cycle ergometer with a duration between
2 and 12 min. The comparison focuses on the onset of $\dot{V}_{\mathrm{O}_2}$
kinetics.
  Results: On average, the hydraulic model predicted peak
$\dot{V}_{\mathrm{O}_2}$ during exercise $216\pm113$~s earlier than observed in
the data. The new hydraulic model also did not predict the so-called
$\dot{V}_{\mathrm{O}_2}$ slow component and made the unrealistic assumption
that there is no $\dot{V}_{\mathrm{O}_2}$ at the onset of exercise.
  Conclusion: While the new hydraulic model may be a powerful tool for
predicting energy recovery, it should not be used to predict metabolic
responses during high-intensity exercise. The present study contributes towards
a more holistic picture of the benefits and limitations of the new hydraulic
model. Data and code are published as open source.

Mammalian cells have about 30,000-fold more protein molecules than mRNA
molecules. This larger number of molecules and the associated larger dynamic
range have major implications in the application of proteomics technologies. We
examine these implications for both liquid chromatography-tandem
mass-spectrometry (LC-MS/MS) and single-molecule counting and provide estimates
on how many molecules are routinely measured in proteomics experiments by
LC-MS/MS. We review strategies that have been helpful for counting billions of
protein molecules by LC-MS/MS and suggest that these strategies can benefit
single-molecule methods, especially in mitigating the challenges of the wide
dynamic range of the proteome. We also examine the theoretical possibilities
for scaling up single-molecule and mass-spectrometry proteomics approaches to
quantifying the billions of protein molecules that make up the proteomes of our
cells.

There has been an increasing recognition of the utility of models of the
spatial dynamics of viral spread within tissues. Multicellular models, where
cells are represented as discrete regions of space coupled to a virus density
surface, are a popular approach to capture these dynamics. Conventionally, such
models are simulated by discretising the viral surface and depending on the
rate of viral diffusion and other considerations, a finer or coarser
discretisation may be used. The impact that this choice may have on the
behaviour of the system has not been studied. Here we demonstrate that, if
rates of viral diffusion are small, then the choice of spatial discretisation
of the viral surface can have quantitative and even qualitative influence on
model outputs. We investigate in detail the mechanisms driving these phenomena
and discuss the constraints on the design and implementation of multicellular
viral dynamics models for different parameter configurations.

An enduring challenge in computational biology is to balance data quality and
quantity with model complexity. Tools such as identifiability analysis and
information criterion have been developed to harmonise this juxtaposition, yet
cannot always resolve the mismatch between available data and the granularity
required in mathematical models to answer important biological questions.
Often, it is only simple phenomenological models, such as the logistic and
Gompertz growth models, that are identifiable from standard experimental
measurements. To draw insights from the complex, non-identifiable models that
incorporate key biological mechanisms of interest, we study the geometry of a
map in parameter space from the complex model to a simple, identifiable,
surrogate model. By studying how non-identifiable parameters in the complex
model quantitatively relate to identifiable parameters in surrogate, we
introduce and exploit a layer of interpretation between the set of
non-identifiable parameters and the goodness-of-fit metric or likelihood
studied in typical identifiability analysis. We demonstrate our approach by
analysing a hierarchy of mathematical models for multicellular tumour spheroid
growth. Typical data from tumour spheroid experiments are limited and noisy,
and corresponding mathematical models are very often made arbitrarily complex.
Our geometric approach is able to predict non-identifiabilities, subset
non-identifiable parameter spaces into identifiable parameter combinations that
relate to individual data features, and overall provide additional biological
insight from complex non-identifiable models.

The connection between the design and delivery of health care services using
information technology is known as health informatics. It involves data usage,
validation, and transfer of an integrated medical analysis using neural
networks of multi-layer deep learning techniques to analyze complex data. For
instance, Google incorporated ''DeepMind'' health mobile tool that integrates
\& leverage medical data needed to enhance professional healthcare delivery to
patients. Moorfield Eye Hospital London introduced DeepMind Research Algorithms
with dozens of retinal scans attributes while DeepMind UCL handled the
identification of cancerous tissues using CT \& MRI Scan tools. Atomise
analyzed drugs and chemicals with Deep Learning Neural Networks to identify
accurate pre-clinical prescriptions. Health informatics makes medical care
intelligent, interactive, cost-effective, and accessible; especially with DL
application tools for detecting the actual cause of diseases. The extensive use
of neural network tools leads to the expansion of different medical disciplines
which mitigates data complexity and enhances 3-4D overlap images using target
point label data detectors that support data augmentation, un-semi-supervised
learning, multi-modality and transfer learning architecture. Health science
over the years focused on artificial intelligence tools for care delivery,
chronic care management, prevention/wellness, clinical supports, and diagnosis.
The outcome of their research leads to cardiac arrest diagnosis through Heart
Signal Computer-Aided Diagnostic tool (CADX) and other multifunctional deep
learning techniques that offer care, diagnosis \& treatment. Health informatics
provides monitored outcomes of human body organs through medical images that
classify interstitial lung disease, detects image nodules for reconstruction \&
tumor segmentation. The emergent medical research applications gave rise to
clinical-pathological human-level performing tools for handling Radiological,
Ophthalmological, and Dental diagnosis. This research will evaluate
methodologies, Deep learning architectures, approaches, bio-informatics,
specified function requirements, monitoring tools, ANN (artificial neural
network), data labeling \& annotation algorithms that control data validation,
modeling, and diagnosis of different diseases using smart monitoring health
informatics applications.

Four different Granger causality-based methods - one linear and three
nonlinear (Granger Causality, Kernel Granger Causality, large-scale Nonlinear
Granger Causality, and Neural Network Granger Causality) were used for
assessment and causal-based quantification of the respiratory sinus arrythmia
(RSA) in the group of pediatric cardiac patients, based on the single-lead ECG
and impedance pneumography signals (the latter as the tidal volume curve
equivalent). Each method was able to detect the dependency (in terms of causal
inference) between respiratory and cardiac signals. The correlations between
quantified RSA and the demographic parameters were also studied, but the
results differ for each method.

Post-translational modifications (PTMs) have key roles in extending the
functional diversity of proteins and as a result, regulating diverse cellular
processes in prokaryotic and eukaryotic organisms. Phosphorylation modification
is a vital PTM that occurs in most proteins and plays a significant role in
many biological processes. Disorders in the phosphorylation process lead to
multiple diseases including neurological disorders and cancers. The purpose of
this review paper is to organize this body of knowledge associated with
phosphorylation site (p-site) prediction to facilitate future research in this
field. At first, we comprehensively reviewed all related databases and
introduced all steps regarding dataset creation, data preprocessing and method
evaluation in p-site prediction. Next, we investigated p-sites prediction
methods which fall into two computational groups: Algorithmic and Machine
Learning (ML). Additionally, it was shown that there are basically two main
approaches for p-sites prediction by ML: conventional and End-to-End deep
learning methods, which were given an overview for both of them. Moreover, this
study introduced the most important feature extraction techniques which have
mostly been used in p-site prediction. Finally, we created three test sets from
new proteins related to the 2022th released version of the dbPTM database based
on general and human species. Evaluation of the available online tools on the
test sets showed quite poor performance for p-sites prediction. Keywords:
Phosphorylation, Machine Learning, Deep Learning, Post Translation
Modification, Databases

Recently, haplo-identical transplantation with multiple HLA mismatches has
become a viable option for system cell transplants. Haplotype sharing detection
requires imputation of donor and recipient. We show that even in
high-resolution typing when all alleles are known, there is a 15% error rate in
haplotype phasing, and even more in low resolution typings. Similarly, in
related donors, parents haplotypes should be imputed to determine what
haplotype each child inherited. We propose GRAMM (GRaph bAsed FaMilly
iMputation) to phase alleles in family pedigree HLA typing data, and in
mother-cord blood unit pairs. We show that GRAMM has practically no phasing
errors when pedigree data are available. We apply GRAMM to simulations with
different typing resolutions as well as paired cord-mother typings, and show
very high phasing accuracy, and improved alleles imputation accuracy. We use
GRAMM to detect recombination events and show that the rate of falsely detected
recombination events (False Positive Rate) in simulations is very low. We then
apply recombination detection to typed families to estimate the recombination
rate in Israeli and Australian population datasets. The estimated recombination
rate has an upper bound of 10-20% per family (1-4% per individual). GRAMM is
available at: https://gramm.math.biu.ac.il/.

Purpose Tumor-infiltrating lymphocytes (TILs) have significant prognostic
values in cancers. However, very few automated, deep-learning-based TIL scoring
algorithms have been developed for colorectal cancers (CRC). Methods We
developed an automated, multiscale LinkNet workflow for quantifying
cellular-level TILs for CRC tumors using H&E-stained images. The predictive
performance of the automatic TIL scores (TIL) for disease progression and
overall survival was evaluate using two international datasets, including 554
CRC patients from The Cancer Genome Atlas (TCGA) and 1130 CRC patients from
Molecular and Cellular Oncology (MCO). Results The LinkNet model provided an
outstanding precision (0.9508), recall (0.9185), and overall F1 score (0.9347).
Clear dose-response relationships were observed between TILs and risk of
disease progression or death decreased in both TCGA and MCO cohorts. Both
univariate and multivariate Cox regression analyses for the TCGA data
demonstrated that patients with high TILs had significant (approx. 75%)
reduction of risk for disease progression. In both MCO and TCGA studies, the
TIL-high group was significantly associated with improved overall survival in
univariate analysis (30% and 54% reduction in risk, respectively). However,
potential confounding was observed in the MCO dataset. The favorable effects of
high TILs were consistently observed in different subgroups according to know
risk factors. Conclusion A deep-learning workflow for automatic TIL
quantification based on LinkNet was successfully developed.

Animal learning has interested ecologists and psychologists for over a
century. Mathematical models that explain how animals store and recall
information have gained attention recently. Central to this work is statistical
decision theory (SDT), which relates information uptake in animals to Bayesian
inference. SDT effectively explains many learning tasks in animals, but
extending this theory to predict how animals will learn in changing
environments still poses a challenge for ecologists. We addressed this
shortcoming with a novel implementation of Bayesian Markov Chain Monte Carlo
(MCMC) sampling to simulate how animals sample environmental information and
learn as a result. We applied our framework to an individual-based model
simulating complex foraging tasks encountered by wild animals. Simulated
``animals" learned behavioral strategies that optimized foraging returns simply
by following the principles of an MCMC sampler. In these simulations,
behavioral plasticity was most conducive to efficient foraging in unpredictable
and uncertain environments. Our model suggests that animals prioritize highly
concentrated resources even when these resources are less available overall, in
line with existing knowledge on optimal foraging and ideal free distribution
theory. Our innovative computational modelling framework can be applied more
widely to simulate the learning of many other tasks in animals and humans.

Real-time deformability cytometry (RT-DC) is an established method that
quantifies features like size, shape, and stiffness for whole cell populations
on a single-cell level in real time. To extract the cell stiffness, a lookup
table (LUT) disentangles the experimentally derived steady state cell
deformation and the projected area, yielding the Young's modulus. So far, two
lookup tables exists, but are limited to simple linear material models and
cylindrical channel geometries. Here, we present two new lookup tables for
RT-DC based on a neo-Hookean hyperelastic material numerically derived by
simulations based on the finite element method in square and cylindrical
channel geometries. At the same time, we quantify the influence of the
shear-thinning behaviour of the surrounding medium on the stationary
deformation of cells in RT-DC and discuss the applicability and impact of the
proposed LUTs regarding past and future RT-DC data analysis. Additionally, we
provide insights about the cell strain and stresses, as well as the influence
resulting from the rotational symmetric assumption on the cell deformation and
volume estimation. The new lookup tables as well as the numerical cell shapes
are made freely available.

Insect outbreaks are biotic disturbances in forests and agroecosystems that
cause economic and ecological damage. This phenomenon depends on a variety of
biological and physical factors. The complexity and practical importance of the
issue have made the problem of predicting outbreaks a focus of recent research.
Here, we propose the Pattern-Based Prediction (PBP) method for predicting
population outbreaks. It is based on the Alert Zone Procedure, combined with
elements from machine learning. It uses information on previous time series
values that precede an outbreak event as predictors of future outbreaks, which
can be useful when monitoring pest species. We illustrate the methodology using
simulated datasets and real time series data obtained by monitoring aphids in
wheat crops in Southern Brazil. We obtained an average test accuracy of
$84.6\%$ in the simulation studies implemented with stochastic models, and
$95.0\%$ for predicting outbreaks using the real dataset. This shows the
feasibility of the PBP method in predicting outbreaks in population dynamics.
We benchmarked our results against established state-of-the-art machine
learning methods, namely Support Vector Machines, Deep Neural Networks, Long
Short Term Memory and Random Forests. The PBP method yielded a competitive
performance, associated with higher true-positive rates in most comparisons,
while being able to provide interpretability rather than being a black-box
method. This is an improvement over current state-of-the-art machine learning
tools, especially when being used by non-specialists, such as ecologists aiming
to use a quantitative approach for pest monitoring. We provide open-source code
to implement the PBP method in Python, through the \texttt{pypbp} package,
which may be directly downloaded from the Python Package Index server or
accessed through \url{https://pypbp-documentation.readthedocs.io}

The understanding of the mechanisms of SARS-CoV-2 evolution and transmission
is one of the greatest challenges of our time. By integrating artificial
intelligence (AI), viral genomes isolated from patients, tens of thousands of
mutational data, biophysics, bioinformatics, and algebraic topology, the
SARS-CoV-2 evolution was revealed to be governed by infectivity-based natural
selection. Two key mutation sites, L452 and N501 on the viral spike protein
receptor-binding domain (RBD), were predicted in summer 2020, long before they
occur in prevailing variants Alpha, Beta, Gamma, Delta, Kappa, Theta, Lambda,
Mu, and Omicron. Recent studies identified a new mechanism of natural
selection: antibody resistance. AI-based forecasting of Omicron's infectivity,
vaccine breakthrough, and antibody resistance was later nearly perfectly
confirmed by experiments. The replacement of dominant BA.1 by BA.2 in later
March was predicted in early February. On May 1, 2022, persistent
Laplacian-based AI projected Omicron BA.4 and BA.5 to become the new dominating
COVID-19 variants. This prediction became reality in late June. Topological AI
models offer accurate prediction of mutational impacts on the efficacy of
monoclonal antibodies (mAbs).

Surface modification is an important topic to improve dental implants.
Corundum residues, which are part of current dental implant blasting,
disappeared on Straumann dental implants in recent publications. In our
investigations of the surface of 4 different Straumann implants using scanning
electron microscopy (SEM) and energy-dispersive X-ray spectroscopy (EDX) we
found the following three main findings: surfaces are nearly corundum-free,
disseminated gap-framed corundum particles and significant molecular carbon
residues. The data strongly suggest that Straumann applies a modified surface
technology on dental implants to remove corundum residues and involving unclear
carbons. One explanation could be, a Straumann patent involving a dextran
coating allowing easy corundum particle removal by aqueous solution, while
unintended molecular carbon residues cannot explain all findings. This change
of the production process without a new approval by the FDA would be a
violation of US federal law and the carbon bindings are a possible danger to
patients.

Phylogenetically informed k-mers, or phylo-k-mers for short, are k-mers that
are predicted to appear within a given genomic region at predefined locations
of a fixed phylogeny. Given a reference alignment for this genomic region and
assuming a phylogenetic model of sequence evolution, we can compute a
probability score for any given k-mer at any given tree node. The k-mers with
sufficiently high probabilities can later be used to perform alignment-free
phylogenetic classification of new sequences-a procedure recently proposed for
the phylogenetic placement of metabarcoding reads and the detection of novel
virus recombinants. While computing phylo-k-mers, we need to consider large
numbers of k-mers at each tree node, which warrants the development of
efficient enumeration algorithms. We consider a formal definition of the
problem of phylo-k-mer computation: How to efficiently find all k-mers whose
probability lies above a user-defined threshold for a given tree node? We
describe and analyze algorithms for this problem, relying on branch-and-bound
and divideand-conquer techniques. We exploit the redundancy of adjacent windows
of the alignment and the structure of the probability matrix to save on
computation. Besides computational complexity analyses, we provide an empirical
evaluation of the relative performance of their implementations on real-world
and simulated data. The divide-and-conquer algorithms, which to the best of our
knowledge are novel, are found to be clear improvements over the
branch-and-bound approach, especially when a large number of phylo-k-mers are
found.

The successful application of epidemic models hinges on our ability to
estimate model parameters from limited observations reliably. An
often-overlooked step before estimating model parameters consists of ensuring
that the model parameters are structurally identifiable from a given dataset.
Structural identifiability analysis uncovers any existing parameter
correlations that preclude their estimation from the observed variables. Here
we review and illustrate methods for structural identifiability analysis based
on a differential algebra approach using DAISY and Mathematica (Wolfram
Research). We demonstrate this approach through examples of compartmental
epidemic models previously employed to study transmission dynamics and control.
We show that lack of structural identifiability may be remedied by
incorporating additional observations from different model states or fixing
some parameters based on existing parameter correlations, or by reducing the
number of parameters or state variables involved in the system dynamics. We
also underscore how structural identifiability analysis can help improve
compartmental diagrams of differential-equation models by indicating the
observed variables and the results of the structural identifiability analysis.

Sound data analysis is essential to retrieve meaningful biological
information from single-cell proteomics experiments. This analysis is carried
out by computational methods that are assembled into workflows, and their
implementations influence the conclusions that can be drawn from the data. In
this work, we explore and compare the computational workflows that have been
used over the last four years and identify a profound lack of consensus on how
to analyze single-cell proteomics data. We highlight the need for benchmarking
of computational workflows, standardization of computational tools and data, as
well as carefully designed experiments. Finally, we cover the current
standardization efforts that aim to fill the gap and list the remaining missing
pieces, and conclude with lessons learned from the replication of published
single-cell proteomics analyses.

Mathematical oncology provides unique and invaluable insights into tumour
growth on both the microscopic and macroscopic levels. This review presents
state-of-the-art modelling techniques and focuses on their role in
understanding glioblastoma, a malignant form of brain cancer. For each
approach, we summarise the scope, drawbacks, and assets. We highlight the
potential clinical applications of each modelling technique and discuss the
connections between the mathematical models and the molecular and imaging data
used to inform them. By doing so, we aim to prime cancer researchers with
current and emerging computational tools for understanding tumour progression.
Finally, by providing an in-depth picture of the different modelling
techniques, we also aim to assist researchers who seek to build and develop
their own models and the associated inference frameworks.

Cholera continues to be a global health threat. Understanding how cholera
spreads between locations is fundamental to the rational, evidence-based design
of intervention and control efforts. Traditionally, cholera transmission models
have utilized cholera case count data. More recently, whole genome sequence
data has qualitatively described cholera transmission. Integrating these data
streams may provide much more accurate models of cholera spread, however no
systematic analyses have been performed so far to compare traditional
case-count models to the phylodynamic models from genomic data for cholera
transmission. Here, we use high-fidelity case count and whole genome sequencing
data from the 1991-1998 cholera epidemic in Argentina to directly compare the
epidemiological model parameters estimated from these two data sources. We find
that phylodynamic methods applied to cholera genomics data provide comparable
estimates that are in line with established methods. Our methodology represents
a critical step in building a framework for integrating case-count and genomic
data sources for cholera epidemiology and other bacterial pathogens.

Urogenital schistosomiasis has been present naturally in the South of Europe
since the beginning of the 20 th century and nowadays its presence is also
known, at least imported by Sub-Saharan emigrants and tourists, in France,
Italy, Portugal and Spain. One of the intermediate hosts of this trematode
present in Europe is the bulinid mollusc Bulinus truncatus, non-native species
that can be reached to Europe by humans and birds. In order to know this
mollusc better, we carried out a morpho-anatomical study, of the shell, the
reproductive system, radula, the respiratory organs and pseudobranch of several
populations from Italy, France and Spain. Spanish conchological material
studied comes from different populations, from material deposited in the "Museo
Nacional de Ciencias Naturales" of Madrid and the "Museu de Ci{\`e}ncies
Naturals" of Barcelona, as well as from its own material deposited in the
"Museu Valenci{\`a} d'Hist{\`o}ria Natural" of Alginet (Valencia). The shell
growth in captivity and the estimation of the population age of B. truncatus
from El Ejido (Almer{\'i}a, Spain), has also been studied. Finally, the finding
of aphallic and euphallic specimens in the different populations of southern
Europe studied is presented and taxonomic and ecological data of the genus
Bulinus are shown.

Abnormalities in motor-control behavior, which have been with concussion and
head acceleration events (HAE), can be quantified using virtual reality (VR)
technologies. Motor-control behavior has been consistently mapped to the
brain's somatomotor network (SM) using both structural (sMRI) and functional
MRI (fMRI). However, no studies habe integrated HAE, motor-control behavior,
sMRI and fMRI measures. Here, brain networks important for motor-control were
hypothesized to show changes in tractography-based diffusion weighted imaging
[difference in fractional anisotropy (dFA)] and resting-state fMRI (rs-fMRI)
measures in collegiate American football players across the season, and that
these measures would relate to VR-based motor-control. We firther tested if
nine inflammation-related miRNAs were associated with
behavior-structure-function variables. Using permutation-based mediation and
moderation methods, we found that across-season dFA from the SM structural
connectome (SM-dFA) mediated the relationship between across-season VR-based
Sensory-motor Reactivity (dSR) and rs-fMRI SM fingerprint similarity (p = 0.007
and Teff = 47%). The interaction between dSR and SM-dFA also predicted (pF =
0.036, pbeta3 = 0.058) across-season levels of dmiRNA-30d through
permutation-based moderation analysis. These results suggest (1) that
motor-control is in a feedback relationship with brain structure and function,
(2) behavior-structure-function can be connected to HAE, and (3)
behavior-structure might predict molecular biology measures.

The microbial community composition in the human gut has a profound effect on
human health. This observation has lead to extensive use of microbiome
therapies, including over-the-counter ``probiotic" treatments intended to alter
the composition of the microbiome. Despite so much promise and commercial
interest, the factors that contribute to the success or failure of
microbiome-targeted treatments remain unclear. We investigate the biotic
interactions that lead to successful engraftment of a novel bacterial strain
introduced to the microbiome as in probiotic treatments. We use pairwise
genome-scale metabolic modeling with a generalized resource allocation
constraint to build a network of interactions between 818 species with well
developed models available in the AGORA database. We create induced sub-graphs
using the taxa present in samples from three experimental engraftment studies
and assess the likelihood of invader engraftment based on network structure. To
do so, we use a set of dynamical models designed to reflect connect network
topology to growth dynamics. We show that a generalized Lotka-Volterra model
has strong ability to predict if a particular invader or probiotic will
successfully engraft into an individual's microbiome. Furthermore, we show that
the mechanistic nature of the model is useful for revealing which
microbe-microbe interactions potentially drive engraftment.

Stem cell-derived organoids are a promising tool to model native human
tissues as they resemble human organs functionally and structurally compared to
traditional monolayer cell-based assays. For instance, colon organoids can
spontaneously develop crypt-like structures similar to those found in the
native colon. While analyzing the structural development of organoids can be a
valuable readout, using traditional image analysis tools makes it challenging
because of the heterogeneities and the abstract nature of organoid
morphologies. To address this limitation, we developed and validated a deep
learning-based image analysis tool, named D-CryptO, for the classification of
organoid morphology. D-CryptO can automatically assess the crypt formation and
opacity of colorectal organoids from brightfield images to determine the extent
of organoid structural maturity. To validate this tool, changes in organoid
morphology were analyzed during organoid passaging and short-term forskolin
stimulation. To further demonstrate the potential of D-CryptO for drug testing,
organoid structures were analyzed following treatments with a panel of
chemotherapeutic drugs. With D-CryptO, subtle variations in how colon organoids
responded to the different chemotherapeutic drugs were detected, which suggest
potentially distinct mechanisms of action. This tool could be expanded to other
organoid types, like intestinal organoids, to facilitate 3D tissue
morphological analysis.

Pulmonary hypertension (PH), defined by a mean pulmonary arterial blood
pressure above 20 mmHg, is a cardiovascular disease impacting the pulmonary
vasculature. PH is accompanied by vascular remodeling, wherein vessels become
stiffer, large vessels dilate, and smaller vessels constrict. Some types of PH,
including hypoxia-induced PH (HPH), lead to microvascular rarefaction. The goal
of this study is to analyze the change in pulmonary arterial network
morphometry in the presence of HPH. To do so, we use novel methods from
topological data analysis (TDA), employing persistent homology to quantify
arterial network morphometry for control and hypertensive mice. These methods
are used to characterize arterial trees extracted from micro-computed
tomography (micro-CT) images. To compare results between control and
hypertensive animals, we normalize generated networks using three pruning
algorithms. This proof-of-concept study shows that the pruning methods effects
the spatial tree statistics and complexities of the trees. Results show that
HPH trees have higher depth and that the directional complexities correlate
with branch number, except for trees pruned by vessel radius, where the left
and anterior complexity are lower compared to control trees. While more data is
required to make a conclusion about the overall effect of HPH on network
topology, this study provides a framework for analyzing the topology of
biological networks and is a step towards the extraction of relevant
information for diagnosing and detecting HPH.

In this study, we present a method of pattern mining based on network theory
that enables the identification of protein structures or complexes from
synthetic volume densities, without the knowledge of predefined templates or
human biases for refinement. We hypothesized that the topological connectivity
of protein structures is invariant, and they are distinctive for the purpose of
protein identification from distorted data presented in volume densities.
Three-dimensional densities of a protein or a complex from simulated
tomographic volumes were transformed into mathematical graphs as observables.
We systematically introduced data distortion or defects such as missing
fullness of data, the tumbling effect, and the missing wedge effect into the
simulated volumes, and varied the distance cutoffs in pixels to capture the
varying connectivity between the density cluster centroids in the presence of
defects. A similarity score between the graphs from the simulated volumes and
the graphs transformed from the physical protein structures in point data was
calculated by comparing their network theory order parameters including node
degrees, betweenness centrality, and graph densities. By capturing the
essential topological features defining the heterogeneous morphologies of a
network, we were able to accurately identify proteins and homo-multimeric
complexes from ten topologically distinctive samples without realistic noise
added. Our approach empowers future developments of tomogram processing by
providing pattern mining with interpretability, to enable the classification of
single-domain protein native topologies as well as distinct single-domain
proteins from multimeric complexes within noisy volumes.

Cellular functions crucially depend on the precise regulation of gene
expression. This process involves complex biochemical reactions taking place on
the chromatin polymer in a tightly confined environment. Despite the
availability of large data sets probing this process from multiple angles, we
still lack a quantitative understanding of how biochemical processes, governed
by genetic sequence affinity, interact with the 3D polymer nature of the
chromatin. Here we propose a new statistical polymer model which naturally
incorporates observational data about sequence-driven biochemical processes,
such as the binding of transcription factor proteins, in a 3D model of
chromatin structure. We introduce a new algorithm for approximate Bayesian
inference and show on a case study that our model not only provides better
predictions of chromatin state compared to purely data-driven approaches, but
also gives a quantitative estimate of the relative importance of biochemical
and polymer terms in determining chromatin state, leading to a significant
revision of previous estimates. Furthermore, we show that, with no additional
input from genome 3D structure data, our model can predict highly non-trivial
geometrical structures in the folding of DNA within the nucleus. Our model
demonstrates the importance of introducing physically realistic statistical
models for predicting chromatin state from data and opens the way to a new
class of approaches to interpreting epigenomic data.

Epitope vaccines are a promising direction to enable precision treatment for
cancer, autoimmune diseases, and allergies. Effectively designing such vaccines
requires accurate prediction of proteasomal cleavage in order to ensure that
the epitopes in the vaccine are presented to T cells by the major
histocompatibility complex (MHC). While direct identification of proteasomal
cleavage \emph{in vitro} is cumbersome and low throughput, it is possible to
implicitly infer cleavage events from the termini of MHC-presented epitopes,
which can be detected in large amounts thanks to recent advances in
high-throughput MHC ligandomics. Inferring cleavage events in such a way
provides an inherently noisy signal which can be tackled with new developments
in the field of deep learning that supposedly make it possible to learn
predictors from noisy labels. Inspired by such innovations, we sought to
modernize proteasomal cleavage predictors by benchmarking a wide range of
recent methods, including LSTMs, transformers, CNNs, and denoising methods, on
a recently introduced cleavage dataset. We found that increasing model scale
and complexity appeared to deliver limited performance gains, as several
methods reached about 88.5% AUC on C-terminal and 79.5% AUC on N-terminal
cleavage prediction. This suggests that the noise and/or complexity of
proteasomal cleavage and the subsequent biological processes of the antigen
processing pathway are the major limiting factors for predictive performance
rather than the specific modeling approach used. While biological complexity
can be tackled by more data and better models, noise and randomness inherently
limit the maximum achievable predictive performance. All our datasets and
experiments are available at
https://github.com/ziegler-ingo/cleavage_prediction.

Action potential propagation along the axons and across the dendrites is the
foundation of the electrical activity observed in the brain and the rest of the
central nervous system. Theoretical and numerical modeling of this action
potential activity has long been a key focus area of electro-chemical neuronal
modeling. Specifically, considering the presence of nodes of Ranvier along the
myelinated axon, single-cable models of the propagation of action potential
have been popular. Building on these models, and considering a secondary
electrical conduction pathway below the myelin sheath, the double-cable model
has been proposed. Such cable theory based treatments have inherent limitations
in their lack of a representation of the spatio-temporal evolution of the
neuronal electro-chemistry. In contrast, a Poisson-Nernst-Planck (PNP) based
electro-diffusive framework accounts for the underlying spatio-temporal ionic
concentration dynamics and is a more comprehensive treatment. In this work, a
high-fidelity implementation of the PNP model is demonstrated. This model is
shown to produce results similar to the cable theory based electrical models,
and in addition, the rich spatio-temporal evolution of the underlying ionic
transport is captured. Novel to this work is the extension of PNP model to
axonal geometries with multiple nodes of Ranvier and multiple variants of the
electro-diffusive model - PNP without myelin, PNP with myelin, and PNP with the
myelin sheath and peri-axonal space. Further, we apply this spatio-temporal
model to numerically estimate conduction velocity in a rat axon. Specifically,
saltatory conduction due to the presence of myelin sheath and the peri-axonal
space is investigated.

We describe several shortcomings of a study by Patone et al, whose findings
were recently published in the American Heart Association Journal Circulation,
including the following:
  * The study's principal conclusion, as initially stated, begins "Overall, the
risk of myocarditis is greater after SARS-CoV-2 infection than after COVID-19
vaccination ...." However, Patone et al never attempt to assess the incidence
of myocarditis in their study population following SARS-CoV-2 infection.
Rather, they make an untenable assumption that all infections occurring in
their study population are associated with (reported) positive COVID-19 tests.
Using publicly available data from the UK's ONS and NHS, we show that Patone et
al's estimates, for the unvaccinated, of myocarditis incidence associated with
infection are likely overestimated by a factor of at least 1.58.
  * The method Patone et al use to compute the incidence of myocarditis among
the unvaccinated after a positive COVID test may overestimate risk. The authors
assume, without justification, that unvaccinated persons hospitalized during
the study period with positive-test-associated myocarditis would later choose
to vaccinate with the same probability as unvaccinated persons who have had a
positive COVID test. We present a plausibility argument that suggests a
possible further exaggeration of myocarditis risk post infection by a factor of
1.5.
  * Patone et al fail to discuss important limitations of their study with
respect to guiding public health recommendations. For instance, at most 0.18%
of SARS-CoV-2 cases that contributed to the study's finding were
Omicron-variant cases. Thus, the study's estimates of myocarditis risk
following infection do not speak to the risk following Omicron infection, which
is recognized to be milder than that of previous variants.

We propose a novel mathematical paradigm for the study of genetic variation
in sequence alignments. This framework originates from extending the notion of
pairwise relations, upon which current analysis is based on, to k-ary
dissimilarity. This dissimilarity naturally leads to a generalization of
simplicial complexes by endowing simplices with weights, compatible with the
boundary operator. We introduce the notion of k-stances and dissimilarity
complex, the former encapsulating arithmetic as well as topological structure
expressing these k-ary relations. We study basic mathematical properties of
dissimilarity complexes and show how this approach captures an entirely new
layer of biologically relevant viral dynamics in the context of SARS-CoV-2 and
H1N1 flu genomic data.

Deep Learning and DRUG-seq (Digital RNA with perturbation of genes) have
attracted attention in drug discovery. However, the public DRUG-seq dataset is
too small to be used for directly training a deep learning neural network from
scratch. Inspired by the transfer learning technique, we pretrain a drug
efficacy prediction neural network model with the Library of Integrated
Network-based Cell-Signature (LINCS) L1000 data and then use human neural cell
DRUG-seq data to fine-tune it. After training, the model is used for virtual
screening to find potential drugs for Alzheimer's disease (AD) treatment.
Finally, we find 27 potential drugs for AD treatment including Irsogladine
(PDE4 inhibitor), Tasquinimod (HDAC4 selective inhibitor), Suprofen (dual
COX-1/COX-2 inhibitor) et al.

Motivation: A crucial step to understand and also to communicate clinical
data either from scientific research or clinical routine practice is
visualizing it. However, visualization generally comes with great effort due to
the lack of suitable tools or significant technical knowledge about data
handling and subsequent visualization. The solution presented in this work
offers an easy-to-use software tool that enables various kinds of plotting
functions. Due to the easy and intuitive design, the entrance barrier to using
the viewer tool is very low, and little to no onboarding is required before
creating the first plot. However, patient data are obviously subject to strict
data protection. Consequently, the traceability of subjects should be avoided,
which can be overcome by generation of synthetic cohorts. Methods: The
visualization tool is developed as an open-source solution implemented as a
web-based service consisting of a data module as well as an interface module
using modern technologies. The virtual cohort was created in the scope of this
work to be able to deploy a version without violating any data protection
guidelines. This cohort containing artificial patients is generated from a
noise-adding model that preserves important aspects of the original data, such
that it is a realistic dataset ready to be explored by users. Results: This
work presents SCAview, an user-friendly, interactive cohort explorer that aims
to enable data handling for visualization in a clickable interface. The service
is deployed and can be tested with a virtual cohort employed for privacy
reasons. In order to get access please contact the corresponding author
(address below). Availability: http://idsn.dzne.de - In order to get a user and
password please contact philipp.wegner@dzne.de Contact: philipp.wegner@dzne.de

The multifunctional nucleoproteins play important role in the life cycle of
coronaviruses. The assessment of their quantities is of general interest for
the assembly of virions and medical applications. The proliferating
nucleoproteins induce the related (auto)immune response and via binding to host
RNA affect various regulation mechanisms. In this report we briefly summarize
and comment the available experimental data on the subject concerned.

The existence of doublets in single-cell RNA sequencing (scRNA-seq) data
poses a great challenge in downstream data analysis. Computational
doublet-detection methods have been developed to remove doublets from scRNA-seq
data. Yet, the default hyperparameter settings of those methods may not provide
optimal performance. Here, we propose a strategy to tune hyperparameters for a
cutting-edge doublet-detection method. We utilize a full factorial design to
explore the relationship between hyperparameters and detection accuracy on 16
real scRNA-seq datasets. The optimal hyperparameters are obtained by a response
surface model and convex optimization. We show that the optimal hyperparameters
provide top performance across scRNA-seq datasets under various biological
conditions. Our tuning strategy can be applied to other computational
doublet-detection methods. It also offers insights into hyperparameter tuning
for broader computational methods in scRNA-seq data analysis.

It is shown that various symptoms could remain in the stage of post-acute
sequelae of SARS-CoV-2 infection (PASC), otherwise known as Long COVID. A
number of COVID patients suffer from heterogeneous symptoms, which severely
impact recovery from the pandemic. While scientists are trying to give an
unambiguous definition of Long COVID, efforts in prediction of Long COVID could
play an important role in understanding the characteristic of this new disease.
Vital measurements (e.g. oxygen saturation, heart rate, blood pressure) could
reflect body's most basic functions and are measured regularly during
hospitalization, so among patients diagnosed COVID positive and hospitalized,
we analyze the vital measurements of first 7 days since the hospitalization
start date to study the pattern of the vital measurements and predict Long
COVID with the information from vital measurements.

Recent advances in biological technologies, such as multi-way chromosome
conformation capture (3C), require development of methods for analysis of
multi-way interactions. Hypergraphs are mathematically tractable objects that
can be utilized to precisely represent and analyze multi-way interactions. Here
we present the Hypergraph Analysis Toolbox (HAT), a software package for
visualization and analysis of multi-way interactions in complex systems.

Motivation: Over the past decade, network-based approaches have proven useful
in identifying disease modules within the human interactome, often providing
insights into key mechanisms and guiding the quest for therapeutic targets.
This is all the more important, since experimental investigation of potential
gene candidates is an expensive task, thus not always a feasible option. On the
other hand, many sources of biological information exist beyond the interactome
and an important research direction is the design of effective techniques for
their integration. Results: In this work, we introduce the Biological Random
Walks (BRW) approach for disease gene prioritization in the human interactome.
The proposed framework leverages multiple biological sources within an
integrated framework. We perform an extensive, comparative study of BRW's
performance against well-established baselines. Availability and
implementation: All code is publicly available and can be downloaded at
\url{https://github.com/LeoM93/BiologicalRandomWalks}. We used publicly
available datasets, details on their retrieval and preprocessing are provided
in the supplementary material.

Acute lymphoblastic leukemia is the most frequent pediatric cancer.
Approximately two third of survivors develop one or more health complications
known as late adverse effects following their treatments. The existing measures
offered to patients during their follow-up visits to the hospital are rather
standardized for all childhood cancer survivors and not necessarily
personalized for childhood ALL survivors. As a result, late adverse effects may
be underdiagnosed and, in most cases, only taken care of following their
appearance. Thus, it is necessary to predict these treatment-related conditions
earlier in order to prevent them and enhance the survivors' health. Multiple
studies have investigated the development of late adverse effects prediction
tools to offer better personalized follow-up methods. However, no solution
integrated the usage of neural networks to date. In this work, we developed
graph-based parameters-efficient neural networks and promoted their
interpretability with multiple post-hoc analyses. We first proposed a new
disease-specific VO$_2$ peak prediction model that does not require patients to
participate to a physical function test (e.g., 6-minute walk test) and further
created an obesity prediction model using clinical variables that are available
from the end of childhood ALL treatment as well as genomic variables. Our
solutions were able to achieve better performance than linear and tree-based
models on small cohorts of patients ($\leq$ 223) for both tasks.

Acute myocardial infarction (AMI) is one of the most severe manifestation of
coronary artery disease. ST-segment elevation myocardial infarction (STEMI) is
the most serious type of AMI. We proposed to develop a machine learning
algorithm based on the home page of electronic medical record (HPEMR) for
predicting in-hospital mortality of patients with STEMI in the early stage.
Methods: This observational study applied clinical information collected
between 2013 and 2017 from 7 tertiary hospitals in Shenzhen, China. The
patients' STEMI data were used to train 4 different machine learning algorithms
to predict in-hospital mortality among the patients with STEMI, including
Logistic Regression, Support Vector Machine, Gradient Boosting Decision Tree,
and Artificial Neuron network. Results: A total of 5865 patients with STEMI
were enrolled in our study. The model was developed by considering 3 types of
variables, which included demographic data, diagnosis and comorbidities, and
hospitalization information basing on HPEMR. The association of selected
features using univariant logistic regression was reported. Specially, for the
comorbidities, atrial fibrillation (OR: 11.0; 95% CI: 5.64 - 20.2), acute renal
failure (OR: 9.75; 95% CI: 3.81 - 25.0), type 2 diabetic nephropathy (OR: 5.45;
95% CI: 1.57 - 19.0), acute heart failure (OR: 6.05; 95% CI: 1.99 - 14.9), and
cardiac function grade IV (OR: 28.6; 95% CI: 20.6 - 39.6) were found to be
associated with a high odds of death. Within the test dataset, our model showed
a good discrimination ability as measured by area under the receiver operating
characteristic curve (AUC; 0.879) (95% CI: 0.825 - 0.933).

Reducing antibiotic usage in livestock animals has become an urgent issue
worldwide to prevent antimicrobial resistance. Here, abuse of chlortetracycline
(CTC), a versatile antibacterial agent, on the performance, blood components,
fecal microbiota, and organic acid concentration in calves was investigated.
Japanese Black calves were fed milk replacer containing CTC at 10 g/kg (CON) or
0 g/kg (EXP). Growth performance was not affected by CTC administration.
However, CTC administration altered the correlation between fecal organic acids
and bacterial genera. Machine learning methods such as association analysis,
linear discriminant analysis, and energy landscape analysis revealed that CTC
administration affected according to certain rules the population of various
types of fecal bacteria. It is particularly interesting that the population of
several methane-producing bacteria was high in the CON, and that of
Lachnospiraceae, a butyrate-producing bacteria, was high in the EXP at 60 d of
age. Furthermore, statistical causal inference based on machine learning data
estimated that CTC treatment affects the entire intestinal environment,
inhibiting butyrate production for growth and biological defense, which may be
attributed to methanogens in feces. Thus, these observations highlight the
multiple harmful impacts of antibiotics on intestinal health and the potential
production of greenhouse gas in the calves.

We live in a period where bio-informatics is rapidly expanding, a significant
quantity of genomic data has been produced as a result of the advancement of
high-throughput genome sequencing technology, raising concerns about the costs
associated with data storage and transmission. The question of how to properly
compress data from genomic sequences is still open. Previously many researcher
proposed many compression method on this topic DNA Compression without machine
learning and with machine learning approach. Extending a previous research, we
propose a new architecture like modified DeepDNA and we have propose a new
methodology be deploying a double base-ed strategy for compression of DNA
sequences. And validated the results by experimenting on three sizes of
datasets are 100, 243, 356. The experimental outcomes highlight our improved
approach's superiority over existing approaches for analyzing the human
mitochondrial genome data, such as DeepDNA.

Background: The availability of high throughput methods for measurement of
mRNA concentrations makes the reliability of conclusions drawn from the data
and global quality control of samples and hybridization important issues. We
address these issues by an information theoretic approach, applied to
discretized expression values in replicated gene expression data.
  Results: Our approach yields a quantitative measure of two important
parameter classes: First, the probability $P(\sigma | S)$ that a gene is in the
biological state $\sigma$ in a certain variety, given its observed expression
$S$ in the samples of that variety. Second, sample specific error probabilities
which serve as consistency indicators of the measured samples of each variety.
The method and its limitations are tested on gene expression data for
developing murine B-cells and a $t$-test is used as reference. On a set of
known genes it performs better than the $t$-test despite the crude
discretization into only two expression levels. The consistency indicators,
i.e. the error probabilities, correlate well with variations in the biological
material and thus prove efficient.
  Conclusions: The proposed method is effective in determining differential
gene expression and sample reliability in replicated microarray data. Already
at two discrete expression levels in each sample, it gives a good explanation
of the data and is comparable to standard techniques.

We report the development and detailed calibration of a multiphoton
fluorescence lifetime imaging system (FLIM) using a streak camera. The present
system is versatile with high spatial (0.2 micron) and temporal (50 psec)
resolution and allows rapid data acquisition and reliable and reproducible
lifetime determinations. The system was calibrated with standard fluorescent
dyes and the lifetime values obtained were in very good agreement with values
reported in literature for these dyes. We also demonstrate the applicability of
the system to FLIM studies in cellular specimens including stained pollen
grains and fibroblast cells expressing green fluorescent protein. The lifetime
values obtained matched well with those reported earlier by other groups for
these same specimens. Potential applications of the present system include the
measurement of intracellular physiology and Fluorescence Resonance Energy
Transfer (FRET) imaging which are discussed in the context of live cell
imaging.

We make available a library of documented IDL .pro files as well as a
shareable object library that allows IDL to call routines from LAPACK. The
routines are for use in the spectral analysis of time series data. The primary
focus of these routines are David Thomson's multitaper methods but a whole
range of functions will be made available in future revisions of the
submission. At present routines are provided to carry out the following
operations: calculate prolate spheroidal sequences and eigenvalues, project
time-series into frequency bands, calculate spectral estimates with or without
moving windows, and calculate the cross-coherence between two time series as a
function of frequency as well as the coherence between frequencies for a single
time series.

We examined the changes in swimming behaviour of the bacterium Rhodobacter
sphaeroides in response to stepwise changes in a nutrient (propionate),
following the prestimulus motion, the initial response and the adaptation to
the sustained concentration of the chemical. This was carried out by tethering
motile cells by their flagella to glass slides and following the rotational
behaviour of their cell bodies in response to the nutrient change. Computerised
motion analysis was used to analyse the behaviour. Distributions of run and
stop times were obtained from rotation data for tethered cells. Exponential and
Weibull fits for these distributions, and variability in individual responses
are discussed. In terms of parameters derived from the run and stop time
distributions, we compare the responses to stepwise changes in the nutrient
concentration and the long-term behaviour of 84 cells under twelve propionate
concentration levels from 1 nM to 25 mM. We discuss traditional assumptions for
the random walk approximation to bacterial swimming and compare them with the
observed R. sphaeroides motile behaviour.

An analysis of the protein content of several crystal forms of proteins has
been performed. We apply a new numerical technique, the Independent Component
Analysis (ICA), to determine the volume fraction of the asymmetric unit
occupied by the protein. This technique requires only the crystallographic data
of structure factors as input.

We introduce, analyze, and implement a new method for parameter
identification for system of ordinary differential equations that are used to
model sets of biochemical reactions. Our method relies on the integral
formulation of the ODE system and a method of linear least squares applied to
the integral equations. Certain variants of this method are also introduced in
this paper.

Microelectromagnet devices, a ring trap and a matrix, were developed for the
microscopic control of biological systems. The ring trap is a circular Au wire
with an insulator on top. The matrix has two arrays of straight Au wires, one
array perpendicular to the other, that are separated and topped by insulating
layers. Microelectromagnets can produce strong magnetic fields to stably
manipulate magnetically tagged biological systems in a fluid. Moreover, by
controlling the currents flowing through the wires, a microelectromagnet matrix
can move a peak in the magnetic field magnitude continuously over the surface
of the device, generate multiple peaks simultaneously and control them
independently. These capabilities of a matrix can be used to trap, continuously
transport, assemble, separate and sort biological samples on micrometer length
scales. Combining microelectromagnets with microfluidic systems, chip-based
experimental systems can be realized for novel applications in biological and
biomedical studies.

A barrier penetration model has been proposed to explain the spontaneous
melting of the DNA oligomers into two separate single strands whereas the
partially melted intermediate states are shown to be the bound state solution
of the same effective potential that generates the barrier.

Large numbers of MS/MS peptide spectra generated in proteomics experiments
require efficient, sensitive and specific algorithms for peptide
identification. In the Open Mass Spectrometry Search Algorithm [OMSSA],
specificity is calculated by a classic probability score using an explicit
model for matching experimental spectra to sequences. At default thresholds,
OMSSA matches more spectra from a standard protein cocktail than a comparable
algorithm. OMSSA is designed to be faster than published algorithms in
searching large MS/MS datasets.

Sequence comparison is a widely used computational technique in modern
molecular biology. In spite of the frequent use of sequence comparisons the
important problem of assigning statistical significance to a given degree of
similarity is still outstanding. Analytical approaches to filling this gap
usually make use of an approximation that neglects certain correlations in the
disorder underlying the sequence comparison algorithm. Here, we use the longest
common subsequence problem, a prototype sequence comparison problem, to
analytically establish that this approximation does make a difference to
certain sequence comparison statistics. In the course of establishing this
difference we develop a method that can systematically deal with these disorder
correlations.

We present a novel classification-based algorithm called GeneClass for
learning to predict gene regulatory response. Our approach is motivated by the
hypothesis that in simple organisms such as Saccharomyces cerevisiae, we can
learn a decision rule for predicting whether a gene is up- or down-regulated in
a particular experiment based on (1) the presence of binding site subsequences
(``motifs'') in the gene's regulatory region and (2) the expression levels of
regulators such as transcription factors in the experiment (``parents''). Thus
our learning task integrates two qualitatively different data sources:
genome-wide cDNA microarray data across multiple perturbation and mutant
experiments along with motif profile data from regulatory sequences. Rather
than focusing on the regression task of predicting real-valued gene expression
measurements, GeneClass performs the classification task of predicting +1 and
-1 labels, corresponding to up- and down-regulation beyond the levels of
biological and measurement noise in microarray measurements. GeneClass uses the
Adaboost learning algorithm with a margin-based generalization of decision
trees called alternating decision trees. In computational experiments based on
the Gasch S. cerevisiae dataset, we show that the GeneClass method predicts up-
and down-regulation on held-out experiments with high accuracy. We explore a
range of experimental setups related to environmental stress response, and we
retrieve important regulators, binding site motifs, and relationships between
regulators and binding sites that are known to be associated to specific stress
response pathways. Our method thus provides predictive hypotheses, suggests
biological experiments, and provides interpretable insight into the structure
of genetic regulatory networks.

Action potential duration (APD) restitution, which relates APD to the
preceding diastolic interval (DI), is a useful tool for predicting the onset of
abnormal cardiac rhythms. However, it is known that different pacing protocols
lead to different APD restitution curves (RCs). This phenomenon, known as APD
rate-dependence, is a consequence of memory in the tissue. In addition to APD
restitution, conduction velocity restitution also plays an important role in
the spatiotemporal dynamics of cardiac tissue. We present new results
concerning rate-dependent restitution in the velocity of propagating action
potentials in a one-dimensional fiber. Our numerical simulations show that,
independent of the amount of memory in the tissue, waveback velocity exhibits
pronounced rate-dependence and the wavefront velocity does not. Moreover, the
discrepancy between waveback velocity RCs is most significant for small DI. We
provide an analytical explanation of these results, using a system of coupled
maps to relate the wavefront and waveback velocities. Our calculations show
that waveback velocity rate-dependence is due to APD restitution, not memory.

A commonly employed measure of the signal amplification properties of an
input/output system is its induced L2 norm, sometimes also known as "H
infinity" gain. In general, however, it is extremely difficult to compute the
numerical value for this norm, or even to check that it is finite, unless the
system being studied is linear. This paper describes a class of systems for
which it is possible to reduce this computation to that of finding the norm of
an associated linear system. In contrast to linearization approaches, a precise
value, not an estimate, is obtained for the full nonlinear model. The class of
systems that we study arose from the modeling of certain biological
intracellular signaling cascades, but the results should be of wider
applicability.

We analyze a microscopic RNA model, which includes two widely used models as
limiting cases, namely it contains terms for bond as well as for stacking
energies. We numerically investigate possible changes in the qualitative and
quantitative behaviour while going from one model to the other; in particular
we test, whether a transition occurs, when continuously moving from one model
to the other. For this we calculate various thermodynamic quantities, both at
zero temperature as well as at finite temperatures. All calculations can be
done efficiently in polynomial time by a dynamic programming algorithm. We do
not find a sign for transition between the models, but the critical exponent
$\nu$ of the correlation length, describing the phase transition in all models
to an ordered low-temperature phase, seems to depend continuously on the model.
Finally, we apply the epsilon-Coupling method, to study low excitations. The
exponent $\theta$ describing the energy-scaling of the excitations seems to
depend not much on the energy model.

An analysis of the RR-interval time series, $t_i$, is presented for the case
in which the average time, $\bar{t}$, changes slowly. In particular, $\bar{t}$
and a short-time scale variability parameter, $V$, are simultaneously measured
while $\bar{t}$ decreases for subjects in the reclined position. The initial
decrease in $\bar{t}$ is usually linear with $V$ yielding parameters that can
be related to physiological quantities.

We present a novel classification-based method for learning to predict gene
regulatory response. Our approach is motivated by the hypothesis that in simple
organisms such as Saccharomyces cerevisiae, we can learn a decision rule for
predicting whether a gene is up- or down-regulated in a particular experiment
based on (1) the presence of binding site subsequences (``motifs'') in the
gene's regulatory region and (2) the expression levels of regulators such as
transcription factors in the experiment (``parents''). Thus our learning task
integrates two qualitatively different data sources: genome-wide cDNA
microarray data across multiple perturbation and mutant experiments along with
motif profile data from regulatory sequences. We convert the regression task of
predicting real-valued gene expression measurement to a classification task of
predicting +1 and -1 labels, corresponding to up- and down-regulation beyond
the levels of biological and measurement noise in microarray measurements. The
learning algorithm employed is boosting with a margin-based generalization of
decision trees, alternating decision trees. This large-margin classifier is
sufficiently flexible to allow complex logical functions, yet sufficiently
simple to give insight into the combinatorial mechanisms of gene regulation. We
observe encouraging prediction accuracy on experiments based on the Gasch S.
cerevisiae dataset, and we show that we can accurately predict up- and
down-regulation on held-out experiments. Our method thus provides predictive
hypotheses, suggests biological experiments, and provides interpretable insight
into the structure of genetic regulatory networks.

We introduce a new heuristic for the multiple alignment of a set of
sequences. The heuristic is based on a set cover of the residue alphabet of the
sequences, and also on the determination of a significant set of blocks
comprising subsequences of the sequences to be aligned. These blocks are
obtained with the aid of a new data structure, called a suffix-set tree, which
is constructed from the input sequences with the guidance of the
residue-alphabet set cover and generalizes the well-known suffix tree of the
sequence set. We provide performance results on selected BAliBASE amino-acid
sequences and compare them with those yielded by some prominent approaches.

We present multiscale models of cancer tumor invasion with components at the
molecular, cellular, and tissue levels. We provide biological justifications
for the model components, present computational results from the model, and
discuss the scientific-computing methodology used to solve the model equations.
The models and methodology presented in this paper form the basis for
developing and treating increasingly complex, mechanistic models of tumor
invasion that will be more predictive and less phenomenological. Because many
of the features of the cancer models, such as taxis, aging and growth, are seen
in other biological systems, the models and methods discussed here also provide
a template for handling a broader range of biological problems.

Random forest is a classification algorithm well suited for microarray data:
it shows excellent performance even when most predictive variables are noise,
can be used when the number of variables is much larger than the number of
observations, and returns measures of variable importance. Thus, it is
important to understand the performance of random forest with microarray data
and its use for gene selection.
  We first show the effects of changes in parameters of random forest on the
prediction error. Then we present an approach for gene selection that uses
measures of variable importance and error rate, and is targeted towards the
selection of small sets of genes. Using simulated and real microarray data, we
show that the gene selection procedure yields small sets of genes while
preserving predictive accuracy.
  Availability: All code is available as an R package, varSelRF, from CRAN,
http://cran.r-project.org/src/contrib/PACKAGES.html, or from the supplementary
material page.
  Supplementary information:
http://ligarto.org/rdiaz/Papers/rfVS/randomForestVarSel.html

We introduce a new methodology for the determination of amino-acid
substitution matrices for use in the alignment of proteins. The new methodology
is based on a pre-existing set cover on the set of residues and on the
undirected graph that describes residue exchangeability given the set cover.
For fixed functional forms indicating how to obtain edge weights from the set
cover and, after that, substitution-matrix elements from weighted distances on
the graph, the resulting substitution matrix can be checked for performance
against some known set of reference alignments and for given gap costs. Finding
the appropriate functional forms and gap costs can then be formulated as an
optimization problem that seeks to maximize the performance of the substitution
matrix on the reference alignment set. We give computational results on the
BAliBASE suite using a genetic algorithm for optimization. Our results indicate
that it is possible to obtain substitution matrices whose performance is either
comparable to or surpasses that of several others, depending on the particular
scenario under consideration.

We introduce a model for describing the dynamics of large numbers of
interacting cells. The fundamental dynamical variables in the model are
sub-cellular elements, which interact with each other through phenomenological
intra- and inter-cellular potentials. Advantages of the model include i)
adaptive cell-shape dynamics, ii) flexible accommodation of additional
intra-cellular biology, and iii) the absence of an underlying grid. We present
here a detailed description of the model, and use successive mean-field
approximations to connect it to more coarse-grained approaches, such as
discrete cell-based algorithms and coupled partial differential equations. We
also discuss efficient algorithms for encoding the model, and give an example
of a simulation of an epithelial sheet. Given the biological flexibility of the
model, we propose that it can be used effectively for modeling a range of
multi-cellular processes, such as tumor dynamics and embryogenesis.

A model of growth of icosahedral viral capsids is proposed. It takes into
account the diversity of hexamers' compositions, leading to definite capsid
size. We show that the observed yield of capsid production implies a very high
level of self-organization of elementary building blocks. The exact number of
different protein dimers composing hexamers is related to the size of a given
capsid, labeled by its T-number. Simple rules determining these numbers for
each value of T are deduced and certain consequences are discussed.

A quantitative measure of stability in stochastic dynamics starts to emerge
in recent experiments on bioswitches. This quantity, similar to the potential
function in mathematics, is deeply rooted in biology, dated back at the
beginning of quantitative description of biological processes: the adaptive
landscape of Wright (1932) and the development landscape of Waddington (1940).
Nevertheless, its quantitative implication has been frequently challenged by
biologists. Recent progresses in quantitative biology begin to meet those
outstanding challenges.

The presented previously indirect optimization method (IOM) developed within
biochemical systems theory (BST) provides a versatile and mathematically
tractable optimization strategy for biochemical systems. However, due to the
local approximations nature of the BST formalism, the iterative version of this
technique possibly does not yield the true optimum solution. In this work, an
algorithm is proposed to obtain the correct and consistent optimum steady-state
operating point of biochemical systems. The existing linear optimization
problem of the direct IOM approach is modified by adding an equality constraint
of describing the consistency of solutions between the S-system and the
original model. Lagrangian analysis is employed to derive the first order
necessary optimality conditions for the above modified optimization problem.
This leads to a procedure that may be regarded as a modified iterative IOM
approach in which the optimization objective function includes an extra linear
term. The extra term contains a comparison of metabolite concentration
derivatives with respect to the enzyme activities between the S-system and the
original model and ensures that the new algorithm is still carried out within
linear programming techniques. The presented framework is applied to several
biochemical systems and shown to the tractability and effectiveness of the
method. The simulation is also studied to investigate the convergence
properties of the algorithm and to give a performance comparison of standard
and modified iterative IOM approach.

Ecological systems are governed by complex interactions which are mainly
nonlinear. In order to capture this complexity and nonlinearity, statistical
models recently gained popularity. However, although these models are commonly
applied in ecology, there are no studies to date aiming to assess the
applicability and performance. We provide an overview for nature of the wide
range of the data sets and predictive variables, from both aquatic and
terrestrial ecosystems with different scales of time-dependent dynamics, and
the applicability and robustness of predictive modeling methods on such data
sets by comparing different statistical modeling approaches. The methods
considered k-NN, LDA, QDA, generalized linear models (GLM) feedforward
multilayer backpropagation networks and pseudo-supervised network ARTMAP. For
ecosystems involving time-dependent dynamics and periodicities whose frequency
are possibly less than the time scale of the data considered, GLM and
connectionist neural network models appear to be most suitable and robust,
provided that a predictive variable reflecting these time-dependent dynamics
included in the model either implicitly or explicitly. For spatial data, which
does not include any time-dependence comparable to the time scale covered by
the data, on the other hand, neighborhood based methods such as k-NN and ARTMAP
proved to be more robust than other methods considered in this study. In
addition, for predictive modeling purposes, first a suitable, computationally
inexpensive method should be applied to the problem at hand a good predictive
performance of which would render the computational cost and efforts associated
with complex variants unnecessary.

Support vector machines and kernel methods are increasingly popular in
genomics and computational biology, due to their good performance in real-world
applications and strong modularity that makes them suitable to a wide range of
problems, from the classification of tumors to the automatic annotation of
proteins. Their ability to work in high dimension, to process non-vectorial
data, and the natural framework they provide to integrate heterogeneous data
are particularly relevant to various problems arising in computational biology.
In this chapter we survey some of the most prominent applications published so
far, highlighting the particular developments in kernel methods triggered by
problems in biology, and mention a few promising research directions likely to
expand in the future.

A central task in the study of molecular sequence data from present-day
species is the reconstruction of the ancestral relationships. The most
established approach to tree reconstruction is the maximum likelihood (ML)
method. In this method, evolution is described in terms of a discrete-state
continuous-time Markov process on a phylogenetic tree. The substitution rate
matrix, that determines the Markov process, can be estimated using the
expectation maximization (EM) algorithm. Unfortunately, an exhaustive search
for the ML phylogenetic tree is computationally prohibitive for large data
sets. In such situations, the neighbor-joining (NJ) method is frequently used
because of its computational speed. The NJ method reconstructs trees by
clustering neighboring sequences recursively, based on pairwise comparisons
between the sequences. The NJ method can be generalized such that
reconstruction is based on comparisons of subtrees rather than pairwise
distances. In this paper, we present an algorithm for simultaneous substitution
rate estimation and phylogenetic tree reconstruction. The algorithm iterates
between the EM algorithm for estimating substitution rates and the generalized
NJ method for tree reconstruction. Preliminary results of the approach are
encouraging.

This technical report provides the supplementary material for a paper
entitled "Information based clustering", to appear shortly in Proceedings of
the National Academy of Sciences (USA). In Section I we present in detail the
iterative clustering algorithm used in our experiments and in Section II we
describe the validation scheme used to determine the statistical significance
of our results. Then in subsequent sections we provide all the experimental
results for three very different applications: the response of gene expression
in yeast to different forms of environmental stress, the dynamics of stock
prices in the Standard and Poor's 500, and viewer ratings of popular movies. In
particular, we highlight some of the results that seem to deserve special
attention. All the experimental results and relevant code, including a freely
available web application, can be found at
http://www.genomics.princeton.edu/biophysics-theory .

In an age of increasingly large data sets, investigators in many different
disciplines have turned to clustering as a tool for data analysis and
exploration. Existing clustering methods, however, typically depend on several
nontrivial assumptions about the structure of data. Here we reformulate the
clustering problem from an information theoretic perspective which avoids many
of these assumptions. In particular, our formulation obviates the need for
defining a cluster "prototype", does not require an a priori similarity metric,
is invariant to changes in the representation of the data, and naturally
captures non-linear relations. We apply this approach to different domains and
find that it consistently produces clusters that are more coherent than those
extracted by existing algorithms. Finally, our approach provides a way of
clustering based on collective notions of similarity rather than the
traditional pairwise measures.

A salient feature of stationary patterns in tip-growing cells is the key role
played by the symports and antiports, membrane proteins that translocate two
ionic species at the same time. It is shown that these co-transporters
destabilize generically the membrane voltage if the two translocated ions
diffuse differently and carry a charge of opposite (same) sign for symports
(antiports). Orders of magnitude obtained for the time and lengthscale are in
agreement with experiments. A weakly nonlinear analysis characterizes the
bifurcation.

The Levenshtein distance is an important tool for the comparison of symbolic
sequences, with many appearances in genome research, linguistics and other
areas. For efficient applications, an approximation by a distance of smaller
computational complexity is highly desirable. However, our comparison of the
Levenshtein with a generic dictionary-based distance indicates their
statistical independence. This suggests that a simplification along this line
might not be possible without restricting the class of sequences. Several other
probabilistic properties are briefly discussed, emphasizing various questions
that deserve further investigation.

Statistical properties of interbeat intervals cascade are evaluated by
considering the joint probability distribution $P(\Delta x_2,\tau_2;\Delta
x_1,\tau_1)$ for two interbeat increments $\Delta x_1$ and $\Delta x_2$ of
different time scales $\tau_1$ and $\tau_2$. We present evidence that the
conditional probability distribution $P(\Delta x_2,\tau_2|\Delta x_1,\tau_1)$
may obey a Chapman-Kolmogorov equation. The corresponding Kramers-Moyal (KM)
coefficients are evaluated. It is shown that while the first and second KM
coefficients, i.e., the drift and diffusion coefficients, take on well-defined
and significant values, the higher-order coefficients in the KM expansion are
very small. As a result, the joint probability distributions of the increments
in the interbeat intervals obey a Fokker-Planck equation. The method provides a
novel technique for distinguishing the two classes of subjects in terms of the
drift and diffusion coefficients, which behave differently for two classes of
the subjects, namely, healthy subjects and those with congestive heart failure.

We investigate the Markov nature, Cascade of information from large time
scale to small scale and extended self similarity properties of the beat to
beat fluctuations of healthy subjects as well as those with congestive heart
failure. To check the Markov nature, we use a novel inverse method that
utilizes a set of data to construct a simple equation that governs the
stochastic process for which the data have been measured, hence enabling us to
reconstruct the stochastic process. The inverse method provides a novel
technique for distinguishing the two classes of subjects in terms of a drift
and a diffusion coefficients which behave completely differently for the two
classes of subjects.To investigate the cascade of information from large to
small time scales we also analyze the statistical properties of interbeat
intervals cascade by considering the joint probability distribution for two
interbeat increments. As a result, the joint probability distributions of the
increments in the interbeat intervals obey a Fokker-Planck equation. Finally we
analyze the extended self-similarity (ESS) in the beat-to-beat fluctuations in
the heart rates of healthy and congestive heart failure subjects.The proposed
methods provide the novel techniques for distinguishing the two classes of
subjects in terms of the drift and diffusion coefficients, intermittency
exponents which behave differently for two classes of the subjects, namely,
healthy subjects and those with congestive heart failure.

We have developed a quantitative model for the creation of cytoplasmic Ca2+
gradients near the inner surface of the plasma membrane (PM). In particular we
simulated the refilling of the sarcoplasmic reticulum (SR) via PM-SR junctions
during asynchronous [Ca2+] oscillations in smooth muscle cells of the rabbit
inferior vena cava. We have combined confocal microscopy data on the [Ca2+]
oscillations, force transduction data from cell contraction studies and
electron microscopic images to build a basis for computational simulations that
model the transport of calcium ions from Na+/Ca2+ exchangers (NCX) on the PM to
sarcoplasmic/endoplasmic reticulum Ca2+ ATPase (SERCA) pumps on the SR as a
three-dimensional random walk through the PM-SR junctional cytoplasmic spaces.
Electron microscopic ultrastructural images of the smooth muscle cells were
elaborated with software algorithms to produce a very clear and dimensionally
accurate picture of the PM-SR junctions. From this study, we conclude that it
is plausible and possible for enough Ca2+ to pass through the PM-SR junctions
to replete the SR during the regenerative Ca2+ release, which underlies agonist
induced asynchronous Ca2+ oscillations in vascular smooth muscle.

We introduce a family of positive definite kernels specifically optimized for
the manipulation of 3D structures of molecules with kernel methods. The kernels
are based on the comparison of the three-points pharmacophores present in the
3D structures of molecul es, a set of molecular features known to be
particularly relevant for virtual screening applications. We present a
computationally demanding exact implementation of these kernels, as well as
fast approximations related to the classical fingerprint-based approa ches.
Experimental results suggest that this new approach outperforms
state-of-the-art algorithms based on the 2D structure of mol ecules for the
detection of inhibitors of several drug targets.

Microarrays have become extremely useful for analysing genetic phenomena, but
establishing a relation between microarray analysis results (typically a list
of genes) and their biological significance is often difficult. Currently, the
standard approach is to map a posteriori the results onto gene networks to
elucidate the functions perturbed at the level of pathways. However,
integrating a priori knowledge of the gene networks could help in the
statistical analysis of gene expression data and in their biological
interpretation. Here we propose a method to integrate a priori the knowledge of
a gene network in the analysis of gene expression data. The approach is based
on the spectral decomposition of gene expression profiles with respect to the
eigenfunctions of the graph, resulting in an attenuation of the high-frequency
components of the expression profiles with respect to the topology of the
graph. We show how to derive unsupervised and supervised classification
algorithms of expression profiles, resulting in classifiers with biological
relevance. We applied the method to the analysis of a set of expression
profiles from irradiated and non-irradiated yeast strains. It performed at
least as well as the usual classification but provides much more biologically
relevant results and allows a direct biological interpretation.

Some species of purple bacteria as, e.g., Rhodobacter sphaeroides contain the
protein PufX. Concurrently, the light harvesting complexes 1 (LH1) form dimers
of open rings. In mutants without PufX, the LH1s are closed rings and
photosynthesis breaks down, because the ubiquinone exchange at the reaction
center is blocked. Thus, PufX is regarded essential for quinone exchange.
  In contrast to this view, which implicitly treats the LH1s as obstacles to
photosynthesis, we propose that the primary purpose of PufX is to improve the
efficiency of light harvesting by inducing the LH1 dimerization. Calculations
with a dipole model, which compare the photosynthetic efficiency of various
configurations of monomeric and dimeric core complexes, show that the dimer can
absorb photons directly into the RC about 30% more efficient, when related to
the number of bacteriochlorophylls, but that the performance of the more
sophisticated dimeric LH1 antenna degrades faster with structural
perturbations. The calculations predict an optimal orientation of the reaction
centers relative to the LH1 dimer, which agrees well with the experimentally
found configuration.
  For the increased required rigidity of the dimer additional modifications of
the LH1 subunits are necessary, which would lead to the observed ubiquinone
blockage, when PufX is missing.

We present a novel method for finding low dimensional views of high
dimensional data: Targeted Projection Pursuit. The method proceeds by finding
projections of the data that best approximate a target view. Two versions of
the method are introduced; one version based on Procrustes analysis and one
based on a single layer perceptron. These versions are capable of finding
orthogonal or non-orthogonal projections respectively. The method is
quantitatively and qualitatively compared with other dimension reduction
techniques. It is shown to find two-dimensional views that display the
classification of cancers from gene expression data with a visual separation
equal to, or better than, existing dimension reduction techniques.

When the same set of genes appear in two top ranking gene lists in two
different studies, it is often of interest to estimate the probability for this
being a chance event. This overlapping probability is well known to follow the
hypergeometric distribution. Usually, the lengths of top-ranking gene lists are
assumed to be fixed, by using a pre-set criterion on, e.g., $p$-value for the
t-test. We investigate how overlapping probability changes with the gene
selection criterion, or simply, with the length of the top-ranking gene lists.
It is concluded that overlapping probability is indeed a function of the gene
list length, and its statistical significance should be quoted in the context
of gene selection criterion.

A common practice in microarray analysis is to transform the microarray raw
data (light intensity) by a logarithmic transformation, and the justification
for this transformation is to make the distribution more symmetric and
Gaussian-like. Since this transformation is not universally practiced in all
microarray analysis, we examined whether the discrepancy of this treatment of
raw data affect the "high level" analysis result. In particular, whether the
differentially expressed genes as obtained by $t$-test, regularized t-test, or
logistic regression have altered rank orders due to presence or absence of the
transformation. We show that as much as 20%--40% of significant genes are
"discordant" (significant only in one form of the data and not in both),
depending on the test being used and the threshold value for claiming
significance. The t-test is more likely to be affected by logarithmic
transformation than logistic regression, and regularized $t$-test more affected
than t-test. On the other hand, the very top ranking genes (e.g. up to top
20--50 genes, depending on the test) are not affected by the logarithmic
transformation.

This submission is a duplicate of arXiv:q-bio/0602024 and has been removed.

Scaling analysis of heart rate time series has emerged as an useful tool for
assessment of autonomic cardiac control. We investigate the heart rate time
series of ten athletes (five males and five females), by applying detrended
fluctuation analysis (DFA). High resolution ECGs are recorded under
standardized resting conditions over 30 minutes and subsequently heart rate
time series are extracted and artefacts filtered. We find three distinct
regions of scale-invariance, which correspond to the well-known VLF, LF, and HF
bands in the power spectra of heart rate variability. The scaling exponents
alpha are alphaHF: 1.15 [0.96-1.22], alphaLF: 0.68 [0.57-0.84], alphaVLF:
0.83[0.82-0.99]; p<10^-5). In conclusion, DFA scaling exponents of heart rate
time series should be fitted to the VLF, LF, and HF ranges, respectively.

In this work we consider a simple, approximate, tending toward exact,
solution of the system of two usual Lotka-Volterra differential equations.
Given solution is obtained by an iterative method. In any finite approximation
order of this solution, exponents of the corresponding Lotka-Volterra variables
have simple, time polynomial form. When approximation order tends to infinity
obtained approximate solution converges toward exact solution in some finite
time interval.

Multi-electrode neurophysiological recordings produce massive quantities of
data. Multivariate time series analysis provides the basic framework for
analyzing the patterns of neural interactions in these data. It has long been
recognized that neural interactions are directional. Being able to assess the
directionality of neuronal interactions is thus a highly desired capability for
understanding the cooperative nature of neural computation. Research over the
last few years has shown that Granger causality is a key technique to furnish
this capability. The main goal of this article is to provide an expository
introduction to the concept of Granger causality. Mathematical frameworks for
both bivariate Granger causality and conditional Granger causality are
developed in detail with particular emphasis on their spectral representations.
The technique is demonstrated in numerical examples where the exact answers of
causal influences are known. It is then applied to analyze multichannel local
field potentials recorded from monkeys performing a visuomotor task. Our
results are shown to be physiologically interpretable and yield new insights
into the dynamical organization of large-scale oscillatory cortical networks.

This paper presents efficient algorithms for solving the problem of aligning
a protein structure template to a query amino-acid sequence, known as protein
threading problem. We consider the problem as a special case of graph matching
problem. We give formal graph and integer programming models of the problem.
After studying the properties of these models, we propose two kinds of
Lagrangian relaxation for solving them. We present experimental results on real
life instances showing the efficiency of our approaches.

A simple harmonic oscillator model is proposed to describe the mechanical
power involved in human locomotion. In this framework, by taking into account
the anthropometric parameters of a standard individual, we are able to
calculate the speed-power curves in human walking. The proposed model accounts
for the well known Margaria's law in which the cost ofthe human running
(independent from the speed) is fixed to 1 Kcal/(Kg Km). The model includes the
effects of a gentle slope (either positive or negative) and the effect due to
the mechanical response of the walking surface. The model results obtained in
the presence of a slope are in qualitative agreement with the experimental data
obtained by A. Leonardi et al.

Motivated by the abundance of directed synaptic couplings in a real
biological neuronal network, we investigate the synchronization behavior of the
Hodgkin-Huxley model in a directed network. We start from the standard model of
the Watts-Strogatz undirected network and then change undirected edges to
directed arcs with a given probability, still preserving the connectivity of
the network. A generalized clustering coefficient for directed networks is
defined and used to investigate the interplay between the synchronization
behavior and underlying structural properties of directed networks. We observe
that the directedness of complex networks plays an important role in emerging
dynamical behaviors, which is also confirmed by a numerical study of the
sociological game theoretic voter model on directed networks.

Motivated by chemical applications, we revisit and extend a family of
positive definite kernels for graphs based on the detection of common subtrees,
initially proposed by Ramon et al. (2003). We propose new kernels with a
parameter to control the complexity of the subtrees used as features to
represent the graphs. This parameter allows to smoothly interpolate between
classical graph kernels based on the count of common walks, on the one hand,
and kernels that emphasize the detection of large common subtrees, on the other
hand. We also propose two modular extensions to this formulation. The first
extension increases the number of subtrees that define the feature space, and
the second one removes noisy features from the graph representations. We
validate experimentally these new kernels on binary classification tasks
consisting in discriminating toxic and non-toxic molecules with support vector
machines.

We present an algorithm for the continuous monitoring of the biomass and
ethanol concentrations and moreover the kinetic rate in the Mezcal fermentation
process. This algorithm performs its task having only available the on-line
measurements of the redox potential. The procedure includes an artificial
neural network (ANN) that relates the redox potential to the ethanol and
biomass concentrations. Then a nonlinear-observer-based algorithm uses the
biomass estimations to infer the kinetic rate of this fermentation process. The
method shows that the redox potential is a valuable indicator of microorganism
metabolic activity during the Mezcal fermentation. In addition, the estimated
kinetic rate can be considered as a direct evidence of the presence of mixed
culture growth in the process. In this work, the detailed design of the
software-sensor is presented, as well as its experimental application at the
laboratory level

Motivated by applications in systems biology, we seek a probabilistic
framework based on Markov processes to represent intracellular processes. We
review the formal relationships between different stochastic models referred to
in the systems biology literature. As part of this review, we present a novel
derivation of the differential Chapman-Kolmogorov equation for a general
multidimensional Markov process made up of both continuous and jump processes.
We start with the definition of a time-derivative for a probability density but
place no restrictions on the probability distribution, in particular, we do not
assume it to be confined to a region that has a surface (on which the
probability is zero). In our derivation, the master equation gives the jump
part of the Markov process while the Fokker-Planck equation gives the
continuous part. We thereby sketch a {}``family tree'' for stochastic models in
systems biology, providing explicit derivations of their formal relationship
and clarifying assumptions involved.

We have developed a luminescence-measurement system for liquid
bio/chemiluminescence that can obtain quantitative luminescence spectra as the
absolute total number of luminescence photons at each wavelength or photon
energy and quantum yields. Calibration of light-collection efficiency in the
system is performed with a reference double-plate cell. This method is
applicable to sample cells of any kind suitable for measurement, which is a
great advantage over previous techniques in practical experiments. Using this
system, the quantum yield of aqueous luminol chemiluminescence was obtained as
1.23-+0.20%, which is in good agreement with previously reported values.

A new type of microfluidic system for biological cell manipulation, a
CMOS/microfluidic hybrid, is demonstrated. The hybrid system starts with a
custom-designed CMOS (complementary metal-oxide semiconductor) chip fabricated
in a semiconductor foundry using standard integration circuit technology. A
microfluidic channel is post-fabricated on top of the CMOS chip to provide
biocompatible environment. The motion of individual biological cells that are
tagged with magnetic beads is directly controlled by the CMOS chip that
generates localized magnetic filed patterns using an on-chip array of
micro-electromagnets. The speed and the programmability of the CMOS chip
further allow for the dynamic reconfiguration of the magnetic fields,
substantially increasing the manipulation capability of the hybrid system. The
concept of a hybrid system is verified by simultaneously manipulating
individual biological cells with microscopic resolution. A new operation
protocol that exploits the fast speed of electronics to trap and move a large
number of cells with less power consumption is also demonstrated. Combining the
advantages of microelectronics, the CMOS/microfluidic hybrid approach presents
a new model for a multifunctional lab-on-a chip for biological and medical
applications.

We propose a computational approach to modeling the collective dynamics of
populations of coupled heterogeneous biological oscillators. In contrast to
Monte Carlo simulation, this approach utilizes generalized Polynomial Chaos
(gPC) to represent random properties of the population, thus reducing the
dynamics of ensembles of oscillators to dynamics of their (typically
significantly fewer) representative gPC coefficients. Equation-Free (EF)
methods are employed to efficiently evolve these gPC coefficients in time and
compute their coarse-grained stationary state and/or limit cycle solutions,
circumventing the derivation of explicit, closed-form evolution equations.
Ensemble realizations of the oscillators and their statistics can be readily
reconstructed from these gPC coefficients. We apply this methodology to the
synchronization of yeast glycolytic oscillators coupled by the membrane
exchange of an intracellular metabolite. The heterogeneity consists of a single
random parameter, which accounts for glucose influx into a cell, with a
Gaussian distribution over the population. Coarse projective integration is
used to accelerate the evolution of the population statistics in time. Coarse
fixed-point algorithms in conjunction with a Poincar\'e return map are used to
compute oscillatory solutions for the cell population and to quantify their
stability.

Affected relatives are essential for pedigree linkage analysis, however, they
cause a violation of the independent sample assumption in case-control
association studies. To avoid the correlation between samples, a common
practice is to take only one affected sample per pedigree in association
analysis. Although several methods exist in handling correlated samples, they
are still not widely used in part because these are not easily implemented, or
because they are not widely known. We advocate the effective sample size method
as a simple and accessible approach for case-control association analysis with
correlated samples. This method modifies the chi-square test statistic,
p-value, and 95% confidence interval of the odds-ratio by replacing the
apparent number of allele or genotype counts with the effective ones in the
standard formula, without the need for specialized computer programs. We
present a simple formula for calculating effective sample size for many types
of relative pairs and relative sets. For allele frequency estimation, the
effective sample size method captures the variance inflation exactly. For
genotype frequency, simulations showed that effective sample size provides a
satisfactory approximation. A gene which is previously identified as a type 1
diabetes susceptibility locus, the interferon-induced helicase gene (IFIH1), is
shown to be significantly associated with rheumatoid arthritis when the
effective sample size method is applied. This significant association is not
established if only one affected sib per pedigree were used in the association
analysis. Relationship between the effective sample size method and other
methods -- the generalized estimation equation, variance of eigenvalues for
correlation matrices, and genomic controls -- are discussed.

An analysis of data from 212 permanent sample plots provided no evidence of
any decline in rainforest productivity after three cycles of selection logging
in the tropical rainforests of north Queensland. Relative productivity was
determined as the difference between observed diameter increments and
increments predicted from a diameter increment function which incorporated tree
size, stand density and site quality. Analyses of variance and regression
analyses revealed no significant decline in productivity after repeated
harvesting. There is evidence to support the assertion that if any permanent
productivity decline exists, it does not exceed six per cent per harvest.

The study of epistasis is of great importance in statistical genetics in
fields such as linkage and association analysis and QTL mapping. In an effort
to classify the types of epistasis in the case of two biallelic loci Li and
Reich listed and described all models in the simplest case of 0/1 penetrance
values. However, they left open the problem of finding a classification of
two-locus models with continuous penetrance values. We provide a complete
classification of biallelic two-locus models. In addition to solving the
classification problem for dichotomous trait disease models, our results apply
to any instance where real numbers are assigned to genotypes, and provide a
complete framework for studying epistasis in QTL data. Our approach is
geometric and we show that there are 387 distinct types of two-locus models,
which can be reduced to 69 when symmetry between loci and alleles is accounted
for. The model types are defined by 86 circuits, which are linear combinations
of genotype values, each of which measures a fundamental unit of interaction.
The circuits provide information on epistasis beyond that contained in the
additive x add, add x dom, and dom x dom interaction terms. We discuss the
connection between our classification and standard epistatic models and
demonstrate its utility by analyzing a previously published dataset.

We introduce the simple parametrization for the space of codons (triples of
nucleotides) by 8\times 8 table. This table (which we call the dyadic plane)
possesses the natural 2-adic ultrametric. We show that after this
parametrization the genetic code will be a locally constant map of the simple
form. The local constancy of this map will describe degeneracy of the genetic
code.
  The map of the genetic code defines 2-adic ultrametric on the space of amino
acids. We show that hydrophobic amino acids will be clustered in two balls with
respect to this ultrametric. Therefore the introduced parametrization of space
of codons exhibits the hidden regularity of the genetic code.

Motivation: In silico methods for the prediction of antigenic peptides
binding to MHC class I molecules play an increasingly important role in the
identification of T-cell epitopes. Statistical and machine learning methods, in
particular, are widely used to score candidate epitopes based on their
similarity with known epitopes and non epitopes. The genes coding for the MHC
molecules, however, are highly polymorphic, and statistical methods have
difficulties to build models for alleles with few known epitopes. In this case,
recent works have demonstrated the utility of leveraging information across
alleles to improve the performance of the prediction. Results: We design a
support vector machine algorithm that is able to learn epitope models for all
alleles simultaneously, by sharing information across similar alleles. The
sharing of information across alleles is controlled by a user-defined measure
of similarity between alleles. We show that this similarity can be defined in
terms of supertypes, or more directly by comparing key residues known to play a
role in the peptide-MHC binding. We illustrate the potential of this approach
on various benchmark experiments where it outperforms other state-of-the-art
methods.

Fourier transform has become a basic tool for analyzing biological signals
1,2,3. Mostly a fast Fourier transform is computed for a finite sequence of
data sample 4. This is the standard way apparatuses and modern computerized
technology provide information, according with their frequency range, of the
well known brain signals Delta, Theta, Alpha 1, Alpha 2, Beta 1 and Beta 2
furnishing experts with electroencephalographic (EEG) profile of clinical use
obtained from these short periods 5,6.
  For long periods, an analogous novel procedure is established as follows:
Assigning certain numerical value, i.e., the absolute power, to each brain
signal at certain sampling times, generates data that can be interpolated and
extrapolated through a long period, yielding an absolute power function of time
for each signal 7. A further Fourier transform is then performed8,9, to analyze
these new functions, finding typical frequencies and their corresponding
periods for each one of these signals and, also, relative phases for coincident
periods between two or more signals. Our procedure of analysis presented here
can be applied, in principle, to any biological signal of interest.

Trajectories of a signal that fluctuates between two states which originate
from single molecule activities have become ubiquitous. Common examples are
trajectories of ionic flux through individual membrane-channels, and of photon
counts collected from diffusion, activity, and conformational changes of
biopolymers. By analyzing the trajectory, one wishes to deduce the underlying
mechanism, which is usually described by a multi-substate kinetic scheme. In
previous works, we divided kinetic schemes that generate two-state trajectories
into two types: reducible schemes and irreducible schemes. We showed that all
the information in trajectories generated from reducible schemes is contained
in the waiting time probability density functions (PDFs) of the two states. It
follows that reducible schemes with the same waiting time PDFs are not
distinguishable. In this work, we further characterize the topologies of
kinetic schemes, now of irreducible schemes, and further study two-state
trajectories from the two types of scheme. We suggest various methods for
extracting information about the underlying kinetic scheme from the trajectory
(e. g., calculate the binned successive waiting times PDF and analyze the
ordered waiting times trajectory), and point out the advantages and
disadvantages of each. We show that the binned successive waiting times PDF is
not only more robust than other functions when analyzing finite trajectories,
but contains, in most cases, more information about the underlying kinetic
scheme than other functions in the limit of infinitely long trajectories. For
some cases however, analyzing the ordered waiting times trajectory may supply
unique information about the underlying kinetic scheme.

Power spectral density is an accepted measure of heart rate variability. Two
estimators of multifractal properties: Wavelet Transform Modulus Maxima and
Multifractal Detrended Fluctuation Analysis are used to investigate
multifractal properties for the three strongly physiologically grounded
components of power spectra: low frequency (LF), very low frequency (VLF) and
ultra low frequency (ULV). Circadian rhythm changes are examined by
discrimination of daily activity from nocturnal rest. Investigations consider
normal sinus rhythms of healthy 39 subjects which are grouped in two sets:
5-hour wake series and 5-hour sleep series. Qualitative arguments are provided
to conjecture the presence of stochastic persistence in LF range, loss of heart
rate variability during night in VLF range and its increase in ULF.

The switch-like character of gene regulation has motivated the use of hybrid,
discrete-continuous models of genetic regulatory networks. While powerful
techniques for the analysis, verification, and control of hybrid systems have
been developed, the specificities of the biological application domain pose a
number of challenges, notably the absence of quantitative information on
parameter values and the size and complexity of networks of biological
interest. We introduce a method for the analysis of reachability properties of
genetic regulatory networks that is based on a class of discontinuous
piecewise-affine (PA) differential equations well-adapted to the above
constraints. More specifically, we introduce a hyperrectangular partition of
the state space that forms the basis for a discrete abstraction preserving the
sign of the derivatives of the state variables. The resulting discrete
transition system provides a conservative approximation of the qualitative
dynamics of the network and can be efficiently computed in a symbolic manner
from inequality constraints on the parameters. The method has been implemented
in the computer tool Genetic Network Analyzer (GNA), which has been applied to
the analysis of a regulatory system whose functioning is not well-understood by
biologists, the nutritional stress response in the bacterium Escherichia coli.

The signal from many single molecule experiments monitoring molecular
processes, such as enzyme turnover via fluorescence and opening and closing of
ion channel via the flux of ions, consists of a time series of stochastic on
and off (or open and closed) periods, termed a two-state trajectory. This
signal reflects the dynamics in the underlying multi-substate on-off kinetic
scheme (KS) of the process. The determination of the underlying KS is difficult
and sometimes even impossible due to the loss of information in the mapping of
the mutli dimensional KS onto two dimensions. Here we introduce a new procedure
that efficiently and optimally relates the signal to all equivalent underlying
KSs. This procedure partitions the space of KSs into canonical (unique) forms
that can handle any KS, and obtains the topology and other details of the
canonical form from the data without the need for fitting. Also established are
relationships between the data and the topology of the canonical form to the
on-off connectivity of a KS. The suggested canonical forms constitute a
powerful tool in discriminating between KSs. Based on our approach, the upper
bound on the information content in two state trajectories is determined.

