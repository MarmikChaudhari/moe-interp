{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23293866eb834cbc9ebb59feabf00301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_model(model_name=\"allenai/OLMoE-1B-7B-0924\"):\n",
    "    # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split text file into tokens for model's context length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_input(file_path, chunk_size=1000, tokenizer=None):\n",
    "    \"\"\"    \n",
    "    args :\n",
    "        file_path (str): Path to the input text file\n",
    "        chunk_size (int): Number of tokens per chunk\n",
    "        tokenizer: HuggingFace tokenizer (if None, will split on whitespace)\n",
    "        \n",
    "    output : List of text chunks of approximately chunk_size tokens\n",
    "    \"\"\"\n",
    "    device = 'cpu'\n",
    "    \n",
    "    # Read the full text file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    if tokenizer:\n",
    "        # Tokenize the full text\n",
    "        tokens = tokenizer.encode(text)\n",
    "        tokens_tensor = torch.tensor(tokens).to(device)\n",
    "        \n",
    "        # Split into chunks\n",
    "        chunks = []\n",
    "        for i in range(0, len(tokens), chunk_size):\n",
    "            chunk_tokens = tokens_tensor[i:i + chunk_size]\n",
    "            # Move to CPU for decoding\n",
    "            chunk_tokens = chunk_tokens.cpu()\n",
    "            # Decode tokens back to text\n",
    "            chunk_text = tokenizer.decode(chunk_tokens)\n",
    "            chunks.append(chunk_text)\n",
    "            \n",
    "    else:\n",
    "        # Simple whitespace tokenization\n",
    "        words = text.split()\n",
    "        \n",
    "        # Split into chunks\n",
    "        chunks = []\n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the router logits for each token across all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_router_logits(model, input_text: str, k: int = 1):\n",
    "    \"\"\"\n",
    "    args :\n",
    "        model: OlmoeForCausalLM model\n",
    "        input_text: Text string to analyze\n",
    "        k: Number of top experts to return per token\n",
    "        \n",
    "    output : dictionary mapping layer indices to lists of [token_text, expert_index, router_probability] for each token in that layer\n",
    "    \"\"\"\n",
    "    device = \"cpu\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Forward pass with router logits enabled\n",
    "    outputs = model(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        output_router_logits=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    \n",
    "    # Get router logits for all layers\n",
    "    router_logits = outputs.router_logits\n",
    "    \n",
    "    all_layer_results = {}\n",
    "    for layer_idx, layer_router_logits in enumerate(router_logits):\n",
    "        # Apply softmax to get probabilities\n",
    "        probs = torch.nn.functional.softmax(layer_router_logits.detach(), dim=-1)\n",
    "        # Reshape to [seq_len, num_experts] since batch_size=1\n",
    "        probs = probs.reshape(inputs['input_ids'].size(1), -1)\n",
    "        # Get top k probabilities and indices for each token\n",
    "        top_probs, top_indices = torch.topk(probs, k=k)\n",
    "        \n",
    "        # Move tensors to CPU for post-processing\n",
    "        top_probs = top_probs.cpu()\n",
    "        top_indices = top_indices.cpu()\n",
    "        \n",
    "        # Convert token IDs to text\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].cpu())\n",
    "        \n",
    "        # Create list of [token, expert, prob] for each token\n",
    "        layer_tokens = []\n",
    "        for i in range(len(tokens)):\n",
    "            for j in range(k):\n",
    "                # Clean special characters from token text\n",
    "                clean_token = tokens[i].replace('Ä ', '')\n",
    "                layer_tokens.append([\n",
    "                    clean_token,\n",
    "                    top_indices[i][j].item(),\n",
    "                    top_probs[i][j].item()\n",
    "                ])\n",
    "        \n",
    "        all_layer_results[layer_idx] = layer_tokens\n",
    "    \n",
    "    return all_layer_results # Dictionary mapping layer index to list of [token, expert_number, probability]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### update/create the router logits json file with new tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_router_logits_json(results, domain, device='cpu'):\n",
    "    \"\"\"\n",
    "    args :\n",
    "        results: Dictionary mapping layer index to list of [token, expert_number, probability]\n",
    "        domain: String indicating the domain (e.g., 'arxiv', 'code')\n",
    "        device: Device to use for tensor operations ('cuda' or 'cpu')\n",
    "    output : updated json file with new tokens\n",
    "    \"\"\"\n",
    "\n",
    "    json_path = f'{domain}_all_layers.json'\n",
    "    \n",
    "    # Initialize an empty dictionary for existing results\n",
    "    existing_results = {}\n",
    "    \n",
    "    if os.path.exists(json_path):\n",
    "        # Load existing results\n",
    "        with open(json_path, 'r') as f:\n",
    "            try:\n",
    "                existing_results = json.load(f)\n",
    "                # Convert string keys to integers\n",
    "                existing_results = {int(k): v for k, v in existing_results.items()}\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: {json_path} is empty or corrupted. Starting with an empty dictionary.\")\n",
    "    \n",
    "    # Move results to GPU if available\n",
    "    if torch.cuda.is_available() and device == 'cuda':\n",
    "        for layer_idx, layer_tokens in results.items():\n",
    "            # Convert lists to tensors and move to GPU\n",
    "            tokens_tensor = torch.tensor([[t[0], t[1], t[2]] for t in layer_tokens]).cuda()\n",
    "            results[layer_idx] = tokens_tensor.tolist()\n",
    "    \n",
    "    # Combine existing and new results for each layer\n",
    "    for layer_idx, layer_tokens in results.items():\n",
    "        if layer_idx in existing_results:\n",
    "            existing_results[layer_idx].extend(layer_tokens)\n",
    "        else:\n",
    "            existing_results[layer_idx] = layer_tokens\n",
    "    \n",
    "    # Save updated results with integer keys\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(existing_results, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "    return existing_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### expert distribution bar graphs for a particular layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_expert_distribution_bar_graph(layer_idx, domain, device='cpu'):\n",
    "    \"\"\"    \n",
    "    args :\n",
    "        json_path: Path to the JSON file containing expert counts\n",
    "        device: Device to use for tensor operations ('cuda' or 'cpu')\n",
    "    output : plot of the expert distribution for a particular layer\n",
    "    \"\"\"\n",
    "    json_path = f'{domain}_all_layers.json'\n",
    "\n",
    "    # Read JSON file\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Extract layer results\n",
    "    layer_results = data[str(layer_idx)]\n",
    "    \n",
    "    # Create a dictionary to store expert counts\n",
    "    expert_counts = defaultdict(int)\n",
    "    \n",
    "    # Move data to GPU if available\n",
    "    if torch.cuda.is_available() and device == 'cuda':\n",
    "        layer_results = torch.tensor(layer_results).cuda()\n",
    "        \n",
    "    # Count how many tokens went to each expert\n",
    "    total_assignments = len(layer_results)\n",
    "    print(f'Total assignments: {total_assignments}')\n",
    "    \n",
    "    # Count occurrences of each expert\n",
    "    if torch.cuda.is_available() and device == 'cuda':\n",
    "        # Process on GPU\n",
    "        for _, expert, _ in layer_results.cpu().numpy():\n",
    "            expert_counts[int(expert)] += 1\n",
    "    else:\n",
    "        # Process on CPU\n",
    "        for _, expert, _ in layer_results:\n",
    "            expert_counts[expert] += 1\n",
    "            \n",
    "    print(f'Expert counts: {expert_counts}')\n",
    "    print(f'Total experts: {len(expert_counts)}')\n",
    "    print(f'Expert count for l0', expert_counts[0])\n",
    "    \n",
    "    # Convert to lists for plotting and calculate percentages\n",
    "    experts = [f'{i}' for i in range(64)]\n",
    "    percentages = [expert_counts[i]/total_assignments * 100 for i in range(64)]\n",
    "    \n",
    "    # Create bar chart\n",
    "    fig = go.Figure(data=[\n",
    "        go.Bar(\n",
    "            x=experts,\n",
    "            y=percentages,\n",
    "            textposition='auto',\n",
    "            marker_color='red'  # You can use any color here - hex code, RGB, or color name\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'percentage of total tokens routed to each expert for layer {layer_idx}',\n",
    "        xaxis_title='expert',\n",
    "        yaxis_title='% of total tokens',\n",
    "        yaxis=dict(range=[0, 100]), # Set y-axis range from 0 to 100%\n",
    "        xaxis_tickangle=-45,\n",
    "        bargap=0.2\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### expert distribution heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_expert_distribution_heatmap(domain, device='cpu'):\n",
    "    \"\"\"    \n",
    "    args :\n",
    "        json_path: Path to the JSON file containing expert counts\n",
    "        device: Device to use for tensor operations ('cuda' or 'cpu')\n",
    "    output : heatmap showing distribution of tokens across experts and layers\n",
    "    \"\"\"\n",
    "    json_path = f'{domain}_all_layers.json'\n",
    "\n",
    "    # Read JSON file\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Create a 16x64 matrix to store percentages\n",
    "    expert_matrix = np.zeros((16, 64))\n",
    "    \n",
    "    # Process each layer\n",
    "    for layer in range(16):\n",
    "        if str(layer) not in data:\n",
    "            continue\n",
    "            \n",
    "        layer_results = data[str(layer)]\n",
    "        total_assignments = len(layer_results)\n",
    "        \n",
    "        # Count expert assignments for this layer\n",
    "        expert_counts = defaultdict(int)\n",
    "        if torch.cuda.is_available() and device == 'cuda':\n",
    "            layer_results = torch.tensor(layer_results).cuda()\n",
    "            for _, expert, _ in layer_results.cpu().numpy():\n",
    "                expert_counts[int(expert)] += 1\n",
    "        else:\n",
    "            for _, expert, _ in layer_results:\n",
    "                expert_counts[expert] += 1\n",
    "                \n",
    "        # Calculate percentages for each expert\n",
    "        for expert in range(64):\n",
    "            expert_matrix[layer][expert] = expert_counts[expert] / total_assignments * 100\n",
    "    \n",
    "    # Create and return a single heatmap\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=expert_matrix,\n",
    "        x=[str(i) for i in range(64)],\n",
    "        y=[str(i) for i in range(16)],\n",
    "        colorscale='Reds'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Distribution of Tokens Across Experts and Layers',\n",
    "        xaxis_title='Expert Index',\n",
    "        yaxis_title='Layer',\n",
    "        width=800,\n",
    "        height=800,\n",
    "        xaxis=dict(\n",
    "            tickangle=-45,\n",
    "            constrain='domain'\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            scaleanchor='x',\n",
    "            scaleratio=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### expert distribution for all text input and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/3\n",
      "Warning: swap_all_layers.json is empty or corrupted. Starting with an empty dictionary.\n",
      "Processing chunk 2/3\n",
      "Processing chunk 3/3\n"
     ]
    }
   ],
   "source": [
    "# Read and chunk input text file\n",
    "file_path = 'data/github.txt'\n",
    "domain = 'github'\n",
    "chunks = prepare_text_input(file_path, chunk_size=1024, tokenizer=tokenizer)\n",
    "\n",
    "# Use CPU device\n",
    "device = 'cpu'\n",
    "model = model.to(device)  # Move model to CPU\n",
    "\n",
    "# Process all chunks\n",
    "all_results = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f'Processing chunk {i+1}/{len(chunks)}')\n",
    "    # print(f'Sample text: {chunk[:10]}...')  \n",
    "    \n",
    "    # Get router logits for the chunk\n",
    "    results = get_router_logits(model, chunk)\n",
    "    all_results.append(results)\n",
    "    \n",
    "    # Save intermediate results \n",
    "    update_router_logits_json(results, domain=domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 16  # OLMoE has 16 layers\n",
    "\n",
    "# Combine results from all chunks\n",
    "combined_results = []\n",
    "for layer_idx in range(len(all_results[0])):  # For each layer\n",
    "    layer_combined = []\n",
    "    for chunk_result in all_results:\n",
    "        # Skip if chunk_result[layer_idx] is not iterable\n",
    "        if not isinstance(chunk_result[layer_idx], (list, tuple)):\n",
    "            continue\n",
    "            \n",
    "        # Move data to GPU if available \n",
    "        if torch.cuda.is_available():\n",
    "            chunk_result = [\n",
    "                (token.cuda() if torch.is_tensor(token) else token,\n",
    "                 expert.cuda() if torch.is_tensor(expert) else expert,\n",
    "                 prob.cuda() if torch.is_tensor(prob) else prob)\n",
    "                for token, expert, prob in chunk_result[layer_idx]\n",
    "            ]\n",
    "        layer_combined.extend(chunk_result)\n",
    "    combined_results.append(layer_combined)\n",
    "\n",
    "# Analyze and plot routing for all layers\n",
    "for layer_to_plot in range(num_layers):\n",
    "    layer_results = combined_results[layer_to_plot]\n",
    "    \n",
    "    # Skip if no valid results for this layer\n",
    "    if not layer_results:\n",
    "        print(f\"No valid results for layer {layer_to_plot}\")\n",
    "        continue\n",
    "        \n",
    "    for token_info in layer_results[:5]:  # Limit to first 5 tokens\n",
    "        # Skip if token_info is not a tuple/list\n",
    "        if not isinstance(token_info, (tuple, list)):\n",
    "            continue\n",
    "            \n",
    "        token, expert, prob = token_info\n",
    "        # Move to CPU for printing\n",
    "        if torch.cuda.is_available() and device == 'cuda':\n",
    "            prob = prob\n",
    "        print(f\"Token: {token}, Expert: {expert}, Probability: {prob:.3f}\")\n",
    "\n",
    "    # Plot expert distribution for all processed data\n",
    "    with torch.cuda.amp.autocast():  # Enable automatic mixed precision\n",
    "        fig = plot_expert_distribution_bar_graph(layer_idx=layer_to_plot, domain=domain)\n",
    "    fig.show()\n",
    "\n",
    "    # Save plot as HTML and image\n",
    "    fig.write_html(f'plots/github/{domain}_layer{layer_to_plot}_expert_dist.html')\n",
    "    fig.write_image(f'plots/github/{domain}_layer{layer_to_plot}_expert_dist.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'github'\n",
    "\n",
    "# Plot expert distribution for all processed data\n",
    "fig = plot_expert_distribution_heatmap(domain=domain)\n",
    "fig.show()\n",
    "\n",
    "# Save plot as HTML and image\n",
    "fig.write_html(f'plots/{domain}_expert_dist.html')\n",
    "fig.write_image(f'plots/{domain}_expert_dist.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
