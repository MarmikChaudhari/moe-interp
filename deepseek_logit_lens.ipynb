{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"Get the optimal available device\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        # Enable TF32 for better performance on Ampere GPUs (A100, A6000, etc)\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        # Set memory allocation settings\n",
    "        torch.cuda.empty_cache()\n",
    "        # Enable CUDNN benchmarking for better performance\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3ec63ad93a404a8d2bbbfab1505d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DeepseekForCausalLM(\n",
       "  (model): DeepseekModel(\n",
       "    (embed_tokens): Embedding(102400, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0): DeepseekDecoderLayer(\n",
       "        (self_attn): DeepseekSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): DeepseekRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): DeepseekMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=10944, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=10944, bias=False)\n",
       "          (down_proj): Linear(in_features=10944, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): DeepseekRMSNorm()\n",
       "        (post_attention_layernorm): DeepseekRMSNorm()\n",
       "      )\n",
       "      (1-27): 27 x DeepseekDecoderLayer(\n",
       "        (self_attn): DeepseekSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): DeepseekRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): DeepseekMoE(\n",
       "          (experts): ModuleList(\n",
       "            (0-63): 64 x DeepseekMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
       "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (gate): MoEGate()\n",
       "          (shared_experts): DeepseekMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=2816, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=2816, bias=False)\n",
       "            (down_proj): Linear(in_features=2816, out_features=2048, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): DeepseekRMSNorm()\n",
       "        (post_attention_layernorm): DeepseekRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): DeepseekRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=102400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-moe-16b-base\",\n",
    "                                             trust_remote_code=True,\n",
    "                                             torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-moe-16b-base\", trust_remote_code=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MOEExpertLens:\n",
    "#     def __init__(self, state_dict: Dict[str, torch.Tensor], tokenizer, device=None):\n",
    "#         \"\"\"Initialize the MoE Expert analyzer.\"\"\"\n",
    "#         self.device = device if device is not None else get_device()\n",
    "#         self.state_dict = {k: v.to(self.device) for k, v in state_dict.items()}\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.hidden_size = self.state_dict[\"model.embed_tokens.weight\"].shape[1]\n",
    "#         self.vocab_size = self.state_dict[\"model.embed_tokens.weight\"].shape[0]\n",
    "#         # Get model dtype from embeddings\n",
    "#         self.dtype = self.state_dict[\"model.embed_tokens.weight\"].dtype\n",
    "\n",
    "#     def _process_expert(self, layer_idx: int, expert_idx: int, hidden_state: torch.Tensor) -> torch.Tensor:\n",
    "#         \"\"\"Process hidden state through an expert's weights.\"\"\"\n",
    "#         # Get expert weights and ensure dtype match\n",
    "#         gate_proj = self.state_dict[f\"model.layers.{layer_idx}.mlp.experts.{expert_idx}.gate_proj.weight\"]\n",
    "#         up_proj = self.state_dict[f\"model.layers.{layer_idx}.mlp.experts.{expert_idx}.up_proj.weight\"]\n",
    "#         down_proj = self.state_dict[f\"model.layers.{layer_idx}.mlp.experts.{expert_idx}.down_proj.weight\"]\n",
    "\n",
    "#         hidden_state = hidden_state.to(self.dtype)\n",
    "        \n",
    "#         # Apply MLPs sequentially while maintaining batch and sequence dimensions\n",
    "#         gate_output = F.silu(F.linear(hidden_state, gate_proj))\n",
    "#         up_output = F.linear(hidden_state, up_proj)\n",
    "        \n",
    "#         # Element-wise multiplication and final projection\n",
    "#         x = gate_output * up_output\n",
    "#         return F.linear(x, down_proj)\n",
    "\n",
    "#     def _get_router_output(self, layer_idx: int, hidden_state: torch.Tensor) -> torch.Tensor:\n",
    "#         \"\"\"Get router logits for a layer.\"\"\"\n",
    "#         router_weights = self.state_dict[f\"model.layers.{layer_idx}.mlp.gate.weight\"]\n",
    "#         hidden_state = hidden_state.to(self.dtype)\n",
    "#         return F.linear(hidden_state, router_weights)\n",
    "\n",
    "#     def _process_attention(self, layer_idx: int, hidden_state: torch.Tensor, \n",
    "#                          attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "#         \"\"\"Process hidden state through self-attention while preserving token-wise information.\"\"\"\n",
    "#         q_proj = self.state_dict[f\"model.layers.{layer_idx}.self_attn.q_proj.weight\"]\n",
    "#         k_proj = self.state_dict[f\"model.layers.{layer_idx}.self_attn.k_proj.weight\"]\n",
    "#         v_proj = self.state_dict[f\"model.layers.{layer_idx}.self_attn.v_proj.weight\"]\n",
    "#         o_proj = self.state_dict[f\"model.layers.{layer_idx}.self_attn.o_proj.weight\"]\n",
    "\n",
    "#         # Ensure hidden state has correct dtype\n",
    "#         hidden_state = hidden_state.to(self.dtype)\n",
    "\n",
    "#         # Get shapes\n",
    "#         batch_size, seq_len, hidden_dim = hidden_state.shape\n",
    "#         num_heads = 32  # DeepSeek specific\n",
    "#         head_dim = hidden_dim // num_heads\n",
    "\n",
    "#         # Compute QKV with shape preservation\n",
    "#         q = F.linear(hidden_state, q_proj).view(batch_size, seq_len, num_heads, head_dim)\n",
    "#         k = F.linear(hidden_state, k_proj).view(batch_size, seq_len, num_heads, head_dim)\n",
    "#         v = F.linear(hidden_state, v_proj).view(batch_size, seq_len, num_heads, head_dim)\n",
    "\n",
    "#         # Transpose for attention computation\n",
    "#         q = q.transpose(1, 2)  # [batch, num_heads, seq_len, head_dim]\n",
    "#         k = k.transpose(1, 2)\n",
    "#         v = v.transpose(1, 2)\n",
    "\n",
    "#         # Compute attention scores\n",
    "#         # Cast to float32 for better numerical stability in attention computation\n",
    "#         q_float = q.float()\n",
    "#         k_float = k.float()\n",
    "#         v_float = v.float()\n",
    "\n",
    "#         attn_weights = torch.matmul(q_float, k_float.transpose(-2, -1)) / math.sqrt(head_dim)\n",
    "        \n",
    "#         # Apply causal mask\n",
    "#         causal_mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool, device=self.device), diagonal=1)\n",
    "#         attn_weights.masked_fill_(causal_mask, float('-inf'))\n",
    "        \n",
    "#         if attention_mask is not None:\n",
    "#             attn_weights = attn_weights + attention_mask\n",
    "\n",
    "#         attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        \n",
    "#         # Apply attention to values\n",
    "#         context = torch.matmul(attn_weights, v_float)\n",
    "        \n",
    "#         # Convert back to original dtype\n",
    "#         context = context.to(self.dtype)\n",
    "        \n",
    "#         # Reshape and project to output\n",
    "#         context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, hidden_dim)\n",
    "#         return F.linear(context, o_proj)\n",
    "\n",
    "#     def _apply_layer_norm(self, layer_idx: int, hidden_state: torch.Tensor, norm_type: str) -> torch.Tensor:\n",
    "#         \"\"\"Apply layer normalization while maintaining token information.\"\"\"\n",
    "#         weight = self.state_dict[f\"model.layers.{layer_idx}.{norm_type}.weight\"]\n",
    "#         hidden_state = hidden_state.to(self.dtype)\n",
    "#         return F.layer_norm(hidden_state, (self.hidden_size,), weight=weight)\n",
    "\n",
    "#     def _project_to_vocab(self, hidden_state: torch.Tensor) -> torch.Tensor:\n",
    "#         \"\"\"Project hidden state to vocabulary space while preserving token-wise information.\"\"\"\n",
    "#         lm_head_weights = self.state_dict[\"lm_head.weight\"]\n",
    "#         hidden_state = hidden_state.to(self.dtype)\n",
    "#         return F.linear(hidden_state, lm_head_weights)\n",
    "\n",
    "#     def analyze_text(self, input_ids: torch.Tensor) -> Dict:\n",
    "#         \"\"\"Analyze text through expert lens with proper token state propagation.\"\"\"\n",
    "#         batch_size, seq_len = input_ids.shape\n",
    "#         # Initialize hidden states from embeddings\n",
    "#         hidden_states = self.state_dict[\"model.embed_tokens.weight\"][input_ids]\n",
    "#         results = {}\n",
    "\n",
    "#         # Create causal attention mask (in float32 for numerical stability)\n",
    "#         causal_mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool, device=self.device), diagonal=1)\n",
    "#         attention_mask = torch.zeros(batch_size, 1, seq_len, seq_len, dtype=torch.float32, device=self.device)\n",
    "#         attention_mask.masked_fill_(causal_mask, float('-inf'))\n",
    "\n",
    "#         for layer_idx in range(1, 28):  # Layers 1-27\n",
    "#             layer_results = {\"tokens\": {}}\n",
    "            \n",
    "#             # Apply input layer norm while preserving token information\n",
    "#             normed_states = self._apply_layer_norm(layer_idx, hidden_states, \"input_layernorm\")\n",
    "            \n",
    "#             # Process through attention mechanism\n",
    "#             attn_output = self._process_attention(layer_idx, normed_states, attention_mask)\n",
    "#             hidden_states = hidden_states + attn_output  # Residual connection\n",
    "            \n",
    "#             # Post-attention layer norm\n",
    "#             normed_states = self._apply_layer_norm(layer_idx, hidden_states, \"post_attention_layernorm\")\n",
    "            \n",
    "#             # Get router decisions for each token\n",
    "#             router_logits = self._get_router_output(layer_idx, normed_states)\n",
    "            \n",
    "#             # Process each token position separately\n",
    "#             for pos in range(seq_len):\n",
    "#                 token = self.tokenizer.decode([input_ids[0, pos].item()])\n",
    "#                 token_state = normed_states[:, pos:pos+1]  # Keep batch dimension\n",
    "                \n",
    "#                 # Get top-k experts for this token\n",
    "#                 top_k_experts = torch.topk(router_logits[:, pos], k=7, dim=-1)\n",
    "#                 expert_weights = F.softmax(top_k_experts.values, dim=-1)\n",
    "                \n",
    "#                 expert_outputs = []\n",
    "#                 # Process through selected experts\n",
    "#                 for idx, expert_idx in enumerate(top_k_experts.indices[0]):\n",
    "#                     expert_output = self._process_expert(layer_idx, expert_idx.item(), token_state)\n",
    "#                     weight = expert_weights[0, idx].item()\n",
    "                    \n",
    "#                     # Project expert output to vocab space for analysis\n",
    "#                     logits = self._project_to_vocab(expert_output)\n",
    "#                     top_tokens = torch.topk(logits.squeeze(1), k=5)\n",
    "                    \n",
    "#                     expert_outputs.append({\n",
    "#                         \"expert_id\": expert_idx.item(),\n",
    "#                         \"weight\": weight,\n",
    "#                         \"top_tokens\": [\n",
    "#                             (self.tokenizer.decode([idx.item()]), prob.item())\n",
    "#                             for idx, prob in zip(top_tokens.indices[0], \n",
    "#                                                F.softmax(top_tokens.values[0], dim=-1))\n",
    "#                         ]\n",
    "#                     })\n",
    "                \n",
    "#                 layer_results[\"tokens\"][token] = {\n",
    "#                     \"position\": pos,\n",
    "#                     \"expert_outputs\": expert_outputs\n",
    "#                 }\n",
    "                \n",
    "#                 # Update hidden states for this token with weighted expert outputs\n",
    "#                 token_output = torch.zeros_like(token_state)\n",
    "#                 for expert_out in expert_outputs:\n",
    "#                     expert_idx = expert_out[\"expert_id\"]\n",
    "#                     weight = expert_out[\"weight\"]\n",
    "#                     expert_output = self._process_expert(layer_idx, expert_idx, token_state)\n",
    "#                     token_output += weight * expert_output\n",
    "                \n",
    "#                 # Update the hidden states for this position\n",
    "#                 hidden_states[:, pos:pos+1] = hidden_states[:, pos:pos+1] + token_output\n",
    "\n",
    "#             results[f\"layer_{layer_idx}\"] = layer_results\n",
    "\n",
    "#         return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hooked version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOELens:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.activations = defaultdict(dict)\n",
    "        self.hook_handles = []\n",
    "        self.setup_hooks()\n",
    "\n",
    "    def setup_hooks(self):\n",
    "        def get_gate_hook(layer_idx):\n",
    "            def hook(module, inp, out):\n",
    "                if isinstance(out, tuple):\n",
    "                    topk_idx, topk_weight, _ = out\n",
    "                    \n",
    "                    # Get hidden states from input\n",
    "                    hidden_states = inp[0]\n",
    "                    batch_size, seq_len, hidden_dim = hidden_states.shape\n",
    "                    \n",
    "                    # Project to vocab space to get token predictions\n",
    "                    with torch.no_grad():\n",
    "                        # Get expert outputs\n",
    "                        expert_outputs = {}\n",
    "                        for expert_idx in range(module.n_routed_experts):\n",
    "                            if hasattr(self.model.model.layers[layer_idx].mlp, 'experts'):\n",
    "                                expert = self.model.model.layers[layer_idx].mlp.experts[expert_idx]\n",
    "                                expert_output = expert(hidden_states.view(-1, hidden_dim))\n",
    "                                # Project to vocabulary space\n",
    "                                logits = self.model.lm_head(expert_output)\n",
    "                                top_tokens = torch.topk(logits, k=5, dim=-1)\n",
    "                                expert_outputs[expert_idx] = {\n",
    "                                    'token_ids': top_tokens.indices,\n",
    "                                    'probs': torch.softmax(top_tokens.values, dim=-1)\n",
    "                                }\n",
    "\n",
    "                    self.activations[f'layer_{layer_idx}'] = {\n",
    "                        'router_weights': topk_weight.detach(),\n",
    "                        'router_indices': topk_idx.detach(),\n",
    "                        'expert_outputs': expert_outputs\n",
    "                    }\n",
    "            return hook\n",
    "\n",
    "        for i, layer in enumerate(self.model.model.layers):\n",
    "            if hasattr(layer.mlp, 'gate'):\n",
    "                handle = layer.mlp.gate.register_forward_hook(get_gate_hook(i))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    def analyze_text(self, input_ids: torch.Tensor) -> dict:\n",
    "        self.activations.clear()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids)\n",
    "\n",
    "        results = {}\n",
    "        for layer_name, acts in self.activations.items():\n",
    "            layer_results = {\"tokens\": {}}\n",
    "            \n",
    "            for pos in range(input_ids.shape[1]):\n",
    "                token = self.tokenizer.decode([input_ids[0, pos].item()])\n",
    "                expert_info = []\n",
    "                \n",
    "                if 'router_indices' in acts and 'router_weights' in acts:\n",
    "                    indices = acts['router_indices'][pos]\n",
    "                    weights = acts['router_weights'][pos]\n",
    "                    expert_outputs = acts['expert_outputs']\n",
    "                    \n",
    "                    for idx, weight in zip(indices, weights):\n",
    "                        expert_id = idx.item()\n",
    "                        if expert_id in expert_outputs:\n",
    "                            expert_output = expert_outputs[expert_id]\n",
    "                            top_tokens = [\n",
    "                                (self.tokenizer.decode([token_id.item()]), prob.item())\n",
    "                                for token_id, prob in zip(\n",
    "                                    expert_output['token_ids'][pos],\n",
    "                                    expert_output['probs'][pos]\n",
    "                                )\n",
    "                            ]\n",
    "                        else:\n",
    "                            top_tokens = []\n",
    "                            \n",
    "                        expert_info.append({\n",
    "                            \"expert_id\": expert_id,\n",
    "                            \"weight\": weight.item(),\n",
    "                            \"top_tokens\": top_tokens\n",
    "                        })\n",
    "                \n",
    "                layer_results[\"tokens\"][token] = {\n",
    "                    \"position\": pos,\n",
    "                    \"expert_outputs\": expert_info\n",
    "                }\n",
    "            \n",
    "            results[layer_name] = layer_results\n",
    "\n",
    "        return results\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        for handle in self.hook_handles:\n",
    "            handle.remove()\n",
    "        self.hook_handles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_layer_analysis(tokenizer, results: Dict, token_position: int, input_text: str):\n",
    "    \"\"\"\n",
    "    Creates a plotly visualization of expert activations across layers for a specific token.\n",
    "    Shows all 64 experts with zero weights for non-selected experts.\n",
    "    \"\"\"\n",
    "    # Create lists to store data \n",
    "    layer_nums = []\n",
    "    expert_ids = []\n",
    "    weights = []\n",
    "    hover_texts = []\n",
    "    \n",
    "    total_experts = 64  # Total number of experts in the model\n",
    "    \n",
    "    # Extract token we're visualizing by tokenizing input text first\n",
    "    tokens = tokenizer.encode(input_text)\n",
    "    token = tokenizer.decode([tokens[token_position]])  # Get tokenized token\n",
    "    print(f\"Visualizing token: {token}\")\n",
    "        \n",
    "    for layer_idx in range(1, 28):  # Layers 1-27\n",
    "        layer_data = results[f\"layer_{layer_idx}\"]\n",
    "        token_data = [data for data in layer_data[\"tokens\"].values() \n",
    "                     if data[\"position\"] == token_position][0]\n",
    "        \n",
    "        # Create a mapping of expert_id to its data for this layer\n",
    "        expert_map = {exp[\"expert_id\"]: exp for exp in token_data[\"expert_outputs\"]}\n",
    "        \n",
    "        # Go through all possible experts\n",
    "        for expert_id in range(total_experts):\n",
    "            layer_nums.append(layer_idx)\n",
    "            expert_ids.append(expert_id)\n",
    "            \n",
    "            if expert_id in expert_map:\n",
    "                # Expert was selected\n",
    "                expert_data = expert_map[expert_id]\n",
    "                weight = expert_data[\"weight\"]\n",
    "                top_tokens_text = \"<br>\".join([\n",
    "                    f\"{token}: {prob:.3f}\" \n",
    "                    for token, prob in expert_data[\"top_tokens\"][:5]\n",
    "                ])\n",
    "                hover_text = f\"Layer: {layer_idx}<br>Expert: {expert_id}<br>Weight: {weight:.3f}<br>Top tokens:<br>{top_tokens_text}\"\n",
    "                hover_texts.append(hover_text)\n",
    "            else:\n",
    "                # Expert was not selected\n",
    "                weight = 0\n",
    "                hover_texts.append(None)  # No hover text for unselected experts\n",
    "            \n",
    "            weights.append(weight)\n",
    "    \n",
    "    # Create plotly heatmap\n",
    "    fig = go.Figure(data=go.Scatter(\n",
    "        x=expert_ids,\n",
    "        y=layer_nums,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=9,\n",
    "            color=weights,\n",
    "            colorscale=[\n",
    "                [0, 'rgba(24, 21, 23, 0.8)'],  # Very dark/transparent for zero weights\n",
    "                [0.0001, 'rgb(68,1,84)'],      # Start of Viridis colorscale\n",
    "                [1, 'rgb(253,231,37)']         # End of Viridis colorscale\n",
    "            ],\n",
    "            cmin=0,  # Set minimum of color scale to 0\n",
    "            cmax=1,  # Set maximum of color scale to 1\n",
    "            showscale=True,\n",
    "            colorbar=dict(\n",
    "                title='Weight',\n",
    "                tickmode='linear',\n",
    "                tick0=0,\n",
    "                dtick=0.2\n",
    "            ),\n",
    "        ),\n",
    "        text=hover_texts,\n",
    "        hoverinfo='text',\n",
    "        hovertemplate='%{text}<extra></extra>',  # Only show hover when text exists\n",
    "    ))\n",
    "    \n",
    "    # Update layout with dark theme\n",
    "    fig.update_layout(\n",
    "        template='plotly_dark',\n",
    "        title=f'Expert Activations for Token \"{token}\" at Position {token_position}',\n",
    "        xaxis_title='Expert ID',\n",
    "        yaxis_title='Layer',\n",
    "        yaxis=dict(autorange='reversed'),  # Reverse y-axis to have layer 1 at top\n",
    "        width=1200,\n",
    "        height=800,\n",
    "        showlegend=False,\n",
    "        plot_bgcolor='black',\n",
    "        paper_bgcolor='black'\n",
    "    )\n",
    "    \n",
    "    # Add grid lines\n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='rgba(128, 128, 128, 0.2)', \n",
    "                     range=[-1, total_experts])\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='rgba(128, 128, 128, 0.2)')\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_enhanced_logit_lens(model_outputs, tokenizer, input_text, position=None):\n",
    "#     \"\"\"\n",
    "#     Creates enhanced visualization of logit lens analysis using plotly.\n",
    "    \n",
    "#     Args:\n",
    "#         model_outputs: Dictionary containing layer outputs and logits\n",
    "#         tokenizer: The model's tokenizer\n",
    "#         input_text: The input text being analyzed\n",
    "#         position: Optional specific position to analyze\n",
    "#     \"\"\"\n",
    "#     import plotly.graph_objects as go\n",
    "#     from plotly.subplots import make_subplots\n",
    "#     import numpy as np\n",
    "    \n",
    "#     # Extract data\n",
    "#     layers = []\n",
    "#     tokens = []\n",
    "#     logits = []\n",
    "#     ranks = []\n",
    "    \n",
    "#     # Process each layer's outputs\n",
    "#     for layer_idx, layer_data in sorted(model_outputs.items()):\n",
    "#         if not layer_idx.startswith('layer_'):\n",
    "#             continue\n",
    "            \n",
    "#         layer_num = int(layer_idx.split('_')[1])\n",
    "#         if position is not None:\n",
    "#             token_data = [data for data in layer_data[\"tokens\"].values() \n",
    "#                          if data[\"position\"] == position][0]\n",
    "#         else:\n",
    "#             token_data = next(iter(layer_data[\"tokens\"].values()))\n",
    "            \n",
    "#         # Get top 5 predictions for this layer\n",
    "#         top_predictions = []\n",
    "#         top_logits = []\n",
    "#         for exp_output in token_data[\"expert_outputs\"]:\n",
    "#             for tok, prob in exp_output[\"top_tokens\"]:\n",
    "#                 if tok not in top_predictions:\n",
    "#                     top_predictions.append(tok)\n",
    "#                     top_logits.append(prob)\n",
    "#                 if len(top_predictions) >= 5:\n",
    "#                     break\n",
    "#             if len(top_predictions) >= 5:\n",
    "#                 break\n",
    "                \n",
    "#         layers.append(f\"Layer {layer_num}\")\n",
    "#         tokens.extend([t for t in top_predictions if t not in tokens])\n",
    "#         logits.append(top_logits)\n",
    "        \n",
    "#     # Create matrix of logits\n",
    "#     logit_matrix = np.zeros((len(layers), len(tokens)))\n",
    "#     for i, layer_logits in enumerate(logits):\n",
    "#         for j, token in enumerate(tokens[:len(layer_logits)]):\n",
    "#             logit_matrix[i, j] = layer_logits[j]\n",
    "            \n",
    "#     # Create subplots\n",
    "#     fig = make_subplots(rows=2, cols=1, \n",
    "#                        subplot_titles=(\"Token Logits Across Layers\", \n",
    "#                                      \"Top Token Rankings\"),\n",
    "#                        vertical_spacing=0.15)\n",
    "    \n",
    "#     # Add heatmap for logit values\n",
    "#     fig.add_trace(\n",
    "#         go.Heatmap(\n",
    "#             z=logit_matrix,\n",
    "#             x=tokens,\n",
    "#             y=layers,\n",
    "#             colorscale='Viridis',\n",
    "#             text=[[f\"{val:.2f}\" for val in row] for row in logit_matrix],\n",
    "#             texttemplate=\"%{text}\",\n",
    "#             textfont={\"size\":10},\n",
    "#             showscale=True,\n",
    "#             name=\"Logits\"\n",
    "#         ),\n",
    "#         row=1, col=1\n",
    "#     )\n",
    "    \n",
    "#     # Add scatter plot for rankings\n",
    "#     rank_data = []\n",
    "#     for i, layer in enumerate(layers):\n",
    "#         sorted_indices = np.argsort(-logit_matrix[i])  # Sort by descending logit value\n",
    "#         for rank, idx in enumerate(sorted_indices):\n",
    "#             rank_data.append({\n",
    "#                 'layer': layer,\n",
    "#                 'token': tokens[idx],\n",
    "#                 'rank': rank + 1,\n",
    "#                 'logit': logit_matrix[i, idx]\n",
    "#             })\n",
    "            \n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=[d['layer'] for d in rank_data],\n",
    "#             y=[d['rank'] for d in rank_data],\n",
    "#             mode='markers+text',\n",
    "#             text=[d['token'] for d in rank_data],\n",
    "#             textposition=\"top center\",\n",
    "#             marker=dict(\n",
    "#                 size=10,\n",
    "#                 color=[d['logit'] for d in rank_data],\n",
    "#                 colorscale='Viridis',\n",
    "#                 showscale=True\n",
    "#             ),\n",
    "#             name=\"Token Rankings\"\n",
    "#         ),\n",
    "#         row=2, col=1\n",
    "#     )\n",
    "    \n",
    "#     # Update layout\n",
    "#     fig.update_layout(\n",
    "#         height=1000,\n",
    "#         width=1200,\n",
    "#         title_text=f\"Enhanced Logit Lens Analysis\" + (f\" - Position {position}\" if position else \"\"),\n",
    "#         showlegend=False,\n",
    "#     )\n",
    "    \n",
    "#     # Update axes\n",
    "#     fig.update_xaxes(title_text=\"Tokens\", row=1, col=1)\n",
    "#     fig.update_yaxes(title_text=\"Layers\", row=1, col=1)\n",
    "#     fig.update_xaxes(title_text=\"Layers\", row=2, col=1)\n",
    "#     fig.update_yaxes(title_text=\"Rank\", row=2, col=1, autorange=\"reversed\")\n",
    "    \n",
    "#     return fig\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_enhanced_logit_lens(model_outputs, tokenizer, input_text, position=None):\n",
    "    \"\"\"\n",
    "    Creates a visualization similar to the original logit lens paper style.\n",
    "    \"\"\"\n",
    "    import plotly.graph_objects as go\n",
    "    import numpy as np\n",
    "    \n",
    "    # Extract data\n",
    "    layers = []\n",
    "    tokens = []\n",
    "    logits = []\n",
    "    \n",
    "    for layer_idx, layer_data in sorted(model_outputs.items()):\n",
    "        if not layer_idx.startswith('layer_'):\n",
    "            continue\n",
    "            \n",
    "        layer_num = int(layer_idx.split('_')[1])\n",
    "        if position is not None:\n",
    "            token_data = [data for data in layer_data[\"tokens\"].values() \n",
    "                         if data[\"position\"] == position][0]\n",
    "        else:\n",
    "            token_data = next(iter(layer_data[\"tokens\"].values()))\n",
    "            \n",
    "        layer_name = f\"h{layer_num}_out\" if layer_num > 0 else \"h_out\"\n",
    "        layers.append(layer_name)\n",
    "        \n",
    "        # Get token predictions and logits\n",
    "        current_predictions = []\n",
    "        current_logits = []\n",
    "        for exp_output in token_data[\"expert_outputs\"]:\n",
    "            top_tokens = [(tok, prob) for tok, prob in exp_output[\"top_tokens\"]]\n",
    "            current_predictions.extend([t[0] for t in top_tokens])\n",
    "            current_logits.extend([t[1] for t in top_tokens])\n",
    "            \n",
    "        # Add new unique tokens to the global list\n",
    "        for tok in current_predictions:\n",
    "            if tok not in tokens:\n",
    "                tokens.append(tok)\n",
    "                \n",
    "        # Create full logit vector for this layer\n",
    "        layer_logits = []\n",
    "        for tok in tokens:\n",
    "            if tok in current_predictions:\n",
    "                idx = current_predictions.index(tok)\n",
    "                layer_logits.append(current_logits[idx])\n",
    "            else:\n",
    "                layer_logits.append(0)  # or some small negative number\n",
    "                \n",
    "        logits.append(layer_logits)\n",
    "    \n",
    "    # Convert to numpy array for easier manipulation\n",
    "    logit_matrix = np.array(logits)\n",
    "    \n",
    "    # Create figure\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=logit_matrix,\n",
    "        x=tokens,\n",
    "        y=layers,\n",
    "        colorscale=[\n",
    "            [0, \"rgb(0,0,0)\"],         # Black for lowest values\n",
    "            [0.25, \"rgb(0,0,128)\"],    # Dark blue\n",
    "            [0.5, \"rgb(30,144,255)\"],  # Dodger blue\n",
    "            [0.75, \"rgb(211,211,211)\"], # Light gray\n",
    "            [1, \"rgb(255,255,0)\"]      # Yellow for highest values\n",
    "        ],\n",
    "        text=[[f\"'{t}'\" for t in tokens] for _ in range(len(layers))],\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 10, \"color\": \"white\"},\n",
    "        showscale=True,\n",
    "        colorbar=dict(\n",
    "            title=\"Logit Value\",\n",
    "            titleside=\"right\",\n",
    "            tickfont={\"size\": 10},\n",
    "            thickness=15,\n",
    "            len=0.75\n",
    "        )\n",
    "    ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': \"Model's Top Token and its Logit\",\n",
    "            'y': 0.95,\n",
    "            'x': 0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top',\n",
    "            'font': {'size': 14}\n",
    "        },\n",
    "        width=1200,\n",
    "        height=1000,\n",
    "        xaxis=dict(\n",
    "            showgrid=False,\n",
    "            tickangle=45,\n",
    "            tickfont=dict(size=10),\n",
    "            tickmode='array',\n",
    "            ticktext=tokens,\n",
    "            tickvals=list(range(len(tokens)))\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            showgrid=False,\n",
    "            tickfont=dict(size=10),\n",
    "            autorange='reversed'  # To match the paper's style with h_out at the top\n",
    "        ),\n",
    "        plot_bgcolor='white',\n",
    "        paper_bgcolor='white'\n",
    "    )\n",
    "    \n",
    "    # Add cell borders using shapes\n",
    "    for i in range(len(layers)+1):\n",
    "        fig.add_shape(\n",
    "            type=\"line\",\n",
    "            x0=-0.5,\n",
    "            x1=len(tokens)-0.5,\n",
    "            y0=i-0.5,\n",
    "            y1=i-0.5,\n",
    "            line=dict(color=\"white\", width=0.5)\n",
    "        )\n",
    "    \n",
    "    for j in range(len(tokens)+1):\n",
    "        fig.add_shape(\n",
    "            type=\"line\",\n",
    "            x0=j-0.5,\n",
    "            x1=j-0.5,\n",
    "            y0=-0.5,\n",
    "            y1=len(layers)-0.5,\n",
    "            line=dict(color=\"white\", width=0.5)\n",
    "        )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_expert_contributions(model_outputs, position=None):\n",
    "    \"\"\"\n",
    "    Creates visualization of expert contributions at each layer.\n",
    "    \n",
    "    Args:\n",
    "        model_outputs: Dictionary containing layer outputs and expert information\n",
    "        position: Optional specific position to analyze\n",
    "    \"\"\"\n",
    "    import plotly.graph_objects as go\n",
    "    import numpy as np\n",
    "    \n",
    "    # Extract expert data\n",
    "    layers = []\n",
    "    expert_weights = []\n",
    "    expert_ids = []\n",
    "    \n",
    "    for layer_idx, layer_data in sorted(model_outputs.items()):\n",
    "        if not layer_idx.startswith('layer_'):\n",
    "            continue\n",
    "            \n",
    "        layer_num = int(layer_idx.split('_')[1])\n",
    "        if position is not None:\n",
    "            token_data = [data for data in layer_data[\"tokens\"].values() \n",
    "                         if data[\"position\"] == position][0]\n",
    "        else:\n",
    "            token_data = next(iter(layer_data[\"tokens\"].values()))\n",
    "            \n",
    "        layers.append(f\"Layer {layer_num}\")\n",
    "        \n",
    "        # Get expert weights and ids\n",
    "        layer_weights = []\n",
    "        layer_ids = []\n",
    "        for exp_output in token_data[\"expert_outputs\"]:\n",
    "            layer_weights.append(exp_output[\"weight\"])\n",
    "            layer_ids.append(exp_output[\"expert_id\"])\n",
    "            \n",
    "        expert_weights.append(layer_weights)\n",
    "        expert_ids.extend([id for id in layer_ids if id not in expert_ids])\n",
    "        \n",
    "    # Create matrix of expert weights\n",
    "    weight_matrix = np.zeros((len(layers), len(expert_ids)))\n",
    "    for i, weights in enumerate(expert_weights):\n",
    "        for w, id in zip(weights, expert_ids[:len(weights)]):\n",
    "            weight_matrix[i, expert_ids.index(id)] = w\n",
    "            \n",
    "    # Create figure\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=weight_matrix,\n",
    "        x=[f\"Expert {id}\" for id in expert_ids],\n",
    "        y=layers,\n",
    "        colorscale='Viridis',\n",
    "        text=[[f\"{val:.3f}\" if val > 0 else \"\" for val in row] for row in weight_matrix],\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\":10},\n",
    "        showscale=True,\n",
    "    ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Expert Contributions by Layer\" + (f\" - Position {position}\" if position else \"\"),\n",
    "        xaxis_title=\"Experts\",\n",
    "        yaxis_title=\"Layers\",\n",
    "        height=800,\n",
    "        width=1200,\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_moe_logit_lens_with_active_expert_outputs(model, inputs, tokenizer, num_active_experts=7):\n",
    "    model.eval()\n",
    "\n",
    "    # Initial embedding\n",
    "    x = model.model.embed_tokens(inputs)\n",
    "\n",
    "    # Process each layer\n",
    "    for layer_idx, layer in enumerate(model.model.layers):\n",
    "        print(f\"Layer {layer_idx + 1}\")\n",
    "        x = layer.input_layernorm(x)\n",
    "\n",
    "        # Self-attention output\n",
    "        # Remove the position_ids argument here, let the model handle it internally\n",
    "        attn_output = layer.self_attn(x, x, x)\n",
    "        x = attn_output + x  # Residual connection\n",
    "        x = layer.post_attention_layernorm(x)\n",
    "\n",
    "        # MoE Layer\n",
    "        moe_output = layer.mlp(x)\n",
    "        gate_values = moe_output[\"gate_values\"]\n",
    "        expert_outputs = moe_output[\"expert_outputs\"]\n",
    "\n",
    "        # Extract active experts\n",
    "        for batch_idx, gates in enumerate(gate_values):\n",
    "            active_experts = gates.argsort(descending=True)[:num_active_experts]\n",
    "            print(f\"  Batch {batch_idx + 1}:\")\n",
    "            for expert_idx in active_experts:\n",
    "                expert_weight = gates[expert_idx].item()\n",
    "                expert_output = expert_outputs[batch_idx, :, expert_idx]\n",
    "                top_tokens = expert_output.topk(5, dim=-1)\n",
    "                top_indices = top_tokens.indices\n",
    "                top_scores = top_tokens.values\n",
    "                decoded_tokens = tokenizer.decode(top_indices.tolist())\n",
    "                print(f\"    Expert {expert_idx}: Weight: {expert_weight:.4f}, Tokens: {decoded_tokens}, Scores: {top_scores.tolist()}\")\n",
    "\n",
    "    # Final logits\n",
    "    logits = model.lm_head(x)\n",
    "    top_tokens = logits.topk(5, dim=-1)\n",
    "    decoded_final_tokens = tokenizer.decode(top_tokens.indices.tolist())\n",
    "    print(f\"Final Layer: Tokens: {decoded_final_tokens}, Scores: {top_tokens.values.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"Get the optimal available device\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        # Enable TF32 for better performance on Ampere GPUs (A100, A6000, etc)\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        # Set memory allocation settings\n",
    "        torch.cuda.empty_cache()\n",
    "        # Enable CUDNN benchmarking for better performance\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_moe_logit_lens_with_active_expert_outputs(model, inputs, tokenizer, num_active_experts=7):\n",
    "    model.eval()\n",
    "\n",
    "    # Initial embedding\n",
    "    x = model.model.embed_tokens(inputs)\n",
    "\n",
    "    # Process each layer\n",
    "    for layer_idx, layer in enumerate(model.model.layers):\n",
    "        print(f\"Layer {layer_idx + 1}\")\n",
    "        x = layer.input_layernorm(x)\n",
    "\n",
    "        # Self-attention output\n",
    "        # Remove the position_ids argument here, let the model handle it internally\n",
    "        attn_output = layer.self_attn(x, x, x)\n",
    "        x = attn_output + x  # Residual connection\n",
    "        x = layer.post_attention_layernorm(x)\n",
    "\n",
    "        # MoE Layer\n",
    "        moe_output = layer.mlp(x)\n",
    "        gate_values = moe_output[\"gate_values\"]\n",
    "        expert_outputs = moe_output[\"expert_outputs\"]\n",
    "\n",
    "        # Extract active experts\n",
    "        for batch_idx, gates in enumerate(gate_values):\n",
    "            active_experts = gates.argsort(descending=True)[:num_active_experts]\n",
    "            print(f\"  Batch {batch_idx + 1}:\")\n",
    "            for expert_idx in active_experts:\n",
    "                expert_weight = gates[expert_idx].item()\n",
    "                expert_output = expert_outputs[batch_idx, :, expert_idx]\n",
    "                top_tokens = expert_output.topk(5, dim=-1)\n",
    "                top_indices = top_tokens.indices\n",
    "                top_scores = top_tokens.values\n",
    "                decoded_tokens = tokenizer.decode(top_indices.tolist())\n",
    "                print(f\"    Expert {expert_idx}: Weight: {expert_weight:.4f}, Tokens: {decoded_tokens}, Scores: {top_scores.tolist()}\")\n",
    "\n",
    "    # Final logits\n",
    "    logits = model.lm_head(x)\n",
    "    top_tokens = logits.topk(5, dim=-1)\n",
    "    decoded_final_tokens = tokenizer.decode(top_tokens.indices.tolist())\n",
    "    print(f\"Final Layer: Tokens: {decoded_final_tokens}, Scores: {top_tokens.values.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_lens(model, input_tokens):\n",
    "    \"\"\"\n",
    "    Applies a logit lens to each layer of a mixture of experts (MoE) model for specific input tokens.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The MoE model.\n",
    "        input_tokens (torch.Tensor): Input tensor with token embeddings, shape [batch_size, seq_len].\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the logit lens outputs for each token at each layer.\n",
    "    \"\"\"\n",
    "    logit_outputs = {}\n",
    "\n",
    "    # Pass the input tokens through the embedding layer\n",
    "    embedding_output = model.embed_tokens(input_tokens)  # Shape: [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "    # Process tokens through each layer\n",
    "    for layer_idx, layer in enumerate(model.layers):\n",
    "        # Apply the layer and get its output\n",
    "        layer_outputs = layer(embedding_output)  # Shape: [batch_size, seq_len, feature_dim]\n",
    "\n",
    "        # Extract the mixture of experts (MoE) components for this layer\n",
    "        if hasattr(layer.mlp, \"experts\"):\n",
    "            gate_outputs = layer.mlp.gate(embedding_output)  # Shape: [batch_size, seq_len, num_experts]\n",
    "            expert_outputs = []\n",
    "\n",
    "            # Process each token separately to extract expert-specific outputs\n",
    "            for token_idx in range(input_tokens.shape[1]):\n",
    "                token_expert_outputs = []\n",
    "\n",
    "                for expert_idx, expert in enumerate(layer.mlp.experts):\n",
    "                    token_input = embedding_output[:, token_idx, :]  # Shape: [batch_size, embedding_dim]\n",
    "                    expert_output = expert(token_input)  # Shape: [batch_size, feature_dim]\n",
    "                    token_expert_outputs.append(expert_output)\n",
    "\n",
    "                # Stack expert outputs and apply gating\n",
    "                token_expert_outputs = torch.stack(token_expert_outputs, dim=1)  # Shape: [batch_size, num_experts, feature_dim]\n",
    "                token_gate_weights = gate_outputs[:, token_idx, :].unsqueeze(-1)  # Shape: [batch_size, num_experts, 1]\n",
    "                activated_experts_output = torch.sum(token_expert_outputs * token_gate_weights, dim=1)  # Shape: [batch_size, feature_dim]\n",
    "\n",
    "                # Save activated expert outputs for this token\n",
    "                logit_outputs[f\"layer_{layer_idx}_token_{token_idx}_activated_experts\"] = activated_experts_output\n",
    "\n",
    "        # Update the embedding for the next layer\n",
    "        embedding_output = layer_outputs\n",
    "\n",
    "    return logit_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize and use as before\n",
    "# analyzer = MOEExpertLens(model.state_dict(), tokenizer)\n",
    "# text = \"the quick brown fox\"\n",
    "# input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(get_device())\n",
    "# results = analyzer.analyze_text(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_layer_analysis(tokenizer, results, token_position=3, input_text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([6])\n",
      " jumps\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "text = \"the quick brown fox\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model.forward(**inputs.to(model.device))\n",
    "# print(outputs)\n",
    "x = outputs.logits[0, -1]\n",
    "x = torch.argmax(x)\n",
    "\n",
    "\n",
    "output2 = model.generate(**inputs.to(model.device), max_new_tokens=1)\n",
    "\n",
    "y = output2[0]\n",
    "# torch.argmax(x)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "result = tokenizer.decode(x)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: oment\n",
      "Layer 1: \">:\n",
      "Layer 2: es\n",
      "Layer 3: es\n",
      "Layer 4: croft\n",
      "Layer 5: croft\n",
      "Layer 6: IEEEeqnarray\n",
      "Layer 7: IEEEeqnarray\n",
      "Layer 8: IEEEeqnarray\n",
      "Layer 9: IEEEeqnarray\n",
      "Layer 10: IEEEeqnarray\n",
      "Layer 11: IEEEeqnarray\n",
      "Layer 12: IEEEeqnarray\n",
      "Layer 13: IEEEeqnarray\n",
      "Layer 14: issin\n",
      "Layer 15: IEEEeqnarray\n",
      "Layer 16: estrat\n",
      "Layer 17: IEEEeqnarray\n",
      "Layer 18: IEEEeqnarray\n",
      "Layer 19: =\"../_\n",
      "Layer 20:  rejo\n",
      "Layer 21:  r\n",
      "Layer 22: Jump\n",
      "Layer 23: Jump\n",
      "Layer 24: Jump\n",
      "Layer 25: Jump\n",
      "Layer 26: Jump\n",
      "Layer 27: \n",
      "\n",
      "Layer 28:  jumps\n"
     ]
    }
   ],
   "source": [
    "outputs = model.forward(**inputs.to(model.device), output_hidden_states=True)\n",
    "\n",
    "\n",
    "for i in range(29):\n",
    "    \n",
    "    x = outputs.hidden_states[i]\n",
    "    y = x[0, -1]\n",
    "    z = model.lm_head(y)\n",
    "    z = torch.argmax(z)\n",
    "    result = tokenizer.decode(z)\n",
    "    print(f\"Layer {i}: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create both visualizations\n",
    "logit_fig = plot_enhanced_logit_lens(results, tokenizer, text, position=0)\n",
    "expert_fig = plot_expert_contributions(results, position=0)\n",
    "\n",
    "# Display the figures\n",
    "logit_fig.show()\n",
    "expert_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create both visualizations\n",
    "logit_fig = plot_enhanced_logit_lens(results, tokenizer, text, position=1)\n",
    "expert_fig = plot_expert_contributions(results, position=1)\n",
    "\n",
    "# Display the figures\n",
    "logit_fig.show()\n",
    "expert_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lens\n",
    "lens = MOELens(model, tokenizer)\n",
    "\n",
    "text_new = \"the quick brown fox\"\n",
    "# Analyze text\n",
    "input_ids = tokenizer(text_new, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "results = lens.analyze_text(input_ids)\n",
    "\n",
    "# Clean up hooks when done\n",
    "lens.remove_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing token:  quick\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "text",
         "hovertemplate": "%{text}<extra></extra>",
         "marker": {
          "cmax": 1,
          "cmin": 0,
          "color": [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0299530029296875,
           0,
           0,
           0.036407470703125,
           0.093505859375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.02911376953125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.09893798828125,
           0,
           0,
           0,
           0.252685546875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.078369140625,
           0,
           0,
           0,
           0.06280517578125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.183349609375,
           0,
           0,
           0,
           0.06982421875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0469970703125,
           0,
           0,
           0.10443115234375,
           0,
           0,
           0,
           0,
           0,
           0,
           0.044921875,
           0,
           0,
           0,
           0,
           0,
           0,
           0.10821533203125,
           0,
           0,
           0,
           0,
           0,
           0,
           0.09417724609375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.06463623046875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0465087890625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.051971435546875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.1090087890625,
           0,
           0,
           0,
           0,
           0.03936767578125,
           0,
           0,
           0,
           0,
           0,
           0,
           0.036712646484375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.10821533203125,
           0,
           0.0782470703125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.158935546875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.031646728515625,
           0,
           0,
           0,
           0,
           0,
           0.035614013671875,
           0,
           0,
           0,
           0,
           0,
           0,
           0.40869140625,
           0,
           0,
           0,
           0.06671142578125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.037078857421875,
           0.060882568359375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.06884765625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.039306640625,
           0,
           0,
           0,
           0.180419921875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.117431640625,
           0,
           0,
           0.0345458984375,
           0,
           0.08880615234375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.1494140625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0753173828125,
           0,
           0.06854248046875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.1263427734375,
           0,
           0.054168701171875,
           0,
           0,
           0,
           0,
           0.08465576171875,
           0,
           0,
           0.05731201171875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0462646484375,
           0,
           0,
           0,
           0,
           0.07275390625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.09857177734375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0931396484375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.060882568359375,
           0,
           0.05633544921875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.09051513671875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.15576171875,
           0.042388916015625,
           0,
           0,
           0,
           0.05889892578125,
           0,
           0,
           0,
           0,
           0,
           0.05322265625,
           0,
           0,
           0,
           0,
           0,
           0.10198974609375,
           0,
           0,
           0,
           0,
           0,
           0.09564208984375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0975341796875,
           0,
           0,
           0.035980224609375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.09307861328125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.08294677734375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.07080078125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.05615234375,
           0,
           0,
           0,
           0,
           0.061126708984375,
           0,
           0,
           0,
           0,
           0,
           0,
           0.07659912109375,
           0,
           0,
           0,
           0.069091796875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0712890625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.169189453125,
           0,
           0,
           0,
           0,
           0,
           0,
           0.03790283203125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.07958984375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.05035400390625,
           0,
           0,
           0,
           0,
           0,
           0,
           0.1002197265625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.09454345703125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.041473388671875,
           0,
           0.0865478515625,
           0,
           0,
           0.057708740234375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.07757568359375,
           0.1405029296875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0604248046875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.09539794921875,
           0.0814208984375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.06121826171875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.045562744140625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.1619873046875,
           0,
           0.0584716796875,
           0,
           0,
           0,
           0,
           0.097900390625,
           0.08843994140625,
           0,
           0,
           0,
           0.0377197265625,
           0,
           0,
           0,
           0,
           0,
           0.27197265625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0841064453125,
           0.056121826171875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.043060302734375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.077392578125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.04217529296875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.039459228515625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.291015625,
           0.077392578125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.075927734375,
           0,
           0.12109375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.2080078125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.051055908203125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.042755126953125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0716552734375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.028717041015625,
           0,
           0,
           0,
           0,
           0,
           0.0767822265625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.450439453125,
           0,
           0.03143310546875,
           0,
           0,
           0,
           0,
           0.073974609375,
           0,
           0,
           0,
           0,
           0,
           0.06787109375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.029327392578125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.33251953125,
           0,
           0,
           0.04913330078125,
           0,
           0.049407958984375,
           0,
           0.1337890625,
           0.0306549072265625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.038818359375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0679931640625,
           0,
           0,
           0,
           0,
           0,
           0.08404541015625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0966796875,
           0,
           0,
           0.07696533203125,
           0,
           0,
           0,
           0,
           0.038970947265625,
           0,
           0,
           0,
           0,
           0,
           0.128662109375,
           0,
           0.07073974609375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.06494140625,
           0,
           0,
           0.045074462890625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.044097900390625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.358642578125,
           0,
           0,
           0,
           0.0689697265625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.2127685546875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.040069580078125,
           0.07647705078125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0310211181640625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.12054443359375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.043487548828125,
           0,
           0.06683349609375,
           0,
           0,
           0.047637939453125,
           0,
           0,
           0,
           0.365478515625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.034088134765625,
           0,
           0,
           0,
           0,
           0,
           0.036468505859375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.09429931640625,
           0,
           0.05194091796875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.08502197265625,
           0,
           0,
           0,
           0.051483154296875,
           0,
           0,
           0,
           0,
           0,
           0,
           0.1734619140625,
           0,
           0,
           0.033111572265625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.05780029296875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.07733154296875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.07611083984375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.06463623046875,
           0,
           0.0836181640625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.053253173828125,
           0,
           0,
           0,
           0.060699462890625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.07275390625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.216796875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.048675537109375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.03729248046875,
           0,
           0,
           0.04327392578125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.13720703125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.035919189453125,
           0,
           0,
           0,
           0,
           0.17626953125,
           0,
           0,
           0,
           0,
           0,
           0.269287109375,
           0,
           0,
           0,
           0.060791015625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0546875
          ],
          "colorbar": {
           "dtick": 0.2,
           "tick0": 0,
           "tickmode": "linear",
           "title": {
            "text": "Weight"
           }
          },
          "colorscale": [
           [
            0,
            "rgba(24, 21, 23, 0.8)"
           ],
           [
            0.0001,
            "rgb(68,1,84)"
           ],
           [
            1,
            "rgb(253,231,37)"
           ]
          ],
          "showscale": true,
          "size": 9
         },
         "mode": "markers",
         "text": [
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 1<br>Expert: 19<br>Weight: 0.030<br>Top tokens:<br> : 0.214<br>: 0.212<br>peed: 0.199<br>BOX: 0.192<br>owers: 0.184",
          null,
          null,
          "Layer: 1<br>Expert: 22<br>Weight: 0.036<br>Top tokens:<br>: 0.207<br>: 0.200<br>INESS: 0.199<br>: 0.197<br>aix: 0.196",
          "Layer: 1<br>Expert: 23<br>Weight: 0.094<br>Top tokens:<br>ynchron: 0.237<br>QUI: 0.199<br>rido: 0.190<br>: 0.187<br>INLINE: 0.186",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 1<br>Expert: 38<br>Weight: 0.029<br>Top tokens:<br>f: 0.211<br>: 0.205<br>: 0.200<br>: 0.192<br> dispos: 0.192",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 1<br>Expert: 59<br>Weight: 0.099<br>Top tokens:<br>: 0.213<br>hatt: 0.201<br>XB: 0.199<br>: 0.197<br>: 0.190",
          null,
          null,
          null,
          "Layer: 1<br>Expert: 63<br>Weight: 0.253<br>Top tokens:<br>itaris: 0.236<br>ndefined: 0.205<br>SPHIN: 0.196<br>OperationKind: 0.184<br>ABLE: 0.179",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 2<br>Expert: 29<br>Weight: 0.078<br>Top tokens:<br>comings: 0.218<br>lectic: 0.204<br>: 0.193<br>ernels: 0.193<br>: 0.192",
          null,
          null,
          null,
          "Layer: 2<br>Expert: 33<br>Weight: 0.063<br>Top tokens:<br>cc: 0.243<br>: 0.204<br>CC: 0.196<br> cc: 0.188<br> CC: 0.169",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 2<br>Expert: 44<br>Weight: 0.183<br>Top tokens:<br>: 0.259<br>Fuse: 0.207<br>yles: 0.198<br>Suite: 0.170<br>themselves: 0.166",
          null,
          null,
          null,
          "Layer: 2<br>Expert: 48<br>Weight: 0.070<br>Top tokens:<br>: 0.216<br>Theme: 0.215<br>ium: 0.197<br>Hon: 0.186<br>: 0.186",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 2<br>Expert: 59<br>Weight: 0.047<br>Top tokens:<br>ive: 0.217<br>cell: 0.214<br>ely: 0.209<br>side: 0.180<br>tr: 0.180",
          null,
          null,
          "Layer: 2<br>Expert: 62<br>Weight: 0.104<br>Top tokens:<br>ies: 0.330<br>ie: 0.217<br>eny: 0.159<br>amen: 0.152<br>imen: 0.143",
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 3<br>Expert: 5<br>Weight: 0.045<br>Top tokens:<br>: 0.263<br>: 0.196<br>ypse: 0.189<br>: 0.176<br>anship: 0.176",
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 3<br>Expert: 12<br>Weight: 0.108<br>Top tokens:<br> v: 0.213<br>l: 0.206<br> u: 0.201<br>u: 0.191<br> l: 0.190",
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 3<br>Expert: 19<br>Weight: 0.094<br>Top tokens:<br>rah: 0.255<br> forth: 0.212<br>bush: 0.186<br>: 0.176<br>ovitch: 0.171",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 3<br>Expert: 28<br>Weight: 0.065<br>Top tokens:<br> : 0.227<br> passat: 0.208<br> tornada: 0.192<br>: 0.188<br> : 0.185",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 3<br>Expert: 38<br>Weight: 0.047<br>Top tokens:<br>: 0.217<br>: 0.214<br>: 0.191<br>: 0.190<br>ist: 0.188",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 3<br>Expert: 54<br>Weight: 0.052<br>Top tokens:<br>arques: 0.226<br>-\\!: 0.208<br>bserv: 0.203<br>ishop: 0.189<br>inical: 0.174",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 4<br>Expert: 1<br>Weight: 0.109<br>Top tokens:<br>ulpt: 0.282<br>: 0.224<br>: 0.165<br>olor: 0.165<br> Enc: 0.164",
          null,
          null,
          null,
          null,
          "Layer: 4<br>Expert: 6<br>Weight: 0.039<br>Top tokens:<br>tingu: 0.205<br>odinger: 0.201<br>bows: 0.198<br>Ingressos: 0.198<br> Grau: 0.198",
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 4<br>Expert: 13<br>Weight: 0.037<br>Top tokens:<br>unz: 0.229<br>: 0.209<br>xiety: 0.190<br>*': 0.187<br>yan: 0.185",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 4<br>Expert: 37<br>Weight: 0.108<br>Top tokens:<br>uelto: 0.215<br>*}[!: 0.202<br>: 0.201<br>imales: 0.197<br>: 0.185",
          null,
          "Layer: 4<br>Expert: 39<br>Weight: 0.078<br>Top tokens:<br>: 0.252<br>: 0.242<br>anine: 0.192<br>gable: 0.160<br>ATERIAL: 0.154",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 4<br>Expert: 62<br>Weight: 0.159<br>Top tokens:<br>ndum: 0.239<br> Serrat: 0.206<br>: 0.188<br> : 0.186<br>eting: 0.181",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 5<br>Expert: 25<br>Weight: 0.032<br>Top tokens:<br>: 0.302<br>ablement: 0.233<br>uci: 0.171<br>onian: 0.149<br>: 0.145",
          null,
          null,
          null,
          null,
          null,
          "Layer: 5<br>Expert: 31<br>Weight: 0.036<br>Top tokens:<br>: 0.232<br>ansi: 0.214<br>ymen: 0.203<br> : 0.176<br>lides: 0.175",
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 5<br>Expert: 38<br>Weight: 0.409<br>Top tokens:<br>iffe: 0.216<br>: 0.200<br>: 0.197<br>: 0.194<br> lud: 0.193",
          null,
          null,
          null,
          "Layer: 5<br>Expert: 42<br>Weight: 0.067<br>Top tokens:<br>ilius: 0.222<br>: 0.200<br>ollers: 0.196<br>itzar: 0.192<br>: 0.190",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 5<br>Expert: 50<br>Weight: 0.037<br>Top tokens:<br>: 0.238<br>ismus: 0.229<br>ocratic: 0.189<br> mas: 0.179<br> : 0.164",
          "Layer: 5<br>Expert: 51<br>Weight: 0.061<br>Top tokens:<br>'::: 0.221<br> : 0.205<br>: 0.203<br>anos: 0.186<br> Mim: 0.185",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 6<br>Expert: 6<br>Weight: 0.069<br>Top tokens:<br>ussen: 0.387<br>onos: 0.184<br>berger: 0.161<br>uster: 0.134<br>tx: 0.133",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 6<br>Expert: 17<br>Weight: 0.039<br>Top tokens:<br>olat: 0.341<br>illed: 0.243<br>: 0.146<br>tona: 0.135<br>ulat: 0.135",
          null,
          null,
          null,
          "Layer: 6<br>Expert: 21<br>Weight: 0.180<br>Top tokens:<br>idel: 0.224<br>Estudi: 0.210<br>ProcAddress: 0.190<br> Schottky: 0.190<br>uju: 0.185",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 6<br>Expert: 34<br>Weight: 0.117<br>Top tokens:<br>eters: 0.276<br>: 0.197<br>VOKE: 0.186<br>bitat: 0.179<br>: 0.162",
          null,
          null,
          "Layer: 6<br>Expert: 37<br>Weight: 0.035<br>Top tokens:<br>ostart: 0.215<br>auce: 0.200<br> : 0.200<br>ockets: 0.197<br>: 0.189",
          null,
          "Layer: 6<br>Expert: 39<br>Weight: 0.089<br>Top tokens:<br>}--: 0.219<br>: 0.205<br>OC: 0.193<br>alan: 0.192<br>ired: 0.190",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 7<br>Expert: 9<br>Weight: 0.149<br>Top tokens:<br>alk: 0.271<br>ito: 0.197<br>ally: 0.183<br>ENC: 0.178<br>iced: 0.172",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 7<br>Expert: 44<br>Weight: 0.075<br>Top tokens:<br>ibus: 0.302<br> Macdonald: 0.205<br> pra: 0.172<br> MacDonald: 0.167<br>lu: 0.154",
          null,
          "Layer: 7<br>Expert: 46<br>Weight: 0.069<br>Top tokens:<br>urus: 0.251<br>eming: 0.232<br>: 0.175<br>esis: 0.173<br>iret: 0.170",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 7<br>Expert: 55<br>Weight: 0.126<br>Top tokens:<br> (: 0.258<br>: 0.243<br> _: 0.174<br> L: 0.165<br> Trent: 0.161",
          null,
          "Layer: 7<br>Expert: 57<br>Weight: 0.054<br>Top tokens:<br>ivistic: 0.239<br>ikipedia: 0.220<br> transmet: 0.191<br>olocation: 0.177<br>ruguai: 0.174",
          null,
          null,
          null,
          null,
          "Layer: 7<br>Expert: 62<br>Weight: 0.085<br>Top tokens:<br>istre: 0.242<br>akala: 0.212<br>lectuals: 0.189<br>onsor: 0.181<br>valor: 0.176",
          null,
          null,
          "Layer: 8<br>Expert: 1<br>Weight: 0.057<br>Top tokens:<br>th: 0.223<br>ether: 0.203<br>ride: 0.202<br>uant: 0.191<br> &: 0.181",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 8<br>Expert: 12<br>Weight: 0.046<br>Top tokens:<br>imentaci: 0.230<br> : 0.199<br>oble: 0.197<br>tbody: 0.195<br> viles: 0.178",
          null,
          null,
          null,
          null,
          "Layer: 8<br>Expert: 17<br>Weight: 0.073<br>Top tokens:<br>: 0.237<br>IQUE: 0.221<br>: 0.195<br>matic: 0.175<br>erent: 0.172",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 8<br>Expert: 33<br>Weight: 0.099<br>Top tokens:<br>guera: 0.354<br>onada: 0.213<br>gue: 0.156<br>onats: 0.141<br>mxd: 0.136",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 8<br>Expert: 46<br>Weight: 0.093<br>Top tokens:<br>nal: 0.268<br>hent: 0.192<br>: 0.188<br>getC: 0.181<br>ollary: 0.172",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 8<br>Expert: 63<br>Weight: 0.061<br>Top tokens:<br>PointerException: 0.259<br>zt: 0.210<br>: 0.191<br>amac: 0.186<br> Moy: 0.154",
          null,
          "Layer: 9<br>Expert: 1<br>Weight: 0.056<br>Top tokens:<br>utant: 0.248<br>adec: 0.209<br>: 0.194<br> companys: 0.190<br> Mosel: 0.159",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 9<br>Expert: 30<br>Weight: 0.091<br>Top tokens:<br>ygon: 0.276<br>orthand: 0.210<br>onstr: 0.200<br>Estudi: 0.158<br>: 0.155",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 9<br>Expert: 48<br>Weight: 0.156<br>Top tokens:<br>ie: 0.282<br>External: 0.225<br>oes: 0.202<br>bang: 0.149<br>external: 0.141",
          "Layer: 9<br>Expert: 49<br>Weight: 0.042<br>Top tokens:<br>ishna: 0.227<br>: 0.213<br>opl: 0.198<br>.(*: 0.197<br>: 0.166",
          null,
          null,
          null,
          "Layer: 9<br>Expert: 53<br>Weight: 0.059<br>Top tokens:<br>omys: 0.244<br>riger: 0.204<br> : 0.203<br>iga: 0.175<br>: 0.174",
          null,
          null,
          null,
          null,
          null,
          "Layer: 9<br>Expert: 59<br>Weight: 0.053<br>Top tokens:<br>: 0.253<br>rivia: 0.211<br>ociety: 0.182<br>: 0.178<br>: 0.177",
          null,
          null,
          null,
          null,
          null,
          "Layer: 10<br>Expert: 1<br>Weight: 0.102<br>Top tokens:<br>right: 0.250<br>: 0.199<br>ideo: 0.195<br>erno: 0.190<br>: 0.166",
          null,
          null,
          null,
          null,
          null,
          "Layer: 10<br>Expert: 7<br>Weight: 0.096<br>Top tokens:<br>Cci: 0.241<br>folios: 0.227<br> internes: 0.193<br> FLA: 0.171<br> variaci: 0.168",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 10<br>Expert: 25<br>Weight: 0.098<br>Top tokens:<br>: 0.440<br>avorite: 0.172<br>utta: 0.138<br>lustr: 0.126<br>etapes: 0.124",
          null,
          null,
          "Layer: 10<br>Expert: 28<br>Weight: 0.036<br>Top tokens:<br>glas: 0.379<br>: 0.163<br>urg: 0.162<br>: 0.154<br>uces: 0.142",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 10<br>Expert: 36<br>Weight: 0.093<br>Top tokens:<br>bered: 0.288<br>ear: 0.212<br>eg: 0.177<br>ADD: 0.169<br>ROM: 0.153",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 10<br>Expert: 49<br>Weight: 0.083<br>Top tokens:<br>anza: 0.320<br>: 0.177<br>spinner: 0.174<br> : 0.173<br> spade: 0.156",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 11<br>Expert: 15<br>Weight: 0.071<br>Top tokens:<br>pole: 0.220<br>: 0.200<br>cked: 0.200<br>Ac: 0.195<br>ins: 0.185",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 11<br>Expert: 28<br>Weight: 0.056<br>Top tokens:<br> H: 0.301<br> statistique: 0.229<br>: 0.174<br>orest: 0.162<br>H: 0.134",
          null,
          null,
          null,
          null,
          "Layer: 11<br>Expert: 33<br>Weight: 0.061<br>Top tokens:<br>sr: 0.251<br>sp: 0.239<br>ecke: 0.191<br>: 0.169<br>sex: 0.149",
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 11<br>Expert: 40<br>Weight: 0.077<br>Top tokens:<br>ames: 0.266<br>herit: 0.227<br>abat: 0.181<br>AMES: 0.172<br>: 0.155",
          null,
          null,
          null,
          "Layer: 11<br>Expert: 44<br>Weight: 0.069<br>Top tokens:<br> i: 0.570<br>anwhile: 0.229<br> ii: 0.101<br>: 0.058<br>reland: 0.043",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 11<br>Expert: 63<br>Weight: 0.071<br>Top tokens:<br>YPT: 0.283<br>ossom: 0.203<br>estir: 0.197<br>: 0.162<br>uega: 0.154",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 12<br>Expert: 7<br>Weight: 0.169<br>Top tokens:<br>: 0.485<br>Msk: 0.181<br>eh: 0.136<br>dor: 0.123<br>wlp: 0.076",
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 12<br>Expert: 14<br>Weight: 0.038<br>Top tokens:<br>istrar: 0.302<br>: 0.186<br>: 0.174<br> Ash: 0.170<br>ving: 0.167",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 12<br>Expert: 32<br>Weight: 0.080<br>Top tokens:<br>: 0.268<br>ingut: 0.196<br> : 0.193<br>ansi: 0.182<br>udis: 0.162",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 12<br>Expert: 47<br>Weight: 0.050<br>Top tokens:<br>rpre: 0.323<br>ciones: 0.177<br>ternoons: 0.171<br>: 0.166<br>kom: 0.164",
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 12<br>Expert: 54<br>Weight: 0.100<br>Top tokens:<br> capd: 0.218<br>: 0.208<br>msgs: 0.198<br>: 0.196<br>lpr: 0.180",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 12<br>Expert: 63<br>Weight: 0.095<br>Top tokens:<br>8: 0.300<br>0: 0.209<br>7: 0.190<br>4: 0.153<br>5: 0.148",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 13<br>Expert: 18<br>Weight: 0.041<br>Top tokens:<br> contemporanis: 0.263<br>ornis: 0.241<br>aggable: 0.193<br>: 0.174<br> present: 0.128",
          null,
          "Layer: 13<br>Expert: 20<br>Weight: 0.087<br>Top tokens:<br>uese: 0.365<br>: 0.319<br>oldt: 0.116<br>: 0.104<br> Stammtafeln: 0.096",
          null,
          null,
          "Layer: 13<br>Expert: 23<br>Weight: 0.058<br>Top tokens:<br>: 0.229<br>kees: 0.215<br>: 0.200<br>isp: 0.188<br>: 0.168",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 13<br>Expert: 37<br>Weight: 0.078<br>Top tokens:<br>: 0.232<br>reason: 0.222<br>: 0.196<br>orical: 0.193<br> sord: 0.157",
          "Layer: 13<br>Expert: 38<br>Weight: 0.141<br>Top tokens:<br>: 0.283<br>ensive: 0.243<br>LAGS: 0.209<br>Hospitalet: 0.141<br> : 0.124",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 13<br>Expert: 59<br>Weight: 0.060<br>Top tokens:<br>pendix: 0.215<br>ymen: 0.213<br>ivot: 0.202<br>ruguai: 0.187<br>gano: 0.184",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 14<br>Expert: 11<br>Weight: 0.095<br>Top tokens:<br>: 0.844<br>ILON: 0.126<br>imentaci: 0.010<br> : 0.010<br>: 0.010",
          "Layer: 14<br>Expert: 12<br>Weight: 0.081<br>Top tokens:<br>verb: 0.235<br>: 0.223<br>: 0.221<br>solic: 0.161<br>: 0.161",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 14<br>Expert: 30<br>Weight: 0.061<br>Top tokens:<br> themselves: 0.263<br>: 0.215<br>: 0.187<br>: 0.168<br> : 0.167",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 14<br>Expert: 44<br>Weight: 0.046<br>Top tokens:<br> : 0.221<br>onada: 0.213<br>umu: 0.209<br>pep: 0.188<br>: 0.169",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 14<br>Expert: 58<br>Weight: 0.162<br>Top tokens:<br>riptor: 0.234<br>skirts: 0.214<br>pan: 0.206<br>: 0.175<br>: 0.171",
          null,
          "Layer: 14<br>Expert: 60<br>Weight: 0.058<br>Top tokens:<br>echa: 0.257<br>cked: 0.210<br>arisation: 0.182<br> suficient: 0.177<br>uko: 0.173",
          null,
          null,
          null,
          null,
          "Layer: 15<br>Expert: 1<br>Weight: 0.098<br>Top tokens:<br>ionista: 0.404<br> : 0.356<br>: 0.088<br>eness: 0.083<br> : 0.069",
          "Layer: 15<br>Expert: 2<br>Weight: 0.088<br>Top tokens:<br>abad: 0.276<br>mel: 0.264<br>olo: 0.156<br>ades: 0.154<br>: 0.150",
          null,
          null,
          null,
          "Layer: 15<br>Expert: 6<br>Weight: 0.038<br>Top tokens:<br>: 0.234<br>phony: 0.209<br> CONSTRAINT: 0.193<br>isabs: 0.182<br>: 0.181",
          null,
          null,
          null,
          null,
          null,
          "Layer: 15<br>Expert: 12<br>Weight: 0.272<br>Top tokens:<br>otis: 0.236<br>appings: 0.208<br>takes: 0.203<br>: 0.177<br>LOPT: 0.175",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 15<br>Expert: 50<br>Weight: 0.084<br>Top tokens:<br>ermat: 0.285<br>aer: 0.200<br>0: 0.178<br>cano: 0.171<br>2: 0.166",
          "Layer: 15<br>Expert: 51<br>Weight: 0.056<br>Top tokens:<br>: 0.357<br>: 0.271<br>: 0.127<br>Trace: 0.125<br>: 0.119",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 16<br>Expert: 9<br>Weight: 0.043<br>Top tokens:<br>odel: 0.222<br> Silver: 0.204<br>ent: 0.193<br>ays: 0.193<br>ctions: 0.188",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 16<br>Expert: 21<br>Weight: 0.077<br>Top tokens:<br>mog: 0.221<br>moz: 0.211<br>: 0.192<br>: 0.189<br>enesis: 0.187",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 16<br>Expert: 29<br>Weight: 0.042<br>Top tokens:<br>omys: 0.326<br>ranos: 0.243<br>irat: 0.169<br>: 0.139<br>: 0.123",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 16<br>Expert: 37<br>Weight: 0.039<br>Top tokens:<br>8: 0.259<br>5: 0.247<br>ce: 0.194<br>3: 0.172<br>6: 0.129",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 16<br>Expert: 50<br>Weight: 0.291<br>Top tokens:<br>ICY: 0.218<br>urb: 0.210<br>l: 0.194<br>ls: 0.189<br>: 0.188",
          "Layer: 16<br>Expert: 51<br>Weight: 0.077<br>Top tokens:<br>: 0.366<br>: 0.316<br> relle: 0.118<br>: 0.105<br>ering: 0.094",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 17<br>Expert: 3<br>Weight: 0.076<br>Top tokens:<br> side: 0.252<br>: 0.210<br>: 0.182<br>etch: 0.179<br>0: 0.177",
          null,
          "Layer: 17<br>Expert: 5<br>Weight: 0.121<br>Top tokens:<br>4: 0.449<br>5: 0.169<br>6: 0.159<br>wide: 0.118<br>7: 0.106",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 17<br>Expert: 19<br>Weight: 0.208<br>Top tokens:<br>rament: 0.242<br>ottom: 0.206<br>tole: 0.186<br>opos: 0.184<br>study: 0.183",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 17<br>Expert: 31<br>Weight: 0.051<br>Top tokens:<br>specs: 0.216<br> volunt: 0.213<br>dered: 0.199<br>kered: 0.197<br>acin: 0.176",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 17<br>Expert: 41<br>Weight: 0.043<br>Top tokens:<br>iament: 0.298<br>illeria: 0.206<br>iboot: 0.181<br>ividual: 0.166<br>clo: 0.150",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 17<br>Expert: 49<br>Weight: 0.072<br>Top tokens:<br>: 0.372<br>: 0.218<br>: 0.165<br>rotron: 0.123<br> : 0.123",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 18<br>Expert: 1<br>Weight: 0.029<br>Top tokens:<br>4: 0.286<br>6: 0.237<br>1: 0.230<br>7: 0.124<br>0: 0.122",
          null,
          null,
          null,
          null,
          null,
          "Layer: 18<br>Expert: 7<br>Weight: 0.077<br>Top tokens:<br> : 0.268<br>malink: 0.218<br>MAN: 0.200<br>ordi: 0.158<br>: 0.157",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 18<br>Expert: 29<br>Weight: 0.450<br>Top tokens:<br>fixes: 0.356<br>facts: 0.271<br>ollow: 0.140<br>amboo: 0.136<br> version: 0.097",
          null,
          "Layer: 18<br>Expert: 31<br>Weight: 0.031<br>Top tokens:<br>xes: 0.272<br>: 0.211<br>: 0.193<br>: 0.166<br>: 0.158",
          null,
          null,
          null,
          null,
          "Layer: 18<br>Expert: 36<br>Weight: 0.074<br>Top tokens:<br>2: 0.327<br>0: 0.291<br>3: 0.136<br>5: 0.126<br>7: 0.119",
          null,
          null,
          null,
          null,
          null,
          "Layer: 18<br>Expert: 42<br>Weight: 0.068<br>Top tokens:<br>short: 0.231<br> SH: 0.226<br>: 0.211<br>by: 0.187<br>uality: 0.145",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 19<br>Expert: 15<br>Weight: 0.029<br>Top tokens:<br> : 0.382<br>series: 0.158<br>: 0.156<br>irections: 0.153<br>Series: 0.151",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 19<br>Expert: 43<br>Weight: 0.333<br>Top tokens:<br>: 0.227<br>: 0.207<br>ists: 0.193<br>: 0.193<br>: 0.180",
          null,
          null,
          "Layer: 19<br>Expert: 46<br>Weight: 0.049<br>Top tokens:<br>ishments: 0.288<br>: 0.232<br>bolds: 0.230<br>Revision: 0.129<br> PARTICULAR: 0.121",
          null,
          "Layer: 19<br>Expert: 48<br>Weight: 0.049<br>Top tokens:<br>ucas: 0.388<br>: 0.168<br>artney: 0.168<br>ucius: 0.138<br>riger: 0.138",
          null,
          "Layer: 19<br>Expert: 50<br>Weight: 0.134<br>Top tokens:<br>ieth: 0.357<br>: 0.198<br>lyph: 0.182<br>: 0.136<br>: 0.127",
          "Layer: 19<br>Expert: 51<br>Weight: 0.031<br>Top tokens:<br>regu: 0.349<br>: 0.191<br>riage: 0.181<br>: 0.154<br>utan: 0.124",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 20<br>Expert: 0<br>Weight: 0.039<br>Top tokens:<br>after: 0.478<br> after: 0.326<br>After: 0.071<br> AFTER: 0.066<br> After: 0.058",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 20<br>Expert: 28<br>Weight: 0.068<br>Top tokens:<br>select: 0.345<br>pon: 0.198<br>sort: 0.198<br> sorts: 0.145<br> select: 0.114",
          null,
          null,
          null,
          null,
          null,
          "Layer: 20<br>Expert: 34<br>Weight: 0.084<br>Top tokens:<br>issos: 0.317<br>ariidae: 0.187<br>: 0.175<br>igg: 0.160<br>: 0.160",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 20<br>Expert: 55<br>Weight: 0.097<br>Top tokens:<br>Msk: 0.373<br>FUNCPTR: 0.208<br>iName: 0.151<br>refn: 0.136<br>firstrow: 0.133",
          null,
          null,
          "Layer: 20<br>Expert: 58<br>Weight: 0.077<br>Top tokens:<br>ovo: 0.450<br>hoff: 0.274<br>geu: 0.104<br>intage: 0.086<br>[--]: 0.086",
          null,
          null,
          null,
          null,
          "Layer: 20<br>Expert: 63<br>Weight: 0.039<br>Top tokens:<br>Cient: 0.255<br> ratio: 0.225<br> Ratio: 0.190<br>ratio: 0.165<br>Ratio: 0.165",
          null,
          null,
          null,
          null,
          null,
          "Layer: 21<br>Expert: 5<br>Weight: 0.129<br>Top tokens:<br>play: 0.593<br>Play: 0.114<br>: 0.114<br>: 0.092<br>PLAY: 0.087",
          null,
          "Layer: 21<br>Expert: 7<br>Weight: 0.071<br>Top tokens:<br>escor: 0.331<br>: 0.219<br>oded: 0.201<br> secs: 0.127<br>WISE: 0.122",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 21<br>Expert: 18<br>Weight: 0.065<br>Top tokens:<br>power: 0.517<br> power: 0.272<br>Power: 0.124<br> Power: 0.080<br> powering: 0.008",
          null,
          null,
          "Layer: 21<br>Expert: 21<br>Weight: 0.045<br>Top tokens:<br>: 0.450<br> : 0.247<br>: 0.124<br>: 0.099<br>: 0.080",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 21<br>Expert: 52<br>Weight: 0.044<br>Top tokens:<br>yd: 0.719<br>que: 0.103<br>hol: 0.075<br>is: 0.053<br> restar: 0.050",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 21<br>Expert: 62<br>Weight: 0.359<br>Top tokens:<br>: 0.402<br> brown: 0.251<br>brown: 0.202<br>: 0.081<br>ACY: 0.064",
          null,
          null,
          null,
          "Layer: 22<br>Expert: 2<br>Weight: 0.069<br>Top tokens:<br>: 0.233<br>Solutions: 0.231<br>: 0.209<br> : 0.179<br>: 0.148",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 22<br>Expert: 10<br>Weight: 0.213<br>Top tokens:<br>jiang: 0.349<br>etria: 0.275<br>enys: 0.180<br>acis: 0.109<br>: 0.087",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 22<br>Expert: 23<br>Weight: 0.040<br>Top tokens:<br>opil: 0.507<br>ab: 0.233<br>ipl: 0.094<br>lai: 0.086<br> coun: 0.081",
          "Layer: 22<br>Expert: 24<br>Weight: 0.076<br>Top tokens:<br>turn: 0.499<br>turns: 0.177<br> turn: 0.160<br> Turn: 0.136<br>Turn: 0.028",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 22<br>Expert: 35<br>Weight: 0.031<br>Top tokens:<br> big: 0.278<br>3: 0.240<br>oku: 0.175<br>6: 0.165<br>7: 0.142",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 22<br>Expert: 62<br>Weight: 0.121<br>Top tokens:<br>acitat: 0.241<br>rop: 0.198<br>: 0.193<br>: 0.190<br>iat: 0.178",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 23<br>Expert: 24<br>Weight: 0.043<br>Top tokens:<br>alion: 0.226<br>initat: 0.212<br>: 0.198<br>otge: 0.198<br>trained: 0.165",
          null,
          "Layer: 23<br>Expert: 26<br>Weight: 0.067<br>Top tokens:<br> low: 0.865<br>Low: 0.047<br>low: 0.041<br> Low: 0.031<br> LOW: 0.016",
          null,
          null,
          "Layer: 23<br>Expert: 29<br>Weight: 0.048<br>Top tokens:<br>:: 0.332<br>Has: 0.265<br>have: 0.159<br>has: 0.129<br>?:: 0.114",
          null,
          null,
          null,
          "Layer: 23<br>Expert: 33<br>Weight: 0.365<br>Top tokens:<br>Standard: 0.270<br>: 0.210<br>ocy: 0.200<br>eri: 0.178<br>asca: 0.141",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 23<br>Expert: 56<br>Weight: 0.034<br>Top tokens:<br>: 0.563<br>low: 0.151<br>pick: 0.124<br>ch: 0.091<br>: 0.071",
          null,
          null,
          null,
          null,
          null,
          "Layer: 23<br>Expert: 62<br>Weight: 0.036<br>Top tokens:<br>: 0.295<br>: 0.271<br>: 0.153<br>: 0.147<br> : 0.133",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 24<br>Expert: 11<br>Weight: 0.094<br>Top tokens:<br>y: 0.320<br>set: 0.274<br>zed: 0.175<br>ing: 0.145<br>ting: 0.086",
          null,
          "Layer: 24<br>Expert: 13<br>Weight: 0.052<br>Top tokens:<br> Fund: 0.892<br> Options: 0.044<br>Option: 0.034<br>Fund: 0.016<br>Options: 0.014",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 24<br>Expert: 22<br>Weight: 0.085<br>Top tokens:<br>NOSCRIPT: 0.496<br>TOOLSET: 0.204<br>Strunz: 0.144<br>anesos: 0.088<br>: 0.067",
          null,
          null,
          null,
          "Layer: 24<br>Expert: 26<br>Weight: 0.051<br>Top tokens:<br> road: 0.845<br>Road: 0.086<br> Road: 0.048<br>road: 0.020<br> roadway: 0.000",
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 24<br>Expert: 33<br>Weight: 0.173<br>Top tokens:<br>assos: 0.378<br>: 0.174<br>: 0.168<br>: 0.142<br>ERS: 0.138",
          null,
          null,
          "Layer: 24<br>Expert: 36<br>Weight: 0.033<br>Top tokens:<br> t: 0.665<br>weight: 0.309<br> weight: 0.018<br> Weight: 0.004<br>\tt: 0.004",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 25<br>Expert: 2<br>Weight: 0.058<br>Top tokens:<br>ime: 0.234<br> : 0.219<br>: 0.186<br> ~[: 0.180<br>ameter: 0.180",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 25<br>Expert: 17<br>Weight: 0.077<br>Top tokens:<br>Nan: 0.364<br>escena: 0.184<br>1: 0.163<br>Dar: 0.148<br>0: 0.141",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 25<br>Expert: 37<br>Weight: 0.076<br>Top tokens:<br> part: 0.496<br>part: 0.287<br>part: 0.106<br>islation: 0.060<br> parte: 0.051",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 25<br>Expert: 49<br>Weight: 0.065<br>Top tokens:<br>DOCKED: 0.290<br>ANCH: 0.212<br>roids: 0.183<br> hisp: 0.167<br>ACE: 0.148",
          null,
          "Layer: 25<br>Expert: 51<br>Weight: 0.084<br>Top tokens:<br>lessly: 0.223<br>tility: 0.220<br>anish: 0.199<br>lated: 0.196<br> //$: 0.161",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 25<br>Expert: 62<br>Weight: 0.053<br>Top tokens:<br>mention: 0.339<br>: 0.175<br>iform: 0.173<br>anes: 0.160<br>ottes: 0.152",
          null,
          null,
          null,
          "Layer: 26<br>Expert: 2<br>Weight: 0.061<br>Top tokens:<br> one: 0.991<br>: 0.007<br>one: 0.001<br>: 0.001<br>PIO: 0.000",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 26<br>Expert: 10<br>Weight: 0.073<br>Top tokens:<br> post: 0.870<br> Post: 0.083<br>post: 0.024<br>Post: 0.012<br> round: 0.012",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 26<br>Expert: 38<br>Weight: 0.217<br>Top tokens:<br> an: 0.504<br>idean: 0.194<br>ajes: 0.106<br>ocation: 0.098<br>umnes: 0.098",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 26<br>Expert: 48<br>Weight: 0.049<br>Top tokens:<br>,: 0.506<br>as: 0.200<br>--: 0.151<br>: 0.076<br>: 0.066",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 26<br>Expert: 60<br>Weight: 0.037<br>Top tokens:<br>itats: 0.225<br>: 0.203<br> judgement: 0.197<br> verdict: 0.194<br>WAYS: 0.180",
          null,
          null,
          "Layer: 26<br>Expert: 63<br>Weight: 0.043<br>Top tokens:<br> dry: 0.286<br> long: 0.233<br>texorpdfstring: 0.228<br>operaci: 0.147<br> deser: 0.105",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 27<br>Expert: 10<br>Weight: 0.137<br>Top tokens:<br>,: 0.300<br> Icel: 0.264<br>est: 0.233<br> : 0.116<br> inhal: 0.087",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 27<br>Expert: 38<br>Weight: 0.036<br>Top tokens:<br> understanding: 0.239<br> event: 0.232<br> piece: 0.230<br> idea: 0.173<br> person: 0.126",
          null,
          null,
          null,
          null,
          "Layer: 27<br>Expert: 43<br>Weight: 0.176<br>Top tokens:<br>man: 0.266<br>cell: 0.253<br>hopper: 0.204<br>mans: 0.151<br>port: 0.126",
          null,
          null,
          null,
          null,
          null,
          "Layer: 27<br>Expert: 49<br>Weight: 0.269<br>Top tokens:<br>outs: 0.259<br>stock: 0.202<br>ball: 0.182<br>book: 0.179<br>bar: 0.177",
          null,
          null,
          null,
          "Layer: 27<br>Expert: 53<br>Weight: 0.061<br>Top tokens:<br> appearance: 0.416<br> mistake: 0.342<br> wound: 0.091<br> appearances: 0.083<br> warning: 0.067",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 27<br>Expert: 63<br>Weight: 0.055<br>Top tokens:<br> dotted: 0.233<br> seam: 0.230<br> div: 0.209<br> branch: 0.182<br> plot: 0.146"
         ],
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63
         ],
         "y": [
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27
         ]
        }
       ],
       "layout": {
        "height": 800,
        "paper_bgcolor": "black",
        "plot_bgcolor": "black",
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#f2f5fa"
            },
            "error_y": {
             "color": "#f2f5fa"
            },
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "baxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#506784"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "header": {
             "fill": {
              "color": "#2a3f5f"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#f2f5fa",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#f2f5fa"
          },
          "geo": {
           "bgcolor": "rgb(17,17,17)",
           "lakecolor": "rgb(17,17,17)",
           "landcolor": "rgb(17,17,17)",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#506784"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "dark"
          },
          "paper_bgcolor": "rgb(17,17,17)",
          "plot_bgcolor": "rgb(17,17,17)",
          "polar": {
           "angularaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "bgcolor": "rgb(17,17,17)",
           "radialaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           },
           "yaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           },
           "zaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#f2f5fa"
           }
          },
          "sliderdefaults": {
           "bgcolor": "#C8D4E3",
           "bordercolor": "rgb(17,17,17)",
           "borderwidth": 1,
           "tickwidth": 0
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "bgcolor": "rgb(17,17,17)",
           "caxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "updatemenudefaults": {
           "bgcolor": "#506784",
           "borderwidth": 0
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Expert Activations for Token \" quick\" at Position 2"
        },
        "width": 1200,
        "xaxis": {
         "gridcolor": "rgba(128, 128, 128, 0.2)",
         "gridwidth": 1,
         "range": [
          -1,
          64
         ],
         "showgrid": true,
         "title": {
          "text": "Expert ID"
         }
        },
        "yaxis": {
         "autorange": "reversed",
         "gridcolor": "rgba(128, 128, 128, 0.2)",
         "gridwidth": 1,
         "showgrid": true,
         "title": {
          "text": "Layer"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing token:  fox\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "text",
         "hovertemplate": "%{text}<extra></extra>",
         "marker": {
          "cmax": 1,
          "cmin": 0,
          "color": [
           0,
           0,
           0,
           0.0341796875,
           0,
           0.1494140625,
           0,
           0,
           0.0819091796875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.034088134765625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.038543701171875,
           0,
           0.1285400390625,
           0,
           0,
           0,
           0.03668212890625,
           0,
           0,
           0.2215576171875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.126220703125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.056793212890625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.060791015625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.032501220703125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.025970458984375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.455322265625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0306549072265625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.07147216796875,
           0.03887939453125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.06573486328125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.025726318359375,
           0,
           0.035919189453125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.1795654296875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.048065185546875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.021209716796875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.469482421875,
           0,
           0,
           0,
           0,
           0,
           0,
           0.04901123046875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.068603515625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0297393798828125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.08148193359375,
           0,
           0,
           0,
           0.483642578125,
           0,
           0,
           0,
           0,
           0.05523681640625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.154541015625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.09918212890625,
           0,
           0,
           0,
           0,
           0,
           0.045501708984375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0538330078125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.20361328125,
           0,
           0,
           0,
           0.1383056640625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.031646728515625,
           0,
           0.031829833984375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0809326171875,
           0,
           0,
           0,
           0,
           0,
           0.06500244140625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.087646484375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.27099609375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0537109375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.07159423828125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.1087646484375,
           0,
           0,
           0.36865234375,
           0,
           0,
           0,
           0,
           0,
           0,
           0.037841796875,
           0.039794921875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.1287841796875,
           0,
           0,
           0,
           0,
           0,
           0.0799560546875,
           0,
           0,
           0,
           0,
           0.1304931640625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.11273193359375,
           0,
           0,
           0,
           0,
           0,
           0.0335693359375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.06561279296875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.051605224609375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.33740234375,
           0,
           0,
           0,
           0.04962158203125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.052764892578125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.091552734375,
           0,
           0.06988525390625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0892333984375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.07000732421875,
           0,
           0,
           0,
           0,
           0,
           0,
           0.04840087890625,
           0.049072265625,
           0,
           0,
           0.13232421875,
           0,
           0.05743408203125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.046630859375,
           0,
           0,
           0,
           0,
           0,
           0.06011962890625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.074462890625,
           0.047698974609375,
           0.054931640625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.211181640625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.049102783203125,
           0,
           0,
           0.032379150390625,
           0,
           0,
           0.0595703125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.19189453125,
           0,
           0,
           0,
           0,
           0.04278564453125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.147705078125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.06488037109375,
           0,
           0,
           0,
           0.0577392578125,
           0,
           0,
           0.069091796875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.06597900390625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.107421875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.068115234375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.1455078125,
           0.062225341796875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.128662109375,
           0,
           0,
           0,
           0.054962158203125,
           0,
           0,
           0,
           0,
           0.05474853515625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.057891845703125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.07293701171875,
           0,
           0,
           0,
           0,
           0.3125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.13623046875,
           0,
           0,
           0.045379638671875,
           0,
           0,
           0,
           0,
           0,
           0,
           0.061920166015625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.031890869140625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.053680419921875,
           0,
           0,
           0,
           0.0347900390625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.292724609375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.034210205078125,
           0,
           0,
           0,
           0,
           0,
           0,
           0.120849609375,
           0,
           0,
           0,
           0,
           0,
           0,
           0.05316162109375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.06951904296875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.053680419921875,
           0,
           0,
           0.203125,
           0,
           0,
           0,
           0,
           0.055206298828125,
           0,
           0,
           0,
           0,
           0,
           0,
           0.13720703125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.044891357421875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.09417724609375,
           0,
           0,
           0,
           0.10443115234375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.05865478515625,
           0,
           0,
           0,
           0,
           0.10406494140625,
           0,
           0.10693359375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0433349609375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.083251953125,
           0,
           0,
           0.06890869140625,
           0,
           0,
           0,
           0,
           0.050506591796875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.123291015625,
           0,
           0.04376220703125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0869140625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0274658203125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.056640625,
           0.314697265625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.1376953125,
           0,
           0,
           0,
           0,
           0.049407958984375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.05731201171875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.057098388671875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.032196044921875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.05096435546875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.07147216796875,
           0,
           0,
           0,
           0.032501220703125,
           0,
           0,
           0,
           0,
           0,
           0,
           0.11688232421875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0950927734375,
           0.11248779296875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.076416015625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.060211181640625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0670166015625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.06292724609375,
           0,
           0,
           0.1033935546875,
           0,
           0,
           0,
           0.07757568359375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.1009521484375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.038482666015625,
           0,
           0,
           0,
           0,
           0.0919189453125,
           0,
           0,
           0,
           0,
           0,
           0.037353515625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.08209228515625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.08148193359375,
           0,
           0.06903076171875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.04791259765625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.212158203125,
           0,
           0,
           0,
           0.04791259765625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.058319091796875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.10479736328125,
           0,
           0,
           0,
           0,
           0.079833984375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0711669921875,
           0,
           0,
           0,
           0.048004150390625,
           0,
           0,
           0,
           0,
           0,
           0,
           0.058074951171875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.07745361328125,
           0,
           0,
           0,
           0,
           0,
           0,
           0.045166015625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0312347412109375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.11798095703125,
           0,
           0,
           0.1695556640625,
           0,
           0,
           0,
           0,
           0,
           0.10699462890625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          "colorbar": {
           "dtick": 0.2,
           "tick0": 0,
           "tickmode": "linear",
           "title": {
            "text": "Weight"
           }
          },
          "colorscale": [
           [
            0,
            "rgba(24, 21, 23, 0.8)"
           ],
           [
            0.0001,
            "rgb(68,1,84)"
           ],
           [
            1,
            "rgb(253,231,37)"
           ]
          ],
          "showscale": true,
          "size": 9
         },
         "mode": "markers",
         "text": [
          null,
          null,
          null,
          "Layer: 1<br>Expert: 3<br>Weight: 0.034<br>Top tokens:<br> R: 0.249<br> C: 0.205<br> S: 0.198<br> : 0.182<br>dashed: 0.166",
          null,
          "Layer: 1<br>Expert: 5<br>Weight: 0.149<br>Top tokens:<br>edom: 0.225<br>: 0.201<br>clud: 0.200<br>quita: 0.195<br> Davy: 0.179",
          null,
          null,
          "Layer: 1<br>Expert: 8<br>Weight: 0.082<br>Top tokens:<br>: 0.248<br>: 0.219<br>: 0.180<br>ractical: 0.177<br>=\"//: 0.176",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 1<br>Expert: 43<br>Weight: 0.034<br>Top tokens:<br>: 0.246<br>ooft: 0.207<br>ondo: 0.191<br>: 0.180<br>ents: 0.176",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 1<br>Expert: 59<br>Weight: 0.039<br>Top tokens:<br>eq: 0.247<br>: 0.209<br>: 0.188<br> : 0.179<br>: 0.176",
          null,
          "Layer: 1<br>Expert: 61<br>Weight: 0.129<br>Top tokens:<br>ront: 0.215<br>opro: 0.208<br>fter: 0.198<br>ugar: 0.198<br> <!--[: 0.181",
          null,
          null,
          null,
          "Layer: 2<br>Expert: 1<br>Weight: 0.037<br>Top tokens:<br>ultors: 0.231<br>: 0.222<br>[--: 0.189<br>precated: 0.185<br>bisbe: 0.173",
          null,
          null,
          "Layer: 2<br>Expert: 4<br>Weight: 0.222<br>Top tokens:<br> s: 0.360<br>chaff: 0.180<br>EXTERNAL: 0.177<br>INLINE: 0.143<br>ifera: 0.141",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 2<br>Expert: 23<br>Weight: 0.126<br>Top tokens:<br>ilateral: 0.215<br> e: 0.213<br>eret: 0.205<br>LaTeX: 0.188<br> Exterior: 0.179",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 2<br>Expert: 32<br>Weight: 0.057<br>Top tokens:<br>uriformes: 0.236<br>ndies: 0.209<br>pload: 0.192<br>CEED: 0.185<br>egl: 0.177",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 2<br>Expert: 43<br>Weight: 0.061<br>Top tokens:<br>ievable: 0.264<br>ordial: 0.199<br>quart: 0.181<br>imuth: 0.179<br>ancel: 0.178",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 2<br>Expert: 54<br>Weight: 0.033<br>Top tokens:<br>oyo: 0.225<br>asz: 0.221<br>usz: 0.188<br>sets: 0.185<br>otine: 0.182",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 3<br>Expert: 1<br>Weight: 0.026<br>Top tokens:<br> present: 0.221<br>: 0.216<br>yntax: 0.195<br> control: 0.185<br>adr: 0.183",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 3<br>Expert: 9<br>Weight: 0.455<br>Top tokens:<br>: 0.315<br>: 0.267<br>uaci: 0.145<br>blink: 0.139<br>idual: 0.134",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 3<br>Expert: 18<br>Weight: 0.031<br>Top tokens:<br>Antecedents: 0.209<br> : 0.201<br>~(\\: 0.200<br>: 0.197<br>zr: 0.193",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 3<br>Expert: 33<br>Weight: 0.071<br>Top tokens:<br>atal: 0.213<br>paramname: 0.205<br>ipschitz: 0.200<br>nsit: 0.195<br>amina: 0.187",
          "Layer: 3<br>Expert: 34<br>Weight: 0.039<br>Top tokens:<br>lef: 0.213<br>annels: 0.206<br> PPA: 0.206<br>: 0.191<br>: 0.185",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 3<br>Expert: 51<br>Weight: 0.066<br>Top tokens:<br>uple: 0.210<br>IMPORTED: 0.208<br>ashian: 0.198<br>strument: 0.194<br>: 0.191",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 4<br>Expert: 8<br>Weight: 0.026<br>Top tokens:<br>: 0.258<br>RN: 0.243<br>vphantom: 0.172<br>: 0.166<br>aneously: 0.161",
          null,
          "Layer: 4<br>Expert: 10<br>Weight: 0.036<br>Top tokens:<br> : 0.228<br>restant: 0.198<br>: 0.194<br>hest: 0.191<br> : 0.189",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 4<br>Expert: 27<br>Weight: 0.180<br>Top tokens:<br> : 0.216<br>escut: 0.209<br>: 0.207<br>mlin: 0.197<br> : 0.172",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 4<br>Expert: 39<br>Weight: 0.048<br>Top tokens:<br>agger: 0.217<br>san: 0.208<br>harp: 0.195<br>rases: 0.191<br> : 0.189",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 4<br>Expert: 47<br>Weight: 0.021<br>Top tokens:<br> : 0.245<br> each: 0.198<br>.\\,: 0.192<br>roquois: 0.189<br>ulsed: 0.176",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 4<br>Expert: 57<br>Weight: 0.469<br>Top tokens:<br>gear: 0.224<br>quo: 0.201<br>illion: 0.195<br>yan: 0.192<br>ntesis: 0.189",
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 5<br>Expert: 0<br>Weight: 0.049<br>Top tokens:<br>virt: 0.236<br> le: 0.198<br> ta: 0.193<br> mon: 0.192<br>: 0.181",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 5<br>Expert: 13<br>Weight: 0.069<br>Top tokens:<br>PROTO: 0.234<br>: 0.208<br>TypeDef: 0.197<br>: 0.182<br>TEL: 0.179",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 5<br>Expert: 22<br>Weight: 0.030<br>Top tokens:<br>: 0.214<br>: 0.209<br>atzem: 0.198<br> : 0.195<br>: 0.184",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 5<br>Expert: 32<br>Weight: 0.081<br>Top tokens:<br> cos: 0.233<br> exp: 0.202<br>: 0.202<br> dev: 0.182<br> brackets: 0.181",
          null,
          null,
          null,
          "Layer: 5<br>Expert: 36<br>Weight: 0.484<br>Top tokens:<br>: 0.243<br>athers: 0.211<br>afone: 0.194<br>accio: 0.178<br>GeneratedMessage: 0.174",
          null,
          null,
          null,
          null,
          "Layer: 5<br>Expert: 41<br>Weight: 0.055<br>Top tokens:<br>dorff: 0.279<br> me: 0.213<br>groovy: 0.177<br> : 0.169<br>agrang: 0.163",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 6<br>Expert: 13<br>Weight: 0.155<br>Top tokens:<br>asma: 0.226<br>wani: 0.217<br>itic: 0.201<br>warz: 0.188<br>ENG: 0.168",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 6<br>Expert: 21<br>Weight: 0.099<br>Top tokens:<br> : 0.232<br>: 0.213<br>widet: 0.203<br>: 0.176<br>topsp: 0.175",
          null,
          null,
          null,
          null,
          null,
          "Layer: 6<br>Expert: 27<br>Weight: 0.046<br>Top tokens:<br>: 0.226<br>: 0.209<br>strap: 0.191<br> h: 0.188<br>;: 0.186",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 6<br>Expert: 37<br>Weight: 0.054<br>Top tokens:<br>: 0.411<br>hedra: 0.184<br>feu: 0.137<br>: 0.135<br>: 0.132",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 6<br>Expert: 54<br>Weight: 0.204<br>Top tokens:<br> xif: 0.237<br>URSS: 0.206<br> aque: 0.196<br> reaccion: 0.182<br>IEEEeqnarray: 0.180",
          null,
          null,
          null,
          "Layer: 6<br>Expert: 58<br>Weight: 0.138<br>Top tokens:<br>osus: 0.229<br>pmyadmin: 0.199<br>: 0.198<br>epsi: 0.194<br>ateral: 0.179",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 7<br>Expert: 11<br>Weight: 0.032<br>Top tokens:<br>ITOR: 0.249<br> ipt: 0.195<br>ned: 0.191<br>iments: 0.190<br>: 0.176",
          null,
          "Layer: 7<br>Expert: 13<br>Weight: 0.032<br>Top tokens:<br>uy: 0.238<br> pat: 0.213<br>: 0.197<br>jon: 0.178<br>: 0.174",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 7<br>Expert: 21<br>Weight: 0.081<br>Top tokens:<br>: 0.229<br> fdisk: 0.215<br>: 0.202<br>tees: 0.177<br>: 0.177",
          null,
          null,
          null,
          null,
          null,
          "Layer: 7<br>Expert: 27<br>Weight: 0.065<br>Top tokens:<br>anda: 0.246<br>ostics: 0.203<br>holds: 0.191<br>: 0.183<br>: 0.177",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 7<br>Expert: 46<br>Weight: 0.088<br>Top tokens:<br>: 0.265<br>: 0.209<br>: 0.191<br>: 0.172<br>: 0.162",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 7<br>Expert: 56<br>Weight: 0.271<br>Top tokens:<br> condicions: 0.224<br>: 0.214<br>TERM: 0.190<br>: 0.188<br>ourn: 0.183",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 8<br>Expert: 0<br>Weight: 0.054<br>Top tokens:<br>ophile: 0.209<br>: 0.205<br> Teod: 0.203<br>leep: 0.198<br> Segura: 0.185",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 8<br>Expert: 20<br>Weight: 0.072<br>Top tokens:<br>igr: 0.242<br>EZ: 0.235<br> trailing: 0.177<br>: 0.176<br>: 0.170",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 8<br>Expert: 32<br>Weight: 0.109<br>Top tokens:<br>oldt: 0.385<br>: 0.180<br>: 0.155<br>ineq: 0.146<br>abet: 0.134",
          null,
          null,
          "Layer: 8<br>Expert: 35<br>Weight: 0.369<br>Top tokens:<br>oiselle: 0.274<br> Pillow: 0.223<br>ostic: 0.181<br>INESS: 0.161<br>izar: 0.161",
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 8<br>Expert: 42<br>Weight: 0.038<br>Top tokens:<br>eting: 0.231<br>oto: 0.229<br>ings: 0.193<br>: 0.189<br>otos: 0.158",
          "Layer: 8<br>Expert: 43<br>Weight: 0.040<br>Top tokens:<br> : 0.252<br>rtype: 0.200<br>ctxt: 0.197<br>instrument: 0.177<br>ressos: 0.174",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 9<br>Expert: 4<br>Weight: 0.129<br>Top tokens:<br>ugar: 0.452<br>ardt: 0.161<br>: 0.141<br>: 0.129<br>ournal: 0.116",
          null,
          null,
          null,
          null,
          null,
          "Layer: 9<br>Expert: 10<br>Weight: 0.080<br>Top tokens:<br> \": 0.257<br> har: 0.209<br> ': 0.185<br> *: 0.175<br> : 0.175",
          null,
          null,
          null,
          null,
          "Layer: 9<br>Expert: 15<br>Weight: 0.130<br>Top tokens:<br>: 0.280<br> : 0.274<br>forgotten: 0.156<br>cheek: 0.147<br>: 0.142",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 9<br>Expert: 37<br>Weight: 0.113<br>Top tokens:<br>ignite: 0.260<br>orry: 0.220<br>ifax: 0.206<br>: 0.180<br>: 0.134",
          null,
          null,
          null,
          null,
          null,
          "Layer: 9<br>Expert: 43<br>Weight: 0.034<br>Top tokens:<br>: 0.245<br>: 0.237<br> passat: 0.229<br>oze: 0.146<br>nteg: 0.143",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 9<br>Expert: 59<br>Weight: 0.066<br>Top tokens:<br>Ungrouped: 0.246<br>: 0.243<br>refn: 0.175<br>rfloor: 0.171<br>)|$: 0.165",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 10<br>Expert: 4<br>Weight: 0.052<br>Top tokens:<br>: 0.280<br>: 0.198<br>avid: 0.186<br>: 0.174<br>: 0.163",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 10<br>Expert: 19<br>Weight: 0.337<br>Top tokens:<br>: 0.253<br>agut: 0.212<br>: 0.183<br> novel: 0.178<br>AAAAAAAAAAAAAAAA: 0.174",
          null,
          null,
          null,
          "Layer: 10<br>Expert: 23<br>Weight: 0.050<br>Top tokens:<br>ornia: 0.451<br>{``: 0.145<br> : 0.140<br>Olot: 0.140<br> solars: 0.124",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 10<br>Expert: 45<br>Weight: 0.053<br>Top tokens:<br>: 0.231<br>: 0.227<br>anta: 0.227<br>icolon: 0.161<br> TAU: 0.154",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 10<br>Expert: 58<br>Weight: 0.092<br>Top tokens:<br>mere: 0.280<br>zul: 0.237<br>: 0.171<br>CONDS: 0.164<br>Ljava: 0.149",
          null,
          "Layer: 10<br>Expert: 60<br>Weight: 0.070<br>Top tokens:<br>: 0.264<br>: 0.205<br>outh: 0.183<br>SIGN: 0.183<br>IFF: 0.164",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 11<br>Expert: 11<br>Weight: 0.089<br>Top tokens:<br>elled: 0.298<br> C: 0.235<br>atges: 0.181<br>: 0.149<br> CE: 0.136",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 11<br>Expert: 21<br>Weight: 0.070<br>Top tokens:<br>: 0.303<br>hyde: 0.179<br>: 0.179<br>: 0.173<br> rap: 0.167",
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 11<br>Expert: 28<br>Weight: 0.048<br>Top tokens:<br>lood: 0.495<br>ssica: 0.152<br>onada: 0.123<br>tats: 0.118<br>ntil: 0.113",
          "Layer: 11<br>Expert: 29<br>Weight: 0.049<br>Top tokens:<br>plete: 0.295<br>UBE: 0.213<br>escue: 0.172<br>ycle: 0.167<br>: 0.152",
          null,
          null,
          "Layer: 11<br>Expert: 32<br>Weight: 0.132<br>Top tokens:<br>out: 0.247<br>/: 0.202<br> extent: 0.189<br>: 0.184<br>rolls: 0.178",
          null,
          "Layer: 11<br>Expert: 34<br>Weight: 0.057<br>Top tokens:<br> : 0.255<br>iades: 0.229<br>: 0.196<br>adol: 0.160<br>iak: 0.159",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 12<br>Expert: 3<br>Weight: 0.047<br>Top tokens:<br>enir: 0.291<br>Occ: 0.243<br>: 0.183<br>renta: 0.152<br>FD: 0.131",
          null,
          null,
          null,
          null,
          null,
          "Layer: 12<br>Expert: 9<br>Weight: 0.060<br>Top tokens:<br>iamond: 0.229<br>: 0.208<br> aapt: 0.192<br>ota: 0.191<br>: 0.180",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 12<br>Expert: 18<br>Weight: 0.074<br>Top tokens:<br>aulay: 0.268<br> Borde: 0.207<br>ustral: 0.182<br>guien: 0.175<br>uals: 0.168",
          "Layer: 12<br>Expert: 19<br>Weight: 0.048<br>Top tokens:<br>: 0.257<br>ipel: 0.200<br>eur: 0.184<br>irut: 0.182<br> Editions: 0.177",
          "Layer: 12<br>Expert: 20<br>Weight: 0.055<br>Top tokens:<br>moz: 0.253<br>udis: 0.229<br>ornis: 0.175<br>: 0.174<br>: 0.169",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 12<br>Expert: 36<br>Weight: 0.211<br>Top tokens:<br>ILED: 0.289<br>ygon: 0.194<br>iterr: 0.192<br>aure: 0.185<br>ospital: 0.141",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 13<br>Expert: 4<br>Weight: 0.049<br>Top tokens:<br>: 0.298<br>: 0.238<br>ocol: 0.179<br>uli: 0.146<br>lax: 0.139",
          null,
          null,
          "Layer: 13<br>Expert: 7<br>Weight: 0.032<br>Top tokens:<br>: 0.302<br>: 0.225<br>yst: 0.220<br>erk: 0.127<br>: 0.125",
          null,
          null,
          "Layer: 13<br>Expert: 10<br>Weight: 0.060<br>Top tokens:<br>crest: 0.298<br>: 0.191<br>istre: 0.187<br>var: 0.166<br>ikip: 0.158",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 13<br>Expert: 18<br>Weight: 0.192<br>Top tokens:<br>olla: 0.334<br> dispersa: 0.177<br>: 0.174<br>orr: 0.159<br> falla: 0.156",
          null,
          null,
          null,
          null,
          "Layer: 13<br>Expert: 23<br>Weight: 0.043<br>Top tokens:<br>ouz: 0.258<br>geu: 0.218<br>: 0.198<br>astre: 0.170<br>astres: 0.156",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 13<br>Expert: 38<br>Weight: 0.148<br>Top tokens:<br>duc: 0.277<br>: 0.198<br>: 0.189<br>ugin: 0.188<br>: 0.147",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 14<br>Expert: 9<br>Weight: 0.065<br>Top tokens:<br>estions: 0.233<br> : 0.229<br>: 0.205<br>: 0.167<br> : 0.167",
          null,
          null,
          null,
          "Layer: 14<br>Expert: 13<br>Weight: 0.058<br>Top tokens:<br> punta: 0.230<br> propriet: 0.220<br> : 0.206<br> ficci: 0.174<br>etext: 0.170",
          null,
          null,
          "Layer: 14<br>Expert: 16<br>Weight: 0.069<br>Top tokens:<br>: 0.301<br>atural: 0.227<br>:///: 0.162<br>: 0.158<br>aturally: 0.152",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 14<br>Expert: 24<br>Weight: 0.066<br>Top tokens:<br> only: 0.299<br> item: 0.211<br>ka: 0.202<br>tri: 0.145<br>oh: 0.143",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 14<br>Expert: 44<br>Weight: 0.107<br>Top tokens:<br>9: 0.268<br>0: 0.262<br>7: 0.160<br>6: 0.158<br>8: 0.151",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 14<br>Expert: 52<br>Weight: 0.068<br>Top tokens:<br>: 0.244<br>amena: 0.241<br>: 0.194<br>acao: 0.171<br>nexi: 0.150",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 15<br>Expert: 2<br>Weight: 0.146<br>Top tokens:<br>0: 0.352<br>7: 0.198<br>9: 0.163<br>4: 0.157<br>2: 0.130",
          "Layer: 15<br>Expert: 3<br>Weight: 0.062<br>Top tokens:<br>aband: 0.280<br>CID: 0.201<br>adera: 0.187<br>culo: 0.169<br> Adels: 0.163",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 15<br>Expert: 21<br>Weight: 0.129<br>Top tokens:<br>colhead: 0.317<br>: 0.191<br>rophe: 0.179<br>ku: 0.162<br>ygon: 0.151",
          null,
          null,
          null,
          "Layer: 15<br>Expert: 25<br>Weight: 0.055<br>Top tokens:<br>aye: 0.245<br>on: 0.232<br>jo: 0.215<br>: 0.155<br>acom: 0.153",
          null,
          null,
          null,
          null,
          "Layer: 15<br>Expert: 30<br>Weight: 0.055<br>Top tokens:<br>: 0.281<br>9: 0.206<br>1: 0.184<br>: 0.180<br>asc: 0.149",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 15<br>Expert: 61<br>Weight: 0.058<br>Top tokens:<br>: 0.394<br>: 0.211<br>epsi: 0.166<br>dfp: 0.127<br>: 0.103",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 16<br>Expert: 5<br>Weight: 0.073<br>Top tokens:<br> : 0.256<br>: 0.255<br>: 0.195<br>: 0.161<br>: 0.134",
          null,
          null,
          null,
          null,
          "Layer: 16<br>Expert: 10<br>Weight: 0.312<br>Top tokens:<br>0: 0.343<br>8: 0.201<br>5: 0.181<br>2: 0.138<br>1: 0.137",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 16<br>Expert: 27<br>Weight: 0.136<br>Top tokens:<br> ter: 0.377<br> Ter: 0.299<br> ret: 0.213<br>Ter: 0.059<br>bre: 0.051",
          null,
          null,
          "Layer: 16<br>Expert: 30<br>Weight: 0.045<br>Top tokens:<br>: 0.234<br>: 0.231<br>iosync: 0.190<br>ording: 0.174<br>: 0.172",
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 16<br>Expert: 37<br>Weight: 0.062<br>Top tokens:<br>upp: 0.244<br>: 0.224<br>raphics: 0.218<br>UFF: 0.157<br>{}): 0.157",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 16<br>Expert: 47<br>Weight: 0.032<br>Top tokens:<br>ffected: 0.219<br>refore: 0.217<br> TRI: 0.210<br> once: 0.179<br>abytes: 0.176",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 17<br>Expert: 12<br>Weight: 0.054<br>Top tokens:<br>: 0.572<br>amet: 0.133<br>/__: 0.125<br>isSet: 0.088<br>chiev: 0.082",
          null,
          null,
          null,
          "Layer: 17<br>Expert: 16<br>Weight: 0.035<br>Top tokens:<br> crossorigin: 0.466<br>ESCO: 0.156<br>autilus: 0.132<br>: 0.123<br>IMIT: 0.122",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 17<br>Expert: 31<br>Weight: 0.293<br>Top tokens:<br>0: 0.307<br>2: 0.242<br>1: 0.181<br>4: 0.141<br>5: 0.129",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 17<br>Expert: 41<br>Weight: 0.034<br>Top tokens:<br>: 0.311<br>rality: 0.190<br>alysis: 0.180<br>ilitat: 0.169<br>gsettings: 0.150",
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 17<br>Expert: 48<br>Weight: 0.121<br>Top tokens:<br>es: 0.938<br>s: 0.050<br>ie: 0.005<br>ter: 0.004<br> adm: 0.002",
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 17<br>Expert: 55<br>Weight: 0.053<br>Top tokens:<br> : 0.255<br>: 0.212<br>: 0.202<br>mith: 0.180<br>ServiceProvider: 0.152",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 18<br>Expert: 17<br>Weight: 0.070<br>Top tokens:<br>TOOLSET: 0.315<br>FUNCPTR: 0.197<br>APIENTRY: 0.197<br>bigarray: 0.149<br>paramtype: 0.142",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 18<br>Expert: 28<br>Weight: 0.054<br>Top tokens:<br> flyback: 0.325<br>nore: 0.229<br>orters: 0.206<br>: 0.126<br>: 0.115",
          null,
          null,
          "Layer: 18<br>Expert: 31<br>Weight: 0.203<br>Top tokens:<br> v: 0.316<br> r: 0.199<br> n: 0.172<br> : 0.163<br> p: 0.150",
          null,
          null,
          null,
          null,
          "Layer: 18<br>Expert: 36<br>Weight: 0.055<br>Top tokens:<br>isses: 0.324<br>arms: 0.202<br>openg: 0.167<br>3: 0.158<br>: 0.148",
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 18<br>Expert: 43<br>Weight: 0.137<br>Top tokens:<br>etimes: 0.319<br>: 0.225<br>ividual: 0.213<br>: 0.129<br>eza: 0.114",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 18<br>Expert: 55<br>Weight: 0.045<br>Top tokens:<br> : 0.383<br>hoe: 0.171<br>jna: 0.152<br> Fabra: 0.150<br>ramfs: 0.144",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 19<br>Expert: 3<br>Weight: 0.094<br>Top tokens:<br> comple: 0.267<br> coc: 0.247<br>avera: 0.224<br>itons: 0.133<br>oserver: 0.130",
          null,
          null,
          null,
          "Layer: 19<br>Expert: 7<br>Weight: 0.104<br>Top tokens:<br>: 0.229<br> : 0.204<br>ori: 0.199<br>iber: 0.195<br>\\!\\!\\!\\!: 0.174",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 19<br>Expert: 33<br>Weight: 0.059<br>Top tokens:<br>: 0.277<br>: 0.188<br>lega: 0.180<br> rem: 0.179<br> Pillow: 0.177",
          null,
          null,
          null,
          null,
          "Layer: 19<br>Expert: 38<br>Weight: 0.104<br>Top tokens:<br>osted: 0.243<br>acenter: 0.222<br>upper: 0.186<br>cross: 0.181<br> apoder: 0.169",
          null,
          "Layer: 19<br>Expert: 40<br>Weight: 0.107<br>Top tokens:<br>lau: 0.292<br>: 0.213<br>Bit: 0.196<br> : 0.156<br> records: 0.143",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 19<br>Expert: 56<br>Weight: 0.043<br>Top tokens:<br>know: 0.319<br>knows: 0.190<br>utenberg: 0.184<br>interp: 0.166<br>Know: 0.142",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 20<br>Expert: 26<br>Weight: 0.083<br>Top tokens:<br> that: 0.886<br>\tthat: 0.073<br>that: 0.034<br>That: 0.004<br> That: 0.002",
          null,
          null,
          "Layer: 20<br>Expert: 29<br>Weight: 0.069<br>Top tokens:<br> return: 0.334<br>: 0.213<br> Carry: 0.173<br>return: 0.173<br>: 0.107",
          null,
          null,
          null,
          null,
          "Layer: 20<br>Expert: 34<br>Weight: 0.051<br>Top tokens:<br>1: 0.403<br>4: 0.175<br>9: 0.159<br>5: 0.148<br>3: 0.115",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 20<br>Expert: 42<br>Weight: 0.123<br>Top tokens:<br> debes: 0.376<br> Pop: 0.225<br>erala: 0.147<br>ages: 0.128<br> POP: 0.123",
          null,
          "Layer: 20<br>Expert: 44<br>Weight: 0.044<br>Top tokens:<br>oka: 0.404<br>}^{[: 0.156<br>omi: 0.156<br>ama: 0.150<br>ICES: 0.135",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 20<br>Expert: 56<br>Weight: 0.087<br>Top tokens:<br>: 0.356<br>: 0.203<br>: 0.177<br>iennes: 0.151<br> lim: 0.114",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 21<br>Expert: 22<br>Weight: 0.027<br>Top tokens:<br> Foods: 0.246<br>: 0.220<br>communic: 0.184<br>yre: 0.182<br>: 0.168",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 21<br>Expert: 32<br>Weight: 0.057<br>Top tokens:<br>aviera: 0.258<br>walk: 0.228<br>: 0.178<br>walker: 0.174<br>: 0.162",
          "Layer: 21<br>Expert: 33<br>Weight: 0.315<br>Top tokens:<br>igh: 0.317<br>ently: 0.230<br>ENTRY: 0.192<br>hopper: 0.131<br>entry: 0.129",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 21<br>Expert: 42<br>Weight: 0.138<br>Top tokens:<br>0: 0.381<br>: 0.226<br>1: 0.186<br>an: 0.105<br>: 0.103",
          null,
          null,
          null,
          null,
          "Layer: 21<br>Expert: 47<br>Weight: 0.049<br>Top tokens:<br>: 0.379<br>na: 0.230<br>: 0.200<br>PC: 0.096<br>mi: 0.095",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 21<br>Expert: 58<br>Weight: 0.057<br>Top tokens:<br>: 0.281<br>: 0.233<br>EventListener: 0.190<br>: 0.162<br>: 0.135",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 22<br>Expert: 3<br>Weight: 0.057<br>Top tokens:<br>: 0.555<br> : 0.345<br>: 0.048<br>: 0.041<br>: 0.011",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 22<br>Expert: 28<br>Weight: 0.032<br>Top tokens:<br>Schmidt: 0.342<br>raim: 0.292<br>: 0.139<br>: 0.121<br>reys: 0.106",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 22<br>Expert: 41<br>Weight: 0.051<br>Top tokens:<br> stand: 0.651<br> Stand: 0.162<br> sit: 0.104<br>Stand: 0.060<br>: 0.023",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 22<br>Expert: 51<br>Weight: 0.071<br>Top tokens:<br> will: 0.625<br> WILL: 0.104<br>cmark: 0.103<br>uega: 0.085<br>: 0.083",
          null,
          null,
          null,
          "Layer: 22<br>Expert: 55<br>Weight: 0.033<br>Top tokens:<br>;: 0.480<br> ;: 0.383<br>();: 0.094<br>2: 0.022<br>;</: 0.021",
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 22<br>Expert: 62<br>Weight: 0.117<br>Top tokens:<br> by: 0.748<br>by: 0.080<br>ply: 0.075<br> munt: 0.055<br>FORM: 0.041",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 23<br>Expert: 9<br>Weight: 0.095<br>Top tokens:<br> around: 0.555<br>around: 0.264<br> round: 0.091<br>round: 0.046<br> called: 0.044",
          "Layer: 23<br>Expert: 10<br>Weight: 0.112<br>Top tokens:<br>: 0.536<br>dig: 0.155<br> Ngram: 0.108<br>illings: 0.102<br>DOCKED: 0.099",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 23<br>Expert: 27<br>Weight: 0.076<br>Top tokens:<br>: 0.491<br> q: 0.223<br>: 0.104<br>: 0.096<br>: 0.086",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 23<br>Expert: 35<br>Weight: 0.060<br>Top tokens:<br>paid: 0.301<br>: 0.236<br>: 0.188<br>iless: 0.139<br>: 0.136",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 23<br>Expert: 43<br>Weight: 0.067<br>Top tokens:<br> like: 0.245<br>Like: 0.204<br> Like: 0.194<br>enci: 0.181<br> need: 0.177",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 23<br>Expert: 62<br>Weight: 0.063<br>Top tokens:<br>@: 0.547<br> @: 0.234<br>rowse: 0.076<br>: 0.075<br>@-: 0.068",
          null,
          null,
          "Layer: 24<br>Expert: 1<br>Weight: 0.103<br>Top tokens:<br>ierarchy: 0.423<br>NonUser: 0.370<br>: 0.109<br> relle: 0.054<br>etapes: 0.044",
          null,
          null,
          null,
          "Layer: 24<br>Expert: 5<br>Weight: 0.078<br>Top tokens:<br>run: 0.411<br>Run: 0.270<br> Run: 0.194<br> RUN: 0.080<br> run: 0.045",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 24<br>Expert: 13<br>Weight: 0.101<br>Top tokens:<br>jean: 0.409<br>Download: 0.165<br>: 0.148<br>legr: 0.145<br>oris: 0.133",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 24<br>Expert: 27<br>Weight: 0.038<br>Top tokens:<br>: 0.604<br>--------------------------------------------------------------------------: 0.136<br>\t\t\t\t\t\t\t\t\t\t\t\t\t: 0.120<br>niques: 0.073<br>=-=-=-=-: 0.067",
          null,
          null,
          null,
          null,
          "Layer: 24<br>Expert: 32<br>Weight: 0.092<br>Top tokens:<br>...\\: 0.607<br>''\\: 0.152<br>=\"../../../../../../../../: 0.091<br>\")));: 0.077<br>..\\: 0.074",
          null,
          null,
          null,
          null,
          null,
          "Layer: 24<br>Expert: 38<br>Weight: 0.037<br>Top tokens:<br>imath: 0.398<br>: 0.265<br>uelo: 0.120<br>WireFormat: 0.111<br>uffed: 0.106",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 25<br>Expert: 5<br>Weight: 0.082<br>Top tokens:<br>onada: 0.503<br>: 0.166<br>icode: 0.149<br>: 0.110<br>codep: 0.073",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 25<br>Expert: 17<br>Weight: 0.081<br>Top tokens:<br>vider: 0.321<br>: 0.217<br> <\\: 0.186<br>ickr: 0.169<br>: 0.106",
          null,
          "Layer: 25<br>Expert: 19<br>Weight: 0.069<br>Top tokens:<br> [: 0.970<br>\t[: 0.013<br> ,[: 0.009<br>ifolds: 0.005<br>IEW: 0.004",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 25<br>Expert: 39<br>Weight: 0.048<br>Top tokens:<br>anell: 0.386<br> rose: 0.261<br>LAY: 0.148<br>ocese: 0.128<br>reth: 0.077",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 25<br>Expert: 51<br>Weight: 0.212<br>Top tokens:<br>tainment: 0.232<br>isat: 0.227<br>cov: 0.205<br>: 0.176<br>: 0.160",
          null,
          null,
          null,
          "Layer: 25<br>Expert: 55<br>Weight: 0.048<br>Top tokens:<br>August: 0.256<br>October: 0.216<br>April: 0.192<br>March: 0.180<br>February: 0.156",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 26<br>Expert: 0<br>Weight: 0.058<br>Top tokens:<br> be: 0.603<br> make: 0.164<br> made: 0.121<br>be: 0.070<br>make: 0.043",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 26<br>Expert: 13<br>Weight: 0.105<br>Top tokens:<br> sure: 0.236<br>ernel: 0.236<br> truly: 0.223<br> just: 0.190<br> totally: 0.115",
          null,
          null,
          null,
          null,
          "Layer: 26<br>Expert: 18<br>Weight: 0.080<br>Top tokens:<br> triplets: 0.293<br>der: 0.225<br> Nation: 0.188<br> nation: 0.147<br>: 0.146",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 26<br>Expert: 44<br>Weight: 0.071<br>Top tokens:<br>: 0.313<br>: 0.194<br> : 0.181<br>: 0.161<br> routine: 0.151",
          null,
          null,
          null,
          "Layer: 26<br>Expert: 48<br>Weight: 0.048<br>Top tokens:<br>9: 0.400<br>8: 0.198<br>7: 0.147<br>6: 0.145<br>5: 0.111",
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 26<br>Expert: 55<br>Weight: 0.058<br>Top tokens:<br>-: 0.980<br> -: 0.014<br>/: 0.003<br> /: 0.002<br>: 0.001",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 27<br>Expert: 2<br>Weight: 0.077<br>Top tokens:<br>res: 0.233<br>So: 0.197<br>By: 0.195<br>Edit: 0.195<br>put: 0.180",
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 27<br>Expert: 9<br>Weight: 0.045<br>Top tokens:<br>\": 0.415<br>: 0.244<br>: 0.135<br>\": 0.112<br>: 0.094",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 27<br>Expert: 23<br>Weight: 0.031<br>Top tokens:<br> post: 0.534<br> link: 0.297<br> article: 0.059<br> permalink: 0.058<br> posts: 0.052",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer: 27<br>Expert: 34<br>Weight: 0.118<br>Top tokens:<br> turned: 0.370<br>turned: 0.178<br> expected: 0.178<br> thought: 0.150<br> stopped: 0.125",
          null,
          null,
          "Layer: 27<br>Expert: 37<br>Weight: 0.170<br>Top tokens:<br> ends: 0.274<br> works: 0.263<br> looks: 0.215<br> lands: 0.124<br> lives: 0.124",
          null,
          null,
          null,
          null,
          null,
          "Layer: 27<br>Expert: 43<br>Weight: 0.107<br>Top tokens:<br>men: 0.432<br>iness: 0.151<br>y: 0.148<br>bars: 0.139<br>rates: 0.131",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null
         ],
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63
         ],
         "y": [
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27
         ]
        }
       ],
       "layout": {
        "height": 800,
        "paper_bgcolor": "black",
        "plot_bgcolor": "black",
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#f2f5fa"
            },
            "error_y": {
             "color": "#f2f5fa"
            },
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "baxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#506784"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "header": {
             "fill": {
              "color": "#2a3f5f"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#f2f5fa",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#f2f5fa"
          },
          "geo": {
           "bgcolor": "rgb(17,17,17)",
           "lakecolor": "rgb(17,17,17)",
           "landcolor": "rgb(17,17,17)",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#506784"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "dark"
          },
          "paper_bgcolor": "rgb(17,17,17)",
          "plot_bgcolor": "rgb(17,17,17)",
          "polar": {
           "angularaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "bgcolor": "rgb(17,17,17)",
           "radialaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           },
           "yaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           },
           "zaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#f2f5fa"
           }
          },
          "sliderdefaults": {
           "bgcolor": "#C8D4E3",
           "bordercolor": "rgb(17,17,17)",
           "borderwidth": 1,
           "tickwidth": 0
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "bgcolor": "rgb(17,17,17)",
           "caxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "updatemenudefaults": {
           "bgcolor": "#506784",
           "borderwidth": 0
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Expert Activations for Token \" fox\" at Position 4"
        },
        "width": 1200,
        "xaxis": {
         "gridcolor": "rgba(128, 128, 128, 0.2)",
         "gridwidth": 1,
         "range": [
          -1,
          64
         ],
         "showgrid": true,
         "title": {
          "text": "Expert ID"
         }
        },
        "yaxis": {
         "autorange": "reversed",
         "gridcolor": "rgba(128, 128, 128, 0.2)",
         "gridwidth": 1,
         "showgrid": true,
         "title": {
          "text": "Layer"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_layer_analysis(tokenizer, results, token_position=2, input_text=text)\n",
    "visualize_layer_analysis(tokenizer, results, token_position=4, input_text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
