{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"Get the optimal available device\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        # Enable TF32 for better performance on Ampere GPUs (A100, A6000, etc)\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        # Set memory allocation settings\n",
    "        torch.cuda.empty_cache()\n",
    "        # Enable CUDNN benchmarking for better performance\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOEExpertLens:\n",
    "    def __init__(self, model_name: str, device=None):\n",
    "        \"\"\"Initialize the MoE Expert analyzer.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name/path of the DeepSeek MoE model\n",
    "            device: Optional device to use. If None, will use get_device()\n",
    "        \"\"\"\n",
    "        self.device = device if device is not None else get_device()\n",
    "        \n",
    "        # Load model and move to device\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Get the embedding matrix for reverse projection\n",
    "        self.embed_matrix = self.model.get_input_embeddings().weight\n",
    "        \n",
    "        # Store expert outputs and routing decisions\n",
    "        self.expert_outputs = []  # Will store (layer_idx, token_idx, expert_idx, output)\n",
    "        self.selected_experts = []  # Will store (layer_idx, token_idx, [expert_indices])\n",
    "        self.register_hooks()\n",
    "\n",
    "    def register_hooks(self):\n",
    "        \"\"\"Register forward hooks to capture expert outputs and routing decisions.\"\"\"\n",
    "        def expert_output_hook(module, input, output, expert_idx):\n",
    "            \"\"\"Hook to capture individual expert outputs.\"\"\"\n",
    "            # Store the raw expert output before any aggregation\n",
    "            self.expert_outputs.append((\n",
    "                self.current_layer,\n",
    "                self.current_token,\n",
    "                expert_idx,\n",
    "                output.detach().cpu()  # Move to CPU to save GPU memory\n",
    "            ))\n",
    "            return output\n",
    "\n",
    "        def router_hook(module, input, output):\n",
    "            \"\"\"Hook to capture router decisions.\"\"\"\n",
    "            # Get top-7 expert selections for each token\n",
    "            gate_logits = output\n",
    "            top_k_experts = torch.topk(gate_logits, k=7, dim=-1)\n",
    "            \n",
    "            for token_idx, token_experts in enumerate(top_k_experts.indices):\n",
    "                self.selected_experts.append((\n",
    "                    self.current_layer,\n",
    "                    token_idx,\n",
    "                    token_experts.cpu().tolist()  # Move to CPU to save GPU memory\n",
    "                ))\n",
    "\n",
    "        # Register hooks for each layer and expert\n",
    "        for layer_idx, layer in enumerate(self.model.model.layers):\n",
    "            # Hook for router\n",
    "            layer.mlp.gate.register_forward_hook(router_hook)\n",
    "            \n",
    "            # Hook for each expert\n",
    "            for expert_idx in range(63):  # Excluding shared expert\n",
    "                expert = layer.mlp.experts[expert_idx]\n",
    "                expert.register_forward_hook(\n",
    "                    lambda mod, inp, out, ei=expert_idx: expert_output_hook(mod, inp, out, ei)\n",
    "                )\n",
    "\n",
    "    def _project_to_vocab(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Project hidden states to vocabulary space using embedding matrix.\"\"\"\n",
    "        hidden_states = hidden_states.to(self.device)\n",
    "        return F.linear(hidden_states, self.embed_matrix)\n",
    "\n",
    "    def get_top_k_tokens(self, logits: torch.Tensor, k: int = 5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Convert logits to top-k tokens and their probabilities.\"\"\"\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        top_k_values, top_k_indices = torch.topk(probs, k, dim=-1)\n",
    "        \n",
    "        results = []\n",
    "        for val, idx in zip(top_k_values.cpu(), top_k_indices.cpu()):\n",
    "            token = self.tokenizer.decode([idx])\n",
    "            results.append((token, val.item()))\n",
    "        return results\n",
    "\n",
    "    @torch.cuda.amp.autocast()  # Enable automatic mixed precision\n",
    "    def analyze_text(self, text: str, k: int = 5) -> Dict:\n",
    "        \"\"\"Perform expert analysis on input text.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to analyze\n",
    "            k: Number of top tokens to return for each expert\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing analysis results for each layer\n",
    "        \"\"\"\n",
    "        self.expert_outputs = []\n",
    "        self.selected_experts = []\n",
    "        \n",
    "        # Process input\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Run inference while collecting expert outputs\n",
    "        with torch.no_grad():\n",
    "            for layer_idx in range(len(self.model.model.layers)):\n",
    "                self.current_layer = layer_idx\n",
    "                for token_idx in range(inputs[\"input_ids\"].size(1)):\n",
    "                    self.current_token = token_idx\n",
    "                    _ = self.model(**inputs)\n",
    "\n",
    "        results = {\n",
    "            \"layers\": {}\n",
    "        }\n",
    "\n",
    "        # Analyze expert outputs for each layer\n",
    "        for layer_idx in range(len(self.model.model.layers)):\n",
    "            layer_results = {\n",
    "                \"tokens\": {}\n",
    "            }\n",
    "            \n",
    "            # For each token position\n",
    "            for token_idx in range(inputs[\"input_ids\"].size(1)):\n",
    "                token = self.tokenizer.decode([inputs[\"input_ids\"][0][token_idx].cpu()])\n",
    "                token_experts = []\n",
    "                \n",
    "                # Get selected experts for this token\n",
    "                selected = [x for x in self.selected_experts \n",
    "                          if x[0] == layer_idx and x[1] == token_idx][0][2]\n",
    "                \n",
    "                # Analyze output of each selected expert\n",
    "                for expert_idx in selected:\n",
    "                    expert_output = [x for x in self.expert_outputs \n",
    "                                   if x[0] == layer_idx and \n",
    "                                   x[1] == token_idx and \n",
    "                                   x[2] == expert_idx]\n",
    "                    \n",
    "                    if expert_output:\n",
    "                        # Project expert output to vocab space\n",
    "                        logits = self._project_to_vocab(expert_output[0][3])\n",
    "                        top_tokens = self.get_top_k_tokens(logits[0], k)\n",
    "                        \n",
    "                        token_experts.append({\n",
    "                            \"expert_id\": expert_idx,\n",
    "                            \"top_tokens\": top_tokens\n",
    "                        })\n",
    "                \n",
    "                layer_results[\"tokens\"][token] = {\n",
    "                    \"position\": token_idx,\n",
    "                    \"selected_experts\": selected,\n",
    "                    \"expert_interpretations\": token_experts\n",
    "                }\n",
    "            \n",
    "            results[\"layers\"][f\"layer_{layer_idx}\"] = layer_results\n",
    "        \n",
    "        # Clear GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        return results\n",
    "\n",
    "def visualize_expert_analysis(results: Dict, layer_idx: int, token_position: int):\n",
    "    \"\"\"Visualize expert interpretations for a specific layer and token.\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    layer_data = results[\"layers\"][f\"layer_{layer_idx}\"]\n",
    "    token_data = [data for data in layer_data[\"tokens\"].values() \n",
    "                 if data[\"position\"] == token_position][0]\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for idx, expert_data in enumerate(token_data[\"expert_interpretations\"]):\n",
    "        tokens, probs = zip(*expert_data[\"top_tokens\"])\n",
    "        \n",
    "        plt.subplot(len(token_data[\"expert_interpretations\"]), 1, idx + 1)\n",
    "        sns.barplot(x=list(tokens), y=list(probs))\n",
    "        plt.title(f\"Expert {expert_data['expert_id']} Interpretations\")\n",
    "        plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = MOEExpertLens(\"deepseek-ai/deepseek-moe-16b-base\")\n",
    "\n",
    "# Analyze text\n",
    "text = \"The quick brown fox\"\n",
    "results = analyzer.analyze_text(text)\n",
    "\n",
    "# Visualize expert interpretations for specific layer and token\n",
    "visualize_expert_analysis(results, layer_idx=0, token_position=1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
