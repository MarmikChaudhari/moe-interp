{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import math\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"Get the optimal available device\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        # Enable TF32 for better performance on Ampere GPUs (A100, A6000, etc)\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        # Set memory allocation settings\n",
    "        torch.cuda.empty_cache()\n",
    "        # Enable CUDNN benchmarking for better performance\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c3420ec9b04ab0b6cf031735948849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DeepseekForCausalLM(\n",
       "  (model): DeepseekModel(\n",
       "    (embed_tokens): Embedding(102400, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0): DeepseekDecoderLayer(\n",
       "        (self_attn): DeepseekSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): DeepseekRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): DeepseekMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=10944, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=10944, bias=False)\n",
       "          (down_proj): Linear(in_features=10944, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): DeepseekRMSNorm()\n",
       "        (post_attention_layernorm): DeepseekRMSNorm()\n",
       "      )\n",
       "      (1-27): 27 x DeepseekDecoderLayer(\n",
       "        (self_attn): DeepseekSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): DeepseekRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): DeepseekMoE(\n",
       "          (experts): ModuleList(\n",
       "            (0-63): 64 x DeepseekMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
       "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (gate): MoEGate()\n",
       "          (shared_experts): DeepseekMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=2816, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=2816, bias=False)\n",
       "            (down_proj): Linear(in_features=2816, out_features=2048, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): DeepseekRMSNorm()\n",
       "        (post_attention_layernorm): DeepseekRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): DeepseekRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=102400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-moe-16b-base\",\n",
    "                                             trust_remote_code=True,\n",
    "                                             torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-moe-16b-base\", trust_remote_code=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOELens:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.activations = defaultdict(dict)\n",
    "        self.hook_handles = []\n",
    "        self.setup_hooks()\n",
    "\n",
    "    def setup_hooks(self):\n",
    "        def get_gate_hook(layer_idx):\n",
    "            def hook(module, inp, out):\n",
    "                if isinstance(out, tuple):\n",
    "                    topk_idx, topk_weight, _ = out\n",
    "                    \n",
    "                    # Get hidden states from input\n",
    "                    hidden_states = inp[0]\n",
    "                    batch_size, seq_len, hidden_dim = hidden_states.shape\n",
    "                    \n",
    "                    # Project to vocab space to get token predictions\n",
    "                    with torch.no_grad():\n",
    "                        # Get expert outputs\n",
    "                        expert_outputs = {}\n",
    "                        for expert_idx in range(module.n_routed_experts):\n",
    "                            if hasattr(self.model.model.layers[layer_idx].mlp, 'experts'):\n",
    "                                expert = self.model.model.layers[layer_idx].mlp.experts[expert_idx]\n",
    "                                expert_output = expert(hidden_states.view(-1, hidden_dim))\n",
    "                                # Project to vocabulary space\n",
    "                                logits = self.model.lm_head(expert_output)\n",
    "                                top_tokens = torch.topk(logits, k=5, dim=-1)\n",
    "                                expert_outputs[expert_idx] = {\n",
    "                                    'token_ids': top_tokens.indices,\n",
    "                                    'probs': torch.softmax(top_tokens.values, dim=-1)\n",
    "                                }\n",
    "\n",
    "                    self.activations[f'layer_{layer_idx}'] = {\n",
    "                        'router_weights': topk_weight.detach(),\n",
    "                        'router_indices': topk_idx.detach(),\n",
    "                        'expert_outputs': expert_outputs\n",
    "                    }\n",
    "            return hook\n",
    "\n",
    "        for i, layer in enumerate(self.model.model.layers):\n",
    "            if hasattr(layer.mlp, 'gate'):\n",
    "                handle = layer.mlp.gate.register_forward_hook(get_gate_hook(i))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    def analyze_text(self, input_ids: torch.Tensor) -> dict:\n",
    "        self.activations.clear()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids)\n",
    "\n",
    "        results = {}\n",
    "        for layer_name, acts in self.activations.items():\n",
    "            layer_results = {\"tokens\": {}}\n",
    "            \n",
    "            for pos in range(input_ids.shape[1]):\n",
    "                token = self.tokenizer.decode([input_ids[0, pos].item()])\n",
    "                expert_info = []\n",
    "                \n",
    "                if 'router_indices' in acts and 'router_weights' in acts:\n",
    "                    indices = acts['router_indices'][pos]\n",
    "                    weights = acts['router_weights'][pos]\n",
    "                    expert_outputs = acts['expert_outputs']\n",
    "                    \n",
    "                    for idx, weight in zip(indices, weights):\n",
    "                        expert_id = idx.item()\n",
    "                        if expert_id in expert_outputs:\n",
    "                            expert_output = expert_outputs[expert_id]\n",
    "                            top_tokens = [\n",
    "                                (self.tokenizer.decode([token_id.item()]), prob.item())\n",
    "                                for token_id, prob in zip(\n",
    "                                    expert_output['token_ids'][pos],\n",
    "                                    expert_output['probs'][pos]\n",
    "                                )\n",
    "                            ]\n",
    "                        else:\n",
    "                            top_tokens = []\n",
    "                            \n",
    "                        expert_info.append({\n",
    "                            \"expert_id\": expert_id,\n",
    "                            \"weight\": weight.item(),\n",
    "                            \"top_tokens\": top_tokens\n",
    "                        })\n",
    "                \n",
    "                layer_results[\"tokens\"][token] = {\n",
    "                    \"position\": pos,\n",
    "                    \"expert_outputs\": expert_info\n",
    "                }\n",
    "            \n",
    "            results[layer_name] = layer_results\n",
    "\n",
    "        return results\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        for handle in self.hook_handles:\n",
    "            handle.remove()\n",
    "        self.hook_handles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_layer_analysis(tokenizer, results: Dict, token_position: int, input_text: str):\n",
    "    \"\"\"\n",
    "    Creates a plotly visualization of expert activations across layers for a specific token.\n",
    "    Shows all 64 experts with zero weights for non-selected experts.\n",
    "    \"\"\"\n",
    "    # Create lists to store data \n",
    "    layer_nums = []\n",
    "    expert_ids = []\n",
    "    weights = []\n",
    "    hover_texts = []\n",
    "    \n",
    "    total_experts = 64  # Total number of experts in the model\n",
    "    \n",
    "    # Extract token we're visualizing by tokenizing input text first\n",
    "    tokens = tokenizer.encode(input_text)\n",
    "    token = tokenizer.decode([tokens[token_position]])  # Get tokenized token\n",
    "    print(f\"Visualizing token: {token}\")\n",
    "        \n",
    "    for layer_idx in range(1, 28):  # Layers 1-27\n",
    "        layer_data = results[f\"layer_{layer_idx}\"]\n",
    "        token_data = [data for data in layer_data[\"tokens\"].values() \n",
    "                     if data[\"position\"] == token_position][0]\n",
    "        \n",
    "        # Create a mapping of expert_id to its data for this layer\n",
    "        expert_map = {exp[\"expert_id\"]: exp for exp in token_data[\"expert_outputs\"]}\n",
    "        \n",
    "        # Go through all possible experts\n",
    "        for expert_id in range(total_experts):\n",
    "            layer_nums.append(layer_idx)\n",
    "            expert_ids.append(expert_id)\n",
    "            \n",
    "            if expert_id in expert_map:\n",
    "                # Expert was selected\n",
    "                expert_data = expert_map[expert_id]\n",
    "                weight = expert_data[\"weight\"]\n",
    "                top_tokens_text = \"<br>\".join([\n",
    "                    f\"{token}: {prob:.3f}\" \n",
    "                    for token, prob in expert_data[\"top_tokens\"][:5]\n",
    "                ])\n",
    "                hover_text = f\"Layer: {layer_idx}<br>Expert: {expert_id}<br>Weight: {weight:.3f}<br>Top tokens:<br>{top_tokens_text}\"\n",
    "                hover_texts.append(hover_text)\n",
    "            else:\n",
    "                # Expert was not selected\n",
    "                weight = 0\n",
    "                hover_texts.append(None)  # No hover text for unselected experts\n",
    "            \n",
    "            weights.append(weight)\n",
    "    \n",
    "    # Create plotly heatmap\n",
    "    fig = go.Figure(data=go.Scatter(\n",
    "        x=expert_ids,\n",
    "        y=layer_nums,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=9,\n",
    "            color=weights,\n",
    "            colorscale=[\n",
    "                [0, 'rgba(24, 21, 23, 0.8)'],  # Very dark/transparent for zero weights\n",
    "                [0.0001, 'rgb(68,1,84)'],      # Start of Viridis colorscale\n",
    "                [1, 'rgb(253,231,37)']         # End of Viridis colorscale\n",
    "            ],\n",
    "            cmin=0,  # Set minimum of color scale to 0\n",
    "            cmax=1,  # Set maximum of color scale to 1\n",
    "            showscale=True,\n",
    "            colorbar=dict(\n",
    "                title='Weight',\n",
    "                tickmode='linear',\n",
    "                tick0=0,\n",
    "                dtick=0.2\n",
    "            ),\n",
    "        ),\n",
    "        text=hover_texts,\n",
    "        hoverinfo='text',\n",
    "        hovertemplate='%{text}<extra></extra>',  # Only show hover when text exists\n",
    "    ))\n",
    "    \n",
    "    # Update layout with dark theme\n",
    "    fig.update_layout(\n",
    "        template='plotly_dark',\n",
    "        title=f'Expert Activations for Token \"{token}\" at Position {token_position}',\n",
    "        xaxis_title='Expert ID',\n",
    "        yaxis_title='Layer',\n",
    "        yaxis=dict(autorange='reversed'),  # Reverse y-axis to have layer 1 at top\n",
    "        width=1200,\n",
    "        height=800,\n",
    "        showlegend=False,\n",
    "        plot_bgcolor='black',\n",
    "        paper_bgcolor='black'\n",
    "    )\n",
    "    \n",
    "    # Add grid lines\n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='rgba(128, 128, 128, 0.2)', \n",
    "                     range=[-1, total_experts])\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='rgba(128, 128, 128, 0.2)')\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_enhanced_logit_lens(model_outputs, tokenizer, input_text, position):\n",
    "    \"\"\"\n",
    "    Creates an enhanced logit lens visualization showing expert activations and their token predictions.\n",
    "    \"\"\"\n",
    "    # Get all unique tokens predicted by experts across layers\n",
    "    all_tokens = set()\n",
    "    token_probs = defaultdict(lambda: defaultdict(float))\n",
    "    layers = []\n",
    "    \n",
    "    # Process each layer's outputs\n",
    "    for layer_idx in range(1, 28):  # Layers 1-27\n",
    "        layer_key = f'layer_{layer_idx}'\n",
    "        if layer_key not in model_outputs:\n",
    "            continue\n",
    "            \n",
    "        layer_data = model_outputs[layer_key]\n",
    "        if 'tokens' not in layer_data:\n",
    "            continue\n",
    "            \n",
    "        token_data = None\n",
    "        # Find the token data for the specified position\n",
    "        for data in layer_data['tokens'].values():\n",
    "            if data['position'] == position:\n",
    "                token_data = data\n",
    "                break\n",
    "                \n",
    "        if token_data is None:\n",
    "            continue\n",
    "            \n",
    "        layers.append(layer_idx)\n",
    "        \n",
    "        # Collect tokens and their probabilities from expert outputs\n",
    "        for expert_output in token_data['expert_outputs']:\n",
    "            expert_weight = expert_output['weight']\n",
    "            for token, prob in expert_output['top_tokens']:\n",
    "                all_tokens.add(token)\n",
    "                token_probs[layer_idx][token] += prob * expert_weight\n",
    "\n",
    "    # Convert to sorted lists for consistent ordering\n",
    "    tokens = sorted(list(all_tokens))\n",
    "    \n",
    "    # Create matrix of probabilities\n",
    "    prob_matrix = []\n",
    "    for layer_idx in layers:\n",
    "        layer_probs = []\n",
    "        for token in tokens:\n",
    "            layer_probs.append(token_probs[layer_idx].get(token, 0.0))\n",
    "        prob_matrix.append(layer_probs)\n",
    "\n",
    "    # Create heatmap\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=prob_matrix,\n",
    "        x=tokens,\n",
    "        y=[f'Layer {idx}' for idx in layers],\n",
    "        colorscale='Viridis',\n",
    "        showscale=True\n",
    "    ))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f'Token Probabilities Across Layers - Position {position}',\n",
    "        xaxis_title='Predicted Tokens',\n",
    "        yaxis_title='Layer',\n",
    "        yaxis={'autorange': 'reversed'},  # Display layer 1 at top\n",
    "        width=1200,\n",
    "        height=800,\n",
    "        plot_bgcolor='black',\n",
    "        paper_bgcolor='black',\n",
    "        font=dict(color='white')\n",
    "    )\n",
    "\n",
    "    # Add grid lines\n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='rgba(128, 128, 128, 0.2)')\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='rgba(128, 128, 128, 0.2)')\n",
    "\n",
    "    return fig\n",
    "\n",
    "def plot_expert_contributions(model_outputs, position=None):\n",
    "    \"\"\"\n",
    "    Creates visualization of expert contributions at each layer.\n",
    "    \n",
    "    Args:\n",
    "        model_outputs: Dictionary containing layer outputs and expert information\n",
    "        position: Optional specific position to analyze\n",
    "    \"\"\"\n",
    "    import plotly.graph_objects as go\n",
    "    import numpy as np\n",
    "    \n",
    "    # Extract expert data\n",
    "    layers = []\n",
    "    expert_weights = []\n",
    "    expert_ids = []\n",
    "    \n",
    "    for layer_idx, layer_data in sorted(model_outputs.items()):\n",
    "        if not layer_idx.startswith('layer_'):\n",
    "            continue\n",
    "            \n",
    "        layer_num = int(layer_idx.split('_')[1])\n",
    "        if position is not None:\n",
    "            token_data = [data for data in layer_data[\"tokens\"].values() \n",
    "                         if data[\"position\"] == position][0]\n",
    "        else:\n",
    "            token_data = next(iter(layer_data[\"tokens\"].values()))\n",
    "            \n",
    "        layers.append(f\"Layer {layer_num}\")\n",
    "        \n",
    "        # Get expert weights and ids\n",
    "        layer_weights = []\n",
    "        layer_ids = []\n",
    "        for exp_output in token_data[\"expert_outputs\"]:\n",
    "            layer_weights.append(exp_output[\"weight\"])\n",
    "            layer_ids.append(exp_output[\"expert_id\"])\n",
    "            \n",
    "        expert_weights.append(layer_weights)\n",
    "        expert_ids.extend([id for id in layer_ids if id not in expert_ids])\n",
    "        \n",
    "    # Create matrix of expert weights\n",
    "    weight_matrix = np.zeros((len(layers), len(expert_ids)))\n",
    "    for i, weights in enumerate(expert_weights):\n",
    "        for w, id in zip(weights, expert_ids[:len(weights)]):\n",
    "            weight_matrix[i, expert_ids.index(id)] = w\n",
    "            \n",
    "    # Create figure\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=weight_matrix,\n",
    "        x=[f\"Expert {id}\" for id in expert_ids],\n",
    "        y=layers,\n",
    "        colorscale='Viridis',\n",
    "        text=[[f\"{val:.3f}\" if val > 0 else \"\" for val in row] for row in weight_matrix],\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\":10},\n",
    "        showscale=True,\n",
    "    ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Expert Contributions by Layer\" + (f\" - Position {position}\" if position else \"\"),\n",
    "        xaxis_title=\"Experts\",\n",
    "        yaxis_title=\"Layers\",\n",
    "        height=800,\n",
    "        width=1200,\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_moe_logit_lens_with_active_expert_outputs(model, inputs, tokenizer, num_active_experts=7):\n",
    "    model.eval()\n",
    "\n",
    "    # Initial embedding\n",
    "    x = model.model.embed_tokens(inputs)\n",
    "\n",
    "    # Process each layer\n",
    "    for layer_idx, layer in enumerate(model.model.layers):\n",
    "        print(f\"Layer {layer_idx + 1}\")\n",
    "        x = layer.input_layernorm(x)\n",
    "\n",
    "        # Self-attention output\n",
    "        # Remove the position_ids argument here, let the model handle it internally\n",
    "        attn_output = layer.self_attn(x, x, x)\n",
    "        x = attn_output + x  # Residual connection\n",
    "        x = layer.post_attention_layernorm(x)\n",
    "\n",
    "        # MoE Layer\n",
    "        moe_output = layer.mlp(x)\n",
    "        gate_values = moe_output[\"gate_values\"]\n",
    "        expert_outputs = moe_output[\"expert_outputs\"]\n",
    "\n",
    "        # Extract active experts\n",
    "        for batch_idx, gates in enumerate(gate_values):\n",
    "            active_experts = gates.argsort(descending=True)[:num_active_experts]\n",
    "            print(f\"  Batch {batch_idx + 1}:\")\n",
    "            for expert_idx in active_experts:\n",
    "                expert_weight = gates[expert_idx].item()\n",
    "                expert_output = expert_outputs[batch_idx, :, expert_idx]\n",
    "                top_tokens = expert_output.topk(5, dim=-1)\n",
    "                top_indices = top_tokens.indices\n",
    "                top_scores = top_tokens.values\n",
    "                decoded_tokens = tokenizer.decode(top_indices.tolist())\n",
    "                print(f\"    Expert {expert_idx}: Weight: {expert_weight:.4f}, Tokens: {decoded_tokens}, Scores: {top_scores.tolist()}\")\n",
    "\n",
    "    # Final logits\n",
    "    logits = model.lm_head(x)\n",
    "    top_tokens = logits.topk(5, dim=-1)\n",
    "    decoded_final_tokens = tokenizer.decode(top_tokens.indices.tolist())\n",
    "    print(f\"Final Layer: Tokens: {decoded_final_tokens}, Scores: {top_tokens.values.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"Get the optimal available device\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        # Enable TF32 for better performance on Ampere GPUs (A100, A6000, etc)\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        # Set memory allocation settings\n",
    "        torch.cuda.empty_cache()\n",
    "        # Enable CUDNN benchmarking for better performance\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_moe_logit_lens_with_active_expert_outputs(model, inputs, tokenizer, num_active_experts=7):\n",
    "    model.eval()\n",
    "\n",
    "    # Initial embedding\n",
    "    x = model.model.embed_tokens(inputs)\n",
    "\n",
    "    # Process each layer\n",
    "    for layer_idx, layer in enumerate(model.model.layers):\n",
    "        print(f\"Layer {layer_idx + 1}\")\n",
    "        x = layer.input_layernorm(x)\n",
    "\n",
    "        # Self-attention output\n",
    "        # Remove the position_ids argument here, let the model handle it internally\n",
    "        attn_output = layer.self_attn(x, x, x)\n",
    "        x = attn_output + x  # Residual connection\n",
    "        x = layer.post_attention_layernorm(x)\n",
    "\n",
    "        # MoE Layer\n",
    "        moe_output = layer.mlp(x)\n",
    "        gate_values = moe_output[\"gate_values\"]\n",
    "        expert_outputs = moe_output[\"expert_outputs\"]\n",
    "\n",
    "        # Extract active experts\n",
    "        for batch_idx, gates in enumerate(gate_values):\n",
    "            active_experts = gates.argsort(descending=True)[:num_active_experts]\n",
    "            print(f\"  Batch {batch_idx + 1}:\")\n",
    "            for expert_idx in active_experts:\n",
    "                expert_weight = gates[expert_idx].item()\n",
    "                expert_output = expert_outputs[batch_idx, :, expert_idx]\n",
    "                top_tokens = expert_output.topk(5, dim=-1)\n",
    "                top_indices = top_tokens.indices\n",
    "                top_scores = top_tokens.values\n",
    "                decoded_tokens = tokenizer.decode(top_indices.tolist())\n",
    "                print(f\"    Expert {expert_idx}: Weight: {expert_weight:.4f}, Tokens: {decoded_tokens}, Scores: {top_scores.tolist()}\")\n",
    "\n",
    "    # Final logits\n",
    "    logits = model.lm_head(x)\n",
    "    top_tokens = logits.topk(5, dim=-1)\n",
    "    decoded_final_tokens = tokenizer.decode(top_tokens.indices.tolist())\n",
    "    print(f\"Final Layer: Tokens: {decoded_final_tokens}, Scores: {top_tokens.values.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_lens(model, input_tokens):\n",
    "    \"\"\"\n",
    "    Applies a logit lens to each layer of a mixture of experts (MoE) model for specific input tokens.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The MoE model.\n",
    "        input_tokens (torch.Tensor): Input tensor with token embeddings, shape [batch_size, seq_len].\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the logit lens outputs for each token at each layer.\n",
    "    \"\"\"\n",
    "    logit_outputs = {}\n",
    "\n",
    "    # Pass the input tokens through the embedding layer\n",
    "    embedding_output = model.embed_tokens(input_tokens)  # Shape: [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "    # Process tokens through each layer\n",
    "    for layer_idx, layer in enumerate(model.layers):\n",
    "        # Apply the layer and get its output\n",
    "        layer_outputs = layer(embedding_output)  # Shape: [batch_size, seq_len, feature_dim]\n",
    "\n",
    "        # Extract the mixture of experts (MoE) components for this layer\n",
    "        if hasattr(layer.mlp, \"experts\"):\n",
    "            gate_outputs = layer.mlp.gate(embedding_output)  # Shape: [batch_size, seq_len, num_experts]\n",
    "            expert_outputs = []\n",
    "\n",
    "            # Process each token separately to extract expert-specific outputs\n",
    "            for token_idx in range(input_tokens.shape[1]):\n",
    "                token_expert_outputs = []\n",
    "\n",
    "                for expert_idx, expert in enumerate(layer.mlp.experts):\n",
    "                    token_input = embedding_output[:, token_idx, :]  # Shape: [batch_size, embedding_dim]\n",
    "                    expert_output = expert(token_input)  # Shape: [batch_size, feature_dim]\n",
    "                    token_expert_outputs.append(expert_output)\n",
    "\n",
    "                # Stack expert outputs and apply gating\n",
    "                token_expert_outputs = torch.stack(token_expert_outputs, dim=1)  # Shape: [batch_size, num_experts, feature_dim]\n",
    "                token_gate_weights = gate_outputs[:, token_idx, :].unsqueeze(-1)  # Shape: [batch_size, num_experts, 1]\n",
    "                activated_experts_output = torch.sum(token_expert_outputs * token_gate_weights, dim=1)  # Shape: [batch_size, feature_dim]\n",
    "\n",
    "                # Save activated expert outputs for this token\n",
    "                logit_outputs[f\"layer_{layer_idx}_token_{token_idx}_activated_experts\"] = activated_experts_output\n",
    "\n",
    "        # Update the embedding for the next layer\n",
    "        embedding_output = layer_outputs\n",
    "\n",
    "    return logit_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prompt_cache(domain: str, data_dict: dict, input_text: str, cache_dir='plots-deepseek') -> int:\n",
    "    \"\"\"Save prompt results with domain-based caching.\"\"\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    cache_info_path = Path(cache_dir) / 'domain_cache_info.json'\n",
    "    \n",
    "    # Load or create cache info\n",
    "    if cache_info_path.exists():\n",
    "        with open(cache_info_path, 'r') as f:\n",
    "            cache_info = json.load(f)\n",
    "    else:\n",
    "        cache_info = {}\n",
    "    \n",
    "    # Get prompt number from existing domain or create new one\n",
    "    if domain in cache_info:\n",
    "        prompt_number = cache_info[domain]['prompt_number']\n",
    "    else:\n",
    "        existing_numbers = [info['prompt_number'] for info in cache_info.values()]\n",
    "        prompt_number = max(existing_numbers, default=0) + 1\n",
    "    \n",
    "    # Update cache info with prompt text\n",
    "    cache_info[domain] = {\n",
    "        'prompt_number': prompt_number,\n",
    "        'timestamp': datetime.datetime.now().isoformat(),\n",
    "        'input_text': input_text  # Add the prompt text\n",
    "    }\n",
    "    \n",
    "    # Save cache info\n",
    "    with open(cache_info_path, 'w') as f:\n",
    "        json.dump(cache_info, f, indent=2)\n",
    "    \n",
    "    # Save data using pickle\n",
    "    data_path = Path(cache_dir) / f'prompt_{prompt_number}.pkl'\n",
    "    with open(data_path, 'wb') as f:\n",
    "        pickle.dump(data_dict, f)\n",
    "        \n",
    "    return prompt_number\n",
    "\n",
    "def load_prompt_cache(prompt_number, cache_dir='plots-deepseek'):\n",
    "    \"\"\"Load a specific prompt's results by number\"\"\"\n",
    "    matrix_path = f'{cache_dir}/prompt_{prompt_number}_matrix.npy'\n",
    "    meta_path = f'{cache_dir}/prompt_{prompt_number}_meta.json'\n",
    "    \n",
    "    if not (os.path.exists(matrix_path) and os.path.exists(meta_path)):\n",
    "        raise ValueError(f\"Prompt {prompt_number} not found in cache\")\n",
    "        \n",
    "    count_matrix = np.load(matrix_path)\n",
    "    with open(meta_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "        \n",
    "    return count_matrix, metadata\n",
    "\n",
    "def list_cached_prompts(cache_dir='plots-deepseek'):\n",
    "    \"\"\"List all cached prompts and their metadata\"\"\"\n",
    "    prompts = []\n",
    "    for f in os.listdir(cache_dir):\n",
    "        if f.endswith('_meta.json'):\n",
    "            with open(os.path.join(cache_dir, f), 'r') as meta_file:\n",
    "                prompts.append(json.load(meta_file))\n",
    "    return sorted(prompts, key=lambda x: x['prompt_number'])\n",
    "\n",
    "def plot_cached_prompt(prompt_number, plot_type='heatmap', layer_id=None, tokenizer=None, cache_dir='plots-deepseek'):\n",
    "    \"\"\"Plot results from a cached prompt\"\"\"\n",
    "    count_matrix, metadata = load_prompt_cache(prompt_number, cache_dir)\n",
    "    domain = metadata['domain']\n",
    "    \n",
    "    if plot_type == 'heatmap':\n",
    "        fig = visualize_layer_analysis(\n",
    "            tokenizer,\n",
    "            count_matrix,\n",
    "            domain=domain\n",
    "        )\n",
    "    elif plot_type == 'logit_lens':\n",
    "        if layer_id is None:\n",
    "            raise ValueError(\"layer_id is required for logit lens plots\")\n",
    "        fig = plot_enhanced_logit_lens(\n",
    "            count_matrix,\n",
    "            tokenizer,\n",
    "            domain=domain,\n",
    "            layer_id=layer_id\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown plot type: {plot_type}\")\n",
    "        \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def migrate_cache_structure(old_cache_info):\n",
    "    \"\"\"\n",
    "    Migrate old cache structure to new format.\n",
    "    \"\"\"\n",
    "    new_cache = {\n",
    "        'domains': {},\n",
    "        'next_id': 1\n",
    "    }\n",
    "    \n",
    "    # Handle existing entries if any\n",
    "    for domain, info in old_cache_info.items():\n",
    "        if domain not in ('domains', 'next_id'):  # Skip if these are already in new format\n",
    "            if isinstance(info, dict):\n",
    "                # Convert old format to new\n",
    "                new_cache['domains'][domain] = [{\n",
    "                    'prompt_number': info.get('prompt_number', new_cache['next_id']),\n",
    "                    'timestamp': info.get('timestamp', datetime.datetime.now().isoformat()),\n",
    "                    'input_text': info.get('input_text', 'No text stored')\n",
    "                }]\n",
    "                new_cache['next_id'] = max(new_cache['next_id'], info['prompt_number'] + 1)\n",
    "    \n",
    "    return new_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prompt_cache(domain: str, data: dict, cache_dir='plots-deepseek') -> int:\n",
    "    \"\"\"\n",
    "    Save prompt results with domain-based caching.\n",
    "    \"\"\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    cache_info_path = Path(cache_dir) / 'domain_cache_info.json'\n",
    "    \n",
    "    # Initialize or load cache info\n",
    "    if cache_info_path.exists():\n",
    "        try:\n",
    "            with open(cache_info_path, 'r') as f:\n",
    "                cache_info = json.load(f)\n",
    "                # Migrate if needed\n",
    "                if 'domains' not in cache_info:\n",
    "                    cache_info = migrate_cache_structure(cache_info)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading cache file, creating new: {e}\")\n",
    "            cache_info = {'domains': {}, 'next_id': 1}\n",
    "    else:\n",
    "        cache_info = {'domains': {}, 'next_id': 1}\n",
    "    \n",
    "    # Get next available ID\n",
    "    prompt_number = cache_info['next_id']\n",
    "    \n",
    "    # Initialize domain if needed\n",
    "    if domain not in cache_info['domains']:\n",
    "        cache_info['domains'][domain] = []\n",
    "    \n",
    "    # Add new entry\n",
    "    entry = {\n",
    "        'prompt_number': prompt_number,\n",
    "        'timestamp': datetime.datetime.now().isoformat(),\n",
    "        'input_text': data.get('input_text', 'No text stored')\n",
    "    }\n",
    "    cache_info['domains'][domain].append(entry)\n",
    "    cache_info['next_id'] = prompt_number + 1\n",
    "    \n",
    "    # Save cache info\n",
    "    with open(cache_info_path, 'w') as f:\n",
    "        json.dump(cache_info, f, indent=2)\n",
    "    \n",
    "    # Save data\n",
    "    data_path = Path(cache_dir) / f'prompt_{prompt_number}.pkl'\n",
    "    with open(data_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "        \n",
    "    return prompt_number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_prompt_cache(domain: str, input_text: str, cache_dir='plots-deepseek'):\n",
    "    \"\"\"\n",
    "    Load cached results for a domain and input text.\n",
    "    \"\"\"\n",
    "    cache_info_path = Path(cache_dir) / 'domain_cache_info.json'\n",
    "    \n",
    "    if not cache_info_path.exists():\n",
    "        raise ValueError(f\"No cache found in {cache_dir}\")\n",
    "    \n",
    "    try:\n",
    "        with open(cache_info_path, 'r') as f:\n",
    "            cache_info = json.load(f)\n",
    "            # Migrate if needed\n",
    "            if 'domains' not in cache_info:\n",
    "                cache_info = migrate_cache_structure(cache_info)\n",
    "                # Save migrated structure\n",
    "                with open(cache_info_path, 'w') as f:\n",
    "                    json.dump(cache_info, f, indent=2)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error reading cache file: {e}\")\n",
    "    \n",
    "    if domain not in cache_info['domains']:\n",
    "        raise ValueError(f\"No cache found for domain '{domain}'\")\n",
    "    \n",
    "    # Find matching entry\n",
    "    matching_entries = [\n",
    "        entry for entry in cache_info['domains'][domain]\n",
    "        if entry['input_text'] == input_text\n",
    "    ]\n",
    "    \n",
    "    if not matching_entries:\n",
    "        raise ValueError(f\"No matching cache entry found for text: {input_text}\")\n",
    "    \n",
    "    # Use most recent\n",
    "    latest_entry = max(matching_entries, key=lambda x: x['timestamp'])\n",
    "    prompt_number = latest_entry['prompt_number']\n",
    "    \n",
    "    data_path = Path(cache_dir) / f'prompt_{prompt_number}.pkl'\n",
    "    if not data_path.exists():\n",
    "        raise ValueError(f\"Cache data file missing for prompt #{prompt_number}\")\n",
    "    \n",
    "    with open(data_path, 'rb') as f:\n",
    "        data_dict = pickle.load(f)\n",
    "        \n",
    "    return data_dict, latest_entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def list_cached_prompts(cache_dir='plots-deepseek'):\n",
    "    \"\"\"List all cached prompts and their metadata.\"\"\"\n",
    "    cache_info_path = Path(cache_dir) / 'domain_cache_info.json'\n",
    "    if not cache_info_path.exists():\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with open(cache_info_path, 'r') as f:\n",
    "            cache_info = json.load(f)\n",
    "            # Migrate if needed\n",
    "            if 'domains' not in cache_info:\n",
    "                cache_info = migrate_cache_structure(cache_info)\n",
    "    except Exception:\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    for domain, entries in cache_info.get('domains', {}).items():\n",
    "        for entry in entries:\n",
    "            results.append({\n",
    "                'domain': domain,\n",
    "                'prompt_number': entry['prompt_number'],\n",
    "                'timestamp': entry['timestamp'],\n",
    "                'input_text': entry.get('input_text', 'No text stored')\n",
    "            })\n",
    "    \n",
    "    return sorted(results, key=lambda x: x['prompt_number'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(\n",
    "    text: str,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    token_position: int = None,\n",
    "    domain: str = 'default',\n",
    "    plot_type: str = 'both',\n",
    "    force_recompute: bool = False,\n",
    "    cache_dir: str = 'plots-deepseek'\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyze text through the model and create visualizations.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        figures = {}\n",
    "        \n",
    "        # Check cache if not forcing recompute\n",
    "        if not force_recompute:\n",
    "            try:\n",
    "                cached_data, cache_info = load_prompt_cache(domain, text, cache_dir)\n",
    "                print(f\"Using cached results from prompt #{cache_info['prompt_number']}\")\n",
    "                \n",
    "                # Extract results from cached data\n",
    "                results = cached_data['results']\n",
    "                \n",
    "                # Create visualizations from cached data\n",
    "                if plot_type in ['heatmap', 'both']:\n",
    "                    figures['heatmap'] = visualize_layer_analysis(\n",
    "                        tokenizer=tokenizer,\n",
    "                        results=results,\n",
    "                        token_position=token_position,\n",
    "                        input_text=text\n",
    "                    )\n",
    "                \n",
    "                if plot_type in ['logit_lens', 'both']:\n",
    "                    figures['logit_lens'] = plot_enhanced_logit_lens(\n",
    "                        model_outputs=results,\n",
    "                        tokenizer=tokenizer,\n",
    "                        input_text=text,\n",
    "                        position=token_position\n",
    "                    )\n",
    "                    \n",
    "                if plot_type in ['expert', 'both']:\n",
    "                    figures['expert'] = plot_expert_contributions(\n",
    "                        results,\n",
    "                        position=token_position\n",
    "                    )\n",
    "                    \n",
    "                return figures, results, None\n",
    "            \n",
    "            except ValueError as e:\n",
    "                print(f\"Cache miss: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading cache: {str(e)}\")\n",
    "\n",
    "        # Initialize the lens analyzer\n",
    "        lens = MOELens(model, tokenizer)\n",
    "        \n",
    "        # Process the input text\n",
    "        input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "        results = lens.analyze_text(input_ids)\n",
    "        \n",
    "        # Create visualizations based on plot_type\n",
    "        if plot_type in ['heatmap', 'both']:\n",
    "            figures['heatmap'] = visualize_layer_analysis(\n",
    "                tokenizer=tokenizer,\n",
    "                results=results,\n",
    "                token_position=token_position,\n",
    "                input_text=text\n",
    "            )\n",
    "            \n",
    "        if plot_type in ['logit_lens', 'both']:\n",
    "            figures['logit_lens'] = plot_enhanced_logit_lens(\n",
    "                model_outputs=results,\n",
    "                tokenizer=tokenizer,\n",
    "                input_text=text,\n",
    "                position=token_position\n",
    "            )\n",
    "            \n",
    "        if plot_type in ['expert', 'both']:\n",
    "            figures['expert'] = plot_expert_contributions(\n",
    "                results,\n",
    "                position=token_position\n",
    "            )\n",
    "            \n",
    "        # Cache the results with metadata\n",
    "        cache_data = {\n",
    "            'results': results,\n",
    "            'input_text': text\n",
    "        }\n",
    "        prompt_number = save_prompt_cache(domain, cache_data, cache_dir)\n",
    "        print(f\"Results cached as prompt #{prompt_number}\")\n",
    "        \n",
    "        return figures, results, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in analyze_dataset: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, str(e)\n",
    "\n",
    "    finally:\n",
    "        # Clean up\n",
    "        if 'lens' in locals():\n",
    "            lens.remove_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_enhanced_logit_lens(model_outputs, tokenizer, input_text, position):\n",
    "    \"\"\"\n",
    "    Creates an enhanced logit lens visualization showing expert activations and their token predictions.\n",
    "    \"\"\"\n",
    "    # Get all unique tokens predicted by experts across layers\n",
    "    all_tokens = set()\n",
    "    token_probs = defaultdict(lambda: defaultdict(float))\n",
    "    layers = []\n",
    "    \n",
    "    # Process each layer's outputs in order\n",
    "    layer_keys = sorted([k for k in model_outputs.keys() if k.startswith('layer_')], \n",
    "                       key=lambda x: int(x.split('_')[1]))\n",
    "                       \n",
    "    for layer_key in layer_keys:\n",
    "        layer_idx = int(layer_key.split('_')[1])\n",
    "        layer_data = model_outputs[layer_key]\n",
    "        if 'tokens' not in layer_data:\n",
    "            continue\n",
    "            \n",
    "        token_data = None\n",
    "        # Find the token data for the specified position\n",
    "        for data in layer_data['tokens'].values():\n",
    "            if data['position'] == position:\n",
    "                token_data = data\n",
    "                break\n",
    "                \n",
    "        if token_data is None:\n",
    "            continue\n",
    "            \n",
    "        layers.append(layer_idx)\n",
    "        \n",
    "        # Collect tokens and their probabilities from expert outputs\n",
    "        for expert_output in token_data['expert_outputs']:\n",
    "            expert_weight = expert_output['weight']\n",
    "            for token, prob in expert_output['top_tokens']:\n",
    "                all_tokens.add(token)\n",
    "                token_probs[layer_idx][token] += prob * expert_weight\n",
    "\n",
    "    # Convert to sorted lists for consistent ordering\n",
    "    tokens = sorted(list(all_tokens))\n",
    "    \n",
    "    # Create matrix of probabilities\n",
    "    prob_matrix = []\n",
    "    for layer_idx in layers:\n",
    "        layer_probs = []\n",
    "        for token in tokens:\n",
    "            layer_probs.append(token_probs[layer_idx].get(token, 0.0))\n",
    "        prob_matrix.append(layer_probs)\n",
    "\n",
    "    # Create heatmap\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=prob_matrix,\n",
    "        x=tokens,\n",
    "        y=[f'Layer {idx}' for idx in layers],\n",
    "        colorscale='Viridis',\n",
    "        showscale=True\n",
    "    ))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f'Token Probabilities Across Layers - Position {position}',\n",
    "        xaxis_title='Predicted Tokens',\n",
    "        yaxis_title='Layer',\n",
    "        yaxis={'autorange': 'reversed'},  # Display layer 1 at top\n",
    "        width=1200,\n",
    "        height=800,\n",
    "        plot_bgcolor='black',\n",
    "        paper_bgcolor='black',\n",
    "        font=dict(color='white')\n",
    "    )\n",
    "\n",
    "    # Add grid lines\n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='rgba(128, 128, 128, 0.2)')\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='rgba(128, 128, 128, 0.2)')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_expert_contributions(model_outputs, position=None):\n",
    "    \"\"\"\n",
    "    Creates detailed visualization of expert contributions with token predictions at each layer.\n",
    "    Shows top token and expert weight in cells, all tokens on hover.\n",
    "    \"\"\"\n",
    "    import plotly.graph_objects as go\n",
    "    import numpy as np\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Extract expert data\n",
    "    layers = []\n",
    "    expert_weights = []\n",
    "    expert_ids = []\n",
    "    expert_tokens = defaultdict(dict)\n",
    "    \n",
    "    # Sort layer keys numerically\n",
    "    layer_keys = sorted([k for k in model_outputs.keys() if k.startswith('layer_')],\n",
    "                       key=lambda x: int(x.split('_')[1]))\n",
    "                       \n",
    "    for layer_key in layer_keys:\n",
    "        layer_num = int(layer_key.split('_')[1])\n",
    "        layer_data = model_outputs[layer_key]\n",
    "            \n",
    "        if position is not None:\n",
    "            token_data = [data for data in layer_data[\"tokens\"].values() \n",
    "                         if data[\"position\"] == position][0]\n",
    "        else:\n",
    "            token_data = next(iter(layer_data[\"tokens\"].values()))\n",
    "            \n",
    "        layers.append(layer_num)\n",
    "        \n",
    "        # Get expert weights, ids, and token predictions\n",
    "        layer_weights = []\n",
    "        layer_ids = []\n",
    "        for exp_output in token_data[\"expert_outputs\"]:\n",
    "            layer_weights.append(exp_output[\"weight\"])\n",
    "            expert_id = exp_output[\"expert_id\"]\n",
    "            layer_ids.append(expert_id)\n",
    "            \n",
    "            expert_tokens[(layer_num, expert_id)] = {\n",
    "                'weight': exp_output[\"weight\"],\n",
    "                'tokens': exp_output[\"top_tokens\"]\n",
    "            }\n",
    "            \n",
    "        expert_weights.append(layer_weights)\n",
    "        expert_ids.extend([id for id in layer_ids if id not in expert_ids])\n",
    "    \n",
    "    expert_ids = sorted(list(set(expert_ids)))\n",
    "    \n",
    "    # Create matrices for weights, hover text, and display text\n",
    "    weight_matrix = np.zeros((len(layers), len(expert_ids)))\n",
    "    hover_text = [['' for _ in range(len(expert_ids))] for _ in range(len(layers))]\n",
    "    text_matrix = [['' for _ in range(len(expert_ids))] for _ in range(len(layers))]\n",
    "    \n",
    "    for i, layer in enumerate(layers):\n",
    "        for j, expert_id in enumerate(expert_ids):\n",
    "            info = expert_tokens.get((layer, expert_id), None)\n",
    "            if info and info['tokens']:\n",
    "                weight = info['weight']\n",
    "                weight_matrix[i, j] = weight\n",
    "                \n",
    "                # Get the top token\n",
    "                top_token, _ = info['tokens'][0]\n",
    "                \n",
    "                # Create cell text: show token first, centered, then weight below\n",
    "                # Using HTML for better text control\n",
    "                text_matrix[i][j] = f'<span style=\"text-align: center\">{top_token}<br>{weight:.3f}</span>'\n",
    "                \n",
    "                # Create hover text with all tokens and their probabilities\n",
    "                hover_lines = [\n",
    "                    f\"Layer: {layer}\",\n",
    "                    f\"Expert: {expert_id}\",\n",
    "                    f\"Weight: {weight:.3f}\",\n",
    "                    \"Top tokens:\"\n",
    "                ]\n",
    "                hover_lines.extend(f\"{token}: {prob:.3f}\" for token, prob in info['tokens'])\n",
    "                hover_text[i][j] = \"<br>\".join(hover_lines)\n",
    "    \n",
    "    # Calculate dimensions to make cells square\n",
    "    cell_size = 50  # Base cell size in pixels\n",
    "    width = cell_size * len(expert_ids)\n",
    "    height = cell_size * len(layers)\n",
    "    \n",
    "    # Create figure\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=weight_matrix,\n",
    "        x=[f\"Expert {id}\" for id in expert_ids],\n",
    "        y=[f\"Layer {layer}\" for layer in layers],\n",
    "        colorscale=[\n",
    "            [0, 'rgba(24, 21, 23, 0.8)'],\n",
    "            [0.0001, 'rgb(68,1,84)'],\n",
    "            [1, 'rgb(253,231,37)']\n",
    "        ],\n",
    "        text=text_matrix,\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 9, \"color\": \"white\", \"family\": \"monospace\"},\n",
    "        hoverongaps=False,\n",
    "        hoverinfo=\"text\",\n",
    "        hovertext=hover_text,\n",
    "        showscale=True,\n",
    "    ))\n",
    "    \n",
    "    # Update layout with equal width and height\n",
    "    fig.update_layout(\n",
    "        title=f\"Expert Contributions by Layer\" + (f\" - Position {position}\" if position else \"\"),\n",
    "        xaxis_title=\"Experts\",\n",
    "        yaxis_title=\"Layers\",\n",
    "        height=height,\n",
    "        width=width,\n",
    "        plot_bgcolor='black',\n",
    "        paper_bgcolor='black',\n",
    "        font=dict(color='white'),\n",
    "        xaxis=dict(\n",
    "            tickangle=45,  # Angle the expert labels\n",
    "            tickfont=dict(size=9),\n",
    "            tickmode='array',\n",
    "            ticktext=[f\"Expert {id}\" for id in expert_ids],\n",
    "            tickvals=list(range(len(expert_ids))),\n",
    "            dtick=1,\n",
    "            showgrid=True,\n",
    "            gridwidth=1,\n",
    "            gridcolor='rgba(128, 128, 128, 0.2)',\n",
    "            showline=True,\n",
    "            linewidth=1,\n",
    "            linecolor='rgba(128, 128, 128, 0.2)',\n",
    "            scaleanchor=\"y\",  # This makes the x and y scales equal\n",
    "            scaleratio=1\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            autorange='reversed',  # Keep layer 1 at top\n",
    "            tickfont=dict(size=9),\n",
    "            dtick=1,\n",
    "            showgrid=True,\n",
    "            gridwidth=1,\n",
    "            gridcolor='rgba(128, 128, 128, 0.2)',\n",
    "            showline=True,\n",
    "            linewidth=1,\n",
    "            linecolor='rgba(128, 128, 128, 0.2)',\n",
    "            scaleanchor=\"x\",  # This makes the x and y scales equal\n",
    "            scaleratio=1\n",
    "        ),\n",
    "        margin=dict(l=50, r=50, t=100, b=100)\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache miss: No cache found in plots-deepseek\n",
      "Results cached as prompt #1\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "colorscale": [
          [
           0,
           "rgba(24, 21, 23, 0.8)"
          ],
          [
           0.0001,
           "rgb(68,1,84)"
          ],
          [
           1,
           "rgb(253,231,37)"
          ]
         ],
         "hoverinfo": "text",
         "hoverongaps": false,
         "hovertext": [
          [
           "",
           "",
           "",
           "Layer: 1<br>Expert: 3<br>Weight: 0.034<br>Top tokens:<br> R: 0.249<br> C: 0.205<br> S: 0.198<br> : 0.182<br>dashed: 0.166",
           "",
           "Layer: 1<br>Expert: 5<br>Weight: 0.149<br>Top tokens:<br>edom: 0.225<br>: 0.201<br>clud: 0.200<br>quita: 0.195<br> Davy: 0.179",
           "",
           "Layer: 1<br>Expert: 8<br>Weight: 0.082<br>Top tokens:<br>: 0.248<br>: 0.219<br>: 0.180<br>ractical: 0.177<br>=\"//: 0.176",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 1<br>Expert: 43<br>Weight: 0.034<br>Top tokens:<br>: 0.246<br>ooft: 0.207<br>ondo: 0.191<br>: 0.180<br>ents: 0.176",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 1<br>Expert: 59<br>Weight: 0.039<br>Top tokens:<br>eq: 0.247<br>: 0.209<br>: 0.188<br> : 0.179<br>: 0.176",
           "",
           "Layer: 1<br>Expert: 61<br>Weight: 0.129<br>Top tokens:<br>ront: 0.215<br>opro: 0.208<br>fter: 0.198<br>ugar: 0.198<br> <!--[: 0.181",
           ""
          ],
          [
           "",
           "Layer: 2<br>Expert: 1<br>Weight: 0.037<br>Top tokens:<br>ultors: 0.231<br>: 0.222<br>[--: 0.189<br>precated: 0.185<br>bisbe: 0.173",
           "",
           "",
           "Layer: 2<br>Expert: 4<br>Weight: 0.222<br>Top tokens:<br> s: 0.360<br>chaff: 0.180<br>EXTERNAL: 0.177<br>INLINE: 0.143<br>ifera: 0.141",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 2<br>Expert: 23<br>Weight: 0.126<br>Top tokens:<br>ilateral: 0.215<br> e: 0.213<br>eret: 0.205<br>LaTeX: 0.188<br> Exterior: 0.179",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 2<br>Expert: 32<br>Weight: 0.057<br>Top tokens:<br>uriformes: 0.236<br>ndies: 0.209<br>pload: 0.192<br>CEED: 0.185<br>egl: 0.177",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 2<br>Expert: 43<br>Weight: 0.061<br>Top tokens:<br>ievable: 0.264<br>ordial: 0.199<br>quart: 0.181<br>imuth: 0.179<br>ancel: 0.178",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 2<br>Expert: 54<br>Weight: 0.033<br>Top tokens:<br>oyo: 0.225<br>asz: 0.221<br>usz: 0.188<br>sets: 0.185<br>otine: 0.182",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "Layer: 3<br>Expert: 1<br>Weight: 0.026<br>Top tokens:<br> present: 0.221<br>: 0.216<br>yntax: 0.195<br> control: 0.185<br>adr: 0.183",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 3<br>Expert: 9<br>Weight: 0.455<br>Top tokens:<br>: 0.315<br>: 0.267<br>uaci: 0.145<br>blink: 0.139<br>idual: 0.134",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 3<br>Expert: 18<br>Weight: 0.031<br>Top tokens:<br>Antecedents: 0.209<br> : 0.201<br>~(\\: 0.200<br>: 0.197<br>zr: 0.193",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 3<br>Expert: 33<br>Weight: 0.071<br>Top tokens:<br>atal: 0.213<br>paramname: 0.205<br>ipschitz: 0.200<br>nsit: 0.195<br>amina: 0.187",
           "Layer: 3<br>Expert: 34<br>Weight: 0.039<br>Top tokens:<br>lef: 0.213<br>annels: 0.206<br> PPA: 0.206<br>: 0.191<br>: 0.185",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 3<br>Expert: 51<br>Weight: 0.066<br>Top tokens:<br>uple: 0.210<br>IMPORTED: 0.208<br>ashian: 0.198<br>strument: 0.194<br>: 0.191",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 4<br>Expert: 8<br>Weight: 0.026<br>Top tokens:<br>: 0.258<br>RN: 0.243<br>vphantom: 0.172<br>: 0.166<br>aneously: 0.161",
           "",
           "Layer: 4<br>Expert: 10<br>Weight: 0.036<br>Top tokens:<br> : 0.228<br>restant: 0.198<br>: 0.194<br>hest: 0.191<br> : 0.189",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 4<br>Expert: 27<br>Weight: 0.180<br>Top tokens:<br> : 0.216<br>escut: 0.209<br>: 0.207<br>mlin: 0.197<br> : 0.172",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 4<br>Expert: 39<br>Weight: 0.048<br>Top tokens:<br>agger: 0.217<br>san: 0.208<br>harp: 0.195<br>rases: 0.191<br> : 0.189",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 4<br>Expert: 47<br>Weight: 0.021<br>Top tokens:<br> : 0.245<br> each: 0.198<br>.\\,: 0.192<br>roquois: 0.189<br>ulsed: 0.176",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 4<br>Expert: 57<br>Weight: 0.469<br>Top tokens:<br>gear: 0.224<br>quo: 0.201<br>illion: 0.195<br>yan: 0.192<br>ntesis: 0.189",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "Layer: 5<br>Expert: 0<br>Weight: 0.049<br>Top tokens:<br>virt: 0.236<br> le: 0.198<br> ta: 0.193<br> mon: 0.192<br>: 0.181",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 5<br>Expert: 13<br>Weight: 0.069<br>Top tokens:<br>PROTO: 0.234<br>: 0.208<br>TypeDef: 0.197<br>: 0.182<br>TEL: 0.179",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 5<br>Expert: 22<br>Weight: 0.030<br>Top tokens:<br>: 0.214<br>: 0.209<br>atzem: 0.198<br> : 0.195<br>: 0.184",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 5<br>Expert: 32<br>Weight: 0.081<br>Top tokens:<br> cos: 0.233<br> exp: 0.202<br>: 0.202<br> dev: 0.182<br> brackets: 0.181",
           "",
           "",
           "",
           "Layer: 5<br>Expert: 36<br>Weight: 0.484<br>Top tokens:<br>: 0.243<br>athers: 0.211<br>afone: 0.194<br>accio: 0.178<br>GeneratedMessage: 0.174",
           "",
           "",
           "",
           "",
           "Layer: 5<br>Expert: 41<br>Weight: 0.055<br>Top tokens:<br>dorff: 0.279<br> me: 0.213<br>groovy: 0.177<br> : 0.169<br>agrang: 0.163",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 6<br>Expert: 13<br>Weight: 0.155<br>Top tokens:<br>asma: 0.226<br>wani: 0.217<br>itic: 0.201<br>warz: 0.188<br>ENG: 0.168",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 6<br>Expert: 21<br>Weight: 0.099<br>Top tokens:<br> : 0.232<br>: 0.213<br>widet: 0.203<br>: 0.176<br>topsp: 0.175",
           "",
           "",
           "",
           "",
           "",
           "Layer: 6<br>Expert: 27<br>Weight: 0.046<br>Top tokens:<br>: 0.226<br>: 0.209<br>strap: 0.191<br> h: 0.188<br>;: 0.186",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 6<br>Expert: 37<br>Weight: 0.054<br>Top tokens:<br>: 0.411<br>hedra: 0.184<br>feu: 0.137<br>: 0.135<br>: 0.132",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 6<br>Expert: 54<br>Weight: 0.204<br>Top tokens:<br> xif: 0.237<br>URSS: 0.206<br> aque: 0.196<br> reaccion: 0.182<br>IEEEeqnarray: 0.180",
           "",
           "",
           "",
           "Layer: 6<br>Expert: 58<br>Weight: 0.138<br>Top tokens:<br>osus: 0.229<br>pmyadmin: 0.199<br>: 0.198<br>epsi: 0.194<br>ateral: 0.179",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 7<br>Expert: 11<br>Weight: 0.032<br>Top tokens:<br>ITOR: 0.249<br> ipt: 0.195<br>ned: 0.191<br>iments: 0.190<br>: 0.176",
           "",
           "Layer: 7<br>Expert: 13<br>Weight: 0.032<br>Top tokens:<br>uy: 0.238<br> pat: 0.213<br>: 0.197<br>jon: 0.178<br>: 0.174",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 7<br>Expert: 21<br>Weight: 0.081<br>Top tokens:<br>: 0.229<br> fdisk: 0.215<br>: 0.202<br>tees: 0.177<br>: 0.177",
           "",
           "",
           "",
           "",
           "",
           "Layer: 7<br>Expert: 27<br>Weight: 0.065<br>Top tokens:<br>anda: 0.246<br>ostics: 0.203<br>holds: 0.191<br>: 0.183<br>: 0.177",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 7<br>Expert: 46<br>Weight: 0.088<br>Top tokens:<br>: 0.265<br>: 0.209<br>: 0.191<br>: 0.172<br>: 0.162",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 7<br>Expert: 56<br>Weight: 0.271<br>Top tokens:<br> condicions: 0.224<br>: 0.214<br>TERM: 0.190<br>: 0.188<br>ourn: 0.183",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "Layer: 8<br>Expert: 0<br>Weight: 0.054<br>Top tokens:<br>ophile: 0.209<br>: 0.205<br> Teod: 0.203<br>leep: 0.198<br> Segura: 0.185",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 8<br>Expert: 20<br>Weight: 0.072<br>Top tokens:<br>igr: 0.242<br>EZ: 0.235<br> trailing: 0.177<br>: 0.176<br>: 0.170",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 8<br>Expert: 32<br>Weight: 0.109<br>Top tokens:<br>oldt: 0.385<br>: 0.180<br>: 0.155<br>ineq: 0.146<br>abet: 0.134",
           "",
           "",
           "Layer: 8<br>Expert: 35<br>Weight: 0.369<br>Top tokens:<br>oiselle: 0.274<br> Pillow: 0.223<br>ostic: 0.181<br>INESS: 0.161<br>izar: 0.161",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 8<br>Expert: 42<br>Weight: 0.038<br>Top tokens:<br>eting: 0.231<br>oto: 0.229<br>ings: 0.193<br>: 0.189<br>otos: 0.158",
           "Layer: 8<br>Expert: 43<br>Weight: 0.040<br>Top tokens:<br> : 0.252<br>rtype: 0.200<br>ctxt: 0.197<br>instrument: 0.177<br>ressos: 0.174",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "Layer: 9<br>Expert: 4<br>Weight: 0.129<br>Top tokens:<br>ugar: 0.452<br>ardt: 0.161<br>: 0.141<br>: 0.129<br>ournal: 0.116",
           "",
           "",
           "",
           "",
           "Layer: 9<br>Expert: 10<br>Weight: 0.080<br>Top tokens:<br> \": 0.257<br> har: 0.209<br> ': 0.185<br> *: 0.175<br> : 0.175",
           "",
           "",
           "",
           "Layer: 9<br>Expert: 15<br>Weight: 0.130<br>Top tokens:<br>: 0.280<br> : 0.274<br>forgotten: 0.156<br>cheek: 0.147<br>: 0.142",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 9<br>Expert: 37<br>Weight: 0.113<br>Top tokens:<br>ignite: 0.260<br>orry: 0.220<br>ifax: 0.206<br>: 0.180<br>: 0.134",
           "",
           "",
           "",
           "",
           "",
           "Layer: 9<br>Expert: 43<br>Weight: 0.034<br>Top tokens:<br>: 0.245<br>: 0.237<br> passat: 0.229<br>oze: 0.146<br>nteg: 0.143",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 9<br>Expert: 59<br>Weight: 0.066<br>Top tokens:<br>Ungrouped: 0.246<br>: 0.243<br>refn: 0.175<br>rfloor: 0.171<br>)|$: 0.165",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "Layer: 10<br>Expert: 4<br>Weight: 0.052<br>Top tokens:<br>: 0.280<br>: 0.198<br>avid: 0.186<br>: 0.174<br>: 0.163",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 10<br>Expert: 19<br>Weight: 0.337<br>Top tokens:<br>: 0.253<br>agut: 0.212<br>: 0.183<br> novel: 0.178<br>AAAAAAAAAAAAAAAA: 0.174",
           "",
           "",
           "",
           "Layer: 10<br>Expert: 23<br>Weight: 0.050<br>Top tokens:<br>ornia: 0.451<br>{``: 0.145<br> : 0.140<br>Olot: 0.140<br> solars: 0.124",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 10<br>Expert: 45<br>Weight: 0.053<br>Top tokens:<br>: 0.231<br>: 0.227<br>anta: 0.227<br>icolon: 0.161<br> TAU: 0.154",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 10<br>Expert: 58<br>Weight: 0.092<br>Top tokens:<br>mere: 0.280<br>zul: 0.237<br>: 0.171<br>CONDS: 0.164<br>Ljava: 0.149",
           "",
           "Layer: 10<br>Expert: 60<br>Weight: 0.070<br>Top tokens:<br>: 0.264<br>: 0.205<br>outh: 0.183<br>SIGN: 0.183<br>IFF: 0.164",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 11<br>Expert: 11<br>Weight: 0.089<br>Top tokens:<br>elled: 0.298<br> C: 0.235<br>atges: 0.181<br>: 0.149<br> CE: 0.136",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 11<br>Expert: 21<br>Weight: 0.070<br>Top tokens:<br>: 0.303<br>hyde: 0.179<br>: 0.179<br>: 0.173<br> rap: 0.167",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 11<br>Expert: 28<br>Weight: 0.048<br>Top tokens:<br>lood: 0.495<br>ssica: 0.152<br>onada: 0.123<br>tats: 0.118<br>ntil: 0.113",
           "Layer: 11<br>Expert: 29<br>Weight: 0.049<br>Top tokens:<br>plete: 0.295<br>UBE: 0.213<br>escue: 0.172<br>ycle: 0.167<br>: 0.152",
           "",
           "",
           "Layer: 11<br>Expert: 32<br>Weight: 0.132<br>Top tokens:<br>out: 0.247<br>/: 0.202<br> extent: 0.189<br>: 0.184<br>rolls: 0.178",
           "",
           "Layer: 11<br>Expert: 34<br>Weight: 0.057<br>Top tokens:<br> : 0.255<br>iades: 0.229<br>: 0.196<br>adol: 0.160<br>iak: 0.159",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "Layer: 12<br>Expert: 3<br>Weight: 0.047<br>Top tokens:<br>enir: 0.291<br>Occ: 0.243<br>: 0.183<br>renta: 0.152<br>FD: 0.131",
           "",
           "",
           "",
           "",
           "Layer: 12<br>Expert: 9<br>Weight: 0.060<br>Top tokens:<br>iamond: 0.229<br>: 0.208<br> aapt: 0.192<br>ota: 0.191<br>: 0.180",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 12<br>Expert: 18<br>Weight: 0.074<br>Top tokens:<br>aulay: 0.268<br> Borde: 0.207<br>ustral: 0.182<br>guien: 0.175<br>uals: 0.168",
           "Layer: 12<br>Expert: 19<br>Weight: 0.048<br>Top tokens:<br>: 0.257<br>ipel: 0.200<br>eur: 0.184<br>irut: 0.182<br> Editions: 0.177",
           "Layer: 12<br>Expert: 20<br>Weight: 0.055<br>Top tokens:<br>moz: 0.253<br>udis: 0.229<br>ornis: 0.175<br>: 0.174<br>: 0.169",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 12<br>Expert: 36<br>Weight: 0.211<br>Top tokens:<br>ILED: 0.289<br>ygon: 0.194<br>iterr: 0.192<br>aure: 0.185<br>ospital: 0.141",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "Layer: 13<br>Expert: 4<br>Weight: 0.049<br>Top tokens:<br>: 0.298<br>: 0.238<br>ocol: 0.179<br>uli: 0.146<br>lax: 0.139",
           "",
           "Layer: 13<br>Expert: 7<br>Weight: 0.032<br>Top tokens:<br>: 0.302<br>: 0.225<br>yst: 0.220<br>erk: 0.127<br>: 0.125",
           "",
           "",
           "Layer: 13<br>Expert: 10<br>Weight: 0.060<br>Top tokens:<br>crest: 0.298<br>: 0.191<br>istre: 0.187<br>var: 0.166<br>ikip: 0.158",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 13<br>Expert: 18<br>Weight: 0.192<br>Top tokens:<br>olla: 0.334<br> dispersa: 0.177<br>: 0.174<br>orr: 0.159<br> falla: 0.156",
           "",
           "",
           "",
           "",
           "Layer: 13<br>Expert: 23<br>Weight: 0.043<br>Top tokens:<br>ouz: 0.258<br>geu: 0.218<br>: 0.198<br>astre: 0.170<br>astres: 0.156",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 13<br>Expert: 38<br>Weight: 0.148<br>Top tokens:<br>duc: 0.277<br>: 0.198<br>: 0.189<br>ugin: 0.188<br>: 0.147",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 14<br>Expert: 9<br>Weight: 0.065<br>Top tokens:<br>estions: 0.233<br> : 0.229<br>: 0.205<br>: 0.167<br> : 0.167",
           "",
           "",
           "",
           "Layer: 14<br>Expert: 13<br>Weight: 0.058<br>Top tokens:<br> punta: 0.230<br> propriet: 0.220<br> : 0.206<br> ficci: 0.174<br>etext: 0.170",
           "",
           "Layer: 14<br>Expert: 16<br>Weight: 0.069<br>Top tokens:<br>: 0.301<br>atural: 0.227<br>:///: 0.162<br>: 0.158<br>aturally: 0.152",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 14<br>Expert: 24<br>Weight: 0.066<br>Top tokens:<br> only: 0.299<br> item: 0.211<br>ka: 0.202<br>tri: 0.145<br>oh: 0.143",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 14<br>Expert: 44<br>Weight: 0.107<br>Top tokens:<br>9: 0.268<br>0: 0.262<br>7: 0.160<br>6: 0.158<br>8: 0.151",
           "",
           "",
           "",
           "",
           "",
           "Layer: 14<br>Expert: 52<br>Weight: 0.068<br>Top tokens:<br>: 0.244<br>amena: 0.241<br>: 0.194<br>acao: 0.171<br>nexi: 0.150",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "Layer: 15<br>Expert: 2<br>Weight: 0.146<br>Top tokens:<br>0: 0.352<br>7: 0.198<br>9: 0.163<br>4: 0.157<br>2: 0.130",
           "Layer: 15<br>Expert: 3<br>Weight: 0.062<br>Top tokens:<br>aband: 0.280<br>CID: 0.201<br>adera: 0.187<br>culo: 0.169<br> Adels: 0.163",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 15<br>Expert: 21<br>Weight: 0.129<br>Top tokens:<br>colhead: 0.317<br>: 0.191<br>rophe: 0.179<br>ku: 0.162<br>ygon: 0.151",
           "",
           "",
           "",
           "Layer: 15<br>Expert: 25<br>Weight: 0.055<br>Top tokens:<br>aye: 0.245<br>on: 0.232<br>jo: 0.215<br>: 0.155<br>acom: 0.153",
           "",
           "",
           "",
           "",
           "Layer: 15<br>Expert: 30<br>Weight: 0.055<br>Top tokens:<br>: 0.281<br>9: 0.206<br>1: 0.184<br>: 0.180<br>asc: 0.149",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 15<br>Expert: 61<br>Weight: 0.058<br>Top tokens:<br>: 0.394<br>: 0.211<br>epsi: 0.166<br>dfp: 0.127<br>: 0.103",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "Layer: 16<br>Expert: 5<br>Weight: 0.073<br>Top tokens:<br> : 0.256<br>: 0.255<br>: 0.195<br>: 0.161<br>: 0.134",
           "",
           "",
           "",
           "Layer: 16<br>Expert: 10<br>Weight: 0.312<br>Top tokens:<br>0: 0.343<br>8: 0.201<br>5: 0.181<br>2: 0.138<br>1: 0.137",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 16<br>Expert: 27<br>Weight: 0.136<br>Top tokens:<br> ter: 0.377<br> Ter: 0.299<br> ret: 0.213<br>Ter: 0.059<br>bre: 0.051",
           "",
           "",
           "Layer: 16<br>Expert: 30<br>Weight: 0.045<br>Top tokens:<br>: 0.234<br>: 0.231<br>iosync: 0.190<br>ording: 0.174<br>: 0.172",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 16<br>Expert: 37<br>Weight: 0.062<br>Top tokens:<br>upp: 0.244<br>: 0.224<br>raphics: 0.218<br>UFF: 0.157<br>{}): 0.157",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 16<br>Expert: 47<br>Weight: 0.032<br>Top tokens:<br>ffected: 0.219<br>refore: 0.217<br> TRI: 0.210<br> once: 0.179<br>abytes: 0.176",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 17<br>Expert: 12<br>Weight: 0.054<br>Top tokens:<br>: 0.572<br>amet: 0.133<br>/__: 0.125<br>isSet: 0.088<br>chiev: 0.082",
           "",
           "",
           "Layer: 17<br>Expert: 16<br>Weight: 0.035<br>Top tokens:<br> crossorigin: 0.466<br>ESCO: 0.156<br>autilus: 0.132<br>: 0.123<br>IMIT: 0.122",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 17<br>Expert: 31<br>Weight: 0.293<br>Top tokens:<br>0: 0.307<br>2: 0.242<br>1: 0.181<br>4: 0.141<br>5: 0.129",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 17<br>Expert: 41<br>Weight: 0.034<br>Top tokens:<br>: 0.311<br>rality: 0.190<br>alysis: 0.180<br>ilitat: 0.169<br>gsettings: 0.150",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 17<br>Expert: 48<br>Weight: 0.121<br>Top tokens:<br>es: 0.938<br>s: 0.050<br>ie: 0.005<br>ter: 0.004<br> adm: 0.002",
           "",
           "",
           "",
           "Layer: 17<br>Expert: 55<br>Weight: 0.053<br>Top tokens:<br> : 0.255<br>: 0.212<br>: 0.202<br>mith: 0.180<br>ServiceProvider: 0.152",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 18<br>Expert: 17<br>Weight: 0.070<br>Top tokens:<br>TOOLSET: 0.315<br>FUNCPTR: 0.197<br>APIENTRY: 0.197<br>bigarray: 0.149<br>paramtype: 0.142",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 18<br>Expert: 28<br>Weight: 0.054<br>Top tokens:<br> flyback: 0.325<br>nore: 0.229<br>orters: 0.206<br>: 0.126<br>: 0.115",
           "",
           "",
           "Layer: 18<br>Expert: 31<br>Weight: 0.203<br>Top tokens:<br> v: 0.316<br> r: 0.199<br> n: 0.172<br> : 0.163<br> p: 0.150",
           "",
           "",
           "",
           "",
           "Layer: 18<br>Expert: 36<br>Weight: 0.055<br>Top tokens:<br>isses: 0.324<br>arms: 0.202<br>openg: 0.167<br>3: 0.158<br>: 0.148",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 18<br>Expert: 43<br>Weight: 0.137<br>Top tokens:<br>etimes: 0.319<br>: 0.225<br>ividual: 0.213<br>: 0.129<br>eza: 0.114",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 18<br>Expert: 55<br>Weight: 0.045<br>Top tokens:<br> : 0.383<br>hoe: 0.171<br>jna: 0.152<br> Fabra: 0.150<br>ramfs: 0.144",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "Layer: 19<br>Expert: 3<br>Weight: 0.094<br>Top tokens:<br> comple: 0.267<br> coc: 0.247<br>avera: 0.224<br>itons: 0.133<br>oserver: 0.130",
           "",
           "",
           "Layer: 19<br>Expert: 7<br>Weight: 0.104<br>Top tokens:<br>: 0.229<br> : 0.204<br>ori: 0.199<br>iber: 0.195<br>\\!\\!\\!\\!: 0.174",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 19<br>Expert: 33<br>Weight: 0.059<br>Top tokens:<br>: 0.277<br>: 0.188<br>lega: 0.180<br> rem: 0.179<br> Pillow: 0.177",
           "",
           "",
           "",
           "",
           "Layer: 19<br>Expert: 38<br>Weight: 0.104<br>Top tokens:<br>osted: 0.243<br>acenter: 0.222<br>upper: 0.186<br>cross: 0.181<br> apoder: 0.169",
           "",
           "Layer: 19<br>Expert: 40<br>Weight: 0.107<br>Top tokens:<br>lau: 0.292<br>: 0.213<br>Bit: 0.196<br> : 0.156<br> records: 0.143",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 19<br>Expert: 56<br>Weight: 0.043<br>Top tokens:<br>know: 0.319<br>knows: 0.190<br>utenberg: 0.184<br>interp: 0.166<br>Know: 0.142",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 20<br>Expert: 26<br>Weight: 0.083<br>Top tokens:<br> that: 0.886<br>\tthat: 0.073<br>that: 0.034<br>That: 0.004<br> That: 0.002",
           "",
           "",
           "Layer: 20<br>Expert: 29<br>Weight: 0.069<br>Top tokens:<br> return: 0.334<br>: 0.213<br> Carry: 0.173<br>return: 0.173<br>: 0.107",
           "",
           "",
           "",
           "",
           "Layer: 20<br>Expert: 34<br>Weight: 0.051<br>Top tokens:<br>1: 0.403<br>4: 0.175<br>9: 0.159<br>5: 0.148<br>3: 0.115",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 20<br>Expert: 42<br>Weight: 0.123<br>Top tokens:<br> debes: 0.376<br> Pop: 0.225<br>erala: 0.147<br>ages: 0.128<br> POP: 0.123",
           "",
           "Layer: 20<br>Expert: 44<br>Weight: 0.044<br>Top tokens:<br>oka: 0.404<br>}^{[: 0.156<br>omi: 0.156<br>ama: 0.150<br>ICES: 0.135",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 20<br>Expert: 56<br>Weight: 0.087<br>Top tokens:<br>: 0.356<br>: 0.203<br>: 0.177<br>iennes: 0.151<br> lim: 0.114",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 21<br>Expert: 22<br>Weight: 0.027<br>Top tokens:<br> Foods: 0.246<br>: 0.220<br>communic: 0.184<br>yre: 0.182<br>: 0.168",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 21<br>Expert: 32<br>Weight: 0.057<br>Top tokens:<br>aviera: 0.258<br>walk: 0.228<br>: 0.178<br>walker: 0.174<br>: 0.162",
           "Layer: 21<br>Expert: 33<br>Weight: 0.315<br>Top tokens:<br>igh: 0.317<br>ently: 0.230<br>ENTRY: 0.192<br>hopper: 0.131<br>entry: 0.129",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 21<br>Expert: 42<br>Weight: 0.138<br>Top tokens:<br>0: 0.381<br>: 0.226<br>1: 0.186<br>an: 0.105<br>: 0.103",
           "",
           "",
           "",
           "",
           "Layer: 21<br>Expert: 47<br>Weight: 0.049<br>Top tokens:<br>: 0.379<br>na: 0.230<br>: 0.200<br>PC: 0.096<br>mi: 0.095",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 21<br>Expert: 58<br>Weight: 0.057<br>Top tokens:<br>: 0.281<br>: 0.233<br>EventListener: 0.190<br>: 0.162<br>: 0.135",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "Layer: 22<br>Expert: 3<br>Weight: 0.057<br>Top tokens:<br>: 0.555<br> : 0.345<br>: 0.048<br>: 0.041<br>: 0.011",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 22<br>Expert: 28<br>Weight: 0.032<br>Top tokens:<br>Schmidt: 0.342<br>raim: 0.292<br>: 0.139<br>: 0.121<br>reys: 0.106",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 22<br>Expert: 41<br>Weight: 0.051<br>Top tokens:<br> stand: 0.651<br> Stand: 0.162<br> sit: 0.104<br>Stand: 0.060<br>: 0.023",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 22<br>Expert: 51<br>Weight: 0.071<br>Top tokens:<br> will: 0.625<br> WILL: 0.104<br>cmark: 0.103<br>uega: 0.085<br>: 0.083",
           "",
           "",
           "Layer: 22<br>Expert: 55<br>Weight: 0.033<br>Top tokens:<br>;: 0.480<br> ;: 0.383<br>();: 0.094<br>2: 0.022<br>;</: 0.021",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 22<br>Expert: 62<br>Weight: 0.117<br>Top tokens:<br> by: 0.748<br>by: 0.080<br>ply: 0.075<br> munt: 0.055<br>FORM: 0.041"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 23<br>Expert: 9<br>Weight: 0.095<br>Top tokens:<br> around: 0.555<br>around: 0.264<br> round: 0.091<br>round: 0.046<br> called: 0.044",
           "Layer: 23<br>Expert: 10<br>Weight: 0.112<br>Top tokens:<br>: 0.536<br>dig: 0.155<br> Ngram: 0.108<br>illings: 0.102<br>DOCKED: 0.099",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 23<br>Expert: 27<br>Weight: 0.076<br>Top tokens:<br>: 0.491<br> q: 0.223<br>: 0.104<br>: 0.096<br>: 0.086",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 23<br>Expert: 35<br>Weight: 0.060<br>Top tokens:<br>paid: 0.301<br>: 0.236<br>: 0.188<br>iless: 0.139<br>: 0.136",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 23<br>Expert: 43<br>Weight: 0.067<br>Top tokens:<br> like: 0.245<br>Like: 0.204<br> Like: 0.194<br>enci: 0.181<br> need: 0.177",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 23<br>Expert: 62<br>Weight: 0.063<br>Top tokens:<br>@: 0.547<br> @: 0.234<br>rowse: 0.076<br>: 0.075<br>@-: 0.068"
          ],
          [
           "",
           "Layer: 24<br>Expert: 1<br>Weight: 0.103<br>Top tokens:<br>ierarchy: 0.423<br>NonUser: 0.370<br>: 0.109<br> relle: 0.054<br>etapes: 0.044",
           "",
           "",
           "",
           "Layer: 24<br>Expert: 5<br>Weight: 0.078<br>Top tokens:<br>run: 0.411<br>Run: 0.270<br> Run: 0.194<br> RUN: 0.080<br> run: 0.045",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 24<br>Expert: 13<br>Weight: 0.101<br>Top tokens:<br>jean: 0.409<br>Download: 0.165<br>: 0.148<br>legr: 0.145<br>oris: 0.133",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 24<br>Expert: 27<br>Weight: 0.038<br>Top tokens:<br>: 0.604<br>--------------------------------------------------------------------------: 0.136<br>\t\t\t\t\t\t\t\t\t\t\t\t\t: 0.120<br>niques: 0.073<br>=-=-=-=-: 0.067",
           "",
           "",
           "",
           "",
           "Layer: 24<br>Expert: 32<br>Weight: 0.092<br>Top tokens:<br>...\\: 0.607<br>''\\: 0.152<br>=\"../../../../../../../../: 0.091<br>\")));: 0.077<br>..\\: 0.074",
           "",
           "",
           "",
           "",
           "",
           "Layer: 24<br>Expert: 38<br>Weight: 0.037<br>Top tokens:<br>imath: 0.398<br>: 0.265<br>uelo: 0.120<br>WireFormat: 0.111<br>uffed: 0.106",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "Layer: 25<br>Expert: 5<br>Weight: 0.082<br>Top tokens:<br>onada: 0.503<br>: 0.166<br>icode: 0.149<br>: 0.110<br>codep: 0.073",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 25<br>Expert: 17<br>Weight: 0.081<br>Top tokens:<br>vider: 0.321<br>: 0.217<br> <\\: 0.186<br>ickr: 0.169<br>: 0.106",
           "",
           "Layer: 25<br>Expert: 19<br>Weight: 0.069<br>Top tokens:<br> [: 0.970<br>\t[: 0.013<br> ,[: 0.009<br>ifolds: 0.005<br>IEW: 0.004",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 25<br>Expert: 39<br>Weight: 0.048<br>Top tokens:<br>anell: 0.386<br> rose: 0.261<br>LAY: 0.148<br>ocese: 0.128<br>reth: 0.077",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 25<br>Expert: 51<br>Weight: 0.212<br>Top tokens:<br>tainment: 0.232<br>isat: 0.227<br>cov: 0.205<br>: 0.176<br>: 0.160",
           "",
           "",
           "Layer: 25<br>Expert: 55<br>Weight: 0.048<br>Top tokens:<br>August: 0.256<br>October: 0.216<br>April: 0.192<br>March: 0.180<br>February: 0.156",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "Layer: 26<br>Expert: 0<br>Weight: 0.058<br>Top tokens:<br> be: 0.603<br> make: 0.164<br> made: 0.121<br>be: 0.070<br>make: 0.043",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 26<br>Expert: 13<br>Weight: 0.105<br>Top tokens:<br> sure: 0.236<br>ernel: 0.236<br> truly: 0.223<br> just: 0.190<br> totally: 0.115",
           "",
           "",
           "",
           "Layer: 26<br>Expert: 18<br>Weight: 0.080<br>Top tokens:<br> triplets: 0.293<br>der: 0.225<br> Nation: 0.188<br> nation: 0.147<br>: 0.146",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 26<br>Expert: 44<br>Weight: 0.071<br>Top tokens:<br>: 0.313<br>: 0.194<br> : 0.181<br>: 0.161<br> routine: 0.151",
           "",
           "",
           "",
           "Layer: 26<br>Expert: 48<br>Weight: 0.048<br>Top tokens:<br>9: 0.400<br>8: 0.198<br>7: 0.147<br>6: 0.145<br>5: 0.111",
           "",
           "",
           "",
           "Layer: 26<br>Expert: 55<br>Weight: 0.058<br>Top tokens:<br>-: 0.980<br> -: 0.014<br>/: 0.003<br> /: 0.002<br>: 0.001",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "Layer: 27<br>Expert: 2<br>Weight: 0.077<br>Top tokens:<br>res: 0.233<br>So: 0.197<br>By: 0.195<br>Edit: 0.195<br>put: 0.180",
           "",
           "",
           "",
           "",
           "",
           "Layer: 27<br>Expert: 9<br>Weight: 0.045<br>Top tokens:<br>\": 0.415<br>: 0.244<br>: 0.135<br>\": 0.112<br>: 0.094",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 27<br>Expert: 23<br>Weight: 0.031<br>Top tokens:<br> post: 0.534<br> link: 0.297<br> article: 0.059<br> permalink: 0.058<br> posts: 0.052",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 27<br>Expert: 34<br>Weight: 0.118<br>Top tokens:<br> turned: 0.370<br>turned: 0.178<br> expected: 0.178<br> thought: 0.150<br> stopped: 0.125",
           "",
           "",
           "Layer: 27<br>Expert: 37<br>Weight: 0.170<br>Top tokens:<br> ends: 0.274<br> works: 0.263<br> looks: 0.215<br> lands: 0.124<br> lives: 0.124",
           "",
           "",
           "",
           "",
           "",
           "Layer: 27<br>Expert: 43<br>Weight: 0.107<br>Top tokens:<br>men: 0.432<br>iness: 0.151<br>y: 0.148<br>bars: 0.139<br>rates: 0.131",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ]
         ],
         "showscale": true,
         "text": [
          [
           "",
           "",
           "",
           "<span style=\"text-align: center\"> R<br>0.034</span>",
           "",
           "<span style=\"text-align: center\">edom<br>0.149</span>",
           "",
           "<span style=\"text-align: center\"><br>0.082</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.034</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">eq<br>0.039</span>",
           "",
           "<span style=\"text-align: center\">ront<br>0.129</span>",
           ""
          ],
          [
           "",
           "<span style=\"text-align: center\">ultors<br>0.037</span>",
           "",
           "",
           "<span style=\"text-align: center\"> s<br>0.222</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">ilateral<br>0.126</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">uriformes<br>0.057</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">ievable<br>0.061</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">oyo<br>0.033</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "<span style=\"text-align: center\"> present<br>0.026</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.455</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">Antecedents<br>0.031</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">atal<br>0.071</span>",
           "<span style=\"text-align: center\">lef<br>0.039</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">uple<br>0.066</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.026</span>",
           "",
           "<span style=\"text-align: center\"> <br>0.036</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"> <br>0.180</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">agger<br>0.048</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"> <br>0.021</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">gear<br>0.469</span>",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "<span style=\"text-align: center\">virt<br>0.049</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">PROTO<br>0.069</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.030</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"> cos<br>0.081</span>",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.484</span>",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">dorff<br>0.055</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">asma<br>0.155</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"> <br>0.099</span>",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.046</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.054</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"> xif<br>0.204</span>",
           "",
           "",
           "",
           "<span style=\"text-align: center\">osus<br>0.138</span>",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">ITOR<br>0.032</span>",
           "",
           "<span style=\"text-align: center\">uy<br>0.032</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.081</span>",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">anda<br>0.065</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.088</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"> condicions<br>0.271</span>",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "<span style=\"text-align: center\">ophile<br>0.054</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">igr<br>0.072</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">oldt<br>0.109</span>",
           "",
           "",
           "<span style=\"text-align: center\">oiselle<br>0.369</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">eting<br>0.038</span>",
           "<span style=\"text-align: center\"> <br>0.040</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">ugar<br>0.129</span>",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"> \"<br>0.080</span>",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.130</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">ignite<br>0.113</span>",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.034</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">Ungrouped<br>0.066</span>",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.052</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.337</span>",
           "",
           "",
           "",
           "<span style=\"text-align: center\">ornia<br>0.050</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.053</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">mere<br>0.092</span>",
           "",
           "<span style=\"text-align: center\"><br>0.070</span>",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">elled<br>0.089</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.070</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">lood<br>0.048</span>",
           "<span style=\"text-align: center\">plete<br>0.049</span>",
           "",
           "",
           "<span style=\"text-align: center\">out<br>0.132</span>",
           "",
           "<span style=\"text-align: center\"> <br>0.057</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "<span style=\"text-align: center\">enir<br>0.047</span>",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">iamond<br>0.060</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">aulay<br>0.074</span>",
           "<span style=\"text-align: center\"><br>0.048</span>",
           "<span style=\"text-align: center\">moz<br>0.055</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">ILED<br>0.211</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.049</span>",
           "",
           "<span style=\"text-align: center\"><br>0.032</span>",
           "",
           "",
           "<span style=\"text-align: center\">crest<br>0.060</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">olla<br>0.192</span>",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">ouz<br>0.043</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">duc<br>0.148</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">estions<br>0.065</span>",
           "",
           "",
           "",
           "<span style=\"text-align: center\"> punta<br>0.058</span>",
           "",
           "<span style=\"text-align: center\"><br>0.069</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"> only<br>0.066</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">9<br>0.107</span>",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.068</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "<span style=\"text-align: center\">0<br>0.146</span>",
           "<span style=\"text-align: center\">aband<br>0.062</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">colhead<br>0.129</span>",
           "",
           "",
           "",
           "<span style=\"text-align: center\">aye<br>0.055</span>",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.055</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.058</span>",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"> <br>0.073</span>",
           "",
           "",
           "",
           "<span style=\"text-align: center\">0<br>0.312</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"> ter<br>0.136</span>",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.045</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">upp<br>0.062</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">ffected<br>0.032</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.054</span>",
           "",
           "",
           "<span style=\"text-align: center\"> crossorigin<br>0.035</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">0<br>0.293</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.034</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">es<br>0.121</span>",
           "",
           "",
           "",
           "<span style=\"text-align: center\"> <br>0.053</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">TOOLSET<br>0.070</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"> flyback<br>0.054</span>",
           "",
           "",
           "<span style=\"text-align: center\"> v<br>0.203</span>",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">isses<br>0.055</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">etimes<br>0.137</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"> <br>0.045</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "<span style=\"text-align: center\"> comple<br>0.094</span>",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.104</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.059</span>",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">osted<br>0.104</span>",
           "",
           "<span style=\"text-align: center\">lau<br>0.107</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">know<br>0.043</span>",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"> that<br>0.083</span>",
           "",
           "",
           "<span style=\"text-align: center\"> return<br>0.069</span>",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">1<br>0.051</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"> debes<br>0.123</span>",
           "",
           "<span style=\"text-align: center\">oka<br>0.044</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.087</span>",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"> Foods<br>0.027</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">aviera<br>0.057</span>",
           "<span style=\"text-align: center\">igh<br>0.315</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">0<br>0.138</span>",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.049</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.057</span>",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.057</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">Schmidt<br>0.032</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"> stand<br>0.051</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"> will<br>0.071</span>",
           "",
           "",
           "<span style=\"text-align: center\">;<br>0.033</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"> by<br>0.117</span>"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"> around<br>0.095</span>",
           "<span style=\"text-align: center\"><br>0.112</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.076</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">paid<br>0.060</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"> like<br>0.067</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">@<br>0.063</span>"
          ],
          [
           "",
           "<span style=\"text-align: center\">ierarchy<br>0.103</span>",
           "",
           "",
           "",
           "<span style=\"text-align: center\">run<br>0.078</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">jean<br>0.101</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.038</span>",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">...\\<br>0.092</span>",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">imath<br>0.037</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">onada<br>0.082</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">vider<br>0.081</span>",
           "",
           "<span style=\"text-align: center\"> [<br>0.069</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">anell<br>0.048</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">tainment<br>0.212</span>",
           "",
           "",
           "<span style=\"text-align: center\">August<br>0.048</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "<span style=\"text-align: center\"> be<br>0.058</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"> sure<br>0.105</span>",
           "",
           "",
           "",
           "<span style=\"text-align: center\"> triplets<br>0.080</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"><br>0.071</span>",
           "",
           "",
           "",
           "<span style=\"text-align: center\">9<br>0.048</span>",
           "",
           "",
           "",
           "<span style=\"text-align: center\">-<br>0.058</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ],
          [
           "",
           "",
           "<span style=\"text-align: center\">res<br>0.077</span>",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">\"<br>0.045</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"> post<br>0.031</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\"> turned<br>0.118</span>",
           "",
           "",
           "<span style=\"text-align: center\"> ends<br>0.170</span>",
           "",
           "",
           "",
           "",
           "",
           "<span style=\"text-align: center\">men<br>0.107</span>",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ]
         ],
         "textfont": {
          "color": "white",
          "family": "monospace",
          "size": 9
         },
         "texttemplate": "%{text}",
         "type": "heatmap",
         "x": [
          "Expert 0",
          "Expert 1",
          "Expert 2",
          "Expert 3",
          "Expert 4",
          "Expert 5",
          "Expert 7",
          "Expert 8",
          "Expert 9",
          "Expert 10",
          "Expert 11",
          "Expert 12",
          "Expert 13",
          "Expert 15",
          "Expert 16",
          "Expert 17",
          "Expert 18",
          "Expert 19",
          "Expert 20",
          "Expert 21",
          "Expert 22",
          "Expert 23",
          "Expert 24",
          "Expert 25",
          "Expert 26",
          "Expert 27",
          "Expert 28",
          "Expert 29",
          "Expert 30",
          "Expert 31",
          "Expert 32",
          "Expert 33",
          "Expert 34",
          "Expert 35",
          "Expert 36",
          "Expert 37",
          "Expert 38",
          "Expert 39",
          "Expert 40",
          "Expert 41",
          "Expert 42",
          "Expert 43",
          "Expert 44",
          "Expert 45",
          "Expert 46",
          "Expert 47",
          "Expert 48",
          "Expert 51",
          "Expert 52",
          "Expert 54",
          "Expert 55",
          "Expert 56",
          "Expert 57",
          "Expert 58",
          "Expert 59",
          "Expert 60",
          "Expert 61",
          "Expert 62"
         ],
         "y": [
          "Layer 1",
          "Layer 2",
          "Layer 3",
          "Layer 4",
          "Layer 5",
          "Layer 6",
          "Layer 7",
          "Layer 8",
          "Layer 9",
          "Layer 10",
          "Layer 11",
          "Layer 12",
          "Layer 13",
          "Layer 14",
          "Layer 15",
          "Layer 16",
          "Layer 17",
          "Layer 18",
          "Layer 19",
          "Layer 20",
          "Layer 21",
          "Layer 22",
          "Layer 23",
          "Layer 24",
          "Layer 25",
          "Layer 26",
          "Layer 27"
         ],
         "z": [
          [
           0,
           0,
           0,
           0.0341796875,
           0,
           0.1494140625,
           0,
           0.0819091796875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.034088134765625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.038543701171875,
           0,
           0.1285400390625,
           0
          ],
          [
           0,
           0.03668212890625,
           0,
           0,
           0.2215576171875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.126220703125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.056793212890625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.060791015625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.032501220703125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0.025970458984375,
           0,
           0,
           0,
           0,
           0,
           0,
           0.455322265625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0306549072265625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.07147216796875,
           0.03887939453125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.06573486328125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.025726318359375,
           0,
           0.035919189453125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.1795654296875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.048065185546875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.021209716796875,
           0,
           0,
           0,
           0,
           0,
           0,
           0.469482421875,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0.04901123046875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.068603515625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0297393798828125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.08148193359375,
           0,
           0,
           0,
           0.483642578125,
           0,
           0,
           0,
           0,
           0.05523681640625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.154541015625,
           0,
           0,
           0,
           0,
           0,
           0,
           0.09918212890625,
           0,
           0,
           0,
           0,
           0,
           0.045501708984375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0538330078125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.20361328125,
           0,
           0,
           0,
           0.1383056640625,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.031646728515625,
           0,
           0.031829833984375,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0809326171875,
           0,
           0,
           0,
           0,
           0,
           0.06500244140625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.087646484375,
           0,
           0,
           0,
           0,
           0,
           0,
           0.27099609375,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0.0537109375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.07159423828125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.1087646484375,
           0,
           0,
           0.36865234375,
           0,
           0,
           0,
           0,
           0,
           0,
           0.037841796875,
           0.039794921875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0.1287841796875,
           0,
           0,
           0,
           0,
           0.0799560546875,
           0,
           0,
           0,
           0.1304931640625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.11273193359375,
           0,
           0,
           0,
           0,
           0,
           0.0335693359375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.06561279296875,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0.051605224609375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.33740234375,
           0,
           0,
           0,
           0.04962158203125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.052764892578125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.091552734375,
           0,
           0.06988525390625,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0892333984375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.07000732421875,
           0,
           0,
           0,
           0,
           0,
           0,
           0.04840087890625,
           0.049072265625,
           0,
           0,
           0.13232421875,
           0,
           0.05743408203125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0.046630859375,
           0,
           0,
           0,
           0,
           0.06011962890625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.074462890625,
           0.047698974609375,
           0.054931640625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.211181640625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0.049102783203125,
           0,
           0.032379150390625,
           0,
           0,
           0.0595703125,
           0,
           0,
           0,
           0,
           0,
           0,
           0.19189453125,
           0,
           0,
           0,
           0,
           0.04278564453125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.147705078125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.06488037109375,
           0,
           0,
           0,
           0.0577392578125,
           0,
           0.069091796875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.06597900390625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.107421875,
           0,
           0,
           0,
           0,
           0,
           0.068115234375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0.1455078125,
           0.062225341796875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.128662109375,
           0,
           0,
           0,
           0.054962158203125,
           0,
           0,
           0,
           0,
           0.05474853515625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.057891845703125,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0.07293701171875,
           0,
           0,
           0,
           0.3125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.13623046875,
           0,
           0,
           0.045379638671875,
           0,
           0,
           0,
           0,
           0,
           0,
           0.061920166015625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.031890869140625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.053680419921875,
           0,
           0,
           0.0347900390625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.292724609375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.034210205078125,
           0,
           0,
           0,
           0,
           0,
           0,
           0.120849609375,
           0,
           0,
           0,
           0.05316162109375,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.06951904296875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.053680419921875,
           0,
           0,
           0.203125,
           0,
           0,
           0,
           0,
           0.055206298828125,
           0,
           0,
           0,
           0,
           0,
           0,
           0.13720703125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.044891357421875,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0.09417724609375,
           0,
           0,
           0.10443115234375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.05865478515625,
           0,
           0,
           0,
           0,
           0.10406494140625,
           0,
           0.10693359375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0433349609375,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.083251953125,
           0,
           0,
           0.06890869140625,
           0,
           0,
           0,
           0,
           0.050506591796875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.123291015625,
           0,
           0.04376220703125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0869140625,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0274658203125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.056640625,
           0.314697265625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.1376953125,
           0,
           0,
           0,
           0,
           0.049407958984375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.05731201171875,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0.057098388671875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.032196044921875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.05096435546875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.07147216796875,
           0,
           0,
           0.032501220703125,
           0,
           0,
           0,
           0,
           0,
           0,
           0.11688232421875
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0950927734375,
           0.11248779296875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.076416015625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.060211181640625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0670166015625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.06292724609375
          ],
          [
           0,
           0.1033935546875,
           0,
           0,
           0,
           0.07757568359375,
           0,
           0,
           0,
           0,
           0,
           0,
           0.1009521484375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.038482666015625,
           0,
           0,
           0,
           0,
           0.0919189453125,
           0,
           0,
           0,
           0,
           0,
           0.037353515625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0.08209228515625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.08148193359375,
           0,
           0.06903076171875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.04791259765625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.212158203125,
           0,
           0,
           0.04791259765625,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0.058319091796875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.10479736328125,
           0,
           0,
           0,
           0.079833984375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0711669921875,
           0,
           0,
           0,
           0.048004150390625,
           0,
           0,
           0,
           0.058074951171875,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0.07745361328125,
           0,
           0,
           0,
           0,
           0,
           0.045166015625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0312347412109375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.11798095703125,
           0,
           0,
           0.1695556640625,
           0,
           0,
           0,
           0,
           0,
           0.10699462890625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ]
         ]
        }
       ],
       "layout": {
        "font": {
         "color": "white"
        },
        "height": 1350,
        "margin": {
         "b": 100,
         "l": 50,
         "r": 50,
         "t": 100
        },
        "paper_bgcolor": "black",
        "plot_bgcolor": "black",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Expert Contributions by Layer - Position 4"
        },
        "width": 2900,
        "xaxis": {
         "dtick": 1,
         "gridcolor": "rgba(128, 128, 128, 0.2)",
         "gridwidth": 1,
         "linecolor": "rgba(128, 128, 128, 0.2)",
         "linewidth": 1,
         "scaleanchor": "y",
         "scaleratio": 1,
         "showgrid": true,
         "showline": true,
         "tickangle": 45,
         "tickfont": {
          "size": 9
         },
         "tickmode": "array",
         "ticktext": [
          "Expert 0",
          "Expert 1",
          "Expert 2",
          "Expert 3",
          "Expert 4",
          "Expert 5",
          "Expert 7",
          "Expert 8",
          "Expert 9",
          "Expert 10",
          "Expert 11",
          "Expert 12",
          "Expert 13",
          "Expert 15",
          "Expert 16",
          "Expert 17",
          "Expert 18",
          "Expert 19",
          "Expert 20",
          "Expert 21",
          "Expert 22",
          "Expert 23",
          "Expert 24",
          "Expert 25",
          "Expert 26",
          "Expert 27",
          "Expert 28",
          "Expert 29",
          "Expert 30",
          "Expert 31",
          "Expert 32",
          "Expert 33",
          "Expert 34",
          "Expert 35",
          "Expert 36",
          "Expert 37",
          "Expert 38",
          "Expert 39",
          "Expert 40",
          "Expert 41",
          "Expert 42",
          "Expert 43",
          "Expert 44",
          "Expert 45",
          "Expert 46",
          "Expert 47",
          "Expert 48",
          "Expert 51",
          "Expert 52",
          "Expert 54",
          "Expert 55",
          "Expert 56",
          "Expert 57",
          "Expert 58",
          "Expert 59",
          "Expert 60",
          "Expert 61",
          "Expert 62"
         ],
         "tickvals": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57
         ],
         "title": {
          "text": "Experts"
         }
        },
        "yaxis": {
         "autorange": "reversed",
         "dtick": 1,
         "gridcolor": "rgba(128, 128, 128, 0.2)",
         "gridwidth": 1,
         "linecolor": "rgba(128, 128, 128, 0.2)",
         "linewidth": 1,
         "scaleanchor": "x",
         "scaleratio": 1,
         "showgrid": true,
         "showline": true,
         "tickfont": {
          "size": 9
         },
         "title": {
          "text": "Layers"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First run - analyze and cache results\n",
    "text = \"the quick brown fox\"\n",
    "figures, results, _ = analyze_dataset(\n",
    "    text=text,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    token_position=4,  # To visualize \"quick\"\n",
    "    domain=\"test1\",\n",
    "    force_recompute=False,\n",
    "    plot_type='expert'\n",
    ")\n",
    "\n",
    "if figures:\n",
    "    if 'heatmap' in figures: #kinda useless one but keeping it for the time being\n",
    "        figures['heatmap'].show()\n",
    "    if 'logit_lens' in figures:\n",
    "        figures['logit_lens'].show()\n",
    "    if 'expert' in figures:\n",
    "        figures['expert'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
