{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "import math\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"Get the optimal available device\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        # Enable TF32 for better performance on Ampere GPUs (A100, A6000, etc)\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        # Set memory allocation settings\n",
    "        torch.cuda.empty_cache()\n",
    "        # Enable CUDNN benchmarking for better performance\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4defbb39fbeb4887b4bc2bd1ed9a78e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of shared experts: 2\n",
      "Number of routed experts: 64\n",
      "Number of experts per token: 6\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"deepseek-ai/deepseek-moe-16b-base\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"deepseek-ai/deepseek-moe-16b-base\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Verify the configuration\n",
    "print(f\"Number of shared experts: {model.config.n_shared_experts}\")\n",
    "print(f\"Number of routed experts: {model.config.n_routed_experts}\")\n",
    "print(f\"Number of experts per token: {model.config.num_experts_per_tok}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOELens:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.activations = defaultdict(dict)\n",
    "        self.hook_handles = []\n",
    "        self.setup_hooks()\n",
    "\n",
    "    def setup_hooks(self):\n",
    "        def get_gate_hook(layer_idx):\n",
    "            def hook(module, inp, out):\n",
    "                if isinstance(out, tuple):\n",
    "                    topk_idx, topk_weight, _ = out\n",
    "                    \n",
    "                    # Get hidden states from input\n",
    "                    hidden_states = inp[0]\n",
    "                    batch_size, seq_len, hidden_dim = hidden_states.shape\n",
    "                    \n",
    "                    # Project to vocab space to get token predictions\n",
    "                    with torch.no_grad():\n",
    "                        expert_outputs = {}\n",
    "                        # Only process the expert indices that were actually selected\n",
    "                        unique_experts = torch.unique(topk_idx)\n",
    "                        for expert_idx in unique_experts:\n",
    "                            expert_idx = expert_idx.item()  # Convert to int\n",
    "                            if hasattr(self.model.model.layers[layer_idx].mlp, 'experts'):\n",
    "                                expert = self.model.model.layers[layer_idx].mlp.experts[expert_idx]\n",
    "                                expert_output = expert(hidden_states.view(-1, hidden_dim))\n",
    "                                logits = self.model.lm_head(expert_output)\n",
    "                                top_tokens = torch.topk(logits, k=5, dim=-1)\n",
    "                                expert_outputs[expert_idx] = {\n",
    "                                    'token_ids': top_tokens.indices,\n",
    "                                    'probs': torch.softmax(top_tokens.values, dim=-1)\n",
    "                                }\n",
    "\n",
    "                    self.activations[f'layer_{layer_idx}'] = {\n",
    "                        'router_weights': topk_weight.detach(),\n",
    "                        'router_indices': topk_idx.detach(),\n",
    "                        'expert_outputs': expert_outputs\n",
    "                    }\n",
    "            return hook\n",
    "\n",
    "        def get_shared_expert_hook(layer_idx):\n",
    "            def hook(module, inp, out):\n",
    "                x = inp[0]\n",
    "                batch_size, seq_len, hidden_dim = x.shape\n",
    "\n",
    "                # Compute gate and up projections\n",
    "                gate_proj = module.gate_proj(x)  # [batch, seq_len, 2816]\n",
    "                up_proj = module.up_proj(x)      # [batch, seq_len, 2816]\n",
    "                act = module.act_fn(gate_proj) * up_proj\n",
    "\n",
    "                # Split into two experts (2816 = 2*1408)\n",
    "                expert0_act = act[..., :1408]\n",
    "                expert1_act = act[..., 1408:]\n",
    "\n",
    "                # Project to vocabulary\n",
    "                with torch.no_grad():\n",
    "                    # Process first expert\n",
    "                    expert0_out = module.down_proj(\n",
    "                        F.pad(expert0_act, (0, 1408))  # Pad to match full width\n",
    "                    )\n",
    "                    logits0 = self.model.lm_head(expert0_out)\n",
    "                    top_tokens0 = torch.topk(logits0, k=5, dim=-1)\n",
    "\n",
    "                    # Process second expert\n",
    "                    expert1_out = module.down_proj(\n",
    "                        F.pad(expert1_act, (1408, 0))  # Pad to match full width\n",
    "                    )\n",
    "                    logits1 = self.model.lm_head(expert1_out)\n",
    "                    top_tokens1 = torch.topk(logits1, k=5, dim=-1)\n",
    "\n",
    "                self.activations[f'layer_{layer_idx}']['shared_experts'] = {\n",
    "                    'expert0': {\n",
    "                        'token_ids': top_tokens0.indices,\n",
    "                        'probs': torch.softmax(top_tokens0.values, dim=-1),\n",
    "                        'weight': 1.0\n",
    "                    },\n",
    "                    'expert1': {\n",
    "                        'token_ids': top_tokens1.indices,\n",
    "                        'probs': torch.softmax(top_tokens1.values, dim=-1),\n",
    "                        'weight': 1.0\n",
    "                    }\n",
    "                }\n",
    "            return hook\n",
    "\n",
    "        for i, layer in enumerate(self.model.model.layers):\n",
    "            if hasattr(layer.mlp, 'gate'):\n",
    "                handle = layer.mlp.gate.register_forward_hook(get_gate_hook(i))\n",
    "                self.hook_handles.append(handle)\n",
    "            if hasattr(layer.mlp, 'shared_experts'):\n",
    "                handle = layer.mlp.shared_experts.register_forward_hook(get_shared_expert_hook(i))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    def analyze_text(self, input_ids: torch.Tensor) -> dict:\n",
    "        \"\"\"\n",
    "        Create a combined analysis showing both expert contributions and final predictions.\n",
    "        \"\"\"\n",
    "        self.activations.clear()\n",
    "        final_logits = None\n",
    "        \n",
    "        def get_final_output_hook(module, inp, out):\n",
    "            nonlocal final_logits\n",
    "            final_logits = out\n",
    "\n",
    "        # Add hook for final output\n",
    "        final_hook = self.model.lm_head.register_forward_hook(get_final_output_hook)\n",
    "        self.hook_handles.append(final_hook)\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids)\n",
    "\n",
    "        # Process the outputs for each layer\n",
    "        for layer_key, layer_data in self.activations.items():\n",
    "            if not layer_key.startswith('layer_'):\n",
    "                continue\n",
    "                \n",
    "            tokens_data = {}\n",
    "            for pos in range(input_ids.shape[1]):\n",
    "                expert_outputs = []\n",
    "                \n",
    "                # Process routed experts\n",
    "                if 'expert_outputs' in layer_data:\n",
    "                    router_indices = layer_data['router_indices'][pos]\n",
    "                    router_weights = layer_data['router_weights'][pos]\n",
    "                    \n",
    "                    for idx, expert_idx in enumerate(router_indices):\n",
    "                        expert_idx = expert_idx.item()\n",
    "                        expert_data = layer_data['expert_outputs'].get(expert_idx)\n",
    "                        if expert_data is not None:\n",
    "                            weight = router_weights[idx].item()\n",
    "                            token_ids = expert_data['token_ids'][pos]\n",
    "                            probs = expert_data['probs'][pos]\n",
    "                            \n",
    "                            expert_outputs.append({\n",
    "                                \"expert_id\": expert_idx,\n",
    "                                \"weight\": weight,\n",
    "                                \"top_tokens\": [\n",
    "                                    (self.tokenizer.decode([tid.item()]), prob.item())\n",
    "                                    for tid, prob in zip(token_ids, probs)\n",
    "                                ],\n",
    "                                \"expert_type\": \"routed\"\n",
    "                            })\n",
    "                \n",
    "                # Process shared experts\n",
    "                if 'shared_experts' in layer_data:\n",
    "                    for expert_key in ['expert0', 'expert1']:\n",
    "                        expert_data = layer_data['shared_experts'][expert_key]\n",
    "                        token_ids = expert_data['token_ids'][0, pos]\n",
    "                        probs = expert_data['probs'][0, pos]\n",
    "                        \n",
    "                        expert_outputs.append({\n",
    "                            'expert_id': f'S{expert_key[-1]}',\n",
    "                            'weight': expert_data['weight'],\n",
    "                            'top_tokens': [\n",
    "                                (self.tokenizer.decode([tid.item()]), prob.item())\n",
    "                                for tid, prob in zip(token_ids, probs)\n",
    "                            ],\n",
    "                            'expert_type': 'shared'\n",
    "                        })\n",
    "                \n",
    "                tokens_data[f\"token_{pos}\"] = {\n",
    "                    \"position\": pos,\n",
    "                    \"expert_outputs\": expert_outputs\n",
    "                }\n",
    "                \n",
    "            self.activations[layer_key][\"tokens\"] = tokens_data\n",
    "\n",
    "        # Add final predictions\n",
    "        if final_logits is not None:\n",
    "            final_predictions = {}\n",
    "            for pos in range(input_ids.shape[1]):\n",
    "                logits = final_logits[0, pos]\n",
    "                top_values, top_indices = torch.topk(logits, k=5)\n",
    "                probs = torch.softmax(top_values, dim=-1)\n",
    "                \n",
    "                final_predictions[pos] = {\n",
    "                    'token_ids': top_indices,\n",
    "                    'probs': probs,\n",
    "                    'tokens': [self.tokenizer.decode([idx.item()]) for idx in top_indices]\n",
    "                }\n",
    "            self.activations['final_predictions'] = final_predictions\n",
    "\n",
    "        return dict(self.activations)\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        for handle in self.hook_handles:\n",
    "            handle.remove()\n",
    "        self.hook_handles = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fixed for shared experts and routed exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_layer_analysis(tokenizer, results: Dict, token_position: int, input_text: str):\n",
    "    \"\"\"\n",
    "    Modified version that includes final hidden state layer in visualization \n",
    "    \"\"\"\n",
    "    layer_nums = []\n",
    "    expert_ids = []\n",
    "    weights = []\n",
    "    hover_texts = []\n",
    "    colors = []  # Add colors array for marker colors\n",
    "    \n",
    "    n_routed_experts = 6  # Number of routed experts\n",
    "    n_shared_experts = 2  # Number of shared experts\n",
    "    \n",
    "    # Extract token we're visualizing\n",
    "    tokens = tokenizer.encode(input_text)\n",
    "    token = tokenizer.decode([tokens[token_position]])\n",
    "    print(f\"Visualizing token: {token}\")\n",
    "    \n",
    "    # Add final hidden layer to layer range\n",
    "    layers = list(range(1, 28)) + ['hidden']  # Add 'hidden' as final layer\n",
    "        \n",
    "    for layer_idx in layers:\n",
    "        layer_data = results.get(f\"layer_{layer_idx}\", {})\n",
    "        if not layer_data:\n",
    "            # For hidden state layer, get the final hidden state before lm_head\n",
    "            if layer_idx == 'hidden':\n",
    "                # Get hidden state representation\n",
    "                hidden_data = results.get(\"final_hidden\", {})\n",
    "                if hidden_data:\n",
    "                    layer_nums.append('hidden')\n",
    "                    expert_ids.append(\"H\")  # Mark as hidden layer\n",
    "                    weight = hidden_data.get('weight', 0)\n",
    "                    weights.append(weight)\n",
    "                    colors.append('rgba(253,231,37,0.8)')  # Yellow color for hidden\n",
    "                    \n",
    "                    top_tokens_text = \"<br>\".join([\n",
    "                        f\"{token}: {prob:.3f}\" \n",
    "                        for token, prob in hidden_data[\"top_tokens\"][:5]\n",
    "                    ])\n",
    "                    hover_text = f\"Final Hidden State<br>Weight: {weight:.3f}<br>Top tokens:<br>{top_tokens_text}\"\n",
    "                    hover_texts.append(hover_text)\n",
    "            continue\n",
    "\n",
    "        token_data = None\n",
    "        for data in layer_data.get(\"tokens\", {}).values():\n",
    "            if data[\"position\"] == token_position:\n",
    "                token_data = data\n",
    "                break\n",
    "                \n",
    "        if not token_data:\n",
    "            continue\n",
    "\n",
    "        # Process routed experts\n",
    "        expert_map = {\n",
    "            exp[\"expert_id\"]: exp \n",
    "            for exp in token_data[\"expert_outputs\"] \n",
    "            if exp[\"expert_type\"] == \"routed\"\n",
    "        }\n",
    "        \n",
    "        # Go through routed experts\n",
    "        for expert_id in range(n_routed_experts):\n",
    "            layer_nums.append(layer_idx)\n",
    "            expert_ids.append(f\"R{expert_id}\")  # Prefix with R for routed\n",
    "            \n",
    "            if expert_id in expert_map:\n",
    "                expert_data = expert_map[expert_id]\n",
    "                weight = expert_data[\"weight\"]\n",
    "                top_tokens_text = \"<br>\".join([\n",
    "                    f\"{token}: {prob:.3f}\" \n",
    "                    for token, prob in expert_data[\"top_tokens\"][:5]\n",
    "                ])\n",
    "                hover_text = f\"Layer: {layer_idx}<br>Routed Expert: {expert_id}<br>Weight: {weight:.3f}<br>Top tokens:<br>{top_tokens_text}\"\n",
    "                hover_texts.append(hover_text)\n",
    "            else:\n",
    "                weight = 0\n",
    "                hover_texts.append(None)\n",
    "            \n",
    "            weights.append(weight)\n",
    "            colors.append('rgba(253,231,37,0.8)')  # Yellow color for routed\n",
    "            \n",
    "        # Process shared experts\n",
    "        shared_experts = [\n",
    "            exp for exp in token_data[\"expert_outputs\"]\n",
    "            if exp.get(\"expert_type\") == \"shared\"\n",
    "        ]\n",
    "        for exp in shared_experts:\n",
    "            expert_id = exp['expert_id']  # S0 or S1\n",
    "            weight = exp['weight']\n",
    "            top_tokens = exp['top_tokens']\n",
    "            \n",
    "            layer_nums.append(layer_idx)\n",
    "            expert_ids.append(expert_id)\n",
    "            weights.append(weight)\n",
    "            colors.append('rgba(0,176,246,0.8)')  # Blue color for shared experts\n",
    "            \n",
    "            top_tokens_text = \"<br>\".join([\n",
    "                f\"{token}: {prob:.3f}\" \n",
    "                for token, prob in top_tokens[:5]\n",
    "            ])\n",
    "            hover_text = f\"Layer: {layer_idx}<br>Shared Expert: {expert_id}<br>Weight: {weight:.3f}<br>Top tokens:<br>{top_tokens_text}\"\n",
    "            hover_texts.append(hover_text)\n",
    "    \n",
    "    # Create plotly scatter plot\n",
    "    fig = go.Figure(data=go.Scatter(\n",
    "        x=expert_ids,\n",
    "        y=layer_nums,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=9,\n",
    "            color=colors,  # Use the colors array instead of colorscale\n",
    "            showscale=True,\n",
    "            colorbar=dict(\n",
    "                title='Weight',\n",
    "                tickmode='linear',\n",
    "                tick0=0,\n",
    "                dtick=0.2\n",
    "            ),\n",
    "        ),\n",
    "        text=hover_texts,\n",
    "        hoverinfo='text',\n",
    "        hovertemplate='%{text}<extra></extra>',\n",
    "    ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        template='plotly_dark',\n",
    "        title=f'Expert Activations for Token \"{token}\" at Position {token_position}',\n",
    "        xaxis_title='Expert ID (R: Routed, S: Shared, H: Hidden)',\n",
    "        yaxis_title='Layer',\n",
    "        yaxis=dict(autorange='reversed'),\n",
    "        width=1200,\n",
    "        height=850,  # Increased height to accommodate extra row\n",
    "        showlegend=False,\n",
    "        plot_bgcolor='black',\n",
    "        paper_bgcolor='black'\n",
    "    )\n",
    "    \n",
    "    # Add grid lines\n",
    "    fig.update_xaxes(\n",
    "        showgrid=True,\n",
    "        gridwidth=1,\n",
    "        gridcolor='rgba(128, 128, 128, 0.2)',\n",
    "        tickmode='array',\n",
    "        ticktext=[f\"R{i}\" for i in range(n_routed_experts)] + [\"S0\", \"S1\", \"H\"],\n",
    "        tickvals=list(range(n_routed_experts + 3))\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        showgrid=True,\n",
    "        gridwidth=1,\n",
    "        gridcolor='rgba(128, 128, 128, 0.2)'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_enhanced_logit_lens(model_outputs, tokenizer, input_text, position):\n",
    "    \"\"\"\n",
    "    Creates an enhanced logit lens visualization showing expert activations and their token predictions.\n",
    "    Handles both routed and shared experts.\n",
    "    \"\"\"\n",
    "    all_tokens = set()\n",
    "    token_probs = defaultdict(lambda: defaultdict(float))\n",
    "    layers = []\n",
    "    \n",
    "    layer_keys = sorted(\n",
    "        [k for k in model_outputs.keys() if k.startswith('layer_')],\n",
    "        key=lambda x: int(x.split('_')[1])\n",
    "    )\n",
    "    \n",
    "    for layer_key in layer_keys:\n",
    "        layer_idx = int(layer_key.split('_')[1])\n",
    "        layer_data = model_outputs[layer_key]\n",
    "        \n",
    "        token_data = None\n",
    "        for data in layer_data.get(\"tokens\", {}).values():\n",
    "            if data[\"position\"] == position:\n",
    "                token_data = data\n",
    "                break\n",
    "                \n",
    "        if not token_data:\n",
    "            continue\n",
    "            \n",
    "        layers.append(layer_idx)\n",
    "        \n",
    "        # Process both routed and shared experts\n",
    "        for expert_output in token_data[\"expert_outputs\"]:\n",
    "            weight = expert_output[\"weight\"]\n",
    "            expert_type = expert_output[\"expert_type\"]\n",
    "            \n",
    "            for token, prob in expert_output[\"top_tokens\"]:\n",
    "                all_tokens.add(token)\n",
    "                # For shared experts, we might want to weight differently\n",
    "                if expert_type == \"shared\":\n",
    "                    token_probs[layer_idx][token] += prob * 0.5  # Adjust weight for shared experts\n",
    "                else:\n",
    "                    token_probs[layer_idx][token] += prob * weight\n",
    "\n",
    "    tokens = sorted(list(all_tokens))\n",
    "    prob_matrix = []\n",
    "    \n",
    "    for layer_idx in layers:\n",
    "        layer_probs = []\n",
    "        for token in tokens:\n",
    "            layer_probs.append(token_probs[layer_idx].get(token, 0.0))\n",
    "        prob_matrix.append(layer_probs)\n",
    "\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=prob_matrix,\n",
    "        x=tokens,\n",
    "        y=[f'Layer {idx}' for idx in layers],\n",
    "        colorscale='Viridis',\n",
    "        showscale=True\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f'Token Probabilities Across Layers - Position {position}',\n",
    "        xaxis_title='Predicted Tokens',\n",
    "        yaxis_title='Layer',\n",
    "        yaxis={'autorange': 'reversed'},\n",
    "        width=1200,\n",
    "        height=800,\n",
    "        plot_bgcolor='black',\n",
    "        paper_bgcolor='black',\n",
    "        font=dict(color='white')\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='rgba(128, 128, 128, 0.2)')\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='rgba(128, 128, 128, 0.2)')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_moe_logit_lens_with_active_expert_outputs(model, inputs, tokenizer, num_active_experts=7):\n",
    "    model.eval()\n",
    "\n",
    "    # Initial embedding\n",
    "    x = model.model.embed_tokens(inputs)\n",
    "\n",
    "    # Process each layer\n",
    "    for layer_idx, layer in enumerate(model.model.layers):\n",
    "        print(f\"Layer {layer_idx + 1}\")\n",
    "        x = layer.input_layernorm(x)\n",
    "\n",
    "        # Self-attention output\n",
    "        # Remove the position_ids argument here, let the model handle it internally\n",
    "        attn_output = layer.self_attn(x, x, x)\n",
    "        x = attn_output + x  # Residual connection\n",
    "        x = layer.post_attention_layernorm(x)\n",
    "\n",
    "        # MoE Layer\n",
    "        moe_output = layer.mlp(x)\n",
    "        gate_values = moe_output[\"gate_values\"]\n",
    "        expert_outputs = moe_output[\"expert_outputs\"]\n",
    "\n",
    "        # Extract active experts\n",
    "        for batch_idx, gates in enumerate(gate_values):\n",
    "            active_experts = gates.argsort(descending=True)[:num_active_experts]\n",
    "            print(f\"  Batch {batch_idx + 1}:\")\n",
    "            for expert_idx in active_experts:\n",
    "                expert_weight = gates[expert_idx].item()\n",
    "                expert_output = expert_outputs[batch_idx, :, expert_idx]\n",
    "                top_tokens = expert_output.topk(5, dim=-1)\n",
    "                top_indices = top_tokens.indices\n",
    "                top_scores = top_tokens.values\n",
    "                decoded_tokens = tokenizer.decode(top_indices.tolist())\n",
    "                print(f\"    Expert {expert_idx}: Weight: {expert_weight:.4f}, Tokens: {decoded_tokens}, Scores: {top_scores.tolist()}\")\n",
    "\n",
    "    # Final logits\n",
    "    logits = model.lm_head(x)\n",
    "    top_tokens = logits.topk(5, dim=-1)\n",
    "    decoded_final_tokens = tokenizer.decode(top_tokens.indices.tolist())\n",
    "    print(f\"Final Layer: Tokens: {decoded_final_tokens}, Scores: {top_tokens.values.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"Get the optimal available device\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        # Enable TF32 for better performance on Ampere GPUs (A100, A6000, etc)\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        # Set memory allocation settings\n",
    "        torch.cuda.empty_cache()\n",
    "        # Enable CUDNN benchmarking for better performance\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_moe_logit_lens_with_active_expert_outputs(model, inputs, tokenizer, num_active_experts=7):\n",
    "    model.eval()\n",
    "\n",
    "    # Initial embedding\n",
    "    x = model.model.embed_tokens(inputs)\n",
    "\n",
    "    # Process each layer\n",
    "    for layer_idx, layer in enumerate(model.model.layers):\n",
    "        print(f\"Layer {layer_idx + 1}\")\n",
    "        x = layer.input_layernorm(x)\n",
    "\n",
    "        # Self-attention output\n",
    "        # Remove the position_ids argument here, let the model handle it internally\n",
    "        attn_output = layer.self_attn(x, x, x)\n",
    "        x = attn_output + x  # Residual connection\n",
    "        x = layer.post_attention_layernorm(x)\n",
    "\n",
    "        # MoE Layer\n",
    "        moe_output = layer.mlp(x)\n",
    "        gate_values = moe_output[\"gate_values\"]\n",
    "        expert_outputs = moe_output[\"expert_outputs\"]\n",
    "\n",
    "        # Extract active experts\n",
    "        for batch_idx, gates in enumerate(gate_values):\n",
    "            active_experts = gates.argsort(descending=True)[:num_active_experts]\n",
    "            print(f\"  Batch {batch_idx + 1}:\")\n",
    "            for expert_idx in active_experts:\n",
    "                expert_weight = gates[expert_idx].item()\n",
    "                expert_output = expert_outputs[batch_idx, :, expert_idx]\n",
    "                top_tokens = expert_output.topk(5, dim=-1)\n",
    "                top_indices = top_tokens.indices\n",
    "                top_scores = top_tokens.values\n",
    "                decoded_tokens = tokenizer.decode(top_indices.tolist())\n",
    "                print(f\"    Expert {expert_idx}: Weight: {expert_weight:.4f}, Tokens: {decoded_tokens}, Scores: {top_scores.tolist()}\")\n",
    "\n",
    "    # Final logits\n",
    "    logits = model.lm_head(x)\n",
    "    top_tokens = logits.topk(5, dim=-1)\n",
    "    decoded_final_tokens = tokenizer.decode(top_tokens.indices.tolist())\n",
    "    print(f\"Final Layer: Tokens: {decoded_final_tokens}, Scores: {top_tokens.values.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_lens(model, input_tokens):\n",
    "    \"\"\"\n",
    "    Applies a logit lens to each layer of a mixture of experts (MoE) model for specific input tokens.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The MoE model.\n",
    "        input_tokens (torch.Tensor): Input tensor with token embeddings, shape [batch_size, seq_len].\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the logit lens outputs for each token at each layer.\n",
    "    \"\"\"\n",
    "    logit_outputs = {}\n",
    "\n",
    "    # Pass the input tokens through the embedding layer\n",
    "    embedding_output = model.embed_tokens(input_tokens)  # Shape: [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "    # Process tokens through each layer\n",
    "    for layer_idx, layer in enumerate(model.layers):\n",
    "        # Apply the layer and get its output\n",
    "        layer_outputs = layer(embedding_output)  # Shape: [batch_size, seq_len, feature_dim]\n",
    "\n",
    "        # Extract the mixture of experts (MoE) components for this layer\n",
    "        if hasattr(layer.mlp, \"experts\"):\n",
    "            gate_outputs = layer.mlp.gate(embedding_output)  # Shape: [batch_size, seq_len, num_experts]\n",
    "            expert_outputs = []\n",
    "\n",
    "            # Process each token separately to extract expert-specific outputs\n",
    "            for token_idx in range(input_tokens.shape[1]):\n",
    "                token_expert_outputs = []\n",
    "\n",
    "                for expert_idx, expert in enumerate(layer.mlp.experts):\n",
    "                    token_input = embedding_output[:, token_idx, :]  # Shape: [batch_size, embedding_dim]\n",
    "                    expert_output = expert(token_input)  # Shape: [batch_size, feature_dim]\n",
    "                    token_expert_outputs.append(expert_output)\n",
    "\n",
    "                # Stack expert outputs and apply gating\n",
    "                token_expert_outputs = torch.stack(token_expert_outputs, dim=1)  # Shape: [batch_size, num_experts, feature_dim]\n",
    "                token_gate_weights = gate_outputs[:, token_idx, :].unsqueeze(-1)  # Shape: [batch_size, num_experts, 1]\n",
    "                activated_experts_output = torch.sum(token_expert_outputs * token_gate_weights, dim=1)  # Shape: [batch_size, feature_dim]\n",
    "\n",
    "                # Save activated expert outputs for this token\n",
    "                logit_outputs[f\"layer_{layer_idx}_token_{token_idx}_activated_experts\"] = activated_experts_output\n",
    "\n",
    "        # Update the embedding for the next layer\n",
    "        embedding_output = layer_outputs\n",
    "\n",
    "    return logit_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prompt_cache(domain: str, data_dict: dict, input_text: str, cache_dir='plots-deepseek') -> int:\n",
    "    \"\"\"Save prompt results with domain-based caching.\"\"\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    cache_info_path = Path(cache_dir) / 'domain_cache_info.json'\n",
    "    \n",
    "    # Load or create cache info\n",
    "    if cache_info_path.exists():\n",
    "        with open(cache_info_path, 'r') as f:\n",
    "            cache_info = json.load(f)\n",
    "    else:\n",
    "        cache_info = {}\n",
    "    \n",
    "    # Get prompt number from existing domain or create new one\n",
    "    if domain in cache_info:\n",
    "        prompt_number = cache_info[domain]['prompt_number']\n",
    "    else:\n",
    "        existing_numbers = [info['prompt_number'] for info in cache_info.values()]\n",
    "        prompt_number = max(existing_numbers, default=0) + 1\n",
    "    \n",
    "    # Update cache info with prompt text\n",
    "    cache_info[domain] = {\n",
    "        'prompt_number': prompt_number,\n",
    "        'timestamp': datetime.datetime.now().isoformat(),\n",
    "        'input_text': input_text  # Add the prompt text\n",
    "    }\n",
    "    \n",
    "    # Save cache info\n",
    "    with open(cache_info_path, 'w') as f:\n",
    "        json.dump(cache_info, f, indent=2)\n",
    "    \n",
    "    # Save data using pickle\n",
    "    data_path = Path(cache_dir) / f'prompt_{prompt_number}.pkl'\n",
    "    with open(data_path, 'wb') as f:\n",
    "        pickle.dump(data_dict, f)\n",
    "        \n",
    "    return prompt_number\n",
    "\n",
    "def load_prompt_cache(prompt_number, cache_dir='plots-deepseek'):\n",
    "    \"\"\"Load a specific prompt's results by number\"\"\"\n",
    "    matrix_path = f'{cache_dir}/prompt_{prompt_number}_matrix.npy'\n",
    "    meta_path = f'{cache_dir}/prompt_{prompt_number}_meta.json'\n",
    "    \n",
    "    if not (os.path.exists(matrix_path) and os.path.exists(meta_path)):\n",
    "        raise ValueError(f\"Prompt {prompt_number} not found in cache\")\n",
    "        \n",
    "    count_matrix = np.load(matrix_path)\n",
    "    with open(meta_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "        \n",
    "    return count_matrix, metadata\n",
    "\n",
    "def list_cached_prompts(cache_dir='plots-deepseek'):\n",
    "    \"\"\"List all cached prompts and their metadata\"\"\"\n",
    "    prompts = []\n",
    "    for f in os.listdir(cache_dir):\n",
    "        if f.endswith('_meta.json'):\n",
    "            with open(os.path.join(cache_dir, f), 'r') as meta_file:\n",
    "                prompts.append(json.load(meta_file))\n",
    "    return sorted(prompts, key=lambda x: x['prompt_number'])\n",
    "\n",
    "def plot_cached_prompt(prompt_number, plot_type='heatmap', layer_id=None, tokenizer=None, cache_dir='plots-deepseek'):\n",
    "    \"\"\"Plot results from a cached prompt\"\"\"\n",
    "    count_matrix, metadata = load_prompt_cache(prompt_number, cache_dir)\n",
    "    domain = metadata['domain']\n",
    "    \n",
    "    if plot_type == 'heatmap':\n",
    "        fig = visualize_layer_analysis(\n",
    "            tokenizer,\n",
    "            count_matrix,\n",
    "            domain=domain\n",
    "        )\n",
    "    elif plot_type == 'logit_lens':\n",
    "        if layer_id is None:\n",
    "            raise ValueError(\"layer_id is required for logit lens plots\")\n",
    "        fig = plot_enhanced_logit_lens(\n",
    "            count_matrix,\n",
    "            tokenizer,\n",
    "            domain=domain,\n",
    "            layer_id=layer_id\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown plot type: {plot_type}\")\n",
    "        \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def migrate_cache_structure(old_cache_info):\n",
    "    \"\"\"\n",
    "    Migrate old cache structure to new format.\n",
    "    \"\"\"\n",
    "    new_cache = {\n",
    "        'domains': {},\n",
    "        'next_id': 1\n",
    "    }\n",
    "    \n",
    "    # Handle existing entries if any\n",
    "    for domain, info in old_cache_info.items():\n",
    "        if domain not in ('domains', 'next_id'):  # Skip if these are already in new format\n",
    "            if isinstance(info, dict):\n",
    "                # Convert old format to new\n",
    "                new_cache['domains'][domain] = [{\n",
    "                    'prompt_number': info.get('prompt_number', new_cache['next_id']),\n",
    "                    'timestamp': info.get('timestamp', datetime.datetime.now().isoformat()),\n",
    "                    'input_text': info.get('input_text', 'No text stored')\n",
    "                }]\n",
    "                new_cache['next_id'] = max(new_cache['next_id'], info['prompt_number'] + 1)\n",
    "    \n",
    "    return new_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prompt_cache(domain: str, data: dict, cache_dir='plots-deepseek') -> int:\n",
    "    \"\"\"\n",
    "    Save prompt results with domain-based caching.\n",
    "    \"\"\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    cache_info_path = Path(cache_dir) / 'domain_cache_info.json'\n",
    "    \n",
    "    # Initialize or load cache info\n",
    "    if cache_info_path.exists():\n",
    "        try:\n",
    "            with open(cache_info_path, 'r') as f:\n",
    "                cache_info = json.load(f)\n",
    "                # Migrate if needed\n",
    "                if 'domains' not in cache_info:\n",
    "                    cache_info = migrate_cache_structure(cache_info)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading cache file, creating new: {e}\")\n",
    "            cache_info = {'domains': {}, 'next_id': 1}\n",
    "    else:\n",
    "        cache_info = {'domains': {}, 'next_id': 1}\n",
    "    \n",
    "    # Get next available ID\n",
    "    prompt_number = cache_info['next_id']\n",
    "    \n",
    "    # Initialize domain if needed\n",
    "    if domain not in cache_info['domains']:\n",
    "        cache_info['domains'][domain] = []\n",
    "    \n",
    "    # Add new entry\n",
    "    entry = {\n",
    "        'prompt_number': prompt_number,\n",
    "        'timestamp': datetime.datetime.now().isoformat(),\n",
    "        'input_text': data.get('input_text', 'No text stored')\n",
    "    }\n",
    "    cache_info['domains'][domain].append(entry)\n",
    "    cache_info['next_id'] = prompt_number + 1\n",
    "    \n",
    "    # Save cache info\n",
    "    with open(cache_info_path, 'w') as f:\n",
    "        json.dump(cache_info, f, indent=2)\n",
    "    \n",
    "    # Save data\n",
    "    data_path = Path(cache_dir) / f'prompt_{prompt_number}.pkl'\n",
    "    with open(data_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "        \n",
    "    return prompt_number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_prompt_cache(domain: str, input_text: str, cache_dir='plots-deepseek'):\n",
    "    \"\"\"\n",
    "    Load cached results for a domain and input text.\n",
    "    \"\"\"\n",
    "    cache_info_path = Path(cache_dir) / 'domain_cache_info.json'\n",
    "    \n",
    "    if not cache_info_path.exists():\n",
    "        raise ValueError(f\"No cache found in {cache_dir}\")\n",
    "    \n",
    "    try:\n",
    "        with open(cache_info_path, 'r') as f:\n",
    "            cache_info = json.load(f)\n",
    "            # Migrate if needed\n",
    "            if 'domains' not in cache_info:\n",
    "                cache_info = migrate_cache_structure(cache_info)\n",
    "                # Save migrated structure\n",
    "                with open(cache_info_path, 'w') as f:\n",
    "                    json.dump(cache_info, f, indent=2)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error reading cache file: {e}\")\n",
    "    \n",
    "    if domain not in cache_info['domains']:\n",
    "        raise ValueError(f\"No cache found for domain '{domain}'\")\n",
    "    \n",
    "    # Find matching entry\n",
    "    matching_entries = [\n",
    "        entry for entry in cache_info['domains'][domain]\n",
    "        if entry['input_text'] == input_text\n",
    "    ]\n",
    "    \n",
    "    if not matching_entries:\n",
    "        raise ValueError(f\"No matching cache entry found for text: {input_text}\")\n",
    "    \n",
    "    # Use most recent\n",
    "    latest_entry = max(matching_entries, key=lambda x: x['timestamp'])\n",
    "    prompt_number = latest_entry['prompt_number']\n",
    "    \n",
    "    data_path = Path(cache_dir) / f'prompt_{prompt_number}.pkl'\n",
    "    if not data_path.exists():\n",
    "        raise ValueError(f\"Cache data file missing for prompt #{prompt_number}\")\n",
    "    \n",
    "    with open(data_path, 'rb') as f:\n",
    "        data_dict = pickle.load(f)\n",
    "        \n",
    "    return data_dict, latest_entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_cached_prompts(cache_dir='plots-deepseek'):\n",
    "    \"\"\"List all cached prompts and their metadata.\"\"\"\n",
    "    cache_info_path = Path(cache_dir) / 'domain_cache_info.json'\n",
    "    if not cache_info_path.exists():\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with open(cache_info_path, 'r') as f:\n",
    "            cache_info = json.load(f)\n",
    "            # Migrate if needed\n",
    "            if 'domains' not in cache_info:\n",
    "                cache_info = migrate_cache_structure(cache_info)\n",
    "    except Exception:\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    for domain, entries in cache_info.get('domains', {}).items():\n",
    "        for entry in entries:\n",
    "            results.append({\n",
    "                'domain': domain,\n",
    "                'prompt_number': entry['prompt_number'],\n",
    "                'timestamp': entry['timestamp'],\n",
    "                'input_text': entry.get('input_text', 'No text stored')\n",
    "            })\n",
    "    \n",
    "    return sorted(results, key=lambda x: x['prompt_number'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_enhanced_logit_lens(model_outputs, tokenizer, input_text, position):\n",
    "    \"\"\"\n",
    "    Creates an enhanced logit lens visualization showing expert activations and their token predictions.\n",
    "    \"\"\"\n",
    "    # Get all unique tokens predicted by experts across layers\n",
    "    all_tokens = set()\n",
    "    token_probs = defaultdict(lambda: defaultdict(float))\n",
    "    layers = []\n",
    "    \n",
    "    # Process each layer's outputs in order\n",
    "    layer_keys = sorted([k for k in model_outputs.keys() if k.startswith('layer_')], \n",
    "                       key=lambda x: int(x.split('_')[1]))\n",
    "                       \n",
    "    for layer_key in layer_keys:\n",
    "        layer_idx = int(layer_key.split('_')[1])\n",
    "        layer_data = model_outputs[layer_key]\n",
    "        if 'tokens' not in layer_data:\n",
    "            continue\n",
    "            \n",
    "        token_data = None\n",
    "        # Find the token data for the specified position\n",
    "        for data in layer_data['tokens'].values():\n",
    "            if data['position'] == position:\n",
    "                token_data = data\n",
    "                break\n",
    "                \n",
    "        if token_data is None:\n",
    "            continue\n",
    "            \n",
    "        layers.append(layer_idx)\n",
    "        \n",
    "        # Collect tokens and their probabilities from expert outputs\n",
    "        for expert_output in token_data['expert_outputs']:\n",
    "            expert_weight = expert_output['weight']\n",
    "            for token, prob in expert_output['top_tokens']:\n",
    "                all_tokens.add(token)\n",
    "                token_probs[layer_idx][token] += prob * expert_weight\n",
    "\n",
    "    # Convert to sorted lists for consistent ordering\n",
    "    tokens = sorted(list(all_tokens))\n",
    "    \n",
    "    # Create matrix of probabilities\n",
    "    prob_matrix = []\n",
    "    for layer_idx in layers:\n",
    "        layer_probs = []\n",
    "        for token in tokens:\n",
    "            layer_probs.append(token_probs[layer_idx].get(token, 0.0))\n",
    "        prob_matrix.append(layer_probs)\n",
    "\n",
    "    # Create heatmap\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=prob_matrix,\n",
    "        x=tokens,\n",
    "        y=[f'Layer {idx}' for idx in layers],\n",
    "        colorscale='Viridis',\n",
    "        showscale=True\n",
    "    ))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f'Token Probabilities Across Layers - Position {position}',\n",
    "        xaxis_title='Predicted Tokens',\n",
    "        yaxis_title='Layer',\n",
    "        yaxis={'autorange': 'reversed'},  # Display layer 1 at top\n",
    "        width=1200,\n",
    "        height=800,\n",
    "        plot_bgcolor='black',\n",
    "        paper_bgcolor='black',\n",
    "        font=dict(color='white')\n",
    "    )\n",
    "\n",
    "    # Add grid lines\n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='rgba(128, 128, 128, 0.2)')\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='rgba(128, 128, 128, 0.2)')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_expert_and_final_predictions(model_outputs, tokenizer, position=None, title=None):\n",
    "    \"\"\"\n",
    "    Enhanced visualization showing both expert contributions and final predictions with improved readability.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    expert_ids = []\n",
    "    expert_tokens = defaultdict(dict)\n",
    "    \n",
    "    # Process layer outputs\n",
    "    layer_keys = sorted([k for k in model_outputs.keys() if k.startswith('layer_')],\n",
    "                       key=lambda x: int(x.split('_')[1]))\n",
    "    \n",
    "    for layer_key in layer_keys:\n",
    "        layer_num = int(layer_key.split('_')[1])\n",
    "        layer_data = model_outputs[layer_key]\n",
    "        \n",
    "        # Find relevant token data\n",
    "        token_data = None\n",
    "        for data in layer_data.get(\"tokens\", {}).values():\n",
    "            if data[\"position\"] == position:\n",
    "                token_data = data\n",
    "                break\n",
    "                \n",
    "        if not token_data:\n",
    "            continue\n",
    "            \n",
    "        layers.append(layer_num)\n",
    "        \n",
    "        # Process experts\n",
    "        for expert_output in token_data[\"expert_outputs\"]:\n",
    "            expert_type = expert_output[\"expert_type\"]\n",
    "            if expert_type == \"routed\":\n",
    "                expert_id = f\"R{expert_output['expert_id']}\"\n",
    "            else:\n",
    "                expert_id = expert_output[\"expert_id\"]  # Already has 'S0' or 'S1' format\n",
    "                \n",
    "            weight = expert_output[\"weight\"]\n",
    "            tokens = expert_output[\"top_tokens\"]\n",
    "            \n",
    "            expert_tokens[(layer_num, expert_id)] = {\n",
    "                'weight': weight,\n",
    "                'tokens': tokens\n",
    "            }\n",
    "            if expert_id not in expert_ids:\n",
    "                expert_ids.append(expert_id)\n",
    "    \n",
    "    # Add final predictions\n",
    "    if 'final_predictions' in model_outputs and position in model_outputs['final_predictions']:\n",
    "        final_preds = model_outputs['final_predictions'][position]\n",
    "        final_layer = max(layers) + 1\n",
    "        layers.append(final_layer)\n",
    "        \n",
    "        for i, (token_id, prob) in enumerate(zip(final_preds['token_ids'], final_preds['probs'])):\n",
    "            expert_id = f'Final_{i+1}'\n",
    "            if expert_id not in expert_ids:\n",
    "                expert_ids.append(expert_id)\n",
    "            token = final_preds['tokens'][i]\n",
    "            expert_tokens[(final_layer, expert_id)] = {\n",
    "                'weight': prob.item(),\n",
    "                'tokens': [(token, prob.item())]\n",
    "            }\n",
    "    \n",
    "    # Sort expert IDs\n",
    "    expert_ids = sorted(list(set(expert_ids)), key=str)\n",
    "    \n",
    "    # Create matrices for visualization\n",
    "    weight_matrix = np.zeros((len(layers), len(expert_ids)))\n",
    "    hover_text = [['' for _ in range(len(expert_ids))] for _ in range(len(layers))]\n",
    "    text_matrix = [['' for _ in range(len(expert_ids))] for _ in range(len(layers))]\n",
    "    \n",
    "    # Color scheme\n",
    "    colors = []\n",
    "    for expert_id in expert_ids:\n",
    "        if expert_id.startswith('R'):\n",
    "            colors.append('rgba(253,231,37,0.9)')  # Yellow for routed\n",
    "        elif expert_id.startswith('S'):\n",
    "            colors.append('rgba(0,176,246,0.9)')   # Blue for shared\n",
    "        else:\n",
    "            colors.append('rgba(94,201,98,0.9)')   # Green for final predictions\n",
    "    \n",
    "    # Fill matrices\n",
    "    for i, layer in enumerate(layers):\n",
    "        for j, expert_id in enumerate(expert_ids):\n",
    "            info = expert_tokens.get((layer, expert_id), None)\n",
    "            if info and info['tokens']:\n",
    "                weight = info['weight']\n",
    "                weight_matrix[i, j] = weight\n",
    "                \n",
    "                top_token, _ = info['tokens'][0]\n",
    "                text_matrix[i][j] = f'{top_token}<br>{weight:.3f}'\n",
    "                \n",
    "                hover_lines = [\n",
    "                    f\"Layer: {'Final' if 'Final' in expert_id else layer}\",\n",
    "                    f\"Expert: {expert_id}\",\n",
    "                    f\"Weight: {weight:.3f}\",\n",
    "                    \"Top tokens:\"\n",
    "                ]\n",
    "                hover_lines.extend(f\"{token}: {prob:.3f}\" for token, prob in info['tokens'])\n",
    "                hover_text[i][j] = \"<br>\".join(hover_lines)\n",
    "    \n",
    "    # Create figure with increased size and improved readability\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=weight_matrix,\n",
    "        x=expert_ids,\n",
    "        y=[f\"Layer {layer}\" if layer != max(layers) else \"Final Layer\" for layer in layers],\n",
    "        colorscale=[\n",
    "            [0, 'rgba(24, 21, 23, 0.9)'],\n",
    "            [0.0001, 'rgb(68,1,84)'],\n",
    "            [1, 'rgb(242, 121, 53)'] #for shared experts\n",
    "        ],\n",
    "        text=text_matrix,\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 16, \"color\": \"white\", \"family\": \"Arial\"},\n",
    "        hoverongaps=False,\n",
    "        hoverinfo=\"text\",\n",
    "        hovertext=hover_text,\n",
    "        showscale=True,\n",
    "    ))\n",
    "    \n",
    "    # Update layout with improved readability\n",
    "    title = title or f\"Expert Contributions and Final Predictions\" + (f\" - Position {position}\" if position else \"\")\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=title,\n",
    "            font=dict(size=24)\n",
    "        ),\n",
    "        xaxis_title=dict(\n",
    "            text=\"Experts\",\n",
    "            font=dict(size=20)\n",
    "        ),\n",
    "        yaxis_title=dict(\n",
    "            text=\"Layers\",\n",
    "            font=dict(size=20)\n",
    "        ),\n",
    "        height=2000,  # Increased height\n",
    "        width=4000,   # Increased width\n",
    "        plot_bgcolor='black',\n",
    "        paper_bgcolor='black',\n",
    "        font=dict(\n",
    "            color='white',\n",
    "            size=16\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            tickangle=45,\n",
    "            tickfont=dict(size=16),\n",
    "            showgrid=True,\n",
    "            gridwidth=1,\n",
    "            gridcolor='rgba(128, 128, 128, 0.2)',\n",
    "            showline=True,\n",
    "            linewidth=1,\n",
    "            linecolor='rgba(128, 128, 128, 0.2)'\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            autorange='reversed',\n",
    "            tickfont=dict(size=16),\n",
    "            showgrid=True,\n",
    "            gridwidth=1,\n",
    "            gridcolor='rgba(128, 128, 128, 0.2)',\n",
    "            showline=True,\n",
    "            linewidth=1,\n",
    "            linecolor='rgba(128, 128, 128, 0.2)'\n",
    "        ),\n",
    "        margin=dict(l=120, r=120, t=160, b=120)  # Increased margins\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(\n",
    "    text: str,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    token_position: int = None,\n",
    "    domain: str = 'default',\n",
    "    plot_type: str = 'both',\n",
    "    force_recompute: bool = False,\n",
    "    cache_dir: str = 'plots-deepseek',\n",
    "    display_plot: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyze text through the model and create visualizations.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        figures = {}\n",
    "        \n",
    "        # Check cache if not forcing recompute\n",
    "        if not force_recompute:\n",
    "            try:\n",
    "                cached_data, cache_info = load_prompt_cache(domain, text, cache_dir)\n",
    "                print(f\"Using cached results from prompt #{cache_info['prompt_number']}\")\n",
    "                \n",
    "                # Extract results from cached data\n",
    "                results = cached_data['results']\n",
    "                \n",
    "                # Create visualizations from cached data\n",
    "                if plot_type in ['heatmap', 'both']:\n",
    "                    figures['heatmap'] = visualize_layer_analysis(\n",
    "                        tokenizer=tokenizer,\n",
    "                        results=results,\n",
    "                        token_position=token_position,\n",
    "                        input_text=text\n",
    "                    )\n",
    "                    if display_plot:\n",
    "                        figures['heatmap'].show()\n",
    "                \n",
    "                if plot_type in ['logit_lens', 'both']:\n",
    "                    figures['logit_lens'] = plot_enhanced_logit_lens(\n",
    "                        model_outputs=results,\n",
    "                        tokenizer=tokenizer,\n",
    "                        input_text=text,\n",
    "                        position=token_position\n",
    "                    )\n",
    "                    if display_plot:\n",
    "                        figures['logit_lens'].show()\n",
    "                    \n",
    "                if plot_type in ['expert', 'both']:\n",
    "                    figures['expert'] = plot_expert_and_final_predictions(\n",
    "                        results,\n",
    "                        tokenizer=tokenizer,\n",
    "                        position=token_position,\n",
    "                        title=f\"Expert Contributions and Predictions for: '{text}'\"\n",
    "                    )\n",
    "                    if display_plot:\n",
    "                        figures['expert'].show()\n",
    "                    \n",
    "                return figures, results, None\n",
    "            \n",
    "            except ValueError as e:\n",
    "                print(f\"Cache miss: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading cache: {str(e)}\")\n",
    "\n",
    "        # Initialize the lens analyzer\n",
    "        lens = MOELens(model, tokenizer)\n",
    "        \n",
    "        # Process the input text\n",
    "        input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "        results = lens.analyze_text(input_ids)\n",
    "        \n",
    "        # Create visualizations based on plot_type\n",
    "        if plot_type in ['heatmap', 'both']:\n",
    "            figures['heatmap'] = visualize_layer_analysis(\n",
    "                tokenizer=tokenizer,\n",
    "                results=results,\n",
    "                token_position=token_position,\n",
    "                input_text=text\n",
    "            )\n",
    "            if display_plot:\n",
    "                figures['heatmap'].show()\n",
    "            \n",
    "        if plot_type in ['logit_lens', 'both']:\n",
    "            figures['logit_lens'] = plot_enhanced_logit_lens(\n",
    "                model_outputs=results,\n",
    "                tokenizer=tokenizer,\n",
    "                input_text=text,\n",
    "                position=token_position\n",
    "            )\n",
    "            if display_plot:\n",
    "                figures['logit_lens'].show()\n",
    "            \n",
    "        if plot_type in ['expert', 'both']:\n",
    "            figures['expert'] = plot_expert_and_final_predictions(\n",
    "                results,\n",
    "                tokenizer=tokenizer,\n",
    "                position=token_position,\n",
    "                title=f\"Expert Contributions and Predictions for: '{text}'\"\n",
    "            )\n",
    "            if display_plot:\n",
    "                figures['expert'].show()\n",
    "            \n",
    "        # Cache the results with metadata\n",
    "        cache_data = {\n",
    "            'results': results,\n",
    "            'input_text': text\n",
    "        }\n",
    "        prompt_number = save_prompt_cache(domain, cache_data, cache_dir)\n",
    "        print(f\"Results cached as prompt #{prompt_number}\")\n",
    "        \n",
    "        return figures, results, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in analyze_dataset: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, str(e)\n",
    "\n",
    "    finally:\n",
    "        # Clean up\n",
    "        if 'lens' in locals():\n",
    "            lens.remove_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached results from prompt #1\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "colorscale": [
          [
           0,
           "rgba(24, 21, 23, 0.9)"
          ],
          [
           0.0001,
           "rgb(68,1,84)"
          ],
          [
           1,
           "rgb(242, 121, 53)"
          ]
         ],
         "hoverinfo": "text",
         "hoverongaps": false,
         "hovertext": [
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 1<br>Expert: R14<br>Weight: 0.077<br>Top tokens:<br>acost: 0.238<br>resents: 0.208<br>ituting: 0.193<br> : 0.180<br>ADDING: 0.180",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 1<br>Expert: R20<br>Weight: 0.028<br>Top tokens:<br>amena: 0.220<br>tig: 0.198<br>iger: 0.197<br>: 0.196<br>oon: 0.188",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 1<br>Expert: R30<br>Weight: 0.031<br>Top tokens:<br>: 0.217<br>aviera: 0.203<br>: 0.198<br>poca: 0.191<br>erland: 0.191",
           "",
           "",
           "",
           "",
           "",
           "Layer: 1<br>Expert: R36<br>Weight: 0.058<br>Top tokens:<br>pte: 0.213<br>zem: 0.200<br>accur: 0.199<br>: 0.195<br>: 0.193",
           "",
           "",
           "",
           "Layer: 1<br>Expert: R4<br>Weight: 0.302<br>Top tokens:<br>: 0.231<br>enu: 0.219<br>: 0.188<br>: 0.183<br>: 0.178",
           "",
           "",
           "",
           "",
           "",
           "Layer: 1<br>Expert: R45<br>Weight: 0.032<br>Top tokens:<br>eqz: 0.215<br>ICA: 0.203<br>IO: 0.197<br>: 0.193<br> Cal: 0.191",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 1<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br>(<: 0.201<br>: 0.201<br>: 0.200<br>: 0.199<br>WHM: 0.199",
           "Layer: 1<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br>pei: 0.202<br>OX: 0.201<br>ertes: 0.200<br>LH: 0.199<br>OID: 0.199"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 2<br>Expert: R12<br>Weight: 0.039<br>Top tokens:<br>: 0.211<br>: 0.210<br>geon: 0.203<br>: 0.193<br>orter: 0.183",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 2<br>Expert: R41<br>Weight: 0.048<br>Top tokens:<br>: 0.227<br>intilla: 0.198<br>: 0.194<br>poca: 0.191<br>: 0.191",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 2<br>Expert: R48<br>Weight: 0.079<br>Top tokens:<br>: 0.223<br>urator: 0.203<br>atum: 0.196<br> convenc: 0.192<br>: 0.186",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 2<br>Expert: R57<br>Weight: 0.049<br>Top tokens:<br> controlador: 0.228<br> ra: 0.201<br>: 0.197<br>lides: 0.188<br>: 0.188",
           "",
           "",
           "",
           "",
           "Layer: 2<br>Expert: R61<br>Weight: 0.181<br>Top tokens:<br>azu: 0.274<br>regs: 0.189<br>: 0.189<br>sterdam: 0.174<br>ergy: 0.173",
           "",
           "",
           "Layer: 2<br>Expert: R7<br>Weight: 0.089<br>Top tokens:<br>umerable: 0.217<br>olate: 0.205<br>ustr: 0.198<br>sembly: 0.190<br>: 0.190",
           "",
           "",
           "Layer: 2<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br>aix: 0.203<br>rar: 0.202<br>2: 0.200<br>4: 0.198<br>enza: 0.197",
           "Layer: 2<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br>: 0.201<br>ailable: 0.200<br> : 0.200<br>ariat: 0.200<br>sting: 0.200"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "Layer: 3<br>Expert: R0<br>Weight: 0.207<br>Top tokens:<br>scul: 0.208<br>ssim: 0.206<br>uliar: 0.203<br>amard: 0.194<br>imod: 0.189",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 3<br>Expert: R25<br>Weight: 0.052<br>Top tokens:<br>fort: 0.208<br>: 0.205<br>: 0.200<br>: 0.196<br>let: 0.191",
           "",
           "",
           "",
           "Layer: 3<br>Expert: R29<br>Weight: 0.041<br>Top tokens:<br>Strunz: 0.244<br>explot: 0.198<br>xkb: 0.190<br> propied: 0.188<br>DOCKED: 0.181",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 3<br>Expert: R55<br>Weight: 0.040<br>Top tokens:<br>: 0.214<br>icae: 0.209<br>ICES: 0.207<br>Roine: 0.191<br>ences: 0.178",
           "",
           "",
           "",
           "",
           "Layer: 3<br>Expert: R60<br>Weight: 0.105<br>Top tokens:<br>anni: 0.218<br>ayers: 0.208<br>prev: 0.201<br>inx: 0.188<br>XMLSchema: 0.185",
           "",
           "",
           "Layer: 3<br>Expert: R63<br>Weight: 0.054<br>Top tokens:<br>pite: 0.224<br>slaught: 0.205<br>->__: 0.205<br>: 0.185<br>:+: 0.182",
           "",
           "",
           "",
           "Layer: 3<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br>memname: 0.205<br>: 0.202<br>: 0.198<br>ObjectMeta: 0.198<br> : 0.197",
           "Layer: 3<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br>ulats: 0.203<br>Oest: 0.203<br>: 0.199<br>: 0.198<br>lir: 0.197"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 4<br>Expert: R2<br>Weight: 0.056<br>Top tokens:<br>: 0.226<br>oremove: 0.201<br> davall: 0.198<br> : 0.190<br>posicions: 0.186",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 4<br>Expert: R30<br>Weight: 0.166<br>Top tokens:<br>3: 0.209<br>5: 0.201<br>4: 0.201<br>adv: 0.195<br>undo: 0.193",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 4<br>Expert: R43<br>Weight: 0.076<br>Top tokens:<br>ored: 0.223<br> Pro: 0.201<br>inn: 0.201<br>ss: 0.188<br>Pro: 0.187",
           "",
           "",
           "",
           "",
           "",
           "Layer: 4<br>Expert: R49<br>Weight: 0.130<br>Top tokens:<br>ban: 0.317<br>ordi: 0.200<br> Ban: 0.166<br>: 0.161<br>rament: 0.156",
           "Layer: 4<br>Expert: R5<br>Weight: 0.061<br>Top tokens:<br>: 0.257<br>: 0.191<br>: 0.186<br>dles: 0.185<br>: 0.181",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 4<br>Expert: R7<br>Weight: 0.070<br>Top tokens:<br>: 0.219<br>apunov: 0.217<br>: 0.194<br>slaught: 0.187<br>#[: 0.183",
           "",
           "",
           "Layer: 4<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br>: 0.206<br> Mosc: 0.201<br>: 0.199<br>uca: 0.197<br>: 0.197",
           "Layer: 4<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br>: 0.201<br>eff: 0.201<br>: 0.200<br>: 0.200<br>orm: 0.199"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 5<br>Expert: R19<br>Weight: 0.060<br>Top tokens:<br>: 0.216<br> Pass: 0.212<br>til: 0.199<br>: 0.198<br>: 0.175",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 5<br>Expert: R30<br>Weight: 0.109<br>Top tokens:<br>cale: 0.239<br>ters: 0.209<br>quier: 0.197<br>: 0.189<br>: 0.166",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 5<br>Expert: R37<br>Weight: 0.172<br>Top tokens:<br>ur: 0.232<br>: 0.222<br>ide: 0.206<br>aire: 0.172<br>bers: 0.168",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 5<br>Expert: R48<br>Weight: 0.090<br>Top tokens:<br>icci: 0.261<br>: 0.212<br>: 0.198<br> : 0.165<br>: 0.164",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 5<br>Expert: R57<br>Weight: 0.057<br>Top tokens:<br>: 0.297<br> Gamb: 0.202<br>ACA: 0.169<br>: 0.169<br>: 0.163",
           "Layer: 5<br>Expert: R58<br>Weight: 0.080<br>Top tokens:<br>[][]{: 0.217<br>enthal: 0.208<br>eper: 0.205<br>: 0.188<br>: 0.181",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 5<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br>2: 0.201<br>0: 0.201<br>3: 0.200<br>4: 0.200<br>9: 0.199",
           "Layer: 5<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br>: 0.202<br>Flip: 0.201<br>: 0.200<br>IR: 0.199<br>IRS: 0.198"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 6<br>Expert: R16<br>Weight: 0.042<br>Top tokens:<br>amb: 0.228<br>: 0.210<br>NE: 0.203<br>Amb: 0.187<br> damages: 0.172",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 6<br>Expert: R4<br>Weight: 0.054<br>Top tokens:<br>**: 0.232<br>*: 0.202<br> : 0.197<br> spect: 0.187<br> : 0.183",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 6<br>Expert: R50<br>Weight: 0.106<br>Top tokens:<br>: 0.257<br>arx: 0.215<br>: 0.183<br>: 0.173<br>sd: 0.172",
           "",
           "",
           "",
           "",
           "",
           "Layer: 6<br>Expert: R57<br>Weight: 0.088<br>Top tokens:<br>roz: 0.272<br>: 0.201<br>cter: 0.198<br>ittings: 0.166<br> : 0.162",
           "",
           "",
           "",
           "Layer: 6<br>Expert: R60<br>Weight: 0.089<br>Top tokens:<br>: 0.263<br>iste: 0.219<br>quista: 0.196<br>ausen: 0.163<br> right: 0.159",
           "",
           "",
           "Layer: 6<br>Expert: R63<br>Weight: 0.040<br>Top tokens:<br>4: 0.252<br>5: 0.197<br>7: 0.196<br>3: 0.181<br>6: 0.174",
           "",
           "",
           "",
           "Layer: 6<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br>: 0.204<br>4: 0.202<br>0: 0.198<br>: 0.198<br>3: 0.198",
           "Layer: 6<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br>: 0.209<br>: 0.200<br> <!--[: 0.197<br>: 0.197<br>: 0.197"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 7<br>Expert: R37<br>Weight: 0.095<br>Top tokens:<br>Jac: 0.229<br> squ: 0.227<br>: 0.193<br>ilde: 0.176<br>jac: 0.175",
           "Layer: 7<br>Expert: R38<br>Weight: 0.066<br>Top tokens:<br>same: 0.240<br>ending: 0.223<br>inic: 0.184<br> Se: 0.178<br>end: 0.174",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 7<br>Expert: R49<br>Weight: 0.044<br>Top tokens:<br>adur: 0.318<br>: 0.210<br>ermost: 0.184<br>ighters: 0.146<br> comprob: 0.143",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 7<br>Expert: R59<br>Weight: 0.092<br>Top tokens:<br>ISTS: 0.240<br>chism: 0.211<br>sica: 0.189<br>trica: 0.182<br>IZED: 0.178",
           "",
           "",
           "",
           "",
           "Layer: 7<br>Expert: R63<br>Weight: 0.077<br>Top tokens:<br> : 0.261<br>: 0.201<br>yer: 0.187<br> : 0.177<br>inisc: 0.174",
           "",
           "",
           "Layer: 7<br>Expert: R9<br>Weight: 0.201<br>Top tokens:<br>uts: 0.216<br>: 0.211<br> R: 0.194<br>ut: 0.191<br>UTH: 0.188",
           "Layer: 7<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br>odin: 0.205<br>metav: 0.203<br>uxer: 0.198<br>: 0.197<br>iad: 0.196",
           "Layer: 7<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br>: 0.203<br>substack: 0.201<br>: 0.201<br>WSER: 0.199<br>: 0.197"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 8<br>Expert: R11<br>Weight: 0.040<br>Top tokens:<br>ermanent: 0.230<br> Catalana: 0.216<br>gico: 0.190<br> : 0.190<br>Enumerator: 0.174",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 8<br>Expert: R3<br>Weight: 0.041<br>Top tokens:<br>sman: 0.472<br>: 0.187<br>atim: 0.129<br>uric: 0.108<br> : 0.104",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 8<br>Expert: R39<br>Weight: 0.035<br>Top tokens:<br>wanda: 0.263<br>itely: 0.201<br>inaci: 0.190<br>: 0.177<br>Cci: 0.170",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 8<br>Expert: R5<br>Weight: 0.093<br>Top tokens:<br> prov: 0.305<br>op: 0.236<br>: 0.205<br>OP: 0.133<br>: 0.120",
           "",
           "",
           "Layer: 8<br>Expert: R52<br>Weight: 0.083<br>Top tokens:<br>coln: 0.250<br> Landing: 0.204<br>atiu: 0.196<br>dua: 0.181<br>landing: 0.169",
           "",
           "",
           "",
           "Layer: 8<br>Expert: R57<br>Weight: 0.082<br>Top tokens:<br>: 0.248<br>estions: 0.201<br>asin: 0.190<br>: 0.186<br>iffe: 0.175",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 8<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br> : 0.210<br>lir: 0.203<br>: 0.197<br>: 0.195<br>}&=&\\: 0.195",
           "Layer: 8<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br>yn: 0.211<br>: 0.209<br>}$~\\: 0.198<br>SSA: 0.193<br>: 0.190"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 9<br>Expert: R21<br>Weight: 0.076<br>Top tokens:<br>LAIN: 0.218<br>: 0.207<br>itudes: 0.199<br>3: 0.195<br> : 0.180",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 9<br>Expert: R46<br>Weight: 0.098<br>Top tokens:<br>: 0.278<br>ruci: 0.191<br>: 0.190<br>ullivan: 0.174<br>Arque: 0.167",
           "",
           "",
           "",
           "",
           "Layer: 9<br>Expert: R50<br>Weight: 0.045<br>Top tokens:<br>: 0.243<br>Solr: 0.243<br> almacenado: 0.208<br>: 0.159<br>issors: 0.147",
           "",
           "",
           "",
           "",
           "",
           "Layer: 9<br>Expert: R57<br>Weight: 0.134<br>Top tokens:<br>ingu: 0.299<br>onte: 0.241<br>oret: 0.173<br>: 0.145<br> Conca: 0.142",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 9<br>Expert: R63<br>Weight: 0.050<br>Top tokens:<br>ibar: 0.305<br>iban: 0.187<br> : 0.183<br>aturally: 0.169<br>: 0.155",
           "",
           "",
           "Layer: 9<br>Expert: R9<br>Weight: 0.071<br>Top tokens:<br>: 0.269<br>thems: 0.200<br>nesses: 0.196<br>niques: 0.167<br> Grup: 0.167",
           "Layer: 9<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br>: 0.212<br>ola: 0.200<br>: 0.198<br>nica: 0.195<br>har: 0.195",
           "Layer: 9<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br>Formaci: 0.207<br>addock: 0.200<br>itaris: 0.199<br> Local: 0.199<br>: 0.195"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 10<br>Expert: R1<br>Weight: 0.152<br>Top tokens:<br>: 0.238<br>isin: 0.196<br>ilst: 0.192<br>mesos: 0.187<br>: 0.187",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 10<br>Expert: R21<br>Weight: 0.044<br>Top tokens:<br>hir: 0.221<br>fusc: 0.208<br>autilus: 0.194<br>: 0.190<br>: 0.187",
           "",
           "Layer: 10<br>Expert: R23<br>Weight: 0.070<br>Top tokens:<br>: 0.348<br>: 0.187<br>: 0.161<br>lotte: 0.158<br>: 0.146",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 10<br>Expert: R41<br>Weight: 0.052<br>Top tokens:<br>aried: 0.302<br>way: 0.214<br>ally: 0.198<br>riac: 0.143<br>: 0.142",
           "",
           "",
           "",
           "",
           "Layer: 10<br>Expert: R46<br>Weight: 0.060<br>Top tokens:<br>imburg: 0.268<br>ISR: 0.209<br>IOR: 0.205<br>ims: 0.160<br>igui: 0.157",
           "",
           "",
           "Layer: 10<br>Expert: R49<br>Weight: 0.082<br>Top tokens:<br>ry: 0.292<br> WELL: 0.239<br>sworth: 0.179<br>.: 0.145<br> P: 0.145",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 10<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br> lo: 0.214<br>rians: 0.201<br>: 0.197<br>: 0.196<br>awk: 0.192",
           "Layer: 10<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br>: 0.219<br>ovn: 0.198<br> m: 0.197<br>',\\: 0.193<br> DBA: 0.193"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 11<br>Expert: R1<br>Weight: 0.054<br>Top tokens:<br>: 0.268<br>habitants: 0.214<br>ippets: 0.191<br>erity: 0.165<br>ouin: 0.163",
           "",
           "",
           "",
           "Layer: 11<br>Expert: R14<br>Weight: 0.048<br>Top tokens:<br>8: 0.325<br>3: 0.210<br>4: 0.167<br>6: 0.159<br>7: 0.140",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 11<br>Expert: R22<br>Weight: 0.057<br>Top tokens:<br>_.--: 0.270<br>ugal: 0.231<br>rede: 0.169<br>ugu: 0.165<br>: 0.164",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 11<br>Expert: R30<br>Weight: 0.101<br>Top tokens:<br>4: 0.293<br>8: 0.218<br>5: 0.172<br>6: 0.170<br>1: 0.146",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 11<br>Expert: R44<br>Weight: 0.076<br>Top tokens:<br>OBJECT: 0.429<br>: 0.211<br>iti: 0.144<br>: 0.124<br>imode: 0.092",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 11<br>Expert: R54<br>Weight: 0.083<br>Top tokens:<br>imath: 0.309<br>lef: 0.208<br>ases: 0.208<br>: 0.156<br>IconError: 0.119",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 11<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br>inclo: 0.202<br>: 0.202<br>firstrow: 0.202<br> Cervera: 0.197<br>CODEGEN: 0.197",
           "Layer: 11<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br>De: 0.204<br>: 0.203<br>(: 0.198<br>: 0.198<br>: 0.198"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 12<br>Expert: R15<br>Weight: 0.055<br>Top tokens:<br>oraci: 0.257<br>rasal: 0.202<br>obrir: 0.198<br>xhtml: 0.172<br>: 0.171",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 12<br>Expert: R24<br>Weight: 0.052<br>Top tokens:<br>: 0.251<br>ansa: 0.210<br>utic: 0.203<br>: 0.182<br> mor: 0.154",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 12<br>Expert: R35<br>Weight: 0.064<br>Top tokens:<br>: 0.315<br>Msk: 0.205<br>dule: 0.177<br>itel: 0.167<br>blem: 0.137",
           "",
           "",
           "",
           "",
           "",
           "Layer: 12<br>Expert: R40<br>Weight: 0.058<br>Top tokens:<br>: 0.234<br> : 0.213<br>arribada: 0.200<br>: 0.195<br>EUA: 0.157",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 12<br>Expert: R54<br>Weight: 0.077<br>Top tokens:<br>icu: 0.357<br>inot: 0.248<br> gif: 0.166<br>: 0.130<br> : 0.099",
           "",
           "",
           "",
           "Layer: 12<br>Expert: R59<br>Weight: 0.055<br>Top tokens:<br>ansen: 0.233<br>sequ: 0.229<br>: 0.181<br> Rifle: 0.181<br>: 0.176",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 12<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br> self: 0.211<br>ISC: 0.208<br>;',: 0.198<br>: 0.196<br>IPP: 0.187",
           "Layer: 12<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br>9: 0.211<br>: 0.208<br>umber: 0.199<br>jh: 0.193<br>opus: 0.189"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 13<br>Expert: R15<br>Weight: 0.046<br>Top tokens:<br>hest: 0.338<br>OTH: 0.250<br>Unsafe: 0.151<br>: 0.130<br>: 0.130",
           "",
           "",
           "Layer: 13<br>Expert: R18<br>Weight: 0.090<br>Top tokens:<br>veless: 0.423<br>arque: 0.195<br> main: 0.143<br> place: 0.120<br>alion: 0.118",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 13<br>Expert: R3<br>Weight: 0.067<br>Top tokens:<br>: 0.274<br>ianisme: 0.231<br>obel: 0.227<br>aguen: 0.145<br>pee: 0.124",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 13<br>Expert: R44<br>Weight: 0.049<br>Top tokens:<br>: 0.344<br>erex: 0.189<br>eted: 0.174<br>IPU: 0.153<br>ario: 0.141",
           "Layer: 13<br>Expert: R45<br>Weight: 0.042<br>Top tokens:<br> radis: 0.221<br> fes: 0.212<br>eqref: 0.206<br>menuitem: 0.186<br> morta: 0.175",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 13<br>Expert: R60<br>Weight: 0.121<br>Top tokens:<br>ODB: 0.478<br> : 0.248<br>ogia: 0.094<br>: 0.094<br> : 0.086",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 13<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br>: 0.210<br>v: 0.209<br>aren: 0.202<br> : 0.192<br>: 0.187",
           "Layer: 13<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br>: 0.206<br>: 0.202<br>ascript: 0.201<br>: 0.196<br> Espasa: 0.196"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 14<br>Expert: R23<br>Weight: 0.059<br>Top tokens:<br>abase: 0.281<br>ianisme: 0.230<br>odo: 0.196<br> Comen: 0.153<br>ODO: 0.140",
           "",
           "Layer: 14<br>Expert: R25<br>Weight: 0.051<br>Top tokens:<br>ishop: 0.269<br>oyer: 0.226<br>YL: 0.176<br>lesa: 0.166<br>: 0.163",
           "",
           "",
           "Layer: 14<br>Expert: R28<br>Weight: 0.104<br>Top tokens:<br>?></: 0.365<br>TOOLSET: 0.351<br>eldorf: 0.157<br>]]></: 0.084<br>insserv: 0.043",
           "",
           "",
           "",
           "",
           "",
           "Layer: 14<br>Expert: R33<br>Weight: 0.091<br>Top tokens:<br>: 0.338<br>enties: 0.287<br>ono: 0.234<br>umbo: 0.072<br>dna: 0.069",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 14<br>Expert: R40<br>Weight: 0.054<br>Top tokens:<br>(: 0.492<br>: 0.156<br><: 0.144<br> <: 0.119<br> : 0.089",
           "",
           "",
           "",
           "Layer: 14<br>Expert: R44<br>Weight: 0.047<br>Top tokens:<br>izes: 0.277<br>oyo: 0.274<br> : 0.161<br>: 0.151<br> EACH: 0.137",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 14<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br>Ev: 0.223<br>beck: 0.211<br>: 0.192<br>EP: 0.188<br>DC: 0.186",
           "Layer: 14<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br> E: 0.204<br> which: 0.201<br>Exec: 0.199<br> Natural: 0.198<br> self: 0.197"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 15<br>Expert: R17<br>Weight: 0.044<br>Top tokens:<br>Re: 0.257<br> _____: 0.190<br> Which: 0.188<br> Quiz: 0.184<br> OP: 0.181",
           "",
           "",
           "Layer: 15<br>Expert: R2<br>Weight: 0.101<br>Top tokens:<br>|,: 0.310<br> : 0.259<br> : 0.158<br><!: 0.145<br>atee: 0.129",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 15<br>Expert: R26<br>Weight: 0.138<br>Top tokens:<br> : 0.551<br>icity: 0.237<br>: 0.114<br>torch: 0.054<br>acity: 0.044",
           "Layer: 15<br>Expert: R27<br>Weight: 0.045<br>Top tokens:<br> Frames: 0.236<br> FRAMES: 0.211<br>: 0.188<br>ORTS: 0.186<br>: 0.179",
           "",
           "Layer: 15<br>Expert: R29<br>Weight: 0.113<br>Top tokens:<br>usen: 0.334<br>ometries: 0.232<br>ometrics: 0.200<br>liminf: 0.134<br>icot: 0.100",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 15<br>Expert: R58<br>Weight: 0.044<br>Top tokens:<br> Net: 0.258<br>net: 0.251<br>NET: 0.204<br>Net: 0.183<br>g: 0.104",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 15<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br>: 0.218<br>: 0.203<br>atas: 0.197<br>osaur: 0.191<br>gard: 0.190",
           "Layer: 15<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br> dy: 0.207<br>IEEEeqnarray: 0.200<br>angle: 0.199<br>ester: 0.197<br>: 0.196"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 16<br>Expert: R15<br>Weight: 0.112<br>Top tokens:<br>ProtoBuf: 0.546<br>: 0.240<br>ospin: 0.086<br>LOAT: 0.077<br>googleads: 0.051",
           "",
           "Layer: 16<br>Expert: R17<br>Weight: 0.043<br>Top tokens:<br>ima: 0.251<br> : 0.204<br>: 0.192<br>f: 0.188<br>: 0.164",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 16<br>Expert: R24<br>Weight: 0.082<br>Top tokens:<br>: 0.279<br>enp: 0.211<br>pse: 0.185<br>uja: 0.183<br>ajo: 0.142",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 16<br>Expert: R47<br>Weight: 0.048<br>Top tokens:<br>ricte: 0.323<br>;->: 0.202<br>: 0.183<br>: 0.146<br>: 0.146",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 16<br>Expert: R59<br>Weight: 0.038<br>Top tokens:<br>: 0.385<br>: 0.174<br>classname: 0.160<br>rids: 0.146<br>: 0.134",
           "",
           "",
           "",
           "Layer: 16<br>Expert: R62<br>Weight: 0.043<br>Top tokens:<br>5: 0.252<br>2: 0.191<br>6: 0.188<br>9: 0.186<br>3: 0.184",
           "",
           "",
           "",
           "",
           "Layer: 16<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br> \": 0.233<br>ince: 0.200<br>: 0.191<br>leave: 0.190<br> ': 0.185",
           "Layer: 16<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br> ...): 0.203<br>QA: 0.203<br> H: 0.201<br> C: 0.197<br> S: 0.195"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "Layer: 17<br>Expert: R0<br>Weight: 0.036<br>Top tokens:<br>subNav: 0.250<br>altColor: 0.242<br>quera: 0.184<br>rowse: 0.164<br>: 0.160",
           "Layer: 17<br>Expert: R1<br>Weight: 0.030<br>Top tokens:<br>: 0.347<br>: 0.230<br>unton: 0.181<br> : 0.133<br>ospital: 0.108",
           "",
           "",
           "",
           "",
           "",
           "Layer: 17<br>Expert: R16<br>Weight: 0.037<br>Top tokens:<br>shift: 0.521<br>shifts: 0.159<br>move: 0.121<br> moved: 0.109<br>: 0.091",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 17<br>Expert: R23<br>Weight: 0.128<br>Top tokens:<br>ength: 0.294<br>: 0.222<br>: 0.180<br>ften: 0.161<br> : 0.142",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 17<br>Expert: R58<br>Weight: 0.071<br>Top tokens:<br>iger: 0.360<br> Picture: 0.180<br>-\": 0.168<br>agona: 0.164<br>1: 0.128",
           "",
           "",
           "",
           "",
           "Layer: 17<br>Expert: R62<br>Weight: 0.034<br>Top tokens:<br>: 0.366<br>: 0.277<br>: 0.215<br>INET: 0.074<br>: 0.067",
           "",
           "",
           "",
           "",
           "Layer: 17<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br>: 0.206<br>oDB: 0.200<br> moder: 0.199<br>CDF: 0.197<br> c: 0.197",
           "Layer: 17<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br>: 0.205<br>: 0.200<br> valenc: 0.199<br> adre: 0.199<br> pilars: 0.197"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 18<br>Expert: R11<br>Weight: 0.028<br>Top tokens:<br> -: 0.230<br>: 0.211<br>: 0.197<br>phe: 0.196<br>: 0.166",
           "",
           "",
           "",
           "",
           "Layer: 18<br>Expert: R16<br>Weight: 0.052<br>Top tokens:<br>iament: 0.303<br>ashian: 0.296<br>$~: 0.168<br>ocre: 0.120<br>iesa: 0.112",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 18<br>Expert: R22<br>Weight: 0.036<br>Top tokens:<br> in: 0.633<br> here: 0.196<br> at: 0.085<br>here: 0.044<br> on: 0.042",
           "",
           "",
           "",
           "Layer: 18<br>Expert: R26<br>Weight: 0.099<br>Top tokens:<br>optera: 0.522<br>aix: 0.148<br>: 0.138<br>ople: 0.109<br>IDX: 0.082",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 18<br>Expert: R45<br>Weight: 0.174<br>Top tokens:<br> Called: 0.453<br> called: 0.260<br>Called: 0.101<br>abel: 0.099<br> also: 0.088",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 18<br>Expert: R62<br>Weight: 0.138<br>Top tokens:<br>|+: 0.266<br>\t|: 0.236<br>acis: 0.175<br>heses: 0.167<br> Aires: 0.156",
           "",
           "",
           "",
           "",
           "Layer: 18<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br>: 0.210<br> (': 0.206<br> ': 0.200<br>esses: 0.193<br>: 0.192",
           "Layer: 18<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br>: 0.207<br> c: 0.199<br>ashi: 0.198<br> Pit: 0.198<br>dome: 0.198"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 19<br>Expert: R18<br>Weight: 0.191<br>Top tokens:<br> called: 0.521<br> Called: 0.166<br>: 0.130<br>ells: 0.107<br>called: 0.076",
           "",
           "",
           "",
           "",
           "Layer: 19<br>Expert: R22<br>Weight: 0.028<br>Top tokens:<br>EOF: 0.338<br>_: 0.197<br> EOF: 0.190<br>like: 0.143<br>: 0.131",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 19<br>Expert: R29<br>Weight: 0.024<br>Top tokens:<br>6: 0.276<br>0: 0.208<br>3: 0.206<br>8: 0.168<br>5: 0.142",
           "",
           "",
           "Layer: 19<br>Expert: R31<br>Weight: 0.063<br>Top tokens:<br> Halloween: 0.298<br>: 0.195<br>: 0.189<br>: 0.185<br>: 0.134",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 19<br>Expert: R51<br>Weight: 0.065<br>Top tokens:<br>le: 0.488<br>tics: 0.174<br>abella: 0.156<br>ometrics: 0.100<br>str: 0.082",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 19<br>Expert: R7<br>Weight: 0.060<br>Top tokens:<br>IS: 0.396<br>||: 0.168<br>obe: 0.150<br>ey: 0.146<br>IPS: 0.140",
           "",
           "",
           "Layer: 19<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br> : 0.209<br> : 0.206<br>Japan: 0.196<br> japonesa: 0.196<br> : 0.193",
           "Layer: 19<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br>: 0.212<br>: 0.199<br>: 0.197<br>: 0.196<br>: 0.196"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 20<br>Expert: R11<br>Weight: 0.036<br>Top tokens:<br>remos: 0.332<br>ech: 0.247<br> ch: 0.150<br> Ch: 0.136<br> Super: 0.135",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 20<br>Expert: R18<br>Weight: 0.039<br>Top tokens:<br>: 0.536<br>: 0.285<br>ombra: 0.066<br>onada: 0.059<br>assis: 0.054",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 20<br>Expert: R25<br>Weight: 0.109<br>Top tokens:<br>: 0.521<br>odin: 0.207<br>: 0.183<br>moil: 0.047<br>bona: 0.042",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 20<br>Expert: R33<br>Weight: 0.067<br>Top tokens:<br> looking: 0.428<br>looking: 0.396<br> holding: 0.077<br> lookin: 0.068<br> LOOK: 0.030",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 20<br>Expert: R48<br>Weight: 0.087<br>Top tokens:<br>tona: 0.549<br> Imperial: 0.229<br>Imper: 0.128<br> : 0.080<br> imperial: 0.013",
           "",
           "",
           "",
           "Layer: 20<br>Expert: R51<br>Weight: 0.056<br>Top tokens:<br>expensive: 0.307<br> expensive: 0.307<br>Exp: 0.194<br> pricey: 0.101<br> costly: 0.091",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 20<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br> E: 0.223<br> M: 0.198<br> R: 0.197<br>/: 0.192<br>..: 0.191",
           "Layer: 20<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br>hdots: 0.217<br>: 0.203<br>: 0.201<br>-------------: 0.190<br>---------: 0.189"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 21<br>Expert: R16<br>Weight: 0.033<br>Top tokens:<br>emat: 0.374<br>ensed: 0.185<br>ibo: 0.160<br>eed: 0.144<br>: 0.136",
           "",
           "",
           "",
           "",
           "Layer: 21<br>Expert: R20<br>Weight: 0.039<br>Top tokens:<br> : 0.231<br> : 0.220<br>ombra: 0.211<br>: 0.175<br> : 0.162",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 21<br>Expert: R42<br>Weight: 0.075<br>Top tokens:<br>_: 0.983<br> _: 0.006<br>**: 0.006<br>/: 0.003<br>||: 0.002",
           "",
           "",
           "",
           "Layer: 21<br>Expert: R46<br>Weight: 0.073<br>Top tokens:<br>riques: 0.736<br> capital: 0.103<br>Imper: 0.080<br>: 0.042<br> CAPITAL: 0.039",
           "Layer: 21<br>Expert: R47<br>Weight: 0.059<br>Top tokens:<br>4: 0.317<br>0: 0.293<br>2: 0.152<br>3: 0.143<br>8: 0.095",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 21<br>Expert: R7<br>Weight: 0.038<br>Top tokens:<br>Strunz: 0.591<br>: 0.112<br> GoString: 0.104<br>arrib: 0.104<br>andid: 0.088",
           "",
           "",
           "Layer: 21<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br>-: 0.355<br>: 0.166<br>-': 0.164<br>most: 0.158<br>-\": 0.157",
           "Layer: 21<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br> n: 0.207<br> m: 0.205<br> j: 0.200<br> b: 0.197<br> z: 0.191"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 22<br>Expert: R20<br>Weight: 0.034<br>Top tokens:<br>: 0.253<br>: 0.227<br>IZE: 0.187<br> Rosen: 0.168<br>hit: 0.165",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 22<br>Expert: R35<br>Weight: 0.034<br>Top tokens:<br>capital: 0.631<br> capital: 0.205<br>Capital: 0.128<br> Capital: 0.030<br> CAPITAL: 0.006",
           "",
           "",
           "",
           "",
           "",
           "Layer: 22<br>Expert: R40<br>Weight: 0.174<br>Top tokens:<br>TO: 0.960<br> Toy: 0.019<br> TO: 0.019<br>toy: 0.001<br>Archive: 0.001",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 22<br>Expert: R49<br>Weight: 0.042<br>Top tokens:<br>given: 0.751<br>;: 0.088<br> given: 0.075<br> ;: 0.047<br>-: 0.038",
           "Layer: 22<br>Expert: R5<br>Weight: 0.062<br>Top tokens:<br> : 0.991<br>1: 0.003<br>2: 0.003<br>:: 0.002<br>3: 0.000",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 22<br>Expert: R58<br>Weight: 0.118<br>Top tokens:<br>ialect: 0.320<br>ciples: 0.263<br>undos: 0.152<br>lica: 0.150<br>lix: 0.114",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 22<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br>: 0.211<br>DOCKED: 0.203<br> : 0.202<br>mil: 0.193<br>StyleID: 0.191",
           "Layer: 22<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br>xiously: 0.210<br>ocity: 0.208<br> of: 0.200<br>sembles: 0.194<br>: 0.188"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 23<br>Expert: R17<br>Weight: 0.048<br>Top tokens:<br>restant: 0.271<br>oldre: 0.233<br>ematic: 0.213<br>bolic: 0.145<br>redible: 0.138",
           "",
           "Layer: 23<br>Expert: R19<br>Weight: 0.041<br>Top tokens:<br>FrameworkBundle: 0.354<br> Climent: 0.215<br>ollary: 0.162<br>adicate: 0.136<br>ceptive: 0.133",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 23<br>Expert: R31<br>Weight: 0.052<br>Top tokens:<br>U: 0.274<br> U: 0.246<br>se: 0.198<br>: 0.145<br>lit: 0.137",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 23<br>Expert: R54<br>Weight: 0.200<br>Top tokens:<br>ICAST: 0.502<br>Balancer: 0.234<br>: 0.153<br>terdam: 0.064<br>: 0.047",
           "",
           "",
           "",
           "",
           "Layer: 23<br>Expert: R6<br>Weight: 0.049<br>Top tokens:<br>riction: 0.562<br>mana: 0.146<br>: 0.112<br>: 0.095<br>uci: 0.084",
           "",
           "",
           "Layer: 23<br>Expert: R62<br>Weight: 0.071<br>Top tokens:<br>ond: 0.338<br> recip: 0.179<br>itely: 0.177<br> : 0.154<br>rowse: 0.153",
           "",
           "",
           "",
           "",
           "Layer: 23<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br>DOCKED: 0.240<br>rbia: 0.203<br>TOOLSET: 0.192<br>: 0.185<br>Ungrouped: 0.180",
           "Layer: 23<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br>full: 0.220<br> full: 0.216<br> Full: 0.196<br> o: 0.190<br> u: 0.178"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 24<br>Expert: R13<br>Weight: 0.105<br>Top tokens:<br>D: 0.764<br>\tD: 0.129<br>odox: 0.088<br> Options: 0.014<br> Fund: 0.004",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 24<br>Expert: R28<br>Weight: 0.051<br>Top tokens:<br>onada: 0.716<br>ionista: 0.128<br>: 0.069<br> back: 0.058<br>ajor: 0.030",
           "",
           "Layer: 24<br>Expert: R3<br>Weight: 0.116<br>Top tokens:<br>elo: 0.700<br>chini: 0.250<br>oy: 0.042<br> : 0.006<br>: 0.003",
           "",
           "",
           "Layer: 24<br>Expert: R32<br>Weight: 0.061<br>Top tokens:<br>[]\": 0.271<br>\">/: 0.245<br>idera: 0.175<br>: 0.171<br>: 0.137",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 24<br>Expert: R49<br>Weight: 0.084<br>Top tokens:<br> goin: 0.652<br> going: 0.308<br>Going: 0.023<br>going: 0.011<br>mael: 0.006",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 24<br>Expert: R9<br>Weight: 0.079<br>Top tokens:<br> to: 0.999<br>\tto: 0.001<br>to: 0.000<br>\tTo: 0.000<br>nto: 0.000",
           "Layer: 24<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br>GeneratedMessage: 0.214<br>CPTR: 0.202<br>:\\\\: 0.196<br>: 0.194<br>:!: 0.193",
           "Layer: 24<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br>6: 0.214<br>2: 0.211<br>3: 0.195<br>0: 0.192<br>8: 0.189"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 25<br>Expert: R11<br>Weight: 0.059<br>Top tokens:<br>: 0.381<br>: 0.210<br>enses: 0.156<br>umped: 0.130<br> deput: 0.123",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 25<br>Expert: R29<br>Weight: 0.092<br>Top tokens:<br>etup: 0.377<br>: 0.193<br>otti: 0.178<br>las: 0.135<br>omitempty: 0.117",
           "",
           "",
           "",
           "",
           "",
           "Layer: 25<br>Expert: R34<br>Weight: 0.059<br>Top tokens:<br>restant: 0.343<br>by: 0.213<br> part: 0.183<br>entin: 0.143<br> corner: 0.118",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 25<br>Expert: R5<br>Weight: 0.061<br>Top tokens:<br>.?: 0.466<br> ?: 0.167<br> ?),: 0.138<br> ?): 0.127<br> : 0.102",
           "",
           "",
           "",
           "Layer: 25<br>Expert: R53<br>Weight: 0.073<br>Top tokens:<br>lich: 0.432<br>ebles: 0.295<br>: 0.111<br>Estructura: 0.091<br>: 0.071",
           "Layer: 25<br>Expert: R54<br>Weight: 0.078<br>Top tokens:<br>essin: 0.731<br>: 0.110<br>mics: 0.064<br>uites: 0.049<br>: 0.046",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 25<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br>...: 0.273<br> ...: 0.212<br>: 0.182<br>..: 0.172<br> -: 0.161",
           "Layer: 25<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br>,: 0.364<br> : 0.341<br>.: 0.141<br>\n: 0.091<br> -: 0.064"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 26<br>Expert: R14<br>Weight: 0.116<br>Top tokens:<br>: 0.433<br>: 0.302<br>: 0.098<br>||: 0.091<br>*/: 0.076",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 26<br>Expert: R26<br>Weight: 0.085<br>Top tokens:<br> not: 0.491<br> actually: 0.188<br> NOT: 0.141<br>nota: 0.098<br> an: 0.083",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 26<br>Expert: R32<br>Weight: 0.062<br>Top tokens:<br> v: 0.259<br>: 0.259<br> mod: 0.188<br> U: 0.181<br> c: 0.114",
           "",
           "",
           "Layer: 26<br>Expert: R35<br>Weight: 0.063<br>Top tokens:<br>1: 0.316<br> : 0.228<br>8: 0.164<br>9: 0.147<br>2: 0.146",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 26<br>Expert: R46<br>Weight: 0.062<br>Top tokens:<br>();: 0.262<br>: 0.250<br>;\\\\: 0.212<br>;[: 0.175<br> : 0.101",
           "",
           "",
           "",
           "Layer: 26<br>Expert: R5<br>Weight: 0.060<br>Top tokens:<br> TX: 0.442<br> OS: 0.333<br> te: 0.100<br> KC: 0.064<br> os: 0.061",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 26<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br>...: 0.373<br> : 0.185<br> \": 0.178<br>\": 0.171<br> -: 0.094",
           "Layer: 26<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br>...: 0.512<br>: 0.155<br>|: 0.151<br>....: 0.095<br>..: 0.087"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 27<br>Expert: R39<br>Weight: 0.142<br>Top tokens:<br> dead: 0.504<br> safe: 0.139<br> full: 0.123<br> worth: 0.119<br> destined: 0.115",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Layer: 27<br>Expert: R52<br>Weight: 0.045<br>Top tokens:<br> remaining: 0.263<br> hosting: 0.239<br> residing: 0.189<br> permitting: 0.160<br> running: 0.150",
           "",
           "",
           "",
           "",
           "",
           "Layer: 27<br>Expert: R59<br>Weight: 0.172<br>Top tokens:<br> taken: 0.326<br> pretty: 0.232<br> super: 0.170<br> seen: 0.160<br> enough: 0.112",
           "",
           "",
           "",
           "",
           "",
           "Layer: 27<br>Expert: R7<br>Weight: 0.122<br>Top tokens:<br> preferable: 0.246<br> common: 0.221<br> imperative: 0.209<br> possible: 0.179<br> advisable: 0.145",
           "Layer: 27<br>Expert: R8<br>Weight: 0.031<br>Top tokens:<br> America: 0.319<br> some: 0.244<br> america: 0.198<br>som: 0.131<br> high: 0.108",
           "Layer: 27<br>Expert: R9<br>Weight: 0.073<br>Top tokens:<br>: 0.887<br>: 0.079<br>: 0.023<br>: 0.009<br>: 0.002",
           "Layer: 27<br>Expert: S0<br>Weight: 1.000<br>Top tokens:<br> : 0.422<br>,: 0.334<br> (: 0.146<br>.: 0.067<br> and: 0.031",
           "Layer: 27<br>Expert: S1<br>Weight: 1.000<br>Top tokens:<br>,: 0.366<br> : 0.277<br> and: 0.141<br> (: 0.139<br>.: 0.077"
          ],
          [
           "Layer: Final<br>Expert: Final_1<br>Weight: 0.407<br>Top tokens:<br> Tokyo: 0.407",
           "Layer: Final<br>Expert: Final_2<br>Weight: 0.247<br>Top tokens:<br> to: 0.247",
           "Layer: Final<br>Expert: Final_3<br>Weight: 0.152<br>Top tokens:<br>\n: 0.152",
           "Layer: Final<br>Expert: Final_4<br>Weight: 0.100<br>Top tokens:<br> a: 0.100",
           "Layer: Final<br>Expert: Final_5<br>Weight: 0.095<br>Top tokens:<br> the: 0.095",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ]
         ],
         "showscale": true,
         "text": [
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "acost<br>0.077",
           "",
           "",
           "",
           "",
           "",
           "",
           "amena<br>0.028",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>0.031",
           "",
           "",
           "",
           "",
           "",
           "pte<br>0.058",
           "",
           "",
           "",
           "<br>0.302",
           "",
           "",
           "",
           "",
           "",
           "eqz<br>0.032",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "(<<br>1.000",
           "pei<br>1.000"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>0.039",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>0.048",
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>0.079",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           " controlador<br>0.049",
           "",
           "",
           "",
           "",
           "azu<br>0.181",
           "",
           "",
           "umerable<br>0.089",
           "",
           "",
           "aix<br>1.000",
           "<br>1.000"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "scul<br>0.207",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "fort<br>0.052",
           "",
           "",
           "",
           "Strunz<br>0.041",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>0.040",
           "",
           "",
           "",
           "",
           "anni<br>0.105",
           "",
           "",
           "pite<br>0.054",
           "",
           "",
           "",
           "memname<br>1.000",
           "ulats<br>1.000"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>0.056",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "3<br>0.166",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "ored<br>0.076",
           "",
           "",
           "",
           "",
           "",
           "ban<br>0.130",
           "<br>0.061",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>0.070",
           "",
           "",
           "<br>1.000",
           "<br>1.000"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>0.060",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "cale<br>0.109",
           "",
           "",
           "",
           "",
           "",
           "",
           "ur<br>0.172",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "icci<br>0.090",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>0.057",
           "[][]{<br>0.080",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "2<br>1.000",
           "<br>1.000"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "amb<br>0.042",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "**<br>0.054",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>0.106",
           "",
           "",
           "",
           "",
           "",
           "roz<br>0.088",
           "",
           "",
           "",
           "<br>0.089",
           "",
           "",
           "4<br>0.040",
           "",
           "",
           "",
           "<br>1.000",
           "<br>1.000"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Jac<br>0.095",
           "same<br>0.066",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "adur<br>0.044",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "ISTS<br>0.092",
           "",
           "",
           "",
           "",
           " <br>0.077",
           "",
           "",
           "uts<br>0.201",
           "odin<br>1.000",
           "<br>1.000"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "ermanent<br>0.040",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "sman<br>0.041",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "wanda<br>0.035",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           " prov<br>0.093",
           "",
           "",
           "coln<br>0.083",
           "",
           "",
           "",
           "<br>0.082",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           " <br>1.000",
           "yn<br>1.000"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "LAIN<br>0.076",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>0.098",
           "",
           "",
           "",
           "",
           "<br>0.045",
           "",
           "",
           "",
           "",
           "",
           "ingu<br>0.134",
           "",
           "",
           "",
           "",
           "",
           "",
           "ibar<br>0.050",
           "",
           "",
           "<br>0.071",
           "<br>1.000",
           "Formaci<br>1.000"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>0.152",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "hir<br>0.044",
           "",
           "<br>0.070",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "aried<br>0.052",
           "",
           "",
           "",
           "",
           "imburg<br>0.060",
           "",
           "",
           "ry<br>0.082",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           " lo<br>1.000",
           "<br>1.000"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>0.054",
           "",
           "",
           "",
           "8<br>0.048",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "_.--<br>0.057",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "4<br>0.101",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "OBJECT<br>0.076",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "imath<br>0.083",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "inclo<br>1.000",
           "De<br>1.000"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "oraci<br>0.055",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>0.052",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>0.064",
           "",
           "",
           "",
           "",
           "",
           "<br>0.058",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "icu<br>0.077",
           "",
           "",
           "",
           "ansen<br>0.055",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           " self<br>1.000",
           "9<br>1.000"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "hest<br>0.046",
           "",
           "",
           "veless<br>0.090",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>0.067",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>0.049",
           " radis<br>0.042",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "ODB<br>0.121",
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>1.000",
           "<br>1.000"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "abase<br>0.059",
           "",
           "ishop<br>0.051",
           "",
           "",
           "?></<br>0.104",
           "",
           "",
           "",
           "",
           "",
           "<br>0.091",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "(<br>0.054",
           "",
           "",
           "",
           "izes<br>0.047",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Ev<br>1.000",
           " E<br>1.000"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Re<br>0.044",
           "",
           "",
           "|,<br>0.101",
           "",
           "",
           "",
           "",
           "",
           "",
           " <br>0.138",
           " Frames<br>0.045",
           "",
           "usen<br>0.113",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           " Net<br>0.044",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>1.000",
           " dy<br>1.000"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "ProtoBuf<br>0.112",
           "",
           "ima<br>0.043",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>0.082",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "ricte<br>0.048",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>0.038",
           "",
           "",
           "",
           "5<br>0.043",
           "",
           "",
           "",
           "",
           " \"<br>1.000",
           " ...)<br>1.000"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "subNav<br>0.036",
           "<br>0.030",
           "",
           "",
           "",
           "",
           "",
           "shift<br>0.037",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "ength<br>0.128",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "iger<br>0.071",
           "",
           "",
           "",
           "",
           "<br>0.034",
           "",
           "",
           "",
           "",
           "<br>1.000",
           "<br>1.000"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           " -<br>0.028",
           "",
           "",
           "",
           "",
           "iament<br>0.052",
           "",
           "",
           "",
           "",
           "",
           "",
           " in<br>0.036",
           "",
           "",
           "",
           "optera<br>0.099",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           " Called<br>0.174",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "|+<br>0.138",
           "",
           "",
           "",
           "",
           "<br>1.000",
           "<br>1.000"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           " called<br>0.191",
           "",
           "",
           "",
           "",
           "EOF<br>0.028",
           "",
           "",
           "",
           "",
           "",
           "",
           "6<br>0.024",
           "",
           "",
           " Halloween<br>0.063",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "le<br>0.065",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "IS<br>0.060",
           "",
           "",
           " <br>1.000",
           "<br>1.000"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "remos<br>0.036",
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>0.039",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>0.109",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           " looking<br>0.067",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "tona<br>0.087",
           "",
           "",
           "",
           "expensive<br>0.056",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           " E<br>1.000",
           "hdots<br>1.000"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "emat<br>0.033",
           "",
           "",
           "",
           "",
           " <br>0.039",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "_<br>0.075",
           "",
           "",
           "",
           "riques<br>0.073",
           "4<br>0.059",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "Strunz<br>0.038",
           "",
           "",
           "-<br>1.000",
           " n<br>1.000"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>0.034",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "capital<br>0.034",
           "",
           "",
           "",
           "",
           "",
           "TO<br>0.174",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "given<br>0.042",
           " <br>0.062",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "ialect<br>0.118",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>1.000",
           "xiously<br>1.000"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "restant<br>0.048",
           "",
           "FrameworkBundle<br>0.041",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "U<br>0.052",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "ICAST<br>0.200",
           "",
           "",
           "",
           "",
           "riction<br>0.049",
           "",
           "",
           "ond<br>0.071",
           "",
           "",
           "",
           "",
           "DOCKED<br>1.000",
           "full<br>1.000"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "D<br>0.105",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "onada<br>0.051",
           "",
           "elo<br>0.116",
           "",
           "",
           "[]\"<br>0.061",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           " goin<br>0.084",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           " to<br>0.079",
           "GeneratedMessage<br>1.000",
           "6<br>1.000"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>0.059",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "etup<br>0.092",
           "",
           "",
           "",
           "",
           "",
           "restant<br>0.059",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           ".?<br>0.061",
           "",
           "",
           "",
           "lich<br>0.073",
           "essin<br>0.078",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "...<br>1.000",
           ",<br>1.000"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "<br>0.116",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           " not<br>0.085",
           "",
           "",
           "",
           "",
           "",
           "",
           " v<br>0.062",
           "",
           "",
           "1<br>0.063",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "();<br>0.062",
           "",
           "",
           "",
           " TX<br>0.060",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "...<br>1.000",
           "...<br>1.000"
          ],
          [
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           " dead<br>0.142",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           " remaining<br>0.045",
           "",
           "",
           "",
           "",
           "",
           " taken<br>0.172",
           "",
           "",
           "",
           "",
           "",
           " preferable<br>0.122",
           " America<br>0.031",
           "<br>0.073",
           " <br>1.000",
           ",<br>1.000"
          ],
          [
           " Tokyo<br>0.407",
           " to<br>0.247",
           "\n<br>0.152",
           " a<br>0.100",
           " the<br>0.095",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           "",
           ""
          ]
         ],
         "textfont": {
          "color": "white",
          "family": "Arial",
          "size": 16
         },
         "texttemplate": "%{text}",
         "type": "heatmap",
         "x": [
          "Final_1",
          "Final_2",
          "Final_3",
          "Final_4",
          "Final_5",
          "R0",
          "R1",
          "R11",
          "R12",
          "R13",
          "R14",
          "R15",
          "R16",
          "R17",
          "R18",
          "R19",
          "R2",
          "R20",
          "R21",
          "R22",
          "R23",
          "R24",
          "R25",
          "R26",
          "R27",
          "R28",
          "R29",
          "R3",
          "R30",
          "R31",
          "R32",
          "R33",
          "R34",
          "R35",
          "R36",
          "R37",
          "R38",
          "R39",
          "R4",
          "R40",
          "R41",
          "R42",
          "R43",
          "R44",
          "R45",
          "R46",
          "R47",
          "R48",
          "R49",
          "R5",
          "R50",
          "R51",
          "R52",
          "R53",
          "R54",
          "R55",
          "R57",
          "R58",
          "R59",
          "R6",
          "R60",
          "R61",
          "R62",
          "R63",
          "R7",
          "R8",
          "R9",
          "S0",
          "S1"
         ],
         "y": [
          "Layer 1",
          "Layer 2",
          "Layer 3",
          "Layer 4",
          "Layer 5",
          "Layer 6",
          "Layer 7",
          "Layer 8",
          "Layer 9",
          "Layer 10",
          "Layer 11",
          "Layer 12",
          "Layer 13",
          "Layer 14",
          "Layer 15",
          "Layer 16",
          "Layer 17",
          "Layer 18",
          "Layer 19",
          "Layer 20",
          "Layer 21",
          "Layer 22",
          "Layer 23",
          "Layer 24",
          "Layer 25",
          "Layer 26",
          "Layer 27",
          "Final Layer"
         ],
         "z": [
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.07659912109375,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0284271240234375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.030914306640625,
           0,
           0,
           0,
           0,
           0,
           0.058135986328125,
           0,
           0,
           0,
           0.3017578125,
           0,
           0,
           0,
           0,
           0,
           0.031951904296875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.03948974609375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.047637939453125,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0794677734375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0489501953125,
           0,
           0,
           0,
           0,
           0.1807861328125,
           0,
           0,
           0.0889892578125,
           0,
           0,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0.2066650390625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.051849365234375,
           0,
           0,
           0,
           0.040618896484375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.04034423828125,
           0,
           0,
           0,
           0,
           0.10498046875,
           0,
           0,
           0.054229736328125,
           0,
           0,
           0,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0562744140625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.1663818359375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.07574462890625,
           0,
           0,
           0,
           0,
           0,
           0.1295166015625,
           0.06121826171875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.07000732421875,
           0,
           0,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.060272216796875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.10870361328125,
           0,
           0,
           0,
           0,
           0,
           0,
           0.1717529296875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.089599609375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.05718994140625,
           0.080322265625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0423583984375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.05352783203125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.1064453125,
           0,
           0,
           0,
           0,
           0,
           0.08807373046875,
           0,
           0,
           0,
           0.08868408203125,
           0,
           0,
           0.03955078125,
           0,
           0,
           0,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.09454345703125,
           0.06622314453125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.043670654296875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0916748046875,
           0,
           0,
           0,
           0,
           0.0770263671875,
           0,
           0,
           0.2005615234375,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.039794921875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.040679931640625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0347900390625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0933837890625,
           0,
           0,
           0.083251953125,
           0,
           0,
           0,
           0.08160400390625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.07635498046875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.09814453125,
           0,
           0,
           0,
           0,
           0.044677734375,
           0,
           0,
           0,
           0,
           0,
           0.13427734375,
           0,
           0,
           0,
           0,
           0,
           0,
           0.050262451171875,
           0,
           0,
           0.0709228515625,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0.1522216796875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.044219970703125,
           0,
           0.06951904296875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.05181884765625,
           0,
           0,
           0,
           0,
           0.05963134765625,
           0,
           0,
           0.0821533203125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0.053924560546875,
           0,
           0,
           0,
           0.047515869140625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0572509765625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.10107421875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.07550048828125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.08331298828125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.055328369140625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.052154541015625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.06390380859375,
           0,
           0,
           0,
           0,
           0,
           0.0577392578125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.07696533203125,
           0,
           0,
           0,
           0.055419921875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.04559326171875,
           0,
           0,
           0.09033203125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.06707763671875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.048736572265625,
           0.041748046875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.1207275390625,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.058624267578125,
           0,
           0.051025390625,
           0,
           0,
           0.1038818359375,
           0,
           0,
           0,
           0,
           0,
           0.09051513671875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.054412841796875,
           0,
           0,
           0,
           0.0474853515625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.044342041015625,
           0,
           0,
           0.1011962890625,
           0,
           0,
           0,
           0,
           0,
           0,
           0.1376953125,
           0.04498291015625,
           0,
           0.11309814453125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.044403076171875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.1116943359375,
           0,
           0.042877197265625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.08172607421875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.04827880859375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.03814697265625,
           0,
           0,
           0,
           0.04302978515625,
           0,
           0,
           0,
           0,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0.035675048828125,
           0.0300140380859375,
           0,
           0,
           0,
           0,
           0,
           0.036773681640625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.1278076171875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0714111328125,
           0,
           0,
           0,
           0,
           0.033935546875,
           0,
           0,
           0,
           0,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0284423828125,
           0,
           0,
           0,
           0,
           0.05169677734375,
           0,
           0,
           0,
           0,
           0,
           0,
           0.036163330078125,
           0,
           0,
           0,
           0.09918212890625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.174072265625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.1376953125,
           0,
           0,
           0,
           0,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.190673828125,
           0,
           0,
           0,
           0,
           0.0277862548828125,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0240020751953125,
           0,
           0,
           0.063232421875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0653076171875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.059906005859375,
           0,
           0,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.036041259765625,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0386962890625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.1087646484375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.06658935546875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.08740234375,
           0,
           0,
           0,
           0.05615234375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.03314208984375,
           0,
           0,
           0,
           0,
           0.039215087890625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.07470703125,
           0,
           0,
           0,
           0.0731201171875,
           0.059234619140625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0377197265625,
           0,
           0,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.034271240234375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.034332275390625,
           0,
           0,
           0,
           0,
           0,
           0.1741943359375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0419921875,
           0.061676025390625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.11761474609375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.048248291015625,
           0,
           0.04071044921875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.051605224609375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.2001953125,
           0,
           0,
           0,
           0,
           0.04864501953125,
           0,
           0,
           0.0709228515625,
           0,
           0,
           0,
           0,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.10455322265625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.051422119140625,
           0,
           0.1162109375,
           0,
           0,
           0.060882568359375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0843505859375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.07879638671875,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.058837890625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.092041015625,
           0,
           0,
           0,
           0,
           0,
           0.05865478515625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.060943603515625,
           0,
           0,
           0,
           0.0726318359375,
           0.078125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.11590576171875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.0853271484375,
           0,
           0,
           0,
           0,
           0,
           0,
           0.061676025390625,
           0,
           0,
           0.06280517578125,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.062042236328125,
           0,
           0,
           0,
           0.060028076171875,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.1416015625,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.044921875,
           0,
           0,
           0,
           0,
           0,
           0.1722412109375,
           0,
           0,
           0,
           0,
           0,
           0.12213134765625,
           0.0312042236328125,
           0.072998046875,
           1,
           1
          ],
          [
           0.40673828125,
           0.2467041015625,
           0.1519775390625,
           0.09967041015625,
           0.0950927734375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ]
         ]
        }
       ],
       "layout": {
        "font": {
         "color": "white",
         "size": 16
        },
        "height": 2000,
        "margin": {
         "b": 120,
         "l": 120,
         "r": 120,
         "t": 160
        },
        "paper_bgcolor": "black",
        "plot_bgcolor": "black",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "size": 24
         },
         "text": "Expert Contributions and Predictions for: 'capital of japan is'"
        },
        "width": 4000,
        "xaxis": {
         "gridcolor": "rgba(128, 128, 128, 0.2)",
         "gridwidth": 1,
         "linecolor": "rgba(128, 128, 128, 0.2)",
         "linewidth": 1,
         "showgrid": true,
         "showline": true,
         "tickangle": 45,
         "tickfont": {
          "size": 16
         },
         "title": {
          "font": {
           "size": 20
          },
          "text": "Experts"
         }
        },
        "yaxis": {
         "autorange": "reversed",
         "gridcolor": "rgba(128, 128, 128, 0.2)",
         "gridwidth": 1,
         "linecolor": "rgba(128, 128, 128, 0.2)",
         "linewidth": 1,
         "showgrid": true,
         "showline": true,
         "tickfont": {
          "size": 16
         },
         "title": {
          "font": {
           "size": 20
          },
          "text": "Layers"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"capital of japan is\"\n",
    "\n",
    "fig, result, _ = analyze_dataset(\n",
    "    text=text,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    token_position=5,  # Analyze full sequence\n",
    "    domain=\"test1\",\n",
    "    force_recompute=False,\n",
    "    plot_type='expert',\n",
    "    display_plot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 102400])\n",
      "i: 4\n",
      "tensor([18.1094, 19.5781, 16.3906,  ...,  2.3262,  2.4473,  2.3262],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "y: 8196\n",
      " Washington\n",
      "torch.Size([2816, 2048])\n",
      "torch.Size([1408, 2048])\n"
     ]
    }
   ],
   "source": [
    "txt = \"capital of usa is\"\n",
    "\n",
    "inputs = tokenizer(txt, return_tensors=\"pt\")\n",
    "outputs = model.forward(**inputs.to(model.device))\n",
    "\n",
    "print(outputs.logits.shape)\n",
    "\n",
    "\n",
    "i = 4\n",
    "\n",
    "print(f\"i: {i}\")\n",
    "x = outputs.logits[0, i]\n",
    "# so basically over hee gpt/these langauge models put a prob over the whole vocab\n",
    "print(x) \n",
    "y = x.argmax()\n",
    "# to get the highest prob token\n",
    "print(f\"y: {y}\") \n",
    "\n",
    "print(tokenizer.decode(y))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(model.model.layers[3].mlp.shared_experts.gate_proj.weight.shape)\n",
    "print(model.model.layers[3].mlp.experts[63].gate_proj.weight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
