{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if DEVICE.type == \"cuda\":\n",
    "    # Print CUDA details\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated()/1024**2:.2f}MB\")\n",
    "    print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved()/1024**2:.2f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"sequential\",\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model.to(DEVICE)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model(\"databricks/dbrx-instruct\") # or base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moe_metadata(model, input_ids):\n",
    "    \"\"\"\n",
    "    Get both router logits and expert indices for all MoE layers in DBRX.\n",
    "    \n",
    "    Args:\n",
    "        model: DBRX model\n",
    "        input_ids: Input token IDs\n",
    "        \n",
    "    Returns:\n",
    "        moe_metadata: Dictionary containing router logits and expert indices\n",
    "    \"\"\"\n",
    "    router_logits_list = []\n",
    "    expert_indices_list = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        # For DBRX, we need to capture router logits and expert indices\n",
    "        # output typically contains: (selected_experts, route_prob, dispatch_mask, combine_weights, expert_layer_output)\n",
    "        hidden_states = input[0]\n",
    "        \n",
    "        # Extract router logits - adjust based on actual DBRX implementation\n",
    "        # This is a simplification and may need to be adjusted based on actual DBRX gate structure\n",
    "        if hasattr(module, 'gate_proj'):\n",
    "            logits = module.gate_proj(hidden_states)\n",
    "            router_logits_list.append(logits.detach())\n",
    "        \n",
    "        # Store expert indices (typically first element of output tuple)\n",
    "        if isinstance(output, tuple) and len(output) > 0:\n",
    "            expert_indices_list.append(output[0].detach())\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    hooks = []\n",
    "    # Iterate through model layers to find MoE layers\n",
    "    for name, module in model.named_modules():\n",
    "        # Look for MoE layers - adapt based on DBRX architecture\n",
    "        if 'moe' in name.lower() and 'block' in name.lower() and hasattr(module, 'gate'):\n",
    "            hook = module.gate.register_forward_hook(hook_fn)\n",
    "            hooks.append(hook)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    \n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    moe_metadata = {\n",
    "        'router_logits': torch.stack(router_logits_list) if router_logits_list else None,\n",
    "        'expert_indices': torch.stack(expert_indices_list) if expert_indices_list else None\n",
    "    }\n",
    "    \n",
    "    if moe_metadata['router_logits'] is not None:\n",
    "        print(f\"Router logits shape: {moe_metadata['router_logits'].shape}\")\n",
    "    if moe_metadata['expert_indices'] is not None:\n",
    "        print(f\"Expert indices shape: {moe_metadata['expert_indices'].shape}\")\n",
    "    \n",
    "    return moe_metadata\n",
    "\n",
    "def prepare_prompt(prompt, tokenizer, max_tokens=2048):\n",
    "    \"\"\"\n",
    "    Prepare a prompt for processing, splitting if necessary to fit within model context.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The text prompt to prepare\n",
    "        tokenizer: The model's tokenizer\n",
    "        max_tokens: Maximum number of tokens per chunk (default: 2048)\n",
    "        \n",
    "    Returns:\n",
    "        List of prompts that fit within token limit\n",
    "    \"\"\"\n",
    "    # Check if the input is a list of lines/prompts\n",
    "    if isinstance(prompt, list):\n",
    "        all_prepared_prompts = []\n",
    "        for single_prompt in prompt:\n",
    "            # Process each line/prompt individually\n",
    "            prepared_chunks = prepare_prompt(single_prompt, tokenizer, max_tokens)\n",
    "            all_prepared_prompts.extend(prepared_chunks)\n",
    "        return all_prepared_prompts\n",
    "    \n",
    "    # Process a single prompt\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    \n",
    "    # If prompt is small enough, return as is\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return [prompt]\n",
    "    \n",
    "    # Split into manageable chunks\n",
    "    prepared_prompts = []\n",
    "    \n",
    "    # Decode tokens into chunks\n",
    "    start_idx = 0\n",
    "    while start_idx < len(tokens):\n",
    "        end_idx = min(start_idx + max_tokens, len(tokens))\n",
    "        chunk_tokens = tokens[start_idx:end_idx]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens)\n",
    "        prepared_prompts.append(chunk_text)\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    print(f\"Long prompt detected! Split into {len(prepared_prompts)} chunks.\")\n",
    "    return prepared_prompts\n",
    "\n",
    "def process_text_file_for_expert_counts(file_path, model, tokenizer, output_path=None, max_tokens=4096):\n",
    "    \"\"\"\n",
    "    Process a text file to analyze MoE routing and count tokens per expert in each layer.\n",
    "    Saves a PyTorch file with expert token counts. Adapted for DBRX architecture.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to text file with prompts (one per line)\n",
    "        model: DBRX model\n",
    "        tokenizer: DBRX tokenizer\n",
    "        output_path: Path to save PyTorch results (default: based on input filename)\n",
    "        max_tokens: Maximum tokens per prompt chunk\n",
    "    \"\"\"\n",
    "    # Load the text file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Check if this is a GitHub code file\n",
    "    if 'github.txt' in file_path:\n",
    "        import re\n",
    "        # Find all code blocks using the file pattern\n",
    "        file_pattern = re.compile(r'.*\\\\b\\\\w+\\\\.(js|py|c|cpp|java|ts|rb|go|rs|cs|swift|kt|php)$', re.MULTILINE)\n",
    "        \n",
    "        # Find all matches (file headers)\n",
    "        matches = list(file_pattern.finditer(content))\n",
    "        \n",
    "        # Extract code blocks between file headers\n",
    "        raw_prompts = []\n",
    "        for i in range(len(matches)):\n",
    "            start_pos = matches[i].start()\n",
    "            # If this is the last match, go to the end of the file\n",
    "            if i == len(matches) - 1:\n",
    "                end_pos = len(content)\n",
    "            else:\n",
    "                end_pos = matches[i+1].start()\n",
    "            \n",
    "            # Extract the code block including the file header\n",
    "            code_block = content[start_pos:end_pos].strip()\n",
    "            raw_prompts.append(code_block)\n",
    "    else:\n",
    "        # Regular text file processing (one prompt per line)\n",
    "        raw_prompts = [line.strip() for line in content.split('\\n') if line.strip()]\n",
    "    \n",
    "    print(f\"Loaded {len(raw_prompts)} raw prompts from {file_path}\")\n",
    "    \n",
    "    # Prepare prompts (handle large prompts by splitting)\n",
    "    prompts = []\n",
    "    for raw_prompt in raw_prompts:\n",
    "        prepared_chunks = prepare_prompt(raw_prompt, tokenizer, max_tokens)\n",
    "        prompts.extend(prepared_chunks)\n",
    "    \n",
    "    print(f\"Processing {len(prompts)} prepared prompts (after splitting large ones)\")\n",
    "    \n",
    "    # Set default output path if not provided\n",
    "    if output_path is None:\n",
    "        output_path = file_path.replace('.txt', '_expert_data.pt')\n",
    "    \n",
    "    # Initialize counter for expert usage\n",
    "    # Structure: {layer_num: {expert_id: count}}\n",
    "    expert_counts = {}\n",
    "    \n",
    "    # Calculate total tokens for progress bar\n",
    "    total_tokens = 0\n",
    "    for prompt in prompts:\n",
    "        tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        total_tokens += tokens.size(1)\n",
    "    \n",
    "    print(f\"Total tokens to process: {total_tokens}\")\n",
    "    \n",
    "    # Initialize progress bar\n",
    "    pbar = tqdm(total=total_tokens, desc=\"Processing tokens\")\n",
    "    processed_tokens = 0\n",
    "    \n",
    "    # Process each prompt\n",
    "    for prompt in prompts:\n",
    "        # Tokenize the prompt\n",
    "        tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        seq_len = tokens.size(1)\n",
    "        \n",
    "        # Get MoE routing metadata\n",
    "        moe_metadata = get_moe_metadata(model, tokens)\n",
    "        \n",
    "        if moe_metadata['expert_indices'] is None:\n",
    "            print(\"No MoE layers detected or no routing information available\")\n",
    "            processed_tokens += seq_len\n",
    "            pbar.update(seq_len)\n",
    "            continue\n",
    "        \n",
    "        # Extract expert indices\n",
    "        expert_indices = moe_metadata['expert_indices']  # shape: [num_layers, seq_len, top_k]\n",
    "        num_moe_layers = expert_indices.size(0)\n",
    "        \n",
    "        # Initialize counter for this batch if needed\n",
    "        for layer_idx in range(num_moe_layers):\n",
    "            layer_num = layer_idx + 1  # 1-based layer indexing\n",
    "            if layer_num not in expert_counts:\n",
    "                expert_counts[layer_num] = {}\n",
    "        \n",
    "        # Count token routing for each layer\n",
    "        for layer_idx in range(num_moe_layers):\n",
    "            layer_num = layer_idx + 1  # 1-based layer indexing\n",
    "            \n",
    "            # Process each token in sequence\n",
    "            for token_idx in range(seq_len):\n",
    "                # Get experts selected for this token in this layer\n",
    "                selected_experts = expert_indices[layer_idx, token_idx].cpu().numpy().tolist()\n",
    "                \n",
    "                # Count each expert\n",
    "                for expert_id in selected_experts:\n",
    "                    if expert_id not in expert_counts[layer_num]:\n",
    "                        expert_counts[layer_num][expert_id] = 0\n",
    "                    expert_counts[layer_num][expert_id] += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        processed_tokens += seq_len\n",
    "        pbar.update(seq_len)\n",
    "    \n",
    "    # Close progress bar\n",
    "    pbar.close()\n",
    "    \n",
    "    # Convert counts to a simple tensor format for saving\n",
    "    expert_token_counts = {}\n",
    "    for layer_num in sorted(expert_counts.keys()):\n",
    "        layer_data = expert_counts[layer_num]\n",
    "        # Create a tensor with counts for each expert (16 experts for DBRX)\n",
    "        counts = torch.zeros(16)\n",
    "        for expert_id, count in layer_data.items():\n",
    "            if 0 <= expert_id < 16:  # Ensure expert ID is valid\n",
    "                counts[expert_id] = count\n",
    "        expert_token_counts[layer_num] = counts\n",
    "    \n",
    "    # Save just the token counts per expert\n",
    "    torch.save(expert_token_counts, output_path)\n",
    "    print(f\"Expert token counts saved to {output_path}\")\n",
    "    \n",
    "    # Create a DataFrame for visualization purposes\n",
    "    rows = []\n",
    "    for layer_num in sorted(expert_counts.keys()):\n",
    "        layer_data = expert_counts[layer_num]\n",
    "        for expert_id in range(16):  # 16 experts for DBRX\n",
    "            count = layer_data.get(expert_id, 0)\n",
    "            rows.append({\n",
    "                'layer': layer_num,\n",
    "                'expert_id': expert_id,\n",
    "                'token_count': count\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "def analyze_text_file_routing(model, tokenizer, file_path):\n",
    "    \"\"\"\n",
    "    Main function to analyze MoE routing for a text file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to text file with prompts (one per line)\n",
    "        model_name: Name of DeepSeek MoE model to use\n",
    "    \"\"\"\n",
    "    # Process the file for expert counts and save as PyTorch file\n",
    "    df = process_text_file_for_expert_counts(file_path, model, tokenizer)\n",
    "    \n",
    "    print(f\"Analysis completed for {file_path}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data-ext/test.txt\"\n",
    "df = analyze_text_file_routing(model, tokenizer, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(\"data-ext/gsm8k_expert_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_graph_all_tokens_paper(expert_data, layer_number, tokenizer, domain=None):\n",
    "    \"\"\"\n",
    "    Visualizes expert distribution for all tokens in a file for a specific layer.\n",
    "    Adapted for DBRX model which has 16 experts rather than 64.\n",
    "    \n",
    "    Args:\n",
    "        expert_data: Dictionary with layer numbers as keys and tensor of expert counts as values\n",
    "                     or path to PyTorch file with this data\n",
    "        layer_number: Layer to analyze\n",
    "        tokenizer: DBRX tokenizer\n",
    "        domain: Optional domain name for title (e.g., 'GSM8K', 'Math', etc.)\n",
    "    \n",
    "    Returns:\n",
    "        fig: Plotly figure object\n",
    "    \"\"\"\n",
    "    # Load data if a file path is provided\n",
    "    if isinstance(expert_data, str):\n",
    "        txt_file_path = expert_data.replace(\"data-ext/pt/\", \"data-ext/\").replace(\"_expert_data.pt\", \".txt\").replace(\"_expert_data_chat.pt\", \".txt\").replace(\"_expert_data_base.pt\", \".txt\")\n",
    "        \n",
    "        num_tokens = None\n",
    "        if os.path.exists(txt_file_path):\n",
    "            # Try different encodings\n",
    "            encodings = ['utf-8', 'latin-1', 'utf-16']\n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    with open(txt_file_path, 'r', encoding=encoding) as f:\n",
    "                        text_content = f.read()\n",
    "                        tokens = tokenizer(text_content, return_tensors=\"pt\")\n",
    "                        num_tokens = tokens.input_ids.numel()\n",
    "                        print(f\"Total tokens in {txt_file_path}: {num_tokens}\")\n",
    "                        break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            else:\n",
    "                print(f\"Could not read {txt_file_path} with any of the attempted encodings\")\n",
    "        \n",
    "        expert_data = torch.load(expert_data)\n",
    "    else:\n",
    "        # If expert_data is already loaded (not a string path)\n",
    "        num_tokens = None\n",
    "    \n",
    "    # Validate layer number is in the data\n",
    "    if layer_number not in expert_data:\n",
    "        raise ValueError(f\"Layer {layer_number} not found in expert data\")\n",
    "    \n",
    "    # Get counts for the specified layer\n",
    "    expert_counts = expert_data[layer_number].numpy()\n",
    "    \n",
    "    # For DBRX, we expect 16 experts rather than 64\n",
    "    num_experts = len(expert_counts)\n",
    "    if num_experts != 16:\n",
    "        print(f\"Warning: Expected 16 experts for DBRX, but found {num_experts}\")\n",
    "        \n",
    "    # Compute percentages\n",
    "    total_tokens = num_tokens or expert_counts.sum() / 4  # Each token routes to 4 experts in DBRX\n",
    "    if total_tokens == 0:\n",
    "        print(\"No tokens found for this layer\")\n",
    "        return\n",
    "    \n",
    "    print(f\"total_tokens: {total_tokens}\")\n",
    "    print(f\"num_tokens: {num_tokens}\")\n",
    "    percentages = (expert_counts / total_tokens) * 100\n",
    "    \n",
    "    # Set discrete opacity based on 25% threshold (4 times expected uniform distribution of 1/16 = 6.25%)\n",
    "    threshold = 25.0\n",
    "    opacities = np.where(percentages >= threshold, 1.0, 0.3)\n",
    "    \n",
    "    # Create plotly figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add bar trace with discrete color and opacity\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=list(range(num_experts)),\n",
    "        y=percentages,\n",
    "        marker=dict(\n",
    "            color='#636EFA',  # Blue color for all bars\n",
    "            opacity=opacities\n",
    "        ),\n",
    "        hovertemplate='Expert ID: %{x}<br>Tokens: %{text}<br>Percentage: %{y:.2f}%<extra></extra>',\n",
    "        text=[f\"{int(count)}\" for count in expert_counts],\n",
    "        textposition='none'  # Ensure no text is displayed on the bars\n",
    "    ))\n",
    "    \n",
    "    # Add horizontal line at threshold\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=-0.5,\n",
    "        x1=num_experts-0.5,\n",
    "        y0=threshold,\n",
    "        y1=threshold,\n",
    "        line=dict(\n",
    "            color=\"red\",\n",
    "            width=2,\n",
    "            dash=\"dash\",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add annotation for the threshold line\n",
    "    fig.add_annotation(\n",
    "        x=num_experts-1,\n",
    "        y=threshold,\n",
    "        text=f\"{threshold}% threshold\",\n",
    "        showarrow=False,\n",
    "        yshift=10,\n",
    "        font=dict(color=\"red\")\n",
    "    )\n",
    "    \n",
    "    routed_tokens = expert_counts.sum()\n",
    "    \n",
    "    # Domain label for title\n",
    "    domain_label = f\" - {domain}\" if domain else \"\"\n",
    "    token_info = f\" (No. of Tokens: {num_tokens}, Routed Tokens: {int(routed_tokens)})\" if num_tokens else f\" (Routed Tokens: {int(routed_tokens)})\"\n",
    "    \n",
    "    # Update layout with white background\n",
    "    fig.update_layout(\n",
    "        title=f'Expert Usage Distribution - Layer {layer_number}{domain_label}{token_info}',\n",
    "        xaxis_title='Expert ID',\n",
    "        yaxis_title='Usage Percentage (%)',\n",
    "        yaxis_range=[0, 100],\n",
    "        xaxis=dict(tickmode='linear', tick0=0, dtick=1),  # Set tick for each expert (0-15)\n",
    "        showlegend=False,\n",
    "        width=1000,\n",
    "        height=600,\n",
    "        plot_bgcolor='white',\n",
    "        paper_bgcolor='white'\n",
    "    )\n",
    "    \n",
    "    # Add gridlines with lighter color\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='rgba(128, 128, 128, 0.1)')\n",
    "    fig.update_xaxes(showgrid=False)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = bar_graph_all_tokens_paper(\"data-ext/arxiv_title_abstract_expert_data_base.pt\", layer_number=15, tokenizer=tokenizer)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
