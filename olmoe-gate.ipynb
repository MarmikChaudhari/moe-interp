{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available, using CPU\n"
     ]
    }
   ],
   "source": [
    "def setup_device():\n",
    "    \"\"\"Set up device and optimizations with graceful CPU fallback\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        # Enable TF32 for better performance on Ampere+ GPUs\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        print(f\"Using CUDA device: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"GPU Memory available: {torch.cuda.get_device_properties(device).total_memory/1e9:.2f} GB\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"CUDA not available, using CPU\")\n",
    "    return device\n",
    "\n",
    "device = setup_device()\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory cache and force garbage collection\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, device):\n",
    "    \"\"\"Load model with appropriate dtype and device placement\"\"\"\n",
    "    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        trust_remote_code=True,\n",
    "    ).eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        model = model.to(device)\n",
    "        \n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model(\"allenai/OLMoE-1B-7B-0924\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moe_metadata(model, input_ids):\n",
    "    \"\"\"Get both router logits and expert indices for all MoE layers\"\"\"\n",
    "    # Clear GPU cache before starting\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    router_logits_list = []\n",
    "    expert_indices_list = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        hidden_states = input[0]\n",
    "        # Use torch.cuda.amp.autocast() for mixed precision\n",
    "        with torch.cuda.amp.autocast('cuda'):\n",
    "            # Optimize matrix multiplication for CUDA\n",
    "            logits = torch.matmul(hidden_states, module.weight.T)\n",
    "            # Use fast CUDA implementation of softmax\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            # Use efficient CUDA topk implementation\n",
    "            _, indices = torch.topk(probs, model.config.num_experts_per_tok, dim=-1)\n",
    "        \n",
    "        # Move tensors to CUDA, detach and clear intermediate tensors\n",
    "        router_logits_list.append(logits.detach().cuda())\n",
    "        expert_indices_list.append(indices.detach().cuda())\n",
    "        del probs  # Clear intermediate tensor\n",
    "        return output\n",
    "    \n",
    "    hooks = []\n",
    "    # Pre-compile layer iteration\n",
    "    layers = [layer for layer in model.model.layers \n",
    "             if layer.mlp.__class__.__name__ == 'OlmoeSparseMoeBlock']\n",
    "    \n",
    "    for layer in layers:\n",
    "        hook = layer.mlp.gate.register_forward_hook(hook_fn)\n",
    "        hooks.append(hook)\n",
    "\n",
    "    # Ensure input is on CUDA\n",
    "    input_ids = input_ids.cuda()\n",
    "    \n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        model(input_ids)\n",
    "        # Clear cache after forward pass\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Clean up hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    # Efficient CUDA stacking with intermediate cache clearing\n",
    "    router_logits = torch.stack(router_logits_list) if router_logits_list else None \n",
    "    del router_logits_list\n",
    "    expert_indices = torch.stack(expert_indices_list) if expert_indices_list else None\n",
    "    del expert_indices_list\n",
    "    \n",
    "    # Final cache clear and sync\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    return router_logits, expert_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "input = \"the quick brown fox\"\n",
    "input_ids = tokenizer.encode(input, return_tensors=\"pt\")\n",
    "print(f\"input_ids shape: {input_ids.shape}\")\n",
    "\n",
    "# Get MoE metadata\n",
    "router_logits, expert_indices = get_moe_metadata(model, input_ids)\n",
    "\n",
    "print(f\"router_logits shape: {router_logits.shape}\")\n",
    "print(f\"expert_indices shape: {expert_indices.shape}\")\n",
    "# print(f\"router_logits shape: {router_logits.shape}\")\n",
    "\n",
    "# Each element in router_logits is tensor of shape:\n",
    "# [batch_size, sequence_length, num_experts]\n",
    "\n",
    "# Each element in expert_indices is tensor of shape:\n",
    "# [batch_size, sequence_length, num_experts_per_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_token_router_probs(router_logits, model_layer_idx):\n",
    "    \"\"\"\n",
    "    Get router probabilities for the last token in the sequence\n",
    "    for a specific MODEL LAYER INDEX (0-15 for MoE layers)\n",
    "    \"\"\"\n",
    "    if model_layer_idx < 0 or model_layer_idx >= router_logits.size(0):\n",
    "        raise ValueError(f\"Invalid model_layer_idx {model_layer_idx}. Must be 0-15 for MoE layers\")\n",
    "    \n",
    "    # Ensure inputs are on CUDA\n",
    "    router_logits = router_logits.cuda()\n",
    "    \n",
    "    layer_logits = router_logits[model_layer_idx]  # [seq_len, num_experts]\n",
    "    \n",
    "    last_token_logits = layer_logits[-1]  # [num_experts]\n",
    "    routing_probs = torch.nn.functional.softmax(last_token_logits, dim=-1)\n",
    "    \n",
    "    return routing_probs\n",
    "\n",
    "def topk(router_probs, k):\n",
    "    \"\"\"zero out all components except top k router probabilities\"\"\"\n",
    "    # Ensure inputs are on CUDA\n",
    "    router_probs = router_probs.cuda()\n",
    "    \n",
    "    values, indices = torch.topk(router_probs, k)\n",
    "    zeroed_probs = torch.zeros_like(router_probs, device='cuda')\n",
    "    zeroed_probs[indices] = values\n",
    "    return zeroed_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = get_last_token_router_probs(router_logits, 3)\n",
    "# print(f\"x: {x}\")\n",
    "# y = topk(x, 8)\n",
    "# print(f\"y: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moe_data(model, tokenizer, prompts):\n",
    "    \"\"\"\n",
    "    Collects both all-token and last-token MoE data in a single forward pass per prompt.\n",
    "    \n",
    "    Args:\n",
    "        model: OLMoE model\n",
    "        tokenizer: OLMoE tokenizer \n",
    "        prompts: List of prompts or list of lists of prompts\n",
    "    \"\"\"\n",
    "    # Clear CUDA cache before starting\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Flatten prompts if it's a list of lists\n",
    "    if isinstance(prompts[0], list):\n",
    "        prompts = [prompt for domain_prompts in prompts for prompt in domain_prompts]\n",
    "    \n",
    "    num_prompts = len(prompts)\n",
    "    \n",
    "    # First tokenize one prompt to get reasonable max length\n",
    "    sample_output = tokenizer(prompts[0], return_length=True)\n",
    "    max_seq_len = min(4096, max(len(tokenizer.encode(prompt)) for prompt in prompts))  # Cap at model's max length\n",
    "    \n",
    "    num_moe_layers = sum(1 for layer in model.model.layers if layer.mlp.__class__.__name__ == 'OlmoeSparseMoeBlock')\n",
    "    print(f\"Number of MoE layers: {num_moe_layers}\")\n",
    "    \n",
    "    num_experts = model.config.num_experts  # 64 for OLMoE\n",
    "    num_experts_per_tok = model.config.num_experts_per_tok  # 8 for OLMoE\n",
    "\n",
    "    # Initialize tensors directly on CUDA\n",
    "    all_token_logits = torch.zeros((num_prompts, num_moe_layers, max_seq_len, num_experts),\n",
    "                               dtype=torch.float16, device='cuda')\n",
    "    all_token_experts = -torch.ones((num_prompts, num_moe_layers, max_seq_len, num_experts_per_tok),\n",
    "                                dtype=torch.long, device='cuda')\n",
    "    last_token_logits = torch.zeros((num_prompts, num_moe_layers, num_experts),\n",
    "                                dtype=torch.float16, device='cuda')\n",
    "    last_token_experts = torch.zeros((num_prompts, num_moe_layers, num_experts_per_tok),\n",
    "                                 dtype=torch.long, device='cuda')\n",
    "\n",
    "    # Process prompts in batches to avoid OOM\n",
    "    batch_size = 8  # Adjust based on GPU memory\n",
    "    for i in range(0, num_prompts, batch_size):\n",
    "        batch_prompts = prompts[i:i + batch_size]\n",
    "        \n",
    "        # Clear cache between batches\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        for j, prompt in enumerate(batch_prompts):\n",
    "            # Add padding and truncation\n",
    "            inputs = tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=max_seq_len\n",
    "            )\n",
    "            \n",
    "            # Move inputs to CUDA\n",
    "            input_ids = inputs['input_ids'].cuda()\n",
    "            attention_mask = inputs['attention_mask'].cuda()\n",
    "            seq_len = attention_mask.sum().item()  # Get actual sequence length without padding\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                router_logits, expert_indices = get_moe_metadata(model, input_ids)\n",
    "            \n",
    "            for layer_idx in range(num_moe_layers):\n",
    "                # Handle router logits - shape should be [1, seq_len, num_experts]\n",
    "                layer_logits = router_logits[layer_idx].squeeze(0)\n",
    "                \n",
    "                # Handle expert indices - shape should be [1, seq_len, num_experts_per_tok]\n",
    "                layer_experts = expert_indices[layer_idx].squeeze(0)\n",
    "                \n",
    "                # Store all tokens data\n",
    "                all_token_logits[i+j, layer_idx, :seq_len] = layer_logits[:seq_len]\n",
    "                all_token_experts[i+j, layer_idx, :seq_len] = layer_experts[:seq_len]\n",
    "                \n",
    "                # Store last token data (using last real token, not padding)\n",
    "                last_token_logits[i+j, layer_idx] = layer_logits[seq_len-1]\n",
    "                last_token_experts[i+j, layer_idx] = layer_experts[seq_len-1]\n",
    "            \n",
    "            # Clear intermediate tensors\n",
    "            del input_ids, attention_mask, router_logits, expert_indices\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return all_token_logits, all_token_experts, last_token_logits, last_token_experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_prompts = ['the quick brown fox', 'the capital of japan is tokyo', 'the capital of france is paris', ]\n",
    "# all_token_logits, all_token_experts, last_token_logits, last_token_experts = get_moe_data(model, tokenizer, prompts = test_prompts)\n",
    "\n",
    "# print(f\"all_token_logits shape: {all_token_logits.shape}\") # [num_prompts, num_layers, seq_len, num_experts]\n",
    "# print(f\"all_token_experts shape: {all_token_experts.shape}\")\n",
    "# print(f\"last_token_logits shape: {last_token_logits.shape}\")\n",
    "# print(f\"last_token_experts shape: {last_token_experts.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompts_from_txt(txt_file_path,  domain = 'english', output_path=f'english.json'):\n",
    "    \"\"\" read prompts from a txt file and save them in json format. \"\"\"\n",
    "    \n",
    "    with open(txt_file_path, 'r', encoding='utf-8') as f:\n",
    "        prompts = [line.strip() for line in f.readlines() if line.strip()]\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump({f\"{domain}\": prompts}, f, indent=4)\n",
    "        \n",
    "    print(f\"{domain} prompts saved to {output_path}\")\n",
    "    return prompts\n",
    "\n",
    "def parse_code_blocks(txt_file_path, output_path='code.json', domain='code'):\n",
    "    \"\"\"parse code blocks between ``` markers from a text file and save them in json format.\"\"\"\n",
    "    code_blocks = []\n",
    "    current_block = []\n",
    "    in_block = False\n",
    "    \n",
    "    with open(txt_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip().startswith('```'):\n",
    "                if in_block:\n",
    "                    # Current block is complete, save it and start new block\n",
    "                    if current_block:\n",
    "                        code_blocks.append('\\n'.join(current_block))\n",
    "                    current_block = []\n",
    "                # Always start a new block since ``` only indicates start\n",
    "                in_block = True\n",
    "                current_block = []\n",
    "            elif in_block:\n",
    "                # Add line to current block\n",
    "                current_block.append(line.rstrip())\n",
    "    \n",
    "    # Save final block if exists\n",
    "    if current_block:\n",
    "        code_blocks.append('\\n'.join(current_block))\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump({domain: code_blocks}, f, indent=4)\n",
    "        \n",
    "    print(f\"code blocks saved to {output_path}\")\n",
    "    return code_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompts_from_json(json_file_path):\n",
    "    \"\"\"load prompts from a json file and return them as a list.\"\"\"\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    # get the first (and only) value from the dictionary\n",
    "    # since the json structure is {\"domain\": [prompts]}\n",
    "    prompts = list(data.values())[0]\n",
    "    return prompts\n",
    "\n",
    "def prepare_multi_domain_prompts(domain_files, output_path='all_domain_prompts.json'):\n",
    "    \"\"\"\n",
    "    prepare a json file containing prompts from multiple domains.\n",
    "    \n",
    "    args:\n",
    "        domain_files: Dict mapping domain names to lists of tuples (file_path, parser_func)\n",
    "            where parser_func is a function that takes a file path and returns a list of prompts\n",
    "            \n",
    "    example:\n",
    "        domain_files = {\n",
    "            'code': [('code.txt', parse_code_blocks)], \n",
    "            'english': [('english.txt', prepare_prompts_from_txt)]\n",
    "        }\n",
    "    \"\"\"\n",
    "    all_prompts = {}\n",
    "    \n",
    "    for domain, file_list in domain_files.items():\n",
    "        domain_prompts = []\n",
    "        for _, prompts in file_list:\n",
    "            # Use load_prompts_from_json if prompts is a dict\n",
    "            if not isinstance(prompts, list):\n",
    "                prompts = load_prompts_from_json(prompts)\n",
    "            domain_prompts.extend(prompts)\n",
    "                \n",
    "        all_prompts[domain] = domain_prompts\n",
    "        \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_prompts, f, indent=4)\n",
    "        \n",
    "    print(f\"all domain prompts saved to {output_path}\")\n",
    "    return all_prompts\n",
    "\n",
    "def convert_all_to_list(all_prompts):\n",
    "    \"\"\"\n",
    "    combines prompts from all domains into a single list of domain-specific prompt lists.\n",
    "    returns a list in the format [[domain1_prompts], [domain2_prompts], ...].\n",
    "    \"\"\"\n",
    "    # Create list of domain-specific prompt lists\n",
    "    combined_prompts = [\n",
    "        prompts for prompts in all_prompts.values()\n",
    "    ]\n",
    "        \n",
    "    total_prompts = sum(len(prompts) for prompts in combined_prompts)\n",
    "    print(f\"total prompts: {total_prompts}\")\n",
    "    print(f\"prompts per domain:\")\n",
    "    for domain, prompts in zip(all_prompts.keys(), combined_prompts):\n",
    "        print(f\"  {domain}: {len(prompts)}\")\n",
    "        \n",
    "    return combined_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_prompts_from_txt('interp-data/engl-lit.txt', domain='english', output_path='english.json')\n",
    "prepare_prompts_from_txt('interp-data/french.txt', domain='french', output_path='french.json')\n",
    "parse_code_blocks('interp-data/code.txt', 'code.json', domain='code')\n",
    "\n",
    "code_prompts = load_prompts_from_json(json_file_path='code.json')\n",
    "print(f\"total code prompts : {len(code_prompts)}\")\n",
    "english_prompts = load_prompts_from_json(json_file_path='english.json')\n",
    "print(f'total english prompts : {len(english_prompts)}')\n",
    "\n",
    "\n",
    "domain_files = {\n",
    "    'code': [('interp-data/code.txt', parse_code_blocks(txt_file_path='interp-data/code.txt', output_path='interp-data/code.json', domain='code'))],\n",
    "    'english': [('interp-data/engl-lit.txt', prepare_prompts_from_txt(txt_file_path='interp-data/engl-lit.txt', output_path='interp-data/english.json', domain='english'))],\n",
    "    'french': [('interp-data/french.txt', prepare_prompts_from_txt(txt_file_path='interp-data/french.txt', output_path='interp-data/french.json', domain='french'))]\n",
    "}\n",
    "all_prompts = prepare_multi_domain_prompts(domain_files, output_path='interp-data/all_prompts.json')\n",
    "print(f\"total domains : {len(all_prompts)}\")\n",
    "\n",
    "# convert all prompts to a single list of domain-specific prompt lists\n",
    "combined_prompts = convert_all_to_list(all_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_prompts_from_txt('interp-data/test.txt', domain='test', output_path='interp-data/test.json')\n",
    "\n",
    "test_prompts = load_prompts_from_json(json_file_path='interp-data/test.json')\n",
    "print(f\"total test prompts : {len(test_prompts)}\")\n",
    "\n",
    "domain_files = {\n",
    "    'test': [('interp-data/test.txt', prepare_prompts_from_txt(txt_file_path='interp-data/test.txt', output_path='interp-data/test.json', domain='test'))]\n",
    "}\n",
    "all_prompts = prepare_multi_domain_prompts(domain_files, output_path='interp-data/all_prompts.json')\n",
    "print(f\"total domains : {len(all_prompts)}\")\n",
    "\n",
    "# convert all prompts to a single list of domain-specific prompt lists\n",
    "combined_prompts = convert_all_to_list(all_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_token_logits, all_token_experts, last_token_logits, last_token_experts = get_moe_data(model, tokenizer, prompts = combined_prompts)\n",
    "\n",
    "print(f'all_token_logits shape: {all_token_logits.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(all_token_logits, \"olmoe-interp-pt/all_token_logits.pt\")\n",
    "torch.save(all_token_experts, \"olmoe-interp-pt/all_token_experts.pt\")\n",
    "torch.save(last_token_logits, \"olmoe-interp-pt/last_token_logits.pt\")\n",
    "torch.save(last_token_experts, \"olmoe-interp-pt/last_token_experts.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_token_logits = torch.load(\"interp-pt/all_token_logits.pt\", map_location=torch.device('cpu'), weights_only=True)\n",
    "all_token_experts = torch.load(\"interp-pt/all_token_experts.pt\", map_location=torch.device('cpu'), weights_only=True)\n",
    "last_token_logits = torch.load(\"interp-pt/last_token_logits.pt\", map_location=torch.device('cpu'), weights_only=True)\n",
    "last_token_experts = torch.load(\"interp-pt/last_token_experts.pt\", map_location=torch.device('cpu'), weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'all_token_logits shape: {all_token_logits.shape}')\n",
    "print(f'all_token_experts shape: {all_token_experts.shape}')\n",
    "print(f'last_token_logits shape: {last_token_logits.shape}')\n",
    "print(f'last_token_experts shape: {last_token_experts.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_graph_visualize(last_token_experts, layer_number, domain, k=8):\n",
    "    \"\"\"\n",
    "    Visualizes expert distribution for last tokens in a domain and layer using plotly.\n",
    "    \n",
    "    Args:\n",
    "        last_token_experts: Tensor [num_prompts, num_layers, num_experts_per_tok]\n",
    "        layer_number: Layer to analyze (1-indexed)\n",
    "        domain: 'code', 'english', or other domain name\n",
    "        k: Number of experts per token (default 8 for OLMoE)\n",
    "    \"\"\"\n",
    "    if domain not in ['code', 'english', 'french']:\n",
    "        raise ValueError(\"Invalid domain\")\n",
    "    \n",
    "    # Get domain slice (adjust size based on your data)\n",
    "    domain_slices = {\n",
    "        'code': slice(0, 200),\n",
    "        'english': slice(200, 400),\n",
    "        'french': slice(400, 600)\n",
    "    }\n",
    "    domain_slice = domain_slices[domain]\n",
    "    \n",
    "    layer_idx = layer_number - 1\n",
    "    if layer_idx < 0 or layer_idx >= last_token_experts.size(1):\n",
    "        raise ValueError(f\"Layer number must be between 1 and {last_token_experts.size(1)}\")\n",
    "    \n",
    "    domain_experts = last_token_experts[domain_slice, layer_idx, :].numpy()\n",
    "    \n",
    "    expert_counts = np.zeros(64)  # OLMoE uses 64 experts\n",
    "    for token_experts in domain_experts:\n",
    "        unique_experts = np.unique(token_experts)\n",
    "        for expert in unique_experts:\n",
    "            expert_counts[expert] += 1\n",
    "            \n",
    "    total_tokens = domain_experts.shape[0]\n",
    "    percentages = (expert_counts / total_tokens) * 100\n",
    "    \n",
    "    # Create plotly bar chart\n",
    "    fig = go.Figure(data=[\n",
    "        go.Bar(\n",
    "            x=list(range(64)),\n",
    "            y=percentages,\n",
    "            marker_color=['darkblue' if p > 0 else 'lightgray' for p in percentages]\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f'Expert Usage Distribution - Layer {layer_number} ({domain})',\n",
    "        xaxis_title='Expert ID',\n",
    "        yaxis_title=f'% of {domain.capitalize()} Domain Tokens',\n",
    "        xaxis=dict(\n",
    "            tickmode='linear',\n",
    "            tick0=0,\n",
    "            dtick=4\n",
    "        ),\n",
    "        yaxis=dict(range=[0, 100]),\n",
    "        showlegend=False,\n",
    "        width=1000,\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    # Add gridlines\n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgray')\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgray')\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_graph_visualize(last_token_experts, layer_number=1, domain='code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_graph_visualize(last_token_experts, layer_number=15, domain='french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_graph_visualize(last_token_experts, layer_number=27, domain='french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_bar_graph(last_token_experts, layer_number):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        last_token_experts: Tensor of shape [600, 27, 6] (prompts, layers, experts)\n",
    "        layer_number: Layer to analyze (1-27)\n",
    "    \"\"\"\n",
    "    # Validate layer\n",
    "    layer_idx = layer_number - 1\n",
    "    if layer_idx < 0 or layer_idx >= last_token_experts.size(1):\n",
    "        raise ValueError(\"Layer number must be between 1 and 27.\")\n",
    "    \n",
    "    # Extract data for the specified layer [600 prompts, 6 experts]\n",
    "    layer_data = last_token_experts[:, layer_idx, :].numpy()\n",
    "    \n",
    "    # Split into domains\n",
    "    code = layer_data[:200]    # First 200 prompts (code)\n",
    "    english = layer_data[200:400]  # Next 200 (english)\n",
    "    french = layer_data[400:600]   # Last 200 (french)\n",
    "    \n",
    "    # Initialize domain-specific expert counts\n",
    "    code_counts = np.zeros(64, dtype=int)\n",
    "    eng_counts = np.zeros(64, dtype=int)\n",
    "    fr_counts = np.zeros(64, dtype=int)\n",
    "    \n",
    "    # Count occurrences for each domain (unique experts per token)\n",
    "    for domain_data, counts in zip([code, english, french], [code_counts, eng_counts, fr_counts]):\n",
    "        for token_experts in domain_data:\n",
    "            unique_experts = np.unique(token_experts)\n",
    "            for expert in unique_experts:\n",
    "                counts[expert] += 1\n",
    "    \n",
    "    # Convert counts to percentages (relative to N_D=600)\n",
    "    total_tokens = 600\n",
    "    code_pct = (code_counts / total_tokens) * 100\n",
    "    eng_pct = (eng_counts / total_tokens) * 100\n",
    "    fr_pct = (fr_counts / total_tokens) * 100\n",
    "    \n",
    "    # Plot settings\n",
    "    experts = np.arange(64)\n",
    "    colors = ['#2c7bb6', '#d7191c', '#fdae61']  # Colorblind-friendly : blue, red, orange\n",
    "    labels = ['Code', 'English', 'French']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20, 8))\n",
    "    ax.bar(experts, code_pct, color=colors[0], label=labels[0])\n",
    "    ax.bar(experts, eng_pct, bottom=code_pct, color=colors[1], label=labels[1])\n",
    "    ax.bar(experts, fr_pct, bottom=code_pct+eng_pct, color=colors[2], label=labels[2])\n",
    "    \n",
    "    ax.set_xlabel('expert ID', fontsize=12)\n",
    "    ax.set_ylabel('percentage of total tokens', fontsize=12)\n",
    "    ax.set_title(f'domain contributions to experts (layer {layer_number})', fontsize=14)\n",
    "    ax.set_xticks(np.arange(0, 64, 4))\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_bar_graph(last_token_experts, layer_number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_graph_all_tokens(all_token_experts, layer_number, domain):\n",
    "    \"\"\"\n",
    "    Visualizes expert distribution for all tokens in a domain and layer.\n",
    "    \n",
    "    Args:\n",
    "        all_token_experts: Tensor of shape [600, 27, 284, 6] (prompts, layers, tokens, experts)\n",
    "        layer_number: Layer to analyze (1-27)\n",
    "        domain: 'code', 'english', or 'french'\n",
    "    \"\"\"\n",
    "    # Validate domain and extract slice\n",
    "    if domain == 'code':\n",
    "        domain_slice = slice(0, 200)\n",
    "    elif domain == 'english':\n",
    "        domain_slice = slice(200, 400)\n",
    "    elif domain == 'french':\n",
    "        domain_slice = slice(400, 600)\n",
    "    else:\n",
    "        raise ValueError(\"Domain must be 'code', 'english', or 'french'.\")\n",
    "    \n",
    "    # Validate layer index\n",
    "    layer_idx = layer_number - 1\n",
    "    if layer_idx < 0 or layer_idx >= all_token_experts.size(1):\n",
    "        raise ValueError(\"Layer number must be between 1 and 27.\")\n",
    "    \n",
    "    # Extract data for domain and layer [200 prompts, 284 tokens, 6 experts]\n",
    "    domain_data = all_token_experts[domain_slice, layer_idx, :, :].numpy()\n",
    "    # Flatten and filter out padding (experts = -1)\n",
    "    flattened = domain_data.reshape(-1, 6)  # [200*284, 6]\n",
    "    valid_tokens_mask = (flattened[:, 0] != -1)  # Padding uses -1\n",
    "    valid_experts = flattened[valid_tokens_mask]\n",
    "    # Count unique experts per token\n",
    "    expert_counts = np.zeros(64, dtype=int)\n",
    "    for token_experts in valid_experts:\n",
    "        unique_experts = np.unique(token_experts)\n",
    "        for expert in unique_experts:\n",
    "            expert_counts[expert] += 1\n",
    "    \n",
    "    # Compute percentages\n",
    "    total_valid_tokens = valid_experts.shape[0]\n",
    "    print(f'total valid tokens : {total_valid_tokens}')\n",
    "    percentages = (expert_counts / total_valid_tokens) * 100\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    bars = plt.bar(range(64), percentages, color='steelblue')\n",
    "    plt.xlabel('expert ID', fontsize=12)\n",
    "    plt.ylabel('% of total tokens selecting expert', fontsize=12)\n",
    "    plt.title(f'expert selection for {domain.capitalize()} domain (layer {layer_number}) - all tokens', fontsize=14)\n",
    "    plt.xticks(np.arange(0, 64, 4))\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Highlight bars with non-zero values\n",
    "    for bar in bars:\n",
    "        if bar.get_height() > 0:\n",
    "            bar.set_color('#2c7bb6')  # Darker blue for emphasis\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_graph_all_tokens(all_token_experts, layer_number=1, domain='french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_bar_graph_all_tokens(all_token_experts, layer_number):\n",
    "    \"\"\"\n",
    "    Visualizes domain contributions to experts across ALL tokens (non-padded) for a given layer.\n",
    "    Normalizes counts per domain to balance influence.\n",
    "    \"\"\"\n",
    "    # Validate layer\n",
    "    layer_idx = layer_number - 1\n",
    "    if layer_idx < 0 or layer_idx >= all_token_experts.size(1):\n",
    "        raise ValueError(\"Layer number must be between 1 and 27.\")\n",
    "    \n",
    "    # Extract layer data [600 prompts, 284 tokens, 6 experts]\n",
    "    layer_data = all_token_experts[:, layer_idx, :, :].numpy()\n",
    "    \n",
    "    # Split into domains\n",
    "    code = layer_data[:200]    # Code domain (prompts 0-199)\n",
    "    english = layer_data[200:400]  # English (prompts 200-399)\n",
    "    french = layer_data[400:600]   # French (prompts 400-599)\n",
    "    \n",
    "    def process_domain(domain_data):\n",
    "        \"\"\"Process a domain's data to count valid tokens and expert usage.\"\"\"\n",
    "        flattened = domain_data.reshape(-1, 6)\n",
    "        valid_mask = flattened[:, 0] != -1\n",
    "        valid_experts = flattened[valid_mask]\n",
    "        counts = np.zeros(64, dtype=int)\n",
    "        for token in valid_experts:\n",
    "            unique_experts = np.unique(token)\n",
    "            for expert in unique_experts:\n",
    "                counts[expert] += 1\n",
    "        return counts, valid_experts.shape[0]\n",
    "    \n",
    "    # Process domains and get raw counts/tokens\n",
    "    code_counts, code_valid = process_domain(code)\n",
    "    eng_counts, eng_valid = process_domain(english)\n",
    "    fr_counts, fr_valid = process_domain(french)\n",
    "    \n",
    "    # Normalize counts to balance domain contributions\n",
    "    def normalize_counts(counts, domain_valid_tokens, scaling_factor=1000):\n",
    "        \"\"\"Scale counts to a common token count (e.g., 1000 tokens per domain).\"\"\"\n",
    "        scale = scaling_factor / domain_valid_tokens\n",
    "        return (counts * scale).astype(int)\n",
    "    \n",
    "    # Choose a scaling factor (e.g., min token count or fixed value)\n",
    "    scaling_factor = min(code_valid, eng_valid, fr_valid)  # Use smallest domain size\n",
    "    print(f'scaling factor : {scaling_factor}')\n",
    "    \n",
    "    # Normalize counts\n",
    "    code_norm = normalize_counts(code_counts, code_valid, scaling_factor)\n",
    "    eng_norm = normalize_counts(eng_counts, eng_valid, scaling_factor)\n",
    "    fr_norm = normalize_counts(fr_counts, fr_valid, scaling_factor)\n",
    "    \n",
    "    # Total tokens after normalization (scaling_factor * 3 for 3 domains)\n",
    "    total_normalized_tokens = scaling_factor * 3\n",
    "    print(f'total normalized tokens : {total_normalized_tokens}')\n",
    "    # Compute percentages\n",
    "    code_pct = (code_norm / total_normalized_tokens) * 100\n",
    "    eng_pct = (eng_norm / total_normalized_tokens) * 100\n",
    "    fr_pct = (fr_norm / total_normalized_tokens) * 100\n",
    "    \n",
    "    # Plot settings\n",
    "    experts = np.arange(64)\n",
    "    colors = ['#2c7bb6', '#d7191c', '#fdae61']\n",
    "    labels = ['Code', 'English', 'French']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20, 8))\n",
    "    ax.bar(experts, code_pct, color=colors[0], label=labels[0])\n",
    "    ax.bar(experts, eng_pct, bottom=code_pct, color=colors[1], label=labels[1])\n",
    "    ax.bar(experts, fr_pct, bottom=code_pct+eng_pct, color=colors[2], label=labels[2])\n",
    "    \n",
    "    ax.set_xlabel('Expert ID', fontsize=12)\n",
    "    ax.set_ylabel('Percentage of Normalized Tokens (%)', fontsize=12)\n",
    "    ax.set_title(f'Balanced Domain Contributions to Experts (Layer {layer_number})', fontsize=14)\n",
    "    ax.set_xticks(np.arange(0, 64, 4))\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_bar_graph_all_tokens(all_token_experts, layer_number=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expert_coactivation(last_token_experts, layer_number, top_k=64):\n",
    "    \"\"\"\n",
    "    Visualizes expert co-activation for a specified layer using last token experts.\n",
    "    \n",
    "    Args:\n",
    "        last_token_experts: Tensor of shape [600, 27, 6] (prompts, layers, top_k=6).\n",
    "        layer_number: Layer to analyze (1-27).\n",
    "        top_k: Number of top experts to display based on maximum co-activation scores.\n",
    "    \"\"\"\n",
    "    # Validate layer index\n",
    "    layer_idx = layer_number - 1\n",
    "    if layer_idx < 0 or layer_idx >= last_token_experts.size(1):\n",
    "        raise ValueError(\"Layer number must be between 1 and 27.\")\n",
    "    \n",
    "    # Extract data for the specified layer [600 prompts, 6 experts]\n",
    "    layer_data = last_token_experts[:, layer_idx, :].numpy()\n",
    "    \n",
    "    # Initialize co-occurrence and expert activation counts\n",
    "    co_occurrence = np.zeros((64, 64), dtype=int)\n",
    "    expert_activations = np.zeros(64, dtype=int)\n",
    "    \n",
    "    # Process each token's expert selections\n",
    "    for token_experts in layer_data:\n",
    "        unique_experts = np.unique(token_experts)  # Remove duplicates\n",
    "        # Update expert activation counts\n",
    "        for expert in unique_experts:\n",
    "            expert_activations[expert] += 1\n",
    "        # Update co-occurrence matrix\n",
    "        for i in unique_experts:\n",
    "            for j in unique_experts:\n",
    "                if i != j:\n",
    "                    co_occurrence[i, j] += 1\n",
    "    \n",
    "    # Compute co-activation matrix (directed)\n",
    "    co_activation = np.zeros((64, 64))\n",
    "    for i in range(64):\n",
    "        if expert_activations[i] > 0:\n",
    "            co_activation[i, :] = (co_occurrence[i, :] / expert_activations[i]) * 100\n",
    "    \n",
    "    # Identify top-k experts with highest max co-activation scores\n",
    "    max_scores = np.max(co_activation, axis=1)\n",
    "    top_experts = np.argsort(-max_scores)[:top_k]\n",
    "    top_experts = np.sort(top_experts)  # Sort for ordered display\n",
    "    \n",
    "    # Filter matrix to include only top experts\n",
    "    filtered_matrix = co_activation[np.ix_(top_experts, top_experts)]\n",
    "    \n",
    "    # Plot settings\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(filtered_matrix, cmap=\"viridis\", interpolation=\"nearest\", aspect=\"auto\")\n",
    "    plt.colorbar(label=\"Co-activation (%)\", shrink=0.8)\n",
    "    \n",
    "    # Label axes with expert IDs\n",
    "    tick_labels = [f\"E{expert}\" for expert in top_experts]\n",
    "    plt.xticks(np.arange(len(top_experts)), tick_labels, rotation=90)\n",
    "    plt.yticks(np.arange(len(top_experts)), tick_labels)\n",
    "    plt.xlabel(\"Expert $E_j$\")\n",
    "    plt.ylabel(\"Expert $E_i$\")\n",
    "    plt.title(f\"Expert Co-activation (Layer {layer_number}) - Top {top_k} Experts\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_coactivation(last_token_experts, layer_number=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
