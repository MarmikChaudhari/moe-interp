{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if DEVICE.type == \"cuda\":\n",
    "    # Print CUDA details\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated()/1024**2:.2f}MB\")\n",
    "    print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved()/1024**2:.2f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad044a503a0542a1b2278b55a56ff077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_model(model_name):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        # use_flash_attention_2=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model.to(DEVICE)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model(\"Qwen/Qwen1.5-MoE-A2.7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2MoeForCausalLM(\n",
       "  (model): Qwen2MoeModel(\n",
       "    (embed_tokens): Embedding(151936, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2MoeDecoderLayer(\n",
       "        (self_attn): Qwen2MoeSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MoeSparseMoeBlock(\n",
       "          (gate): Linear(in_features=2048, out_features=60, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-59): 60 x Qwen2MoeMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
       "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (shared_expert): Qwen2MoeMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
       "        )\n",
       "        (input_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moe_metadata(model, input_ids):\n",
    "    \"\"\"Get both router logits and expert indices for all MoE layers for Qwen1.5-MoE model.\"\"\"\n",
    "    router_logits_list = []\n",
    "    expert_indices_list = []\n",
    "    \n",
    "    # We need different hooks for gate and forward pass\n",
    "    def gate_hook_fn(module, input, output):\n",
    "        # Get router logits from gate module\n",
    "        hidden_states = input[0]\n",
    "        logits = torch.matmul(hidden_states, module.weight.T)\n",
    "        router_logits_list.append(logits.detach())\n",
    "        return output\n",
    "    \n",
    "    # This second hook will capture the selected expert indices\n",
    "    def forward_hook_fn(module, input, output):\n",
    "        # Capture the routing choices\n",
    "        # For testing/debugging, store something simpler\n",
    "        batch_size, seq_len = input[0].shape[:2]\n",
    "        # Just store dummy indices for now (we'll update with proper extraction later)\n",
    "        dummy_indices = torch.zeros((batch_size, seq_len, 4), dtype=torch.long)\n",
    "        expert_indices_list.append(dummy_indices.detach())\n",
    "        return output\n",
    "    \n",
    "    hooks = []\n",
    "    # For Qwen1.5-MoE, hook the gate module in each MoE layer\n",
    "    for layer_idx, layer in enumerate(model.model.layers):\n",
    "        if hasattr(layer, 'mlp') and hasattr(layer.mlp, 'gate'):\n",
    "            # Hook for router logits\n",
    "            hook = layer.mlp.gate.register_forward_hook(gate_hook_fn)\n",
    "            hooks.append(hook)\n",
    "            \n",
    "            # Hook for the whole MoE block\n",
    "            hook = layer.mlp.register_forward_hook(forward_hook_fn)\n",
    "            hooks.append(hook)\n",
    "\n",
    "    # Process the input\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    \n",
    "    # Remove all hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    moe_metadata = {\n",
    "        'router_logits': torch.stack(router_logits_list) if router_logits_list else None,\n",
    "        'expert_indices': expert_indices_list if expert_indices_list else None\n",
    "    }\n",
    "    \n",
    "    if moe_metadata['router_logits'] is not None:\n",
    "        print(f\"Router logits shape: {moe_metadata['router_logits'].shape}\")\n",
    "    if moe_metadata['expert_indices'] is not None:\n",
    "        print(f\"Expert indices shape: {len(moe_metadata['expert_indices'])}\")\n",
    "    \n",
    "    return moe_metadata\n",
    "\n",
    "def prepare_prompt(prompt, tokenizer, max_tokens=2048):\n",
    "    \"\"\"\n",
    "    Prepare a prompt for processing, splitting if necessary to fit within model context.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The text prompt to prepare\n",
    "        tokenizer: The model's tokenizer\n",
    "        max_tokens: Maximum number of tokens per chunk (default: 2048)\n",
    "        \n",
    "    Returns:\n",
    "        List of prompts that fit within token limit\n",
    "    \"\"\"\n",
    "    # Check if the input is a list of lines/prompts\n",
    "    if isinstance(prompt, list):\n",
    "        all_prepared_prompts = []\n",
    "        for single_prompt in prompt:\n",
    "            # Process each line/prompt individually\n",
    "            prepared_chunks = prepare_prompt(single_prompt, tokenizer, max_tokens)\n",
    "            all_prepared_prompts.extend(prepared_chunks)\n",
    "        return all_prepared_prompts\n",
    "    \n",
    "    # Process a single prompt\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    \n",
    "    # If prompt is small enough, return as is\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return [prompt]\n",
    "    \n",
    "    # Split into manageable chunks\n",
    "    prepared_prompts = []\n",
    "    \n",
    "    # Decode tokens into chunks\n",
    "    start_idx = 0\n",
    "    while start_idx < len(tokens):\n",
    "        end_idx = min(start_idx + max_tokens, len(tokens))\n",
    "        chunk_tokens = tokens[start_idx:end_idx]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens)\n",
    "        prepared_prompts.append(chunk_text)\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    print(f\"Long prompt detected! Split into {len(prepared_prompts)} chunks.\")\n",
    "    return prepared_prompts\n",
    "\n",
    "def process_text_file_for_expert_counts(file_path, model, tokenizer, output_path=None, max_tokens=4096):\n",
    "    \"\"\"\n",
    "    Process a text file to analyze MoE routing and count tokens per expert in each layer.\n",
    "    Saves a PyTorch file with expert token counts.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to text file with prompts (one per line)\n",
    "        model: Qwen1.5-MoE model\n",
    "        tokenizer: Qwen tokenizer\n",
    "        output_path: Path to save PyTorch results (default: based on input filename)\n",
    "        max_tokens: Maximum tokens per prompt chunk\n",
    "    \"\"\"\n",
    "    # Load the text file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Check if this is a GitHub code file\n",
    "    if 'github.txt' in file_path:\n",
    "        import re\n",
    "        # Find all code blocks using the file pattern\n",
    "        file_pattern = re.compile(r'.*\\b\\w+\\.(js|py|c|cpp|java|ts|rb|go|rs|cs|swift|kt|php)$', re.MULTILINE)\n",
    "        \n",
    "        # Find all matches (file headers)\n",
    "        matches = list(file_pattern.finditer(content))\n",
    "        \n",
    "        # Extract code blocks between file headers\n",
    "        raw_prompts = []\n",
    "        for i in range(len(matches)):\n",
    "            start_pos = matches[i].start()\n",
    "            # If this is the last match, go to the end of the file\n",
    "            if i == len(matches) - 1:\n",
    "                end_pos = len(content)\n",
    "            else:\n",
    "                end_pos = matches[i+1].start()\n",
    "            \n",
    "            # Extract the code block including the file header\n",
    "            code_block = content[start_pos:end_pos].strip()\n",
    "            raw_prompts.append(code_block)\n",
    "    else:\n",
    "        # Regular text file processing (one prompt per line)\n",
    "        raw_prompts = [line.strip() for line in content.split('\\n') if line.strip()]\n",
    "    \n",
    "    print(f\"Loaded {len(raw_prompts)} raw prompts from {file_path}\")\n",
    "    \n",
    "    # Prepare prompts (handle large prompts by splitting)\n",
    "    prompts = []\n",
    "    for raw_prompt in raw_prompts:\n",
    "        prepared_chunks = prepare_prompt(raw_prompt, tokenizer, max_tokens)\n",
    "        prompts.extend(prepared_chunks)\n",
    "    \n",
    "    print(f\"Processing {len(prompts)} prepared prompts (after splitting large ones)\")\n",
    "    \n",
    "    # Set default output path if not provided\n",
    "    if output_path is None:\n",
    "        output_path = file_path.replace('.txt', '_expert_data.pt')\n",
    "    \n",
    "    # Initialize counter for expert usage\n",
    "    # Structure: {layer_num: {expert_id: count}}\n",
    "    expert_counts = {}\n",
    "    \n",
    "    # Calculate total tokens for progress bar\n",
    "    total_tokens = 0\n",
    "    for prompt in prompts:\n",
    "        tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        total_tokens += tokens.size(1)\n",
    "    \n",
    "    print(f\"Total tokens to process: {total_tokens}\")\n",
    "    \n",
    "    # Initialize progress bar\n",
    "    pbar = tqdm(total=total_tokens, desc=\"Processing tokens\")\n",
    "    processed_tokens = 0\n",
    "    \n",
    "    # Process each prompt\n",
    "    for prompt in prompts:\n",
    "        # Tokenize the prompt\n",
    "        tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        seq_len = tokens.size(1)\n",
    "        \n",
    "        # Get MoE routing metadata\n",
    "        moe_metadata = get_moe_metadata(model, tokens)\n",
    "        \n",
    "        # For Qwen1.5-MoE, we need to handle the expert indices differently\n",
    "        if 'router_logits' in moe_metadata and moe_metadata['router_logits'] is not None:\n",
    "            router_logits = moe_metadata['router_logits']\n",
    "            num_moe_layers = router_logits.size(0)\n",
    "            \n",
    "            # Initialize counter for this batch if needed\n",
    "            for layer_idx in range(num_moe_layers):\n",
    "                layer_num = layer_idx + 1  # 1-based layer indexing\n",
    "                if layer_num not in expert_counts:\n",
    "                    expert_counts[layer_num] = {}\n",
    "            \n",
    "            # For each layer, determine which experts were selected for each token\n",
    "            for layer_idx in range(num_moe_layers):\n",
    "                layer_num = layer_idx + 1  # 1-based layer indexing\n",
    "                \n",
    "                # Get router logits for this layer\n",
    "                layer_logits = router_logits[layer_idx]  # [seq_len, num_experts]\n",
    "                \n",
    "                # Take top-k (4 for Qwen1.5-MoE) experts for each token\n",
    "                # This only includes the 60 routing experts (excluding shared experts)\n",
    "                top_experts = torch.topk(layer_logits, k=4, dim=-1).indices\n",
    "                \n",
    "                # Process each token in sequence\n",
    "                for token_idx in range(seq_len):\n",
    "                    # Get experts selected for this token in this layer\n",
    "                    selected_experts = top_experts[token_idx].cpu().numpy().tolist()\n",
    "                    \n",
    "                    # Count each expert\n",
    "                    for expert_id in selected_experts:\n",
    "                        # Make sure this is a routing expert (0-59) and not a shared expert\n",
    "                        if 0 <= expert_id < 60:\n",
    "                            if expert_id not in expert_counts[layer_num]:\n",
    "                                expert_counts[layer_num][expert_id] = 0\n",
    "                            expert_counts[layer_num][expert_id] += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        processed_tokens += seq_len\n",
    "        pbar.update(seq_len)\n",
    "    \n",
    "    # Close progress bar\n",
    "    pbar.close()\n",
    "    \n",
    "    # Convert counts to a simple tensor format for saving\n",
    "    expert_token_counts = {}\n",
    "    for layer_num in sorted(expert_counts.keys()):\n",
    "        layer_data = expert_counts[layer_num]\n",
    "        # Create a tensor with counts for each expert (60 for Qwen1.5-MoE routing experts)\n",
    "        counts = torch.zeros(60)\n",
    "        for expert_id, count in layer_data.items():\n",
    "            if 0 <= expert_id < 60:  # Only include routing experts\n",
    "                counts[expert_id] = count\n",
    "        expert_token_counts[layer_num] = counts\n",
    "    \n",
    "    # Save just the token counts per expert\n",
    "    torch.save(expert_token_counts, output_path)\n",
    "    print(f\"Expert token counts saved to {output_path}\")\n",
    "    \n",
    "    # Create a DataFrame for visualization purposes\n",
    "    rows = []\n",
    "    for layer_num in sorted(expert_counts.keys()):\n",
    "        layer_data = expert_counts[layer_num]\n",
    "        for expert_id in range(60):  # 60 routing experts for Qwen1.5-MoE\n",
    "            count = layer_data.get(expert_id, 0)\n",
    "            rows.append({\n",
    "                'layer': layer_num,\n",
    "                'expert_id': expert_id,\n",
    "                'token_count': count\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "def analyze_text_file_routing(model, tokenizer, file_path):\n",
    "    \"\"\"\n",
    "    Main function to analyze MoE routing for a text file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to text file with prompts (one per line)\n",
    "        model_name: Name of DeepSeek MoE model to use\n",
    "    \"\"\"\n",
    "    # Process the file for expert counts and save as PyTorch file\n",
    "    df = process_text_file_for_expert_counts(file_path, model, tokenizer)\n",
    "    \n",
    "    print(f\"Analysis completed for {file_path}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 raw prompts from data-ext/test.txt\n",
      "Processing 2 prepared prompts (after splitting large ones)\n",
      "Total tokens to process: 478\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eedde4253d524e19b7a8f1ff300b20a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing tokens:   0%|          | 0/478 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Router logits shape: torch.Size([24, 239, 60])\n",
      "Expert indices shape: 24\n",
      "Router logits shape: torch.Size([24, 239, 60])\n",
      "Expert indices shape: 24\n",
      "Expert token counts saved to data-ext/test_expert_data.pt\n",
      "Analysis completed for data-ext/test.txt\n"
     ]
    }
   ],
   "source": [
    "file_path = \"data-ext/test.txt\"\n",
    "df = analyze_text_file_routing(model, tokenizer, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wk/sgrd2bsj1msgt6bs3kfgj9hw0000gn/T/ipykernel_10507/61385568.py:1: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = torch.load(\"data-ext/test_expert_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1912.)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_graph_all_tokens_paper(expert_data, layer_number, tokenizer, domain=None, actual_token_count=None):\n",
    "    \"\"\"\n",
    "    Visualizes expert distribution for all tokens in a file for a specific layer.\n",
    "    \n",
    "    Args:\n",
    "        expert_data: Dictionary with layer numbers as keys and tensor of expert counts as values\n",
    "                     or path to PyTorch file with this data\n",
    "        layer_number: Layer to analyze (1-24 for Qwen1.5-MoE)\n",
    "        tokenizer: Tokenizer used\n",
    "        domain: Optional domain name for title (e.g., 'GSM8K', 'Math', etc.)\n",
    "        actual_token_count: Optional parameter to override token count\n",
    "    \n",
    "    Returns:\n",
    "        fig: Plotly figure object\n",
    "    \"\"\"\n",
    "    # Load data if a file path is provided\n",
    "    if isinstance(expert_data, str):\n",
    "        # Adjust the path for txt files - they are directly under data-ext/\n",
    "        txt_file_path = expert_data.replace(\"data-ext/pt/\", \"data-ext/\").replace(\"_expert_data.pt\", \".txt\").replace(\"_expert_data_qwen.pt\", \".txt\")\n",
    "        \n",
    "        # Get the actual token count from the original text file\n",
    "        num_tokens = actual_token_count  # Use override if provided\n",
    "        \n",
    "        if num_tokens is None and os.path.exists(txt_file_path):\n",
    "            # Try different encodings\n",
    "            encodings = ['utf-8', 'latin-1', 'utf-16']\n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    with open(txt_file_path, 'r', encoding=encoding) as f:\n",
    "                        text_content = f.read()\n",
    "                        tokens = tokenizer(text_content, return_tensors=\"pt\")\n",
    "                        num_tokens = tokens.input_ids.numel()\n",
    "                        print(f\"Total tokens in {txt_file_path}: {num_tokens}\")\n",
    "                        break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            else:\n",
    "                print(f\"Could not read {txt_file_path} with any of the attempted encodings\")\n",
    "        \n",
    "        expert_data = torch.load(expert_data)\n",
    "    else:\n",
    "        # If expert_data is already loaded (not a string path)\n",
    "        num_tokens = actual_token_count\n",
    "    \n",
    "    # Validate layer number is in the data\n",
    "    if layer_number not in expert_data:\n",
    "        raise ValueError(f\"Layer {layer_number} not found in expert data\")\n",
    "    \n",
    "    # Get counts for the specified layer\n",
    "    expert_counts = expert_data[layer_number].numpy()\n",
    "    \n",
    "    # Calculate the total number of routing decisions (sum of all expert counts)\n",
    "    # For Qwen1.5-MoE, top-k = 4, so each token is routed to 4 experts\n",
    "    # Therefore, the actual number of tokens processed = total routing decisions / 4\n",
    "    routed_tokens = expert_counts.sum()\n",
    "    tokens_processed = routed_tokens / 4  # Each token gets routed to 4 experts\n",
    "    \n",
    "    # Use the override, then the calculated value, then fallback to the file token count\n",
    "    if num_tokens is None:\n",
    "        num_tokens = int(tokens_processed)\n",
    "    \n",
    "    if num_tokens == 0:\n",
    "        print(\"No tokens found for this layer\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Total tokens (from file): {num_tokens}\")\n",
    "    print(f\"Estimated tokens processed: {tokens_processed}\")\n",
    "    print(f\"Total routing decisions: {routed_tokens}\")\n",
    "    \n",
    "    # Compute percentages based on the total number of processed tokens\n",
    "    percentages = (expert_counts / num_tokens) * 100\n",
    "    \n",
    "    # Set discrete opacity based on threshold\n",
    "    # For Qwen1.5-MoE with 60 experts, uniform would be 1.667%, so threshold is 10%\n",
    "    threshold = 6.667\n",
    "    opacities = np.where(percentages >= threshold, 1.0, 0.3)\n",
    "    \n",
    "    # Create plotly figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add bar trace with discrete color and opacity\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=list(range(60)),\n",
    "        y=percentages,\n",
    "        marker=dict(\n",
    "            color='#636EFA',  # Blue color for all bars\n",
    "            opacity=opacities\n",
    "        ),\n",
    "        hovertemplate='Expert ID: %{x}<br>Tokens: %{text}<br>Percentage: %{y:.2f}%<extra></extra>',\n",
    "        text=[f\"{int(count)}\" for count in expert_counts],\n",
    "        textposition='none'  # Ensure no text is displayed on the bars\n",
    "    ))\n",
    "    \n",
    "    # Add horizontal line at threshold\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=-0.5,\n",
    "        x1=59.5,\n",
    "        y0=threshold,\n",
    "        y1=threshold,\n",
    "        line=dict(\n",
    "            color=\"red\",\n",
    "            width=2,\n",
    "            dash=\"dash\",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add annotation for the threshold line\n",
    "    fig.add_annotation(\n",
    "        x=59,\n",
    "        y=threshold,\n",
    "        text=f\"{threshold}% threshold\",\n",
    "        showarrow=False,\n",
    "        yshift=10,\n",
    "        font=dict(color=\"red\")\n",
    "    )\n",
    "    \n",
    "    # Domain label for title\n",
    "    domain_label = f\" - {domain}\" if domain else \"\"\n",
    "    token_info = f\" (No. of Tokens: {int(num_tokens)}, Routing Decisions: {int(routed_tokens)})\"\n",
    "    \n",
    "    # Calculate y-axis max based on the max percentage\n",
    "    y_max = max(100, percentages.max() * 1.1)\n",
    "    \n",
    "    # Update layout with white background\n",
    "    fig.update_layout(\n",
    "        title=f'Expert Usage Distribution - Layer {layer_number}{domain_label}{token_info}',\n",
    "        xaxis_title='Expert ID',\n",
    "        yaxis_title='Usage Percentage (%)',\n",
    "        yaxis_range=[0, y_max],\n",
    "        xaxis=dict(tickmode='linear', tick0=0, dtick=4, range=[-0.5, 59.5]),\n",
    "        showlegend=False,\n",
    "        width=1000,\n",
    "        height=600,\n",
    "        plot_bgcolor='white',\n",
    "        paper_bgcolor='white'\n",
    "    )\n",
    "    \n",
    "    # Add gridlines with lighter color\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='rgba(128, 128, 128, 0.1)')\n",
    "    fig.update_xaxes(showgrid=False)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in data-ext/test.txt: 479\n",
      "Total tokens (from file): 479\n",
      "Estimated tokens processed: 478.0\n",
      "Total routing decisions: 1912.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wk/sgrd2bsj1msgt6bs3kfgj9hw0000gn/T/ipykernel_10507/225860388.py:40: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Expert ID: %{x}<br>Tokens: %{text}<br>Percentage: %{y:.2f}%<extra></extra>",
         "marker": {
          "color": "#636EFA",
          "opacity": [
           1,
           0.3,
           0.3,
           1,
           0.3,
           1,
           1,
           1,
           0.3,
           0.3,
           0.3,
           1,
           1,
           0.3,
           0.3,
           0.3,
           1,
           0.3,
           0.3,
           0.3,
           1,
           0.3,
           0.3,
           1,
           1,
           0.3,
           0.3,
           0.3,
           0.3,
           1,
           0.3,
           0.3,
           1,
           0.3,
           0.3,
           1,
           1,
           1,
           0.3,
           0.3,
           1,
           0.3,
           0.3,
           0.3,
           0.3,
           0.3,
           0.3,
           1,
           1,
           0.3,
           1,
           1,
           0.3,
           1,
           1,
           0.3,
           0.3,
           1,
           0.3,
           0.3
          ]
         },
         "text": [
          "80",
          "23",
          "25",
          "103",
          "27",
          "46",
          "34",
          "44",
          "19",
          "30",
          "29",
          "59",
          "35",
          "13",
          "29",
          "24",
          "41",
          "16",
          "25",
          "26",
          "37",
          "20",
          "17",
          "38",
          "44",
          "15",
          "21",
          "13",
          "31",
          "39",
          "28",
          "20",
          "35",
          "28",
          "28",
          "52",
          "44",
          "33",
          "22",
          "26",
          "57",
          "27",
          "6",
          "14",
          "25",
          "24",
          "20",
          "57",
          "46",
          "18",
          "36",
          "40",
          "26",
          "54",
          "39",
          "20",
          "19",
          "49",
          "5",
          "11"
         ],
         "textposition": "none",
         "type": "bar",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59
         ],
         "y": [
          16.701461791992188,
          4.801670074462891,
          5.219206809997559,
          21.503131866455078,
          5.636743068695068,
          9.603340148925781,
          7.098121166229248,
          9.185803413391113,
          3.966597080230713,
          6.26304817199707,
          6.054279804229736,
          12.317327499389648,
          7.306889533996582,
          2.713987350463867,
          6.054279804229736,
          5.010438442230225,
          8.55949878692627,
          3.340292453765869,
          5.219206809997559,
          5.427974700927734,
          7.724425792694092,
          4.175365447998047,
          3.549060583114624,
          7.933194160461426,
          9.185803413391113,
          3.131524085998535,
          4.384133815765381,
          2.713987350463867,
          6.471816539764404,
          8.141962051391602,
          5.845511436462402,
          4.175365447998047,
          7.306889533996582,
          5.845511436462402,
          5.845511436462402,
          10.855949401855469,
          9.185803413391113,
          6.889352798461914,
          4.592901706695557,
          5.427974700927734,
          11.89979076385498,
          5.636743068695068,
          1.2526096105575562,
          2.922755718231201,
          5.219206809997559,
          5.010438442230225,
          4.175365447998047,
          11.89979076385498,
          9.603340148925781,
          3.757828950881958,
          7.515657901763916,
          8.350730895996094,
          5.427974700927734,
          11.273486137390137,
          8.141962051391602,
          4.175365447998047,
          3.966597080230713,
          10.229644775390625,
          1.0438413619995117,
          2.2964508533477783
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "color": "red"
          },
          "showarrow": false,
          "text": "6.667% threshold",
          "x": 59,
          "y": 6.667,
          "yshift": 10
         }
        ],
        "height": 600,
        "paper_bgcolor": "white",
        "plot_bgcolor": "white",
        "shapes": [
         {
          "line": {
           "color": "red",
           "dash": "dash",
           "width": 2
          },
          "type": "line",
          "x0": -0.5,
          "x1": 59.5,
          "y0": 6.667,
          "y1": 6.667
         }
        ],
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Expert Usage Distribution - Layer 15 (No. of Tokens: 479, Routing Decisions: 1912)"
        },
        "width": 1000,
        "xaxis": {
         "dtick": 4,
         "range": [
          -0.5,
          59.5
         ],
         "showgrid": false,
         "tick0": 0,
         "tickmode": "linear",
         "title": {
          "text": "Expert ID"
         }
        },
        "yaxis": {
         "gridcolor": "rgba(128, 128, 128, 0.1)",
         "gridwidth": 1,
         "range": [
          0,
          100
         ],
         "showgrid": true,
         "title": {
          "text": "Usage Percentage (%)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = bar_graph_all_tokens_paper(\"data-ext/pt/test_expert_data_qwen.pt\", layer_number=15, tokenizer=tokenizer)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
